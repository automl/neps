{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction and Installation Installation Using pip pip install neural-pipeline-search","title":"Introduction and Installation"},{"location":"#introduction-and-installation","text":"","title":"Introduction and Installation"},{"location":"#installation","text":"Using pip pip install neural-pipeline-search","title":"Installation"},{"location":"alternatives/","text":"Some Alternatives SMAC Optuna","title":"Alternatives"},{"location":"alternatives/#some-alternatives","text":"SMAC Optuna","title":"Some Alternatives"},{"location":"analyse/","text":"Analysing Runs NePS has some convenient utilities to help you to understand the results of your run. Saved to disk In the root directory, NePS maintains several files at all times that are human readable and can be useful ROOT_DIRECTORY \u251c\u2500\u2500 results \u2502 \u2514\u2500\u2500 config_1 \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 metadata.yaml \u2502 \u2514\u2500\u2500 result.yaml \u251c\u2500\u2500 all_losses_and_configs.txt \u251c\u2500\u2500 best_loss_trajectory.txt \u2514\u2500\u2500 best_loss_with_config_trajectory.txt Status To show status information about a neural pipeline search run, use python -m neps.status ROOT_DIRECTORY If you need more status information than is printed per default (e.g., the best config over time), please have a look at python -m neps.status --help To show the status repeatedly, on unix systems you can use watch --interval 30 python -m neps.status ROOT_DIRECTORY Visualizations To generate plots to the root directory, run python -m neps.plot ROOT_DIRECTORY Currently, this creates one plot that shows the best error value across the number of evaluations.","title":"Analysing Runs"},{"location":"analyse/#analysing-runs","text":"NePS has some convenient utilities to help you to understand the results of your run.","title":"Analysing Runs"},{"location":"analyse/#saved-to-disk","text":"In the root directory, NePS maintains several files at all times that are human readable and can be useful ROOT_DIRECTORY \u251c\u2500\u2500 results \u2502 \u2514\u2500\u2500 config_1 \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 metadata.yaml \u2502 \u2514\u2500\u2500 result.yaml \u251c\u2500\u2500 all_losses_and_configs.txt \u251c\u2500\u2500 best_loss_trajectory.txt \u2514\u2500\u2500 best_loss_with_config_trajectory.txt","title":"Saved to disk"},{"location":"analyse/#status","text":"To show status information about a neural pipeline search run, use python -m neps.status ROOT_DIRECTORY If you need more status information than is printed per default (e.g., the best config over time), please have a look at python -m neps.status --help To show the status repeatedly, on unix systems you can use watch --interval 30 python -m neps.status ROOT_DIRECTORY","title":"Status"},{"location":"analyse/#visualizations","text":"To generate plots to the root directory, run python -m neps.plot ROOT_DIRECTORY Currently, this creates one plot that shows the best error value across the number of evaluations.","title":"Visualizations"},{"location":"getting_started/","text":"","title":"Getting Started"},{"location":"neps_run/","text":"","title":"The neps.run Function"},{"location":"parallelization/","text":"Parallelization In order to run a neural pipeline search with multiple processes or multiple machines, simply call neps.run multiple times. All calls to neps.run need to use the same root_directory on the same filesystem, otherwise there is no synchronization between the neps.run 's.","title":"Parallelization"},{"location":"parallelization/#parallelization","text":"In order to run a neural pipeline search with multiple processes or multiple machines, simply call neps.run multiple times. All calls to neps.run need to use the same root_directory on the same filesystem, otherwise there is no synchronization between the neps.run 's.","title":"Parallelization"},{"location":"pipeline_space/","text":"","title":"The pipeline_space"},{"location":"run_pipeline/","text":"","title":"The run_pipeline Function"},{"location":"contributing/","text":"Introduction Getting Help Ask in the neps contributor chat on mattermost or any contributor directly. If you are not in the mattermost chat yet, ask to get access. Development Workflow We loosely practice trunk-based-development : We work almost exclusively on the master branch We commit, push, and pull often We automatically run code quality checks before every commit (using pre-commit ) We manually run tests (using pytest ) before every critical push and automatically afterwards (using github actions ) Installation For the contributor installation process see Installation . Checks and tests The documentation also includes an overview on the protocols and tools we use for code quality checks and tests.","title":"Introduction"},{"location":"contributing/#introduction","text":"","title":"Introduction"},{"location":"contributing/#getting-help","text":"Ask in the neps contributor chat on mattermost or any contributor directly. If you are not in the mattermost chat yet, ask to get access.","title":"Getting Help"},{"location":"contributing/#development-workflow","text":"We loosely practice trunk-based-development : We work almost exclusively on the master branch We commit, push, and pull often We automatically run code quality checks before every commit (using pre-commit ) We manually run tests (using pytest ) before every critical push and automatically afterwards (using github actions )","title":"Development Workflow"},{"location":"contributing/#installation","text":"For the contributor installation process see Installation .","title":"Installation"},{"location":"contributing/#checks-and-tests","text":"The documentation also includes an overview on the protocols and tools we use for code quality checks and tests.","title":"Checks and tests"},{"location":"contributing/dependencies/","text":"Managing Dependencies To manage dependencies and for package distribution we use poetry (replaces pip). Add dependencies To install a dependency use poetry add dependency and commit the updated pyproject.toml to git. For more advanced dependency management see examples in pyproject.toml or have a look at the poetry documentation . Install dependencies added by others When other contributors added dependencies to pyproject.toml , you can install them via poetry lock poetry install","title":"Managing Dependencies"},{"location":"contributing/dependencies/#managing-dependencies","text":"To manage dependencies and for package distribution we use poetry (replaces pip).","title":"Managing Dependencies"},{"location":"contributing/dependencies/#add-dependencies","text":"To install a dependency use poetry add dependency and commit the updated pyproject.toml to git. For more advanced dependency management see examples in pyproject.toml or have a look at the poetry documentation .","title":"Add dependencies"},{"location":"contributing/dependencies/#install-dependencies-added-by-others","text":"When other contributors added dependencies to pyproject.toml , you can install them via poetry lock poetry install","title":"Install dependencies added by others"},{"location":"contributing/documentation/","text":"Documentation We use MkDocs , more specifically Material for MkDocs for documentation. To support documentation for multiple versions, we use the plugin mike . Source files for the documentation are at docs and configuration at mkdocs.yml . To build and view the documentation run mike deploy 0 .5.1 latest mike serve and open the URL shown by the mike serve command. To publish the documentation run mike deploy 0 .5.1 latest -p","title":"Documentation"},{"location":"contributing/documentation/#documentation","text":"We use MkDocs , more specifically Material for MkDocs for documentation. To support documentation for multiple versions, we use the plugin mike . Source files for the documentation are at docs and configuration at mkdocs.yml . To build and view the documentation run mike deploy 0 .5.1 latest mike serve and open the URL shown by the mike serve command. To publish the documentation run mike deploy 0 .5.1 latest -p","title":"Documentation"},{"location":"contributing/installation/","text":"Installation for Contributors There are three required steps and one optional: Optional: Install miniconda and create an environment Install poetry Install the neps package using poetry Activate pre-commit for the repository For instructions see below. 1. Optional: Install miniconda and create an environment To manage python versions install e.g., miniconda with wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O install_miniconda.sh bash install_miniconda.sh -b -p $HOME /.conda # Change to place of preference rm install_miniconda.sh Consider running ~/.conda/bin/conda init or ~/.conda/bin/conda init zsh . Then finally create the environment and activate it conda create -n neps python = 3 .7.5 conda activate neps 2. Install poetry First, install poetry, e.g., via curl -sSL https://install.python-poetry.org | python3 - Then consider appending export PATH = \" $HOME /.local/bin: $PATH \" to your .zshrc / .bashrc or alternatively simply running the export manually. 3. Install the neps Package Using poetry Inside the main directory of neps run poetry install 4. Activate pre-commit for the repository With the python environment used to install the neps package run in the main directory of neps pre-commit install","title":"Installation for Contributors"},{"location":"contributing/installation/#installation-for-contributors","text":"There are three required steps and one optional: Optional: Install miniconda and create an environment Install poetry Install the neps package using poetry Activate pre-commit for the repository For instructions see below.","title":"Installation for Contributors"},{"location":"contributing/installation/#1-optional-install-miniconda-and-create-an-environment","text":"To manage python versions install e.g., miniconda with wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O install_miniconda.sh bash install_miniconda.sh -b -p $HOME /.conda # Change to place of preference rm install_miniconda.sh Consider running ~/.conda/bin/conda init or ~/.conda/bin/conda init zsh . Then finally create the environment and activate it conda create -n neps python = 3 .7.5 conda activate neps","title":"1. Optional: Install miniconda and create an environment"},{"location":"contributing/installation/#2-install-poetry","text":"First, install poetry, e.g., via curl -sSL https://install.python-poetry.org | python3 - Then consider appending export PATH = \" $HOME /.local/bin: $PATH \" to your .zshrc / .bashrc or alternatively simply running the export manually.","title":"2. Install poetry"},{"location":"contributing/installation/#3-install-the-neps-package-using-poetry","text":"Inside the main directory of neps run poetry install","title":"3. Install the neps Package Using poetry"},{"location":"contributing/installation/#4-activate-pre-commit-for-the-repository","text":"With the python environment used to install the neps package run in the main directory of neps pre-commit install","title":"4. Activate pre-commit for the repository"},{"location":"contributing/releasing/","text":"Releasing a New Version There are four steps to releasing a new version of neps: Understand Semantic Versioning Update the Package Version Commit and Push With a Version Tag Update Documentation Publish on PyPI 0. Understand Semantic Versioning We follow the semantic versioning scheme. 1. Update the Package Version and CITATION.cff poetry version v0.4.10 and manually change the version specified in CITATION.cff . 2. Commit with a Version Tag First commit and test git add pyproject.toml git commit -m \"Bump version from v0.4.9 to v0.4.10\" pytest Then tag and push git tag v0.4.10 git push --tags git push 3. Update Documentation First check if the documentation has any issues via mike deploy 0 .4.10 latest mike serve and then looking at it. Afterwards, publish it via mike deploy 0 .4.10 latest -p 4. Publish on PyPI To publish to PyPI: Get publishing rights, e.g., asking Danny or Maciej. Be careful, once on PyPI we can not change things. Run poetry publish --build This will ask for your PyPI credentials.","title":"Releasing a New Version"},{"location":"contributing/releasing/#releasing-a-new-version","text":"There are four steps to releasing a new version of neps: Understand Semantic Versioning Update the Package Version Commit and Push With a Version Tag Update Documentation Publish on PyPI","title":"Releasing a New Version"},{"location":"contributing/releasing/#0-understand-semantic-versioning","text":"We follow the semantic versioning scheme.","title":"0. Understand Semantic Versioning"},{"location":"contributing/releasing/#1-update-the-package-version-and-citationcff","text":"poetry version v0.4.10 and manually change the version specified in CITATION.cff .","title":"1. Update the Package Version and CITATION.cff"},{"location":"contributing/releasing/#2-commit-with-a-version-tag","text":"First commit and test git add pyproject.toml git commit -m \"Bump version from v0.4.9 to v0.4.10\" pytest Then tag and push git tag v0.4.10 git push --tags git push","title":"2. Commit with a Version Tag"},{"location":"contributing/releasing/#3-update-documentation","text":"First check if the documentation has any issues via mike deploy 0 .4.10 latest mike serve and then looking at it. Afterwards, publish it via mike deploy 0 .4.10 latest -p","title":"3. Update Documentation"},{"location":"contributing/releasing/#4-publish-on-pypi","text":"To publish to PyPI: Get publishing rights, e.g., asking Danny or Maciej. Be careful, once on PyPI we can not change things. Run poetry publish --build This will ask for your PyPI credentials.","title":"4. Publish on PyPI"},{"location":"contributing/roadmap/","text":"Roadmap Next up Features Utility to get best HPs and (built) architecture* Utility to get incumbent losses over time* Windows support* Documentation Fill up the core documentation pages* Fix NAS examples* Fixes Fix autograd warning? Refactoring remove graph_dense API* run_pipeline = evaluate_pipeline?* loss = validation_error | error | pipeline_error?* IntegerParameter = IntegerSpace Clean up unused branches Merge GP and hierarchical GP Merge gpytorch branch Tests and tooling Add simple regression tests to run on each push Before 1.0.0 version Features Seamless ddp via cli launcher Finegrained control over HP user prior Top vs all vs bottom distribution plots Tensorboard visualizations (incumbent plot, ..) Print search space upon run Add comprehensive regression tests to run manually on the cluster on each version release Fixes Printing architecture search spaces / search spaces in general Metahyper Refine jobtimelimit feature Optimize dependencies Refactoring clean up search spaces classes, unused methods break up search space and config aspect Documentation Keep a changelog Later version Features Optional argparse adder like pytorch lightning Utility neps.clean to manage existing run results Collect data optionally via phone-home to webserver Add Info dict to status BO improvements via Hebo tricks + Mll replacement Miscellaneous Twitter handle and domain, e.g., neural-pipeline.search Doing research with NePS / Documentation on that or full setup","title":"Roadmap"},{"location":"contributing/roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"contributing/roadmap/#next-up","text":"","title":"Next up"},{"location":"contributing/roadmap/#features","text":"Utility to get best HPs and (built) architecture* Utility to get incumbent losses over time* Windows support*","title":"Features"},{"location":"contributing/roadmap/#documentation","text":"Fill up the core documentation pages* Fix NAS examples*","title":"Documentation"},{"location":"contributing/roadmap/#fixes","text":"Fix autograd warning?","title":"Fixes"},{"location":"contributing/roadmap/#refactoring","text":"remove graph_dense API* run_pipeline = evaluate_pipeline?* loss = validation_error | error | pipeline_error?* IntegerParameter = IntegerSpace Clean up unused branches Merge GP and hierarchical GP Merge gpytorch branch","title":"Refactoring"},{"location":"contributing/roadmap/#tests-and-tooling","text":"Add simple regression tests to run on each push","title":"Tests and tooling"},{"location":"contributing/roadmap/#before-100-version","text":"","title":"Before 1.0.0 version"},{"location":"contributing/roadmap/#features_1","text":"Seamless ddp via cli launcher Finegrained control over HP user prior Top vs all vs bottom distribution plots Tensorboard visualizations (incumbent plot, ..) Print search space upon run Add comprehensive regression tests to run manually on the cluster on each version release","title":"Features"},{"location":"contributing/roadmap/#fixes_1","text":"Printing architecture search spaces / search spaces in general Metahyper Refine jobtimelimit feature Optimize dependencies","title":"Fixes"},{"location":"contributing/roadmap/#refactoring_1","text":"clean up search spaces classes, unused methods break up search space and config aspect","title":"Refactoring"},{"location":"contributing/roadmap/#documentation_1","text":"Keep a changelog","title":"Documentation"},{"location":"contributing/roadmap/#later-version","text":"","title":"Later version"},{"location":"contributing/roadmap/#features_2","text":"Optional argparse adder like pytorch lightning Utility neps.clean to manage existing run results Collect data optionally via phone-home to webserver Add Info dict to status BO improvements via Hebo tricks + Mll replacement","title":"Features"},{"location":"contributing/roadmap/#miscellaneous","text":"Twitter handle and domain, e.g., neural-pipeline.search Doing research with NePS / Documentation on that or full setup","title":"Miscellaneous"},{"location":"contributing/tests/","text":"Checks and Tests We have setup checks and tests at several points in the development flow: At every commit we automatically run a suite of pre-commit hooks that perform static code analysis, autoformating, and sanity checks. This is setup during our installation process . At every commit / push locally running a minimal suite of integration tests is encouraged. The tests correspond directly to examples in neps_examples and only check for crash-causing errors. At every push all integration tests and regression tests are run automatically using github actions . Examples and Integration Tests We use examples in neps_examples as integration tests, which we run from the main directory via pytest before every critical push. Creating an Integration Test If you want an implementation to be included in the above testing procedure: Create an example in neps_examples . Add the example to test_examples.py . Running all integration tests locally To speedup testing for developers, we only run a core set of tests per default. To run all tests use pytest -m all_examples On github, we always run all examples. What to do if tests fail If tests fail for you on the master: Try running the tests with a fresh environment install. If issues persist, notify others in the neps developers chat on mattermost. Regression Tests Regression tests are run on each push to the repository to assure the performance of the optimizers don't degrade. Currently, regression runs are recorded on JAHS-Bench-201 data for 2 tasks: cifar10 and fashion_mnist and only for optimizers: random_search , bayesian_optimization , mf_bayesian_optimization , regularized_evolution . This information is stored in the tests/regression_runner.py as two lists: TASKS , OPTIMIZERS . The recorded results are stored as a json dictionary in the tests/losses.json file. Adding new optimizer algorithms Once a new algorithm is added to NEPS library, we need to first record the performance of the algorithm for 100 optimization runs. If the algorithm expects standard loss function (pipeline) and accepts fidelity hyperparameters in pipeline space, then recording results only requires adding the optimizer name into OPTIMIZERS list in tests/regression_runner.py and running tests/regression_runner.py In case your algorithm requires custom pipeline and/or pipeline space you can modify the runner.run_pipeline and runner.pipeline_space attributes of the RegressionRunner after initialization (around line #322 in tests/regression_runner.py ) You can verify the optimizer is recorded by rerunning the regression_runner.py . Now regression test will be run on your new optimizer as well on every push. Regression test metrics For each regression test the algorithm is run 10 times to sample its performance, then they are statistically compared to the 100 recorded runs. We use these 3 boolean metrics to define the performance of the algorithm on any task: Kolmogorov-Smirnov test for goodness of fit - pvalue >= 10% Absolute median distance - bounded within 92.5% confidence range of the expected median distance Median improvement - Median improvement over the recorded median Test metrics are run for each (optimizer, task) combination separately and then collected. The collected metrics are then further combined into 2 metrics Task pass - either both Kolmogorov-Smirnov test and Absolute median distance test passes or just Median improvement Test aggregate - Sum_over_tasks( Kolmogorov-Smirnov test + Absolute median distance + 2 * Median improvement ) Finally, a test for an optimizer only passes when at least for one of the tasks Task pass is true, and Test aggregate is higher than 1 + number of tasks On regression test failures Regression tests are stochastic by nature, so they might fail occasionally even the algorithm performance didn't degrade. In the case of regression test failure, try running it again first, if the problem still persists, then you can contact Danny Stoll or Samir . You can also run tests locally by running: poetry run pytest -m regression_all Disabling and Skipping Checks etc. Pre-commit: How to not run hooks? To commit without running pre-commit use git commit --no-verify -m <COMMIT MESSAGE> . Pylint: How to ignore warnings? There are two options: Disable the warning locally: code = \"foo\" # pylint: disable=ERROR_CODE Make sure to use the named version of the error (e.g., unspecified-encoding , not W1514 ). Remove warning in pyproject.toml that we do not consider useful (do not catch bugs, do not increase code quality). Mypy: How to ignore warnings? There are two options: Disable the warning locally: code = \"foo\" # type: ignore[ERROR_CODE] If you know what you are doing, you can add the whole module to the [[tool.mypy.overrides]] section. This is useful e.g., when adding new files that are in early stage development. Black: How to not format code parts? x = 2 # fmt: off or for blocks # fmt: off x = 2 y = x + 1 # fmt: on","title":"Checks and Tests"},{"location":"contributing/tests/#checks-and-tests","text":"We have setup checks and tests at several points in the development flow: At every commit we automatically run a suite of pre-commit hooks that perform static code analysis, autoformating, and sanity checks. This is setup during our installation process . At every commit / push locally running a minimal suite of integration tests is encouraged. The tests correspond directly to examples in neps_examples and only check for crash-causing errors. At every push all integration tests and regression tests are run automatically using github actions .","title":"Checks and Tests"},{"location":"contributing/tests/#examples-and-integration-tests","text":"We use examples in neps_examples as integration tests, which we run from the main directory via pytest before every critical push.","title":"Examples and Integration Tests"},{"location":"contributing/tests/#creating-an-integration-test","text":"If you want an implementation to be included in the above testing procedure: Create an example in neps_examples . Add the example to test_examples.py .","title":"Creating an Integration Test"},{"location":"contributing/tests/#running-all-integration-tests-locally","text":"To speedup testing for developers, we only run a core set of tests per default. To run all tests use pytest -m all_examples On github, we always run all examples.","title":"Running all integration tests locally"},{"location":"contributing/tests/#what-to-do-if-tests-fail","text":"If tests fail for you on the master: Try running the tests with a fresh environment install. If issues persist, notify others in the neps developers chat on mattermost.","title":"What to do if tests fail"},{"location":"contributing/tests/#regression-tests","text":"Regression tests are run on each push to the repository to assure the performance of the optimizers don't degrade. Currently, regression runs are recorded on JAHS-Bench-201 data for 2 tasks: cifar10 and fashion_mnist and only for optimizers: random_search , bayesian_optimization , mf_bayesian_optimization , regularized_evolution . This information is stored in the tests/regression_runner.py as two lists: TASKS , OPTIMIZERS . The recorded results are stored as a json dictionary in the tests/losses.json file.","title":"Regression Tests"},{"location":"contributing/tests/#adding-new-optimizer-algorithms","text":"Once a new algorithm is added to NEPS library, we need to first record the performance of the algorithm for 100 optimization runs. If the algorithm expects standard loss function (pipeline) and accepts fidelity hyperparameters in pipeline space, then recording results only requires adding the optimizer name into OPTIMIZERS list in tests/regression_runner.py and running tests/regression_runner.py In case your algorithm requires custom pipeline and/or pipeline space you can modify the runner.run_pipeline and runner.pipeline_space attributes of the RegressionRunner after initialization (around line #322 in tests/regression_runner.py ) You can verify the optimizer is recorded by rerunning the regression_runner.py . Now regression test will be run on your new optimizer as well on every push.","title":"Adding new optimizer algorithms"},{"location":"contributing/tests/#regression-test-metrics","text":"For each regression test the algorithm is run 10 times to sample its performance, then they are statistically compared to the 100 recorded runs. We use these 3 boolean metrics to define the performance of the algorithm on any task: Kolmogorov-Smirnov test for goodness of fit - pvalue >= 10% Absolute median distance - bounded within 92.5% confidence range of the expected median distance Median improvement - Median improvement over the recorded median Test metrics are run for each (optimizer, task) combination separately and then collected. The collected metrics are then further combined into 2 metrics Task pass - either both Kolmogorov-Smirnov test and Absolute median distance test passes or just Median improvement Test aggregate - Sum_over_tasks( Kolmogorov-Smirnov test + Absolute median distance + 2 * Median improvement ) Finally, a test for an optimizer only passes when at least for one of the tasks Task pass is true, and Test aggregate is higher than 1 + number of tasks","title":"Regression test metrics"},{"location":"contributing/tests/#on-regression-test-failures","text":"Regression tests are stochastic by nature, so they might fail occasionally even the algorithm performance didn't degrade. In the case of regression test failure, try running it again first, if the problem still persists, then you can contact Danny Stoll or Samir . You can also run tests locally by running: poetry run pytest -m regression_all","title":"On regression test failures"},{"location":"contributing/tests/#disabling-and-skipping-checks-etc","text":"","title":"Disabling and Skipping Checks etc."},{"location":"contributing/tests/#pre-commit-how-to-not-run-hooks","text":"To commit without running pre-commit use git commit --no-verify -m <COMMIT MESSAGE> .","title":"Pre-commit: How to not run hooks?"},{"location":"contributing/tests/#pylint-how-to-ignore-warnings","text":"There are two options: Disable the warning locally: code = \"foo\" # pylint: disable=ERROR_CODE Make sure to use the named version of the error (e.g., unspecified-encoding , not W1514 ). Remove warning in pyproject.toml that we do not consider useful (do not catch bugs, do not increase code quality).","title":"Pylint: How to ignore warnings?"},{"location":"contributing/tests/#mypy-how-to-ignore-warnings","text":"There are two options: Disable the warning locally: code = \"foo\" # type: ignore[ERROR_CODE] If you know what you are doing, you can add the whole module to the [[tool.mypy.overrides]] section. This is useful e.g., when adding new files that are in early stage development.","title":"Mypy: How to ignore warnings?"},{"location":"contributing/tests/#black-how-to-not-format-code-parts","text":"x = 2 # fmt: off or for blocks # fmt: off x = 2 y = x + 1 # fmt: on","title":"Black: How to not format code parts?"},{"location":"contributing/metahyper/","text":"Metahyper TODO","title":"Metahyper"},{"location":"contributing/metahyper/#metahyper","text":"TODO","title":"Metahyper"},{"location":"contributing/metahyper/api/","text":"","title":"Api"},{"location":"contributing/metahyper/evaluation_fn/","text":"","title":"Evaluation fn"},{"location":"contributing/metahyper/getting_started/","text":"","title":"Getting started"},{"location":"contributing/metahyper/parallelization/","text":"","title":"Parallelization"},{"location":"contributing/metahyper/sampler/","text":"","title":"Sampler"}]}