{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neural Pipeline Search (NePS)","text":"<p>Welcome to NePS, a powerful and flexible Python library for hyperparameter optimization (HPO) and neural architecture search (NAS) with its primary goal: make HPO and NAS usable for deep learners in practice.</p> <p>NePS houses recently published and also well-established algorithms that can all be run massively parallel on distributed setups, with tools to analyze runs, restart runs, etc., all tailored to the needs of deep learning experts.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>In addition to the features offered by traditional HPO and NAS libraries, NePS stands out with:</p> <ol> <li>Hyperparameter Optimization (HPO) Efficient Enough For Deep Learning:      NePS excels in efficiently tuning hyperparameters using algorithms that enable users to make use of their prior knowledge, while also using many other efficiency boosters.<ul> <li>PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning (NeurIPS 2023)</li> <li>\u03c0BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization (ICLR 2022) </li> </ul> </li> <li>Neural Architecture Search (NAS) with Expressive Search Spaces:      NePS provides capabilities for designing and optimizing architectures in an expressive and natural fashion.<ul> <li>Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars (NeurIPS 2023) </li> </ul> </li> <li>Zero-effort Parallelization and an Experience Tailored to DL:       NePS simplifies the process of parallelizing optimization tasks both on individual computers and in distributed      computing environments. As NePS is made for deep learners, all technical choices are made with DL in mind and common      DL tools such as Tensorboard are embraced.</li> </ol> <p>Tip</p> <p>Check out:</p> <ul> <li>Reference documentation for a quick overview.</li> <li>API for a more detailed reference.</li> <li>Colab Tutorial walking through NePS's main features.</li> <li>Examples for basic code snippets to get started.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the latest release from PyPI run</p> <pre><code>pip install neural-pipeline-search\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Using <code>neps</code> always follows the same pattern:</p> <ol> <li>Define a <code>evaluate_pipeline</code> function capable of evaluating different architectural and/or hyperparameter configurations    for your problem.</li> <li>Define a search space named <code>pipeline_space</code> of those Parameters e.g. via a dictionary</li> <li>Call <code>neps.run</code> to optimize <code>evaluate_pipeline</code> over <code>pipeline_space</code></li> </ol> <p>In code, the usage pattern can look like this:</p> <pre><code>import neps\nimport logging\n\n\n# 1. Define a function that accepts hyperparameters and computes the validation error\ndef evaluate_pipeline(\n        hyperparameter_a: float, hyperparameter_b: int, architecture_parameter: str\n) -&gt; dict:\n    # Create your model\n    model = MyModel(architecture_parameter)\n\n    # Train and evaluate the model with your training pipeline\n    validation_error = train_and_eval(\n        model, hyperparameter_a, hyperparameter_b\n    )\n    return validation_error\n\n\n# 2. Define a search space of parameters; use the same parameter names as in evaluate_pipeline\npipeline_space = dict(\n    hyperparameter_a=neps.Float(\n        lower=0.001, upper=0.1, log=True  # The search space is sampled in log space\n    ),\n    hyperparameter_b=neps.Integer(lower=1, upper=42),\n    architecture_parameter=neps.Categorical([\"option_a\", \"option_b\"]),\n)\n\n# 3. Run the NePS optimization\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"path/to/save/results\",  # Replace with the actual path.\n    max_evaluations_total=100,\n)\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p>Discover how NePS works through these examples:</p> <ul> <li> <p>Hyperparameter Optimization: Learn the essentials of hyperparameter optimization with NePS.</p> </li> <li> <p>Multi-Fidelity Optimization: Understand how to leverage multi-fidelity optimization for efficient model tuning.</p> </li> <li> <p>Utilizing Expert Priors for Hyperparameters: Learn how to incorporate expert priors for more efficient hyperparameter selection.</p> </li> <li> <p>Additional NePS Examples: Explore more examples, including various use cases and advanced configurations in NePS.</p> </li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Please see the documentation for contributors.</p>"},{"location":"#citations","title":"Citations","text":"<p>For pointers on citing the NePS package and papers refer to our documentation on citations.</p>"},{"location":"citations/","title":"Citations","text":""},{"location":"citations/#citation-of-the-software","title":"Citation of The Software","text":"<p>For citing NePS, please refer to the following:</p>"},{"location":"citations/#apa-style","title":"APA Style","text":"<pre><code>Stoll, D., Mallik, N., Schrodi, S., Bergman, E., Janowski, M., Garibov, S., Abou Chakra, T., Rogalla, D., Bergman, E., Hvarfner, C., Binxin, R., &amp; Hutter, F. (2023). Neural Pipeline Search (NePS) (Version 0.12.2) [Computer software]. https://github.com/automl/neps\n</code></pre>"},{"location":"citations/#bibtex-style","title":"BibTex Style","text":"<pre><code>@software{Stoll_Neural_Pipeline_Search_2023,\nauthor = {Stoll, Danny and Mallik, Neeratyoy and Schrodi, Simon and Bergmann, Eddie and Janowski, Maciej and Garibov, Samir and Abou Chakra, Tarek and Rogalla, Daniel and Bergman, Eddie and Hvarfner, Carl and Binxin, Ru and Hutter, Frank},\nmonth = oct,\ntitle = {{Neural Pipeline Search (NePS)}},\nurl = {https://github.com/automl/neps},\nversion = {0.12.2},\nyear = {2024}\n}\n</code></pre>"},{"location":"citations/#citation-of-papers","title":"Citation of Papers","text":""},{"location":"citations/#priorband","title":"PriorBand","text":"<p>If you have used PriorBand as the optimizer, please use the bibtex below:</p> <pre><code>@inproceedings{mallik2023priorband,\ntitle = {PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning},\nauthor = {Neeratyoy Mallik and Eddie Bergman and Carl Hvarfner and Danny Stoll and Maciej Janowski and Marius Lindauer and Luigi Nardi and Frank Hutter},\nyear = {2023},\nbooktitle = {Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)},\nkeywords = {}\n}\n</code></pre>"},{"location":"citations/#hierarchichal-nas-with-context-free-grammars","title":"Hierarchichal NAS with Context-free Grammars","text":"<p>If you have used the context-free grammar search space and the graph kernels implemented in NePS for the paper Hierarchical NAS, please use the bibtex below:</p> <pre><code>@inproceedings{schrodi2023hierarchical,\ntitle = {Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars},\nauthor = {Simon Schrodi and Danny Stoll and Binxin Ru and Rhea Sanjay Sukthanker and Thomas Brox and Frank Hutter},\nyear = {2023},\nbooktitle = {Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)},\nkeywords = {}\n}\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Getting started with NePS involves a straightforward yet powerful process, centering around its three main components. This approach ensures flexibility and efficiency in evaluating different architecture and hyperparameter configurations for your problem.</p> <p>NePS requires Python 3.10 or higher. You can install it via <code>pip</code> or from source.</p> <pre><code>pip install neural-pipeline-search\n</code></pre>"},{"location":"getting_started/#the-3-main-components","title":"The 3 Main Components","text":"<ol> <li>Establish a <code>pipeline_space=</code>:</li> </ol> <pre><code>pipeline_space={\n    \"some_parameter\": (0.0, 1.0),   # float\n    \"another_parameter\": (0, 10),   # integer\n    \"optimizer\": [\"sgd\", \"adam\"],   # categorical\n    \"epoch\": neps.Integer(lower=1, upper=100, is_fidelity=True),\n    \"learning_rate\": neps.Float(lower=1e-5, upper=1, log=True),\n    \"alpha\": neps.Float(lower=0.1, upper=1.0, prior=0.99, prior_confidence=\"high\")\n}\n</code></pre> <ol> <li>Define an <code>evaluate_pipeline()</code> function:</li> </ol> <pre><code>def evaluate_pipeline(some_parameter: float,\n                 another_parameter: float,\n                 optimizer: str, epoch: int,\n                 learning_rate: float, alpha: float) -&gt; float:\n    model = make_model(...)\n    loss = eval_model(model)\n    return loss\n</code></pre> <ol> <li>Execute with <code>neps.run()</code>:</li> </ol> <pre><code>neps.run(evaluate_pipeline, pipeline_space)\n</code></pre>"},{"location":"getting_started/#whats-next","title":"What's Next?","text":"<p>The reference section provides detailed information on the individual components of NePS.</p> <ol> <li>How to use the <code>neps.run()</code> function to start the optimization process.</li> <li>The different search space options available.</li> <li>How to choose and configure the optimizer used.</li> <li>How to define the <code>evaluate_pipeline()</code> function.</li> <li>How to analyze the optimization runs.</li> </ol> <p>Or discover the features of NePS through these practical examples:</p> <ul> <li> <p>Hyperparameter Optimization (HPO): Learn the essentials of hyperparameter optimization with NePS.</p> </li> <li> <p>Multi-Fidelity Optimization: Understand how to leverage multi-fidelity optimization for efficient model tuning.</p> </li> <li> <p>Utilizing Expert Priors for Hyperparameters: Learn how to incorporate expert priors for more efficient hyperparameter selection.</p> </li> <li> <p>Additional NePS Examples: Explore more examples, including various use cases and advanced configurations in NePS.</p> </li> </ul>"},{"location":"api/","title":"API","text":"<p>Use the tree to navigate the API documentation.</p>"},{"location":"api/neps/api/","title":"Api","text":"<p>API for the neps package.</p>"},{"location":"api/neps/api/#neps.api.run","title":"run","text":"<pre><code>run(\n    evaluate_pipeline: (\n        Callable[..., EvaluatePipelineReturn] | str\n    ),\n    pipeline_space: (\n        Mapping[str, dict | str | int | float | Parameter]\n        | SearchSpace\n        | ConfigurationSpace\n        | Pipeline\n    ),\n    *,\n    root_directory: str | Path = \"neps_results\",\n    overwrite_working_directory: bool = False,\n    post_run_summary: bool = True,\n    max_evaluations_total: int | None = None,\n    max_evaluations_per_run: int | None = None,\n    continue_until_max_evaluation_completed: bool = False,\n    max_cost_total: int | float | None = None,\n    ignore_errors: bool = False,\n    objective_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    sample_batch_size: int | None = None,\n    optimizer: (\n        OptimizerChoice\n        | Mapping[str, Any]\n        | tuple[OptimizerChoice, Mapping[str, Any]]\n        | Callable[\n            Concatenate[SearchSpace, ...], AskFunction\n        ]\n        | Callable[Concatenate[Pipeline, ...], AskFunction]\n        | Callable[\n            Concatenate[SearchSpace | Pipeline, ...],\n            AskFunction,\n        ]\n        | CustomOptimizer\n        | Literal[\"auto\"]\n    ) = \"auto\"\n) -&gt; None\n</code></pre> <p>Run the optimization.</p> <p>Parallelization</p> <p>To run with multiple processes or machines, execute the script that calls <code>neps.run()</code> multiple times. They will keep in sync using the file-sytem, requiring that <code>root_directory</code> be shared between them.</p> <pre><code>import neps\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef evaluate_pipeline(some_parameter: float) -&gt; float:\n    validation_error = -some_parameter\n    return validation_error\n\npipeline_space = dict(some_parameter=neps.Float(lower=0, upper=1))\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space={\n        \"some_parameter\": (0.0, 1.0),   # float\n        \"another_parameter\": (0, 10),   # integer\n        \"optimizer\": [\"sgd\", \"adam\"],   # categorical\n        \"epoch\": neps.Integer(          # fidelity integer\n            lower=1,\n            upper=100,\n            is_fidelity=True\n        ),\n        \"learning_rate\": neps.Float(    # log spaced float\n            lower=1e-5,\n            uperr=1,\n            log=True\n        ),\n        \"alpha\": neps.Float(            # float with a prior\n            lower=0.1,\n            upper=1.0,\n            prior=0.99,\n            prior_confidence=\"high\",\n        )\n    },\n    root_directory=\"usage_example\",\n    max_evaluations_total=5,\n)\n</code></pre> PARAMETER DESCRIPTION <code>evaluate_pipeline</code> <p>The objective function to minimize. This will be called with a configuration from the <code>pipeline_space=</code> that you define.</p> <p>The function should return one of the following:</p> <ul> <li>A <code>float</code>, which is the objective value to minimize.</li> <li> <p>A <code>dict</code> which can have the following keys:</p> <pre><code>{\n    \"objective_to_minimize\": float,  # The thing to minimize (required)\n    \"cost\": float,  # The cost of the evaluate_pipeline, used by some algorithms\n    \"info_dict\": dict,  # Any additional information you want to store, should be YAML serializable\n}\n</code></pre> </li> </ul> <code>str</code> usage for dynamic imports <p>If a string, it should be in the format <code>\"/path/to/:function\"</code>. to specify the function to call. You may also directly provide an mode to import, e.g., <code>\"my.module.something:evaluate_pipeline\"</code>.</p> <p> TYPE: <code>Callable[..., EvaluatePipelineReturn] | str</code> </p> <code>pipeline_space</code> <p>The search space to minimize over.</p> <p>This most direct way to specify the search space is as follows:</p> <pre><code>neps.run(\n    pipeline_space={\n        \"dataset\": \"mnist\",             # constant\n        \"nlayers\": (2, 10),             # integer\n        \"alpha\": (0.1, 1.0),            # float\n        \"optimizer\": [                  # categorical\n            \"adam\", \"sgd\", \"rmsprop\"\n        ],\n        \"learning_rate\": neps.Float(,   # log spaced float\n            lower=1e-5, upper=1, log=True\n        ),\n        \"epochs\": neps.Integer(         # fidelity integer\n            lower=1, upper=100, is_fidelity=True\n        ),\n        \"batch_size\": neps.Integer(     # integer with a prior\n            lower=32, upper=512, prior=128\n        ),\n\n    }\n)\n</code></pre> <p>You can also directly instantiate any of the parameters defined by <code>Parameter</code> and provide them directly.</p> <p>Some important properties you can set on parameters are:</p> <ul> <li><code>prior=</code>: If you have a good idea about what a good setting     for a parameter may be, you can set this as the prior for     a parameter. You can specify this along with <code>prior_confidence</code>     if you would like to assign a <code>\"low\"</code>, <code>\"medium\"</code>, or <code>\"high\"</code>     confidence to the prior.</li> </ul> <p>Yaml support</p> <p>To support spaces defined in yaml, you may also define the parameters as dictionarys, e.g.,</p> <pre><code>neps.run(\n    pipeline_space={\n        \"dataset\": \"mnist\",\n        \"nlayers\": {\"type\": \"int\", \"lower\": 2, \"upper\": 10},\n        \"alpha\": {\"type\": \"float\", \"lower\": 0.1, \"upper\": 1.0},\n        \"optimizer\": {\"type\": \"cat\", \"choices\": [\"adam\", \"sgd\", \"rmsprop\"]},\n        \"learning_rate\": {\"type\": \"float\", \"lower\": 1e-5, \"upper\": 1, \"log\": True},\n        \"epochs\": {\"type\": \"int\", \"lower\": 1, \"upper\": 100, \"is_fidelity\": True},\n        \"batch_size\": {\"type\": \"int\", \"lower\": 32, \"upper\": 512, \"prior\": 128},\n    }\n)\n</code></pre> <p>ConfigSpace support</p> <p>You may also use a <code>ConfigurationSpace</code> object from the <code>ConfigSpace</code> library.</p> <p> TYPE: <code>Mapping[str, dict | str | int | float | Parameter] | SearchSpace | ConfigurationSpace | Pipeline</code> </p> <code>root_directory</code> <p>The directory to save progress to.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>'neps_results'</code> </p> <code>overwrite_working_directory</code> <p>If true, delete the working directory at the start of the run. This is, e.g., useful when debugging a evaluate_pipeline function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>post_run_summary</code> <p>If True, creates a csv file after each worker is done, holding summary information about the configs and results.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_evaluations_per_run</code> <p>Number of evaluations this specific call should do.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>max_evaluations_total</code> <p>Number of evaluations after which to terminate. This is shared between all workers operating in the same <code>root_directory</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>continue_until_max_evaluation_completed</code> <p>If true, only stop after max_evaluations_total have been completed. This is only relevant in the parallel setting.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>max_cost_total</code> <p>No new evaluations will start when this cost is exceeded. Requires returning a cost in the evaluate_pipeline function, e.g., <code>return dict(loss=loss, cost=cost)</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>None</code> </p> <code>ignore_errors</code> <p>Ignore hyperparameter settings that threw an error and do not raise an error. Error configs still count towards max_evaluations_total.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>objective_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error and will use given objective_to_minimize value instead. default: None</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and objective_value_on_error to any float will supress any error and will use given cost value instead. default: None</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>sample_batch_size</code> <p>The number of samples to ask for in a single call to the optimizer.</p> When to use this? <p>This is only useful in scenarios where you have many workers available, and the optimizers sample time prevents full worker utilization, as can happen with Bayesian optimizers.</p> <p>In this case, the currently active worker will first check if there are any new configurations to evaluate, and if not, generate <code>sample_batch_size</code> new configurations that the proceeding workers will then pick up and evaluate.</p> <p>We advise to only use this if:</p> <ul> <li>You are using a <code>\"ifbo\"</code> or <code>\"bayesian_optimization\"</code>.</li> <li>You have a fast to evaluate <code>evaluate_pipeline</code></li> <li>You have a significant amount of workers available, relative to the time it takes to evaluate a single configuration.</li> </ul> Downsides of batching <p>The primary downside of batched optimization is that the next <code>sample_batch_size</code> configurations will not be able to take into account the results of any new evaluations, even if they were to come in relatively quickly.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>Which optimizer to use.</p> <p>Not sure which to use? Leave this at <code>\"auto\"</code> and neps will choose the optimizer based on the search space given.</p> Available optimizers <ul> <li> <p><code>\"bayesian_optimization\"</code>,</p> <p>Models the relation between hyperparameters in your <code>pipeline_space</code> and the results of <code>evaluate_pipeline</code> using bayesian optimization. This acts as a cheap surrogate model of you <code>evaluate_pipeline</code> function that can be used for optimization.</p> <p>When to use this?</p> <p>Bayesion optimization is a good general purpose choice, especially if the size of your search space is not too large. It is also the best option to use if you do not have or want to use a fidelity parameter.</p> <p>Note that acquiring the next configuration to evaluate with bayesian optimization can become prohibitvely expensive as the number of configurations evaluated increases.</p> <p>If there is some numeric cost associated with evaluating a configuration, you can provide this as a <code>cost</code> when returning the results from your <code>evaluate_pipeline</code> function. By specifying <code>cost_aware=True</code>, the optimizer will attempt to balance getting the best result while minimizing the cost.</p> <p>If you have priors, we recommend looking at <code>pibo</code>.</p> <p>For Multi-objective optimization (i.e., no. of objectives in trials &gt; 1), the algorithm automatically switches to the qLogNoisyExpectedHypervolumeImprovement acquisition function.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>initial_design_size</code> <p>Number of samples used before using the surrogate model. If \"ndim\", it will use the number of parameters in the search space.</p> <p> TYPE: <code>int | Literal['ndim']</code> DEFAULT: <code>'ndim'</code> </p> <code>cost_aware</code> <p>Whether to consider reported \"cost\" from configurations in decision making. If True, the optimizer will weigh potential candidates by how much they cost, incentivising the optimizer to explore cheap, good performing configurations. This amount is modified over time. If \"log\", the cost will be log-transformed before being used.</p> <p>Warning</p> <p>If using <code>cost</code>, cost must be provided in the reports of the trials.</p> <p> TYPE: <code>bool | Literal['log']</code> DEFAULT: <code>False</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>device</code> <p>Device to use for the optimization.</p> <p> TYPE: <code>device | str | None</code> DEFAULT: <code>None</code> </p> <code>reference_point</code> <p>The reference point to use got multi-objective bayesian optimization. If <code>None</code>, the reference point will be calculated automatically.</p> <p> TYPE: <code>tuple[float, ...] | None</code> DEFAULT: <code>None</code> </p> </li> </ul> <ul> <li> <p><code>\"ifbo\"</code></p> <p>A transformer that has been trained to predict loss curves of deep-learing models, used to guide the optimization procedure and select configurations which are most promising to evaluate.</p> <p>When to use this?</p> <p>Use this when you think that early signal in your loss curve could be used to distinguish which configurations are likely to achieve a good performance.</p> <p>This algorithm will take many small steps in evaluating your configuration so we also advise that saving and loading your model checkpoint should be relatively fast.</p> <p>This algorithm requires a fidelity parameter, such as <code>epochs</code>, to be present. Each time we evaluate a configuration, we will only evaluate it for a single epoch, before returning back to the ifbo algorithm to select the next configuration.</p> Fidelities? <p>A fidelity parameter lets you control how many resources to invest in a single evaluation. For example, a common one for deep-learing is <code>epochs</code>. We can evaluate a model for just a single epoch, (fidelity step) to gain more information about the model's performance and decide what to do next.</p> <ul> <li>Paper: openreview.net/forum?id=VyoY3Wh9Wd</li> <li>Github: github.com/automl/ifBO/tree/main</li> </ul> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>step_size</code> <p>The size of the step to take in the fidelity domain.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>1</code> </p> <code>sample_prior_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>initial_design_size</code> <p>Number of configs to sample before starting optimization</p> <p>If <code>None</code>, the number of configs will be equal to the number of dimensions.</p> <p> TYPE: <code>int | Literal['ndim']</code> DEFAULT: <code>'ndim'</code> </p> <code>device</code> <p>Device to use for the model</p> <p> TYPE: <code>device | str | None</code> DEFAULT: <code>None</code> </p> <code>surrogate_path</code> <p>Path to the surrogate model to use</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>surrogate_version</code> <p>Version of the surrogate model to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'0.0.1'</code> </p> </li> </ul> <ul> <li> <p><code>\"successive_halving\"</code>:</p> <p>A bandit-based optimization algorithm that uses a fidelity parameter to gradually invest resources into more promising configurations.</p> Fidelities? <p>A fidelity parameter lets you control how many resources to invest in a single evaluation. For example, a common one for deep-learing is <code>epochs</code>. By evaluating a model for just a few epochs, we can quickly get a sense if the model is promising or not. Only those that perform well get promoted and evaluated at a higher epoch.</p> <p>When to use this?</p> <p>When you think that the rank of N configurations at a lower fidelity correlates very well with the rank if you were to evaluate those configurations at higher fidelities.</p> <p>It does this by creating a competition between N configurations and racing them in a bracket against each other. This bracket has a series of incrementing rungs, where lower rungs indicate less resources invested. The amount of resources is related to your fidelity parameter, with the highest rung relating to the maximum of your fidelity parameter.</p> <p>Those that perform well get promoted and evaluated with more resources.</p> <pre><code># A bracket indicating the rungs and configurations.\n# Those which performed best get promoted through the rungs.\n\n|        | fidelity    | c1 | c2 | c3 | c4 | c5 | ... | cN |\n| Rung 0 | (3 epochs)  |  o |  o |  o |  o |  o | ... | o  |\n| Rung 1 | (9 epochs)  |  o |    |  o |  o |    | ... | o  |\n| Rung 2 | (27 epochs) |  o |    |    |    |    | ... |    |\n</code></pre> <p>By default, new configurations are sampled using a uniform distribution, however you can also specify to prefer sampling from around a distribution you think is more promising by setting the <code>prior</code> and the <code>prior_confidence</code> of parameters of your search space.</p> <p>You can choose between these by setting <code>sampler=\"uniform\"</code> or <code>sampler=\"prior\"</code>.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>early_stopping_rate</code> <p>Determines the number of rungs in a bracket Choosing 0 creates maximal rungs given the fidelity bounds.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first, and if so, should it be at the highest fidelity level.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> </li> </ul> <ul> <li> <p><code>\"hyperband\"</code>:</p> <p>Another bandit-based optimization algorithm that uses a fidelity parameter, very similar to <code>successive_halving</code>, but hedges a bit more on the safe side, just incase your fidelity parameters isn't as well correlated as you'd like.</p> <p>When to use this?</p> <p>Use this when you think lower fidelity evaluations of your configurations carries some signal about their ranking at higher fidelities, but not enough to be certain</p> <p>Hyperband is like Successive Halving but it instead of always having the same bracket layout, it runs different brackets with different rungs.</p> <p>This helps hedge against scenarios where rankings at the lowest fidelity do not correlate well with the upper fidelity.</p> <pre><code># Hyperband runs different successive halving brackets\n\n| Bracket 1 |         | Bracket 2 |        | Bracket 3 |\n| Rung 0    | ... |   | (skipped) |        | (skipped) |\n| Rung 1    | ... |   | Rung 1    | ... |  | (skipped) |\n| Rung 2    | ... |   | Rung 2    | ... |  | Rung 2    | ... |\n</code></pre> <p>For more information, see the <code>successive_halving</code> documentation, as this algorithm could be considered an extension of it.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first, and if so, should it be at the highest fidelity level.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> </li> </ul> <ul> <li> <p><code>\"priorband\"</code>:</p> <p>Priorband is also a bandit-based optimization algorithm that uses a fidelity, providing a general purpose sampling extension to other algorithms. It makes better use of the prior information you provide in the search space along with the fact that you can afford to explore and take more risk at lower fidelities.</p> <p>When to use this?</p> <p>Use this when you have a good idea of what good parameters look like and can specify them through the <code>prior</code> and <code>prior_confidence</code> parameters in the search space.</p> <p>As <code>priorband</code> is flexible, you may choose between the existing tradeoffs the other algorithms provide through the use of <code>base=</code>.</p> <p>Priorband works by adjusting the sampling procedure to sample from one of the following three distributions:</p> <ul> <li>1) a uniform distribution</li> <li>2) a prior distribution</li> <li>3) a distribution around the best found configuration so far.</li> </ul> <p>By weighing the likelihood of good configurations having been sampled from each of these distribution, we can score them against each other to aid selection. We further use the fact that we can afford to explore and take more risk at lower fidelities, which is factored into the sampling procedure.</p> <p>See: openreview.net/forum?id=uoiwugtpCH&amp;noteId=xECpK2WH6k</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> <code>base</code> <p>The base algorithm to use for the bracketing.</p> <p> TYPE: <code>Literal['successive_halving', 'hyperband', 'asha', 'async_hb']</code> DEFAULT: <code>'hyperband'</code> </p> <code>bayesian_optimization_kick_in_point</code> <p>If a number <code>N</code>, after <code>N</code> * <code>maximum_fidelity</code> worth of fidelity has been evaluated, proceed with bayesian optimization when sampling a new configuration.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>None</code> </p> </li> </ul> <ul> <li> <p><code>\"asha\"</code>:</p> <p>A bandit-based optimization algorithm that uses a fidelity parameter, the asynchronous version of <code>successive_halving</code>. one that scales better to many parallel workers.</p> <p>When to use this?</p> <p>Use this when you think lower fidelity evaluations of your configurations carries a strong signal about their ranking at higher fidelities, and you have many workers available to evaluate configurations in parallel.</p> <p>It does this by maintaining one big bracket, i.e. one big on-going competition, with a promotion rule based on the sizes of each rung.</p> <pre><code># ASHA maintains one big bracket with an exponentially decreasing amount of\n# configurations promoted, relative to those in the rung below.\n\n|        | fidelity    | c1 | c2 | c3 | c4 | c5 | ...\n| Rung 0 | (3 epochs)  |  o |  o |  o |  o |  o | ...\n| Rung 1 | (9 epochs)  |  o |    |  o |  o |    | ...\n| Rung 2 | (27 epochs) |  o |    |    |  o |    | ...\n</code></pre> <p>For more information, see the <code>successive_halving</code> documentation, as this algorithm could be considered an extension of it.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first, and if so, should it be at the highest fidelity.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> </li> </ul> <ul> <li> <p><code>\"async_hb\"</code>:</p> <p>An asynchronous version of <code>hyperband</code>, where the brackets are run asynchronously, and the promotion rule is based on the number of evaluations each configuration has had.</p> <p>When to use this?</p> <p>Use this when you think lower fidelity evaluations of your configurations carries some signal about their ranking at higher fidelities, but not confidently, and you have many workers available to evaluate configurations in parallel.</p> <pre><code># Async HB runs different \"asha\" brackets, which are unbounded in the number\n# of configurations that can be in each. The bracket chosen at each iteration\n# is a sampling function based on the resources invested in each bracket.\n\n| Bracket 1 |         | Bracket 2 |        | Bracket 3 |\n| Rung 0    | ...     | (skipped) |        | (skipped) |\n| Rung 1    | ...     | Rung 1    | ...    | (skipped) |\n| Rung 2    | ...     | Rung 2    | ...    | Rung 2    | ...\n</code></pre> <p>For more information, see the <code>hyperband</code> documentation, <code>successive_halving</code> documentation, and the <code>asha</code> documentation, as this algorithm takes elements from each.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> </li> </ul> <ul> <li> <p><code>\"random_search\"</code>:</p> <p>A simple random search algorithm that samples configurations uniformly at random.</p> <p>You may also <code>use_priors=</code> to sample from a distribution centered around your defined priors.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>use_priors</code> <p>Whether to use priors when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore fidelity when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool | Literal['highest fidelity']</code> DEFAULT: <code>False</code> </p> </li> </ul> <ul> <li> <p><code>\"grid_search\"</code>:</p> <p>A simple grid search algorithm which discretizes the search space and evaluates all possible configurations.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore fidelity when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> </li> </ul> <p>With any optimizer choice, you also may provide some additional parameters to the optimizers. We do not recommend this unless you are familiar with the optimizer you are using. You may also specify an optimizer as a dictionary for supporting reading in serialized yaml formats:</p> <pre><code>neps.run(\n    ...,\n    optimzier={\n        \"name\": \"priorband\",\n        \"sample_prior_first\": True,\n    }\n)\n</code></pre> Own optimzier <p>Lastly, you may also provide your own optimizer which must satisfy the <code>AskFunction</code> signature.</p> <pre><code>class MyOpt:\n\n    def __init__(self, space: SearchSpace):\n        ...\n\n    def __call__(\n        self,\n        trials: Mapping[str, Trial],\n        budget_info: BudgetInfo | None,\n        n: int | None = None,\n    ) -&gt; SampledConfig | list[SampledConfig]:\n        # Sample a new configuration.\n        #\n        # Args:\n        #   trials: All of the trials that are known about.\n        #   budget_info: information about the budget constraints.\n        #\n        # Returns:\n        #   The sampled configuration(s)\n\n\nneps.run(\n    ...,\n    optimizer=MyOpt,\n)\n</code></pre> <p>This is mainly meant for internal development but allows you to use the NePS runtime to run your optimizer.</p> <p> TYPE: <code>OptimizerChoice | Mapping[str, Any] | tuple[OptimizerChoice, Mapping[str, Any]] | Callable[Concatenate[SearchSpace, ...], AskFunction] | Callable[Concatenate[Pipeline, ...], AskFunction] | Callable[Concatenate[SearchSpace | Pipeline, ...], AskFunction] | CustomOptimizer | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> Source code in <code>neps\\api.py</code> <pre><code>def run(  # noqa: PLR0913\n    evaluate_pipeline: Callable[..., EvaluatePipelineReturn] | str,\n    pipeline_space: (\n        Mapping[str, dict | str | int | float | Parameter]\n        | SearchSpace\n        | ConfigurationSpace\n        | Pipeline\n    ),\n    *,\n    root_directory: str | Path = \"neps_results\",\n    overwrite_working_directory: bool = False,\n    post_run_summary: bool = True,\n    max_evaluations_total: int | None = None,\n    max_evaluations_per_run: int | None = None,\n    continue_until_max_evaluation_completed: bool = False,\n    max_cost_total: int | float | None = None,\n    ignore_errors: bool = False,\n    objective_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    sample_batch_size: int | None = None,\n    optimizer: (\n        OptimizerChoice\n        | Mapping[str, Any]\n        | tuple[OptimizerChoice, Mapping[str, Any]]\n        | Callable[Concatenate[SearchSpace, ...], AskFunction]  # Hack, while we transit\n        | Callable[Concatenate[Pipeline, ...], AskFunction]  # from SearchSpace to\n        | Callable[Concatenate[SearchSpace | Pipeline, ...], AskFunction]  # Pipeline\n        | CustomOptimizer\n        | Literal[\"auto\"]\n    ) = \"auto\",\n) -&gt; None:\n    \"\"\"Run the optimization.\n\n    !!! tip \"Parallelization\"\n\n        To run with multiple processes or machines, execute the script that\n        calls `neps.run()` multiple times. They will keep in sync using\n        the file-sytem, requiring that `root_directory` be shared between them.\n\n\n    ```python\n    import neps\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n\n    def evaluate_pipeline(some_parameter: float) -&gt; float:\n        validation_error = -some_parameter\n        return validation_error\n\n    pipeline_space = dict(some_parameter=neps.Float(lower=0, upper=1))\n    neps.run(\n        evaluate_pipeline=evaluate_pipeline,\n        pipeline_space={\n            \"some_parameter\": (0.0, 1.0),   # float\n            \"another_parameter\": (0, 10),   # integer\n            \"optimizer\": [\"sgd\", \"adam\"],   # categorical\n            \"epoch\": neps.Integer(          # fidelity integer\n                lower=1,\n                upper=100,\n                is_fidelity=True\n            ),\n            \"learning_rate\": neps.Float(    # log spaced float\n                lower=1e-5,\n                uperr=1,\n                log=True\n            ),\n            \"alpha\": neps.Float(            # float with a prior\n                lower=0.1,\n                upper=1.0,\n                prior=0.99,\n                prior_confidence=\"high\",\n            )\n        },\n        root_directory=\"usage_example\",\n        max_evaluations_total=5,\n    )\n    ```\n\n    Args:\n        evaluate_pipeline: The objective function to minimize. This will be called\n            with a configuration from the `pipeline_space=` that you define.\n\n            The function should return one of the following:\n\n            * A `float`, which is the objective value to minimize.\n            * A `dict` which can have the following keys:\n\n                ```python\n                {\n                    \"objective_to_minimize\": float,  # The thing to minimize (required)\n                    \"cost\": float,  # The cost of the evaluate_pipeline, used by some algorithms\n                    \"info_dict\": dict,  # Any additional information you want to store, should be YAML serializable\n                }\n                ```\n\n            ??? note \"`str` usage for dynamic imports\"\n\n                If a string, it should be in the format `\"/path/to/:function\"`.\n                to specify the function to call. You may also directly provide\n                an mode to import, e.g., `\"my.module.something:evaluate_pipeline\"`.\n\n        pipeline_space: The search space to minimize over.\n\n            This most direct way to specify the search space is as follows:\n\n            ```python\n            neps.run(\n                pipeline_space={\n                    \"dataset\": \"mnist\",             # constant\n                    \"nlayers\": (2, 10),             # integer\n                    \"alpha\": (0.1, 1.0),            # float\n                    \"optimizer\": [                  # categorical\n                        \"adam\", \"sgd\", \"rmsprop\"\n                    ],\n                    \"learning_rate\": neps.Float(,   # log spaced float\n                        lower=1e-5, upper=1, log=True\n                    ),\n                    \"epochs\": neps.Integer(         # fidelity integer\n                        lower=1, upper=100, is_fidelity=True\n                    ),\n                    \"batch_size\": neps.Integer(     # integer with a prior\n                        lower=32, upper=512, prior=128\n                    ),\n\n                }\n            )\n            ```\n\n            You can also directly instantiate any of the parameters\n            defined by [`Parameter`][neps.space.parameters.Parameter]\n            and provide them directly.\n\n            Some important properties you can set on parameters are:\n\n            * `prior=`: If you have a good idea about what a good setting\n                for a parameter may be, you can set this as the prior for\n                a parameter. You can specify this along with `prior_confidence`\n                if you would like to assign a `\"low\"`, `\"medium\"`, or `\"high\"`\n                confidence to the prior.\n\n\n            !!! note \"Yaml support\"\n\n                To support spaces defined in yaml, you may also define the parameters\n                as dictionarys, e.g.,\n\n                ```python\n                neps.run(\n                    pipeline_space={\n                        \"dataset\": \"mnist\",\n                        \"nlayers\": {\"type\": \"int\", \"lower\": 2, \"upper\": 10},\n                        \"alpha\": {\"type\": \"float\", \"lower\": 0.1, \"upper\": 1.0},\n                        \"optimizer\": {\"type\": \"cat\", \"choices\": [\"adam\", \"sgd\", \"rmsprop\"]},\n                        \"learning_rate\": {\"type\": \"float\", \"lower\": 1e-5, \"upper\": 1, \"log\": True},\n                        \"epochs\": {\"type\": \"int\", \"lower\": 1, \"upper\": 100, \"is_fidelity\": True},\n                        \"batch_size\": {\"type\": \"int\", \"lower\": 32, \"upper\": 512, \"prior\": 128},\n                    }\n                )\n                ```\n\n            !!! note \"ConfigSpace support\"\n\n                You may also use a `ConfigurationSpace` object from the\n                `ConfigSpace` library.\n\n        root_directory: The directory to save progress to.\n\n        overwrite_working_directory: If true, delete the working directory at the start of\n            the run. This is, e.g., useful when debugging a evaluate_pipeline function.\n\n        post_run_summary: If True, creates a csv file after each worker is done,\n            holding summary information about the configs and results.\n\n        max_evaluations_per_run: Number of evaluations this specific call should do.\n\n        max_evaluations_total: Number of evaluations after which to terminate.\n            This is shared between all workers operating in the same `root_directory`.\n\n        continue_until_max_evaluation_completed:\n            If true, only stop after max_evaluations_total have been completed.\n            This is only relevant in the parallel setting.\n\n        max_cost_total: No new evaluations will start when this cost is exceeded. Requires\n            returning a cost in the evaluate_pipeline function, e.g.,\n            `return dict(loss=loss, cost=cost)`.\n        ignore_errors: Ignore hyperparameter settings that threw an error and do not raise\n            an error. Error configs still count towards max_evaluations_total.\n        objective_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error and will use given objective_to_minimize value instead. default: None\n        cost_value_on_error: Setting this and objective_value_on_error to any float will\n            supress any error and will use given cost value instead. default: None\n\n        sample_batch_size:\n            The number of samples to ask for in a single call to the optimizer.\n\n            ??? tip \"When to use this?\"\n\n                This is only useful in scenarios where you have many workers\n                available, and the optimizers sample time prevents full\n                worker utilization, as can happen with Bayesian optimizers.\n\n                In this case, the currently active worker will first\n                check if there are any new configurations to evaluate,\n                and if not, generate `sample_batch_size` new configurations\n                that the proceeding workers will then pick up and evaluate.\n\n                We advise to only use this if:\n\n                * You are using a `#!python \"ifbo\"` or `#!python \"bayesian_optimization\"`.\n                * You have a fast to evaluate `evaluate_pipeline`\n                * You have a significant amount of workers available, relative to the\n                time it takes to evaluate a single configuration.\n\n            ??? warning \"Downsides of batching\"\n\n                The primary downside of batched optimization is that\n                the next `sample_batch_size` configurations will not\n                be able to take into account the results of any new\n                evaluations, even if they were to come in relatively\n                quickly.\n\n        optimizer: Which optimizer to use.\n\n            Not sure which to use? Leave this at `\"auto\"` and neps will\n            choose the optimizer based on the search space given.\n\n            ??? note \"Available optimizers\"\n\n                ---\n\n                * `#!python \"bayesian_optimization\"`,\n\n                    ::: neps.optimizers.algorithms.bayesian_optimization\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"ifbo\"`\n\n                    ::: neps.optimizers.algorithms.ifbo\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"successive_halving\"`:\n\n                    ::: neps.optimizers.algorithms.successive_halving\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"hyperband\"`:\n\n                    ::: neps.optimizers.algorithms.hyperband\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"priorband\"`:\n\n                    ::: neps.optimizers.algorithms.priorband\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"asha\"`:\n\n                    ::: neps.optimizers.algorithms.asha\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"async_hb\"`:\n\n                    ::: neps.optimizers.algorithms.async_hb\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"random_search\"`:\n\n                    ::: neps.optimizers.algorithms.random_search\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n                * `#!python \"grid_search\"`:\n\n                    ::: neps.optimizers.algorithms.grid_search\n                        options:\n                            show_root_heading: false\n                            show_signature: false\n                            show_source: false\n\n                ---\n\n\n            With any optimizer choice, you also may provide some additional parameters to the optimizers.\n            We do not recommend this unless you are familiar with the optimizer you are using. You\n            may also specify an optimizer as a dictionary for supporting reading in serialized yaml\n            formats:\n\n            ```python\n            neps.run(\n                ...,\n                optimzier={\n                    \"name\": \"priorband\",\n                    \"sample_prior_first\": True,\n                }\n            )\n            ```\n\n            ??? tip \"Own optimzier\"\n\n                Lastly, you may also provide your own optimizer which must satisfy\n                the [`AskFunction`][neps.optimizers.optimizer.AskFunction] signature.\n\n                ```python\n                class MyOpt:\n\n                    def __init__(self, space: SearchSpace):\n                        ...\n\n                    def __call__(\n                        self,\n                        trials: Mapping[str, Trial],\n                        budget_info: BudgetInfo | None,\n                        n: int | None = None,\n                    ) -&gt; SampledConfig | list[SampledConfig]:\n                        # Sample a new configuration.\n                        #\n                        # Args:\n                        #   trials: All of the trials that are known about.\n                        #   budget_info: information about the budget constraints.\n                        #\n                        # Returns:\n                        #   The sampled configuration(s)\n\n\n                neps.run(\n                    ...,\n                    optimizer=MyOpt,\n                )\n                ```\n\n                This is mainly meant for internal development but allows you to use the NePS\n                runtime to run your optimizer.\n\n    \"\"\"  # noqa: E501\n    if (\n        max_evaluations_total is None\n        and max_evaluations_per_run is None\n        and max_cost_total is None\n    ):\n        warnings.warn(\n            \"None of the following were set, this will run idefinitely until the worker\"\n            \" process is stopped.\"\n            f\"\\n * {max_evaluations_total=}\"\n            f\"\\n * {max_evaluations_per_run=}\"\n            f\"\\n * {max_cost_total=}\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    logger.info(f\"Starting neps.run using root directory {root_directory}\")\n\n    if isinstance(pipeline_space, Pipeline):\n        assert not isinstance(evaluate_pipeline, str)\n        evaluate_pipeline = adjust_evaluation_pipeline_for_neps_space(\n            evaluate_pipeline, pipeline_space\n        )\n\n    space = convert_to_space(pipeline_space)\n    _optimizer_ask, _optimizer_info = load_optimizer(optimizer=optimizer, space=space)\n\n    _eval: Callable\n    if isinstance(evaluate_pipeline, str):\n        module, funcname = evaluate_pipeline.rsplit(\":\", 1)\n        eval_pipeline = dynamic_load_object(module, funcname)\n        if not callable(eval_pipeline):\n            raise ValueError(\n                f\"'{funcname}' in module '{module}' is not a callable function.\"\n            )\n        _eval = eval_pipeline\n    elif callable(evaluate_pipeline):\n        _eval = evaluate_pipeline\n    else:\n        raise ValueError(\n            \"evaluate_pipeline must be a callable or a string in the format\"\n            \"'module:function'.\"\n        )\n\n    _launch_runtime(\n        evaluation_fn=_eval,  # type: ignore\n        optimizer=_optimizer_ask,\n        optimizer_info=_optimizer_info,\n        max_cost_total=max_cost_total,\n        optimization_dir=Path(root_directory),\n        max_evaluations_total=max_evaluations_total,\n        max_evaluations_for_worker=max_evaluations_per_run,\n        continue_until_max_evaluation_completed=continue_until_max_evaluation_completed,\n        objective_value_on_error=objective_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        overwrite_optimization_dir=overwrite_working_directory,\n        sample_batch_size=sample_batch_size,\n    )\n\n    if post_run_summary:\n        full_frame_path, short_path = post_run_csv(root_directory)\n        logger.info(\n            \"The post run summary has been created, which is a csv file with the \"\n            \"output of all data in the run.\"\n            f\"\\nYou can find a full dataframe at: {full_frame_path}.\"\n            f\"\\nYou can find a quick summary at: {short_path}.\"\n        )\n    else:\n        logger.info(\n            \"Skipping the creation of the post run summary, which is a csv file with the \"\n            \" output of all data in the run.\"\n            \"\\nSet `post_run_summary=True` to enable it.\"\n        )\n</code></pre>"},{"location":"api/neps/env/","title":"Env","text":"<p>Environment variable parsing for the state.</p>"},{"location":"api/neps/env/#neps.env.get_env","title":"get_env","text":"<pre><code>get_env(\n    key: str, parse: Callable[[str], T], default: V\n) -&gt; T | V\n</code></pre> <p>Get an environment variable or return a default value.</p> Source code in <code>neps\\env.py</code> <pre><code>def get_env(key: str, parse: Callable[[str], T], default: V) -&gt; T | V:\n    \"\"\"Get an environment variable or return a default value.\"\"\"\n    if (e := os.environ.get(key)) is not None:\n        value = parse(e)\n        ENV_VARS_USED[key] = (e, value)\n        return value\n\n    ENV_VARS_USED[key] = (default, default)\n    return default\n</code></pre>"},{"location":"api/neps/env/#neps.env.is_nullable","title":"is_nullable","text":"<pre><code>is_nullable(e: str) -&gt; bool\n</code></pre> <p>Check if an environment variable is nullable.</p> Source code in <code>neps\\env.py</code> <pre><code>def is_nullable(e: str) -&gt; bool:\n    \"\"\"Check if an environment variable is nullable.\"\"\"\n    return e.lower() in (\"none\", \"n\", \"null\")\n</code></pre>"},{"location":"api/neps/env/#neps.env.yaml_or_json","title":"yaml_or_json","text":"<pre><code>yaml_or_json(e: str) -&gt; Literal['yaml', 'json']\n</code></pre> <p>Check if an environment variable is either yaml or json.</p> Source code in <code>neps\\env.py</code> <pre><code>def yaml_or_json(e: str) -&gt; Literal[\"yaml\", \"json\"]:\n    \"\"\"Check if an environment variable is either yaml or json.\"\"\"\n    if e.lower() in (\"yaml\", \"json\"):\n        return e.lower()  # type: ignore\n    raise ValueError(f\"Expected 'yaml' or 'json', got '{e}'.\")\n</code></pre>"},{"location":"api/neps/exceptions/","title":"Exceptions","text":"<p>Exceptions for NePS that don't belong in a specific module.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.LockFailedError","title":"LockFailedError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a lock cannot be acquired.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.NePSError","title":"NePSError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all NePS exceptions.</p> <p>This allows an easier way to catch all NePS exceptions if we inherit all exceptions from this class.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.TrialAlreadyExistsError","title":"TrialAlreadyExistsError","text":"<pre><code>TrialAlreadyExistsError(trial_id: str, *args: Any)\n</code></pre> <p>               Bases: <code>NePSError</code></p> <p>Raised when a trial already exists in the store.</p> Source code in <code>neps\\exceptions.py</code> <pre><code>def __init__(self, trial_id: str, *args: Any) -&gt; None:\n    \"\"\"Initialize the exception with the trial id.\"\"\"\n    super().__init__(trial_id, *args)\n    self.trial_id = trial_id\n</code></pre>"},{"location":"api/neps/exceptions/#neps.exceptions.TrialNotFoundError","title":"TrialNotFoundError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a trial already exists in the store.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.WorkerFailedToGetPendingTrialsError","title":"WorkerFailedToGetPendingTrialsError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a worker failed to get pending trials.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.WorkerRaiseError","title":"WorkerRaiseError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised from a worker when an error is raised.</p> <p>Includes additional information on how to recover</p>"},{"location":"api/neps/runtime/","title":"Runtime","text":"<p>TODO.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker","title":"DefaultWorker  <code>dataclass</code>","text":"<pre><code>DefaultWorker(\n    state: NePSState,\n    settings: WorkerSettings,\n    evaluation_fn: Callable[..., EvaluatePipelineReturn],\n    optimizer: AskFunction,\n    worker_id: str,\n    worker_cumulative_eval_count: int = 0,\n    worker_cumulative_eval_cost: float = 0.0,\n    worker_cumulative_evaluation_time_seconds: float = 0.0,\n    _GRACE: ClassVar = FS_SYNC_GRACE_BASE,\n)\n</code></pre> <p>A default worker for the NePS system.</p> <p>This is the worker that is used by default in the neps.run() loop.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.evaluation_fn","title":"evaluation_fn  <code>instance-attribute</code>","text":"<pre><code>evaluation_fn: Callable[..., EvaluatePipelineReturn]\n</code></pre> <p>The evaluation function to use for the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer: AskFunction\n</code></pre> <p>The optimizer that is in use by the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.settings","title":"settings  <code>instance-attribute</code>","text":"<pre><code>settings: WorkerSettings\n</code></pre> <p>The settings for the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.state","title":"state  <code>instance-attribute</code>","text":"<pre><code>state: NePSState\n</code></pre> <p>The state of the NePS system.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_cumulative_eval_cost","title":"worker_cumulative_eval_cost  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worker_cumulative_eval_cost: float = 0.0\n</code></pre> <p>The cost of the evaluations done by this worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_cumulative_eval_count","title":"worker_cumulative_eval_count  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worker_cumulative_eval_count: int = 0\n</code></pre> <p>The number of evaluations done by this worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_cumulative_evaluation_time_seconds","title":"worker_cumulative_evaluation_time_seconds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worker_cumulative_evaluation_time_seconds: float = 0.0\n</code></pre> <p>The time spent evaluating configurations by this worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_id","title":"worker_id  <code>instance-attribute</code>","text":"<pre><code>worker_id: str\n</code></pre> <p>The id of the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(\n    *,\n    state: NePSState,\n    optimizer: AskFunction,\n    settings: WorkerSettings,\n    evaluation_fn: Callable[..., EvaluatePipelineReturn],\n    worker_id: str | None = None\n) -&gt; DefaultWorker\n</code></pre> <p>Create a new worker.</p> Source code in <code>neps\\runtime.py</code> <pre><code>@classmethod\ndef new(\n    cls,\n    *,\n    state: NePSState,\n    optimizer: AskFunction,\n    settings: WorkerSettings,\n    evaluation_fn: Callable[..., EvaluatePipelineReturn],\n    worker_id: str | None = None,\n) -&gt; DefaultWorker:\n    \"\"\"Create a new worker.\"\"\"\n    return DefaultWorker(\n        state=state,\n        optimizer=optimizer,\n        settings=settings,\n        evaluation_fn=evaluation_fn,\n        worker_id=worker_id if worker_id is not None else _default_worker_name(),\n    )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the worker.</p> <p>Will keep running until one of the criterion defined by the <code>WorkerSettings</code> is met.</p> Source code in <code>neps\\runtime.py</code> <pre><code>def run(self) -&gt; None:  # noqa: C901, PLR0912, PLR0915\n    \"\"\"Run the worker.\n\n    Will keep running until one of the criterion defined by the `WorkerSettings`\n    is met.\n    \"\"\"\n    _set_workers_neps_state(self.state)\n\n    logger.info(\"Launching NePS\")\n\n    _time_monotonic_start = time.monotonic()\n    _error_from_evaluation: Exception | None = None\n\n    _repeated_fail_get_next_trial_count = 0\n    n_repeated_failed_check_should_stop = 0\n    while True:\n        try:\n            # First check local worker settings\n            should_stop = self._check_worker_local_settings(\n                time_monotonic_start=_time_monotonic_start,\n                error_from_this_worker=_error_from_evaluation,\n            )\n            if should_stop is not False:\n                logger.info(should_stop)\n                break\n\n            # Next check global errs having occured\n            should_stop = self._check_shared_error_stopping_criterion()\n            if should_stop is not False:\n                logger.info(should_stop)\n                break\n\n        except WorkerRaiseError as e:\n            # If we raise a specific error, we should stop the worker\n            raise e\n        except Exception as e:\n            # An unknown exception, check our retry countk\n            n_repeated_failed_check_should_stop += 1\n            if (\n                n_repeated_failed_check_should_stop\n                &gt;= MAX_RETRIES_WORKER_CHECK_SHOULD_STOP\n            ):\n                raise WorkerRaiseError(\n                    f\"Worker {self.worker_id} failed to check if it should stop\"\n                    f\" {MAX_RETRIES_WORKER_CHECK_SHOULD_STOP} times in a row. Bailing\"\n                ) from e\n\n            logger.error(\n                \"Unexpected error from worker '%s' while checking if it should stop.\",\n                self.worker_id,\n                exc_info=True,\n            )\n            time.sleep(1)  # Help stagger retries\n            continue\n\n        # From here, we now begin sampling or getting the next pending trial.\n        # As the global stopping criterion requires us to check all trials, and\n        # needs to be in locked in-step with sampling and is done inside\n        # _get_next_trial\n        try:\n            trial_to_eval = self._get_next_trial()\n            if trial_to_eval == \"break\":\n                break\n            _repeated_fail_get_next_trial_count = 0\n        except Exception as e:\n            _repeated_fail_get_next_trial_count += 1\n            if isinstance(e, portalocker.exceptions.LockException):\n                logger.debug(\n                    \"Worker '%s': Timeout while trying to get the next trial to\"\n                    \" evaluate. If you are using a model based optimizer, such as\"\n                    \" Bayesian Optimization, this can occur as the number of\"\n                    \" configurations get large. There's not much to do here\"\n                    \" and we will retry to obtain the lock.\",\n                    self.worker_id,\n                    exc_info=True,\n                )\n            else:\n                logger.debug(\n                    \"Worker '%s': Error while trying to get the next trial to\"\n                    \" evaluate.\",\n                    self.worker_id,\n                    exc_info=True,\n                )\n                time.sleep(1)  # Help stagger retries\n            # NOTE: This is to prevent any infinite loops if we can't get a trial\n            if _repeated_fail_get_next_trial_count &gt;= MAX_RETRIES_GET_NEXT_TRIAL:\n                raise WorkerFailedToGetPendingTrialsError(\n                    f\"Worker {self.worker_id} failed to get pending trials\"\n                    f\" {MAX_RETRIES_GET_NEXT_TRIAL} times in\"\n                    \" a row. Bailing!\"\n                ) from e\n\n            continue\n\n        # We (this worker) has managed to set it to evaluating, now we can evaluate it\n        with _set_global_trial(trial_to_eval):\n            evaluated_trial, report = evaluate_trial(\n                trial=trial_to_eval,\n                evaluation_fn=self.evaluation_fn,\n                default_report_values=self.settings.default_report_values,\n            )\n            evaluation_duration = evaluated_trial.metadata.evaluation_duration\n            assert evaluation_duration is not None\n            self.worker_cumulative_evaluation_time_seconds += evaluation_duration\n\n        self.worker_cumulative_eval_count += 1\n\n        logger.info(\n            \"Worker '%s' evaluated trial: %s as %s.\",\n            self.worker_id,\n            evaluated_trial.id,\n            evaluated_trial.metadata.state,\n        )\n\n        if report.cost is not None:\n            self.worker_cumulative_eval_cost += report.cost\n\n        if report.err is not None:\n            logger.error(\n                f\"Error during evaluation of '{evaluated_trial.id}'\"\n                f\" : {evaluated_trial.config}.\"\n            )\n            logger.exception(report.err)\n            _error_from_evaluation = report.err\n\n        # We do not retry this, as if some other worker has\n        # managed to manipulate this trial in the meantime,\n        # then something has gone wrong\n        with self.state._trial_lock.lock(worker_id=self.worker_id):\n            self.state._report_trial_evaluation(\n                trial=evaluated_trial,\n                report=report,\n                worker_id=self.worker_id,\n            )\n            # This is mostly for `tblogger`\n            for _key, callback in _TRIAL_END_CALLBACKS.items():\n                callback(trial_to_eval)\n\n        logger.debug(\"Config %s: %s\", evaluated_trial.id, evaluated_trial.config)\n        logger.debug(\"Loss %s: %s\", evaluated_trial.id, report.objective_to_minimize)\n        logger.debug(\"Cost %s: %s\", evaluated_trial.id, report.objective_to_minimize)\n        logger.debug(\n            \"Learning Curve %s: %s\", evaluated_trial.id, report.learning_curve\n        )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.get_in_progress_trial","title":"get_in_progress_trial","text":"<pre><code>get_in_progress_trial() -&gt; Trial\n</code></pre> <p>Get the currently running trial in this process.</p> Source code in <code>neps\\runtime.py</code> <pre><code>def get_in_progress_trial() -&gt; Trial:\n    \"\"\"Get the currently running trial in this process.\"\"\"\n    if _CURRENTLY_RUNNING_TRIAL_IN_PROCESS is None:\n        raise RuntimeError(\n            \"The worker's NePS state has not been set! This should only be called\"\n            \" from within a `evaluate_pipeline` context. If you are not running a\"\n            \" pipeline and you did not call this function (`get_workers_neps_state`)\"\n            \" yourself, this is a bug and should be reported to NePS.\"\n        )\n    return _CURRENTLY_RUNNING_TRIAL_IN_PROCESS\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.get_workers_neps_state","title":"get_workers_neps_state","text":"<pre><code>get_workers_neps_state() -&gt; NePSState\n</code></pre> <p>Get the worker's NePS state.</p> Source code in <code>neps\\runtime.py</code> <pre><code>def get_workers_neps_state() -&gt; NePSState:\n    \"\"\"Get the worker's NePS state.\"\"\"\n    if _WORKER_NEPS_STATE is None:\n        raise RuntimeError(\n            \"The worker's NePS state has not been set! This should only be called\"\n            \" from within a `evaluate_pipeline` context. If you are not running a\"\n            \" pipeline and you did not call this function (`get_workers_neps_state`)\"\n            \" yourself, this is a bug and should be reported to NePS.\"\n        )\n    return _WORKER_NEPS_STATE\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.register_notify_trial_end","title":"register_notify_trial_end","text":"<pre><code>register_notify_trial_end(\n    key: str, callback: Callable[[Trial], None]\n) -&gt; None\n</code></pre> <p>Register a callback to be called when a trial ends.</p> Source code in <code>neps\\runtime.py</code> <pre><code>def register_notify_trial_end(key: str, callback: Callable[[Trial], None]) -&gt; None:\n    \"\"\"Register a callback to be called when a trial ends.\"\"\"\n    _TRIAL_END_CALLBACKS[key] = callback\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/","title":"Algorithms","text":"<p>The selection of optimization algorithms available in NePS.</p> <p>This module conveniently starts with 'a' to be at the top and is where most of the code documentation for optimizers can be found.</p> <p>Below you will find some functions with some sane defaults documenting the parameters available. You can pass these functoins to <code>neps.run()</code> if you like, otherwise you may also refer to them by their string name.</p>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.CustomOptimizer","title":"CustomOptimizer  <code>dataclass</code>","text":"<pre><code>CustomOptimizer(\n    name: str,\n    optimizer: (\n        Callable[Concatenate[SearchSpace, ...], AskFunction]\n        | AskFunction\n    ),\n    kwargs: Mapping[str, Any] = dict(),\n    initialized: bool = False,\n)\n</code></pre> <p>Custom optimizer that allows you to define your own optimizer function.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The optimizer function to use.</p> <p> TYPE: <code>Callable[Concatenate[SearchSpace, ...], AskFunction] | AskFunction</code> </p>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.asha","title":"asha","text":"<pre><code>asha(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: (\n        bool | Literal[\"highest_fidelity\"]\n    ) = False\n) -&gt; BracketOptimizer\n</code></pre> <p>A bandit-based optimization algorithm that uses a fidelity parameter, the asynchronous version of <code>successive_halving</code>. one that scales better to many parallel workers.</p> <p>When to use this?</p> <p>Use this when you think lower fidelity evaluations of your configurations carries a strong signal about their ranking at higher fidelities, and you have many workers available to evaluate configurations in parallel.</p> <p>It does this by maintaining one big bracket, i.e. one big on-going competition, with a promotion rule based on the sizes of each rung.</p> <pre><code># ASHA maintains one big bracket with an exponentially decreasing amount of\n# configurations promoted, relative to those in the rung below.\n\n|        | fidelity    | c1 | c2 | c3 | c4 | c5 | ...\n| Rung 0 | (3 epochs)  |  o |  o |  o |  o |  o | ...\n| Rung 1 | (9 epochs)  |  o |    |  o |  o |    | ...\n| Rung 2 | (27 epochs) |  o |    |    |  o |    | ...\n</code></pre> <p>For more information, see the <code>successive_halving</code> documentation, as this algorithm could be considered an extension of it.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first, and if so, should it be at the highest fidelity.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def asha(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: bool | Literal[\"highest_fidelity\"] = False,\n) -&gt; BracketOptimizer:\n    \"\"\"A bandit-based optimization algorithm that uses a _fidelity_ parameter,\n    the _asynchronous_ version of\n    [`successive_halving`][neps.optimizers.algorithms.successive_halving].\n    one that scales better to many parallel workers.\n\n    !!! tip \"When to use this?\"\n\n        Use this when you think lower fidelity evaluations of your configurations carries\n        a strong signal about their ranking at higher fidelities, and you have many\n        workers available to evaluate configurations in parallel.\n\n    It does this by maintaining one big bracket, i.e. one\n    big on-going competition, with a promotion rule based on the sizes of each rung.\n\n    ```\n    # ASHA maintains one big bracket with an exponentially decreasing amount of\n    # configurations promoted, relative to those in the rung below.\n\n    |        | fidelity    | c1 | c2 | c3 | c4 | c5 | ...\n    | Rung 0 | (3 epochs)  |  o |  o |  o |  o |  o | ...\n    | Rung 1 | (9 epochs)  |  o |    |  o |  o |    | ...\n    | Rung 2 | (27 epochs) |  o |    |    |  o |    | ...\n    ```\n\n    For more information, see the\n    [`successive_halving`][neps.optimizers.algorithms.successive_halving] documentation,\n    as this algorithm could be considered an extension of it.\n\n    Args:\n        space: The search space to sample from.\n        eta: The reduction factor used for building brackets\n        sampler: The type of sampling procedure to use:\n\n            * If `#!python \"uniform\"`, samples uniformly from the space when\n                it needs to sample.\n            * If `#!python \"prior\"`, samples from the prior\n                distribution built from the `prior` and `prior_confidence`\n                values in the search space.\n\n        sample_prior_first: Whether to sample the prior configuration first,\n            and if so, should it be at the highest fidelity.\n    \"\"\"\n\n    return _bracket_optimizer(\n        pipeline_space=space,\n        bracket_type=\"asha\",\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        sampler=sampler,\n        sample_prior_first=sample_prior_first,\n        # TODO: Implement this\n        bayesian_optimization_kick_in_point=None,\n        device=None,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.async_hb","title":"async_hb","text":"<pre><code>async_hb(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: bool = False\n) -&gt; BracketOptimizer\n</code></pre> <p>An asynchronous version of <code>hyperband</code>, where the brackets are run asynchronously, and the promotion rule is based on the number of evaluations each configuration has had.</p> <p>When to use this?</p> <p>Use this when you think lower fidelity evaluations of your configurations carries some signal about their ranking at higher fidelities, but not confidently, and you have many workers available to evaluate configurations in parallel.</p> <pre><code># Async HB runs different \"asha\" brackets, which are unbounded in the number\n# of configurations that can be in each. The bracket chosen at each iteration\n# is a sampling function based on the resources invested in each bracket.\n\n| Bracket 1 |         | Bracket 2 |        | Bracket 3 |\n| Rung 0    | ...     | (skipped) |        | (skipped) |\n| Rung 1    | ...     | Rung 1    | ...    | (skipped) |\n| Rung 2    | ...     | Rung 2    | ...    | Rung 2    | ...\n</code></pre> <p>For more information, see the <code>hyperband</code> documentation, <code>successive_halving</code> documentation, and the <code>asha</code> documentation, as this algorithm takes elements from each.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def async_hb(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: bool = False,\n) -&gt; BracketOptimizer:\n    \"\"\"An _asynchronous_ version of [`hyperband`][neps.optimizers.algorithms.hyperband],\n    where the brackets are run asynchronously, and the promotion rule is based on the\n    number of evaluations each configuration has had.\n\n    !!! tip \"When to use this?\"\n\n        Use this when you think lower fidelity evaluations of your configurations carries\n        some signal about their ranking at higher fidelities, but not confidently, and\n        you have many workers available to evaluate configurations in parallel.\n\n    ```\n    # Async HB runs different \"asha\" brackets, which are unbounded in the number\n    # of configurations that can be in each. The bracket chosen at each iteration\n    # is a sampling function based on the resources invested in each bracket.\n\n    | Bracket 1 |         | Bracket 2 |        | Bracket 3 |\n    | Rung 0    | ...     | (skipped) |        | (skipped) |\n    | Rung 1    | ...     | Rung 1    | ...    | (skipped) |\n    | Rung 2    | ...     | Rung 2    | ...    | Rung 2    | ...\n    ```\n\n    For more information, see the\n    [`hyperband`][neps.optimizers.algorithms.hyperband] documentation,\n    [`successive_halving`][neps.optimizers.algorithms.successive_halving] documentation,\n    and the [`asha`][neps.optimizers.algorithms.asha] documentation, as this algorithm\n    takes elements from each.\n\n    Args:\n        space: The search space to sample from.\n        eta: The reduction factor used for building brackets\n        sampler: The type of sampling procedure to use:\n\n            * If `#!python \"uniform\"`, samples uniformly from the space when\n                it needs to sample.\n            * If `#!python \"prior\"`, samples from the prior\n                distribution built from the `prior` and `prior_confidence`\n                values in the search space.\n\n        sample_prior_first: Whether to sample the prior configuration first.\n    \"\"\"\n    return _bracket_optimizer(\n        pipeline_space=space,\n        bracket_type=\"async_hb\",\n        eta=eta,\n        sampler=sampler,\n        sample_prior_first=sample_prior_first,\n        early_stopping_rate=None,\n        # TODO: Implement this\n        bayesian_optimization_kick_in_point=None,\n        device=None,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.bayesian_optimization","title":"bayesian_optimization","text":"<pre><code>bayesian_optimization(\n    space: SearchSpace,\n    *,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    cost_aware: bool | Literal[\"log\"] = False,\n    ignore_fidelity: bool = False,\n    device: device | str | None = None,\n    reference_point: tuple[float, ...] | None = None\n) -&gt; BayesianOptimization\n</code></pre> <p>Models the relation between hyperparameters in your <code>pipeline_space</code> and the results of <code>evaluate_pipeline</code> using bayesian optimization. This acts as a cheap surrogate model of you <code>evaluate_pipeline</code> function that can be used for optimization.</p> <p>When to use this?</p> <p>Bayesion optimization is a good general purpose choice, especially if the size of your search space is not too large. It is also the best option to use if you do not have or want to use a fidelity parameter.</p> <p>Note that acquiring the next configuration to evaluate with bayesian optimization can become prohibitvely expensive as the number of configurations evaluated increases.</p> <p>If there is some numeric cost associated with evaluating a configuration, you can provide this as a <code>cost</code> when returning the results from your <code>evaluate_pipeline</code> function. By specifying <code>cost_aware=True</code>, the optimizer will attempt to balance getting the best result while minimizing the cost.</p> <p>If you have priors, we recommend looking at <code>pibo</code>.</p> <p>For Multi-objective optimization (i.e., no. of objectives in trials &gt; 1), the algorithm automatically switches to the qLogNoisyExpectedHypervolumeImprovement acquisition function.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>initial_design_size</code> <p>Number of samples used before using the surrogate model. If \"ndim\", it will use the number of parameters in the search space.</p> <p> TYPE: <code>int | Literal['ndim']</code> DEFAULT: <code>'ndim'</code> </p> <code>cost_aware</code> <p>Whether to consider reported \"cost\" from configurations in decision making. If True, the optimizer will weigh potential candidates by how much they cost, incentivising the optimizer to explore cheap, good performing configurations. This amount is modified over time. If \"log\", the cost will be log-transformed before being used.</p> <p>Warning</p> <p>If using <code>cost</code>, cost must be provided in the reports of the trials.</p> <p> TYPE: <code>bool | Literal['log']</code> DEFAULT: <code>False</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>device</code> <p>Device to use for the optimization.</p> <p> TYPE: <code>device | str | None</code> DEFAULT: <code>None</code> </p> <code>reference_point</code> <p>The reference point to use got multi-objective bayesian optimization. If <code>None</code>, the reference point will be calculated automatically.</p> <p> TYPE: <code>tuple[float, ...] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def bayesian_optimization(\n    space: SearchSpace,\n    *,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    cost_aware: bool | Literal[\"log\"] = False,\n    ignore_fidelity: bool = False,\n    device: torch.device | str | None = None,\n    reference_point: tuple[float, ...] | None = None,\n) -&gt; BayesianOptimization:\n    \"\"\"Models the relation between hyperparameters in your `pipeline_space`\n    and the results of `evaluate_pipeline` using bayesian optimization.\n    This acts as a cheap _surrogate model_ of you `evaluate_pipeline` function\n    that can be used for optimization.\n\n    !!! tip \"When to use this?\"\n\n        Bayesion optimization is a good general purpose choice, especially\n        if the size of your search space is not too large. It is also the best\n        option to use if you do not have or want to use a _fidelity_ parameter.\n\n        Note that acquiring the next configuration to evaluate with bayesian\n        optimization can become prohibitvely expensive as the number of\n        configurations evaluated increases.\n\n    If there is some numeric cost associated with evaluating a configuration,\n    you can provide this as a `cost` when returning the results from your\n    `evaluate_pipeline` function. By specifying `#!python cost_aware=True`,\n    the optimizer will attempt to balance getting the best result while\n    minimizing the cost.\n\n    If you have _priors_, we recommend looking at\n    [`pibo`][neps.optimizers.algorithms.pibo].\n\n    For Multi-objective optimization (i.e., no. of objectives in trials &gt; 1),\n    the algorithm automatically switches to the qLogNoisyExpectedHypervolumeImprovement\n    acquisition function.\n\n    Args:\n        space: The search space to sample from.\n        initial_design_size: Number of samples used before using the surrogate model.\n            If \"ndim\", it will use the number of parameters in the search space.\n        cost_aware: Whether to consider reported \"cost\" from configurations in decision\n            making. If True, the optimizer will weigh potential candidates by how much\n            they cost, incentivising the optimizer to explore cheap, good performing\n            configurations. This amount is modified over time. If \"log\", the cost\n            will be log-transformed before being used.\n\n            !!! warning\n\n                If using `cost`, cost must be provided in the reports of the trials.\n\n        ignore_fidelity: Whether to ignore the fidelity parameter when sampling.\n            In this case, the max fidelity is always used.\n        device: Device to use for the optimization.\n\n        reference_point: The reference point to use got multi-objective bayesian\n            optimization. If `None`, the reference point will be calculated\n            automatically.\n    \"\"\"\n\n    if not ignore_fidelity and space.fidelity is not None:\n        raise ValueError(\n            \"Fidelities are not supported for BayesianOptimization. Consider setting the\"\n            \" fidelity to a constant value or ignoring it using ignore_fidelity to\"\n            f\" always sample at max fidelity. Got fidelity: {space.fidelities} \"\n        )\n    if ignore_fidelity and space.fidelity is None:\n        logger.warning(\n            \"Warning: You are using ignore_fidelity, but no fidelity is defined in the\"\n            \" search space. Consider setting ignore_fidelity to False.\"\n        )\n\n    if any(parameter.prior is not None for parameter in space.searchables.values()):\n        priors = [\n            parameter\n            for parameter in space.searchables.values()\n            if parameter.prior is not None\n        ]\n        raise ValueError(\n            \"Bayesian optimization does not support priors. Consider using pibo instead.\"\n            f\" Got priors: {priors}\"\n        )\n\n    return _bo(\n        pipeline_space=space,\n        initial_design_size=initial_design_size,\n        cost_aware=cost_aware,\n        device=device,\n        use_priors=False,\n        sample_prior_first=False,\n        ignore_fidelity=ignore_fidelity,\n        reference_point=reference_point,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.custom","title":"custom","text":"<pre><code>custom(\n    name: str,\n    optimizer: (\n        Callable[Concatenate[SearchSpace, ...], AskFunction]\n        | AskFunction\n    ),\n    *,\n    initialized: bool = False,\n    kwargs: Mapping[str, Any] | None = None\n) -&gt; CustomOptimizer\n</code></pre> <p>Create a custom optimizer that allows you to define your own optimizer function.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the optimizer.</p> <p> TYPE: <code>str</code> </p> <code>optimizer</code> <p>The optimizer function to use.</p> <p> TYPE: <code>Callable[Concatenate[SearchSpace, ...], AskFunction] | AskFunction</code> </p> <code>initialized</code> <p>Whether the optimizer has already been initialized.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional arguments to pass to the optimizer function.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def custom(\n    name: str,\n    optimizer: Callable[Concatenate[SearchSpace, ...], AskFunction] | AskFunction,\n    *,\n    initialized: bool = False,\n    kwargs: Mapping[str, Any] | None = None,\n) -&gt; CustomOptimizer:\n    \"\"\"Create a custom optimizer that allows you to define your own optimizer function.\n\n    Args:\n        name: The name of the optimizer.\n        optimizer: The optimizer function to use.\n        initialized: Whether the optimizer has already been initialized.\n        **kwargs: Additional arguments to pass to the optimizer function.\n    \"\"\"\n    return CustomOptimizer(\n        name=name,\n        optimizer=optimizer,\n        kwargs=kwargs or {},\n        initialized=initialized,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.grid_search","title":"grid_search","text":"<pre><code>grid_search(\n    pipeline_space: SearchSpace,\n    *,\n    ignore_fidelity: bool = False\n) -&gt; GridSearch\n</code></pre> <p>A simple grid search algorithm which discretizes the search space and evaluates all possible configurations.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore fidelity when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def grid_search(\n    pipeline_space: SearchSpace,\n    *,\n    ignore_fidelity: bool = False,\n) -&gt; GridSearch:\n    \"\"\"A simple grid search algorithm which discretizes the search\n    space and evaluates all possible configurations.\n\n    Args:\n        pipeline_space: The search space to sample from.\n        ignore_fidelity: Whether to ignore fidelity when sampling.\n            In this case, the max fidelity is always used.\n    \"\"\"\n    from neps.optimizers.utils.grid import make_grid\n\n    if any(\n        parameter.prior is not None for parameter in pipeline_space.searchables.values()\n    ):\n        raise ValueError(\"Grid search does not support priors.\")\n    if ignore_fidelity and pipeline_space.fidelity is None:\n        logger.warning(\n            \"Warning: You are using ignore_fidelity, but no fidelity is defined in the\"\n            \" search space. Consider setting ignore_fidelity to False.\"\n        )\n\n    return GridSearch(\n        configs_list=make_grid(pipeline_space, ignore_fidelity=ignore_fidelity)\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.hyperband","title":"hyperband","text":"<pre><code>hyperband(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: (\n        bool | Literal[\"highest_fidelity\"]\n    ) = False\n) -&gt; BracketOptimizer\n</code></pre> <p>Another bandit-based optimization algorithm that uses a fidelity parameter, very similar to <code>successive_halving</code>, but hedges a bit more on the safe side, just incase your fidelity parameters isn't as well correlated as you'd like.</p> <p>When to use this?</p> <p>Use this when you think lower fidelity evaluations of your configurations carries some signal about their ranking at higher fidelities, but not enough to be certain</p> <p>Hyperband is like Successive Halving but it instead of always having the same bracket layout, it runs different brackets with different rungs.</p> <p>This helps hedge against scenarios where rankings at the lowest fidelity do not correlate well with the upper fidelity.</p> <pre><code># Hyperband runs different successive halving brackets\n\n| Bracket 1 |         | Bracket 2 |        | Bracket 3 |\n| Rung 0    | ... |   | (skipped) |        | (skipped) |\n| Rung 1    | ... |   | Rung 1    | ... |  | (skipped) |\n| Rung 2    | ... |   | Rung 2    | ... |  | Rung 2    | ... |\n</code></pre> <p>For more information, see the <code>successive_halving</code> documentation, as this algorithm could be considered an extension of it.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first, and if so, should it be at the highest fidelity level.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def hyperband(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: bool | Literal[\"highest_fidelity\"] = False,\n) -&gt; BracketOptimizer:\n    \"\"\"Another bandit-based optimization algorithm that uses a _fidelity_ parameter,\n    very similar to [`successive_halving`][neps.optimizers.algorithms.successive_halving],\n    but hedges a bit more on the safe side, just incase your _fidelity_ parameters\n    isn't as well correlated as you'd like.\n\n    !!! tip \"When to use this?\"\n\n        Use this when you think lower fidelity evaluations of your configurations carries\n        some signal about their ranking at higher fidelities, but not enough to be certain\n\n    Hyperband is like Successive Halving but it instead of always having the same bracket\n    layout, it runs different brackets with different rungs.\n\n    This helps hedge against scenarios where rankings at the lowest fidelity do\n    not correlate well with the upper fidelity.\n\n\n    ```\n    # Hyperband runs different successive halving brackets\n\n    | Bracket 1 |         | Bracket 2 |        | Bracket 3 |\n    | Rung 0    | ... |   | (skipped) |        | (skipped) |\n    | Rung 1    | ... |   | Rung 1    | ... |  | (skipped) |\n    | Rung 2    | ... |   | Rung 2    | ... |  | Rung 2    | ... |\n    ```\n\n    For more information, see the\n    [`successive_halving`][neps.optimizers.algorithms.successive_halving] documentation,\n    as this algorithm could be considered an extension of it.\n\n    Args:\n        space: The search space to sample from.\n        eta: The reduction factor used for building brackets\n        sampler: The type of sampling procedure to use:\n\n            * If `#!python \"uniform\"`, samples uniformly from the space when\n                it needs to sample.\n            * If `#!python \"prior\"`, samples from the prior\n                distribution built from the `prior` and `prior_confidence`\n                values in the search space.\n\n        sample_prior_first: Whether to sample the prior configuration first,\n            and if so, should it be at the highest fidelity level.\n    \"\"\"\n    return _bracket_optimizer(\n        pipeline_space=space,\n        bracket_type=\"hyperband\",\n        eta=eta,\n        sampler=sampler,\n        sample_prior_first=sample_prior_first,\n        early_stopping_rate=None,\n        # TODO: Implement this\n        bayesian_optimization_kick_in_point=None,\n        device=None,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.ifbo","title":"ifbo","text":"<pre><code>ifbo(\n    pipeline_space: SearchSpace,\n    *,\n    step_size: int | float = 1,\n    use_priors: bool = False,\n    sample_prior_first: bool = False,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    device: device | str | None = None,\n    surrogate_path: str | Path | None = None,\n    surrogate_version: str = \"0.0.1\"\n) -&gt; IFBO\n</code></pre> <p>A transformer that has been trained to predict loss curves of deep-learing models, used to guide the optimization procedure and select configurations which are most promising to evaluate.</p> <p>When to use this?</p> <p>Use this when you think that early signal in your loss curve could be used to distinguish which configurations are likely to achieve a good performance.</p> <p>This algorithm will take many small steps in evaluating your configuration so we also advise that saving and loading your model checkpoint should be relatively fast.</p> <p>This algorithm requires a fidelity parameter, such as <code>epochs</code>, to be present. Each time we evaluate a configuration, we will only evaluate it for a single epoch, before returning back to the ifbo algorithm to select the next configuration.</p> Fidelities? <p>A fidelity parameter lets you control how many resources to invest in a single evaluation. For example, a common one for deep-learing is <code>epochs</code>. We can evaluate a model for just a single epoch, (fidelity step) to gain more information about the model's performance and decide what to do next.</p> <ul> <li>Paper: openreview.net/forum?id=VyoY3Wh9Wd</li> <li>Github: github.com/automl/ifBO/tree/main</li> </ul> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>step_size</code> <p>The size of the step to take in the fidelity domain.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>1</code> </p> <code>sample_prior_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>initial_design_size</code> <p>Number of configs to sample before starting optimization</p> <p>If <code>None</code>, the number of configs will be equal to the number of dimensions.</p> <p> TYPE: <code>int | Literal['ndim']</code> DEFAULT: <code>'ndim'</code> </p> <code>device</code> <p>Device to use for the model</p> <p> TYPE: <code>device | str | None</code> DEFAULT: <code>None</code> </p> <code>surrogate_path</code> <p>Path to the surrogate model to use</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>surrogate_version</code> <p>Version of the surrogate model to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'0.0.1'</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def ifbo(\n    pipeline_space: SearchSpace,\n    *,\n    step_size: int | float = 1,\n    use_priors: bool = False,\n    sample_prior_first: bool = False,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    device: torch.device | str | None = None,\n    surrogate_path: str | Path | None = None,\n    surrogate_version: str = \"0.0.1\",\n) -&gt; IFBO:\n    \"\"\"A transformer that has been trained to predict loss curves of deep-learing\n    models, used to guide the optimization procedure and select configurations which\n    are most promising to evaluate.\n\n    !!! tip \"When to use this?\"\n\n        Use this when you think that early signal in your loss curve could be used\n        to distinguish which configurations are likely to achieve a good performance.\n\n        This algorithm will take many small steps in evaluating your configuration\n        so we also advise that saving and loading your model checkpoint should\n        be relatively fast.\n\n    This algorithm requires a _fidelity_ parameter, such as `epochs`, to be present.\n    Each time we evaluate a configuration, we will only evaluate it for a single\n    epoch, before returning back to the ifbo algorithm to select the next configuration.\n\n    ??? tip \"Fidelities?\"\n\n        A fidelity parameter lets you control how many resources to invest in\n        a single evaluation. For example, a common one for deep-learing is\n        `epochs`. We can evaluate a model for just a single epoch, (fidelity step)\n        to gain more information about the model's performance and decide what\n        to do next.\n\n    * **Paper**: https://openreview.net/forum?id=VyoY3Wh9Wd\n    * **Github**: https://github.com/automl/ifBO/tree/main\n\n    Args:\n        pipeline_space: Space in which to search\n        step_size: The size of the step to take in the fidelity domain.\n        sample_prior_first: Whether to sample the default configuration first\n        initial_design_size: Number of configs to sample before starting optimization\n\n            If `None`, the number of configs will be equal to the number of dimensions.\n\n        device: Device to use for the model\n        surrogate_path: Path to the surrogate model to use\n        surrogate_version: Version of the surrogate model to use\n    \"\"\"\n    from neps.optimizers.ifbo import _adjust_space_to_match_stepsize\n\n    if pipeline_space.fidelity is None:\n        raise ValueError(\"Fidelity is required for IFBO.\")\n\n    # TODO: I'm not sure how this might effect tables, whose lowest fidelity\n    # might be below to possibly increased lower bound.\n    space, fid_bins = _adjust_space_to_match_stepsize(pipeline_space, step_size)\n    parameters = space.searchables\n\n    if use_priors and not any(\n        parameter.prior is not None for parameter in parameters.values()\n    ):\n        logger.warning(\n            \"Warning: You are using priors, but no priors are defined in the search\"\n            \" space. Consider setting use_priors to False.\"\n        )\n\n    if not use_priors and any(\n        parameter.prior is not None for parameter in parameters.values()\n    ):\n        priors = [\n            parameter for parameter in parameters.values() if parameter.prior is not None\n        ]\n        raise ValueError(\n            f\"To use priors, you must set use_priors=True. Got priors: {priors}\"\n        )\n\n    match initial_design_size:\n        case \"ndim\":\n            _initial_design_size = len(parameters)\n        case _:\n            _initial_design_size = initial_design_size\n\n    match device:\n        case str():\n            device = torch.device(device)\n        case None:\n            device = torch.get_default_device()\n        case torch.device():\n            pass\n        case _:\n            raise ValueError(\"device should be a string, torch.device or None\")\n\n    return IFBO(\n        space=pipeline_space,\n        n_fidelity_bins=fid_bins,\n        device=device,\n        sample_prior_first=sample_prior_first,\n        n_initial_design=_initial_design_size,\n        prior=Prior.from_parameters(parameters) if use_priors else None,\n        ftpfn=FTPFNSurrogate(\n            target_path=Path(surrogate_path) if surrogate_path is not None else None,\n            version=surrogate_version,\n            device=device,\n        ),\n        encoder=ConfigEncoder.from_parameters(\n            parameters,\n            # FTPFN doesn't support categoricals and we were recomended\n            # to just evenly distribute in the unit norm\n            custom_transformers={\n                cat_name: CategoricalToUnitNorm(choices=cat.choices)\n                for cat_name, cat in space.categoricals.items()\n            },\n        ),\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.mo_hyperband","title":"mo_hyperband","text":"<pre><code>mo_hyperband(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: (\n        bool | Literal[\"highest_fidelity\"]\n    ) = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\"\n) -&gt; BracketOptimizer\n</code></pre> <p>Multi-objective version of hyperband using the same candidate selection method as MOASHA.</p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def mo_hyperband(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    sample_prior_first: bool | Literal[\"highest_fidelity\"] = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n) -&gt; BracketOptimizer:\n    \"\"\"Multi-objective version of hyperband using the same\n    candidate selection method as MOASHA.\n    \"\"\"\n    return _bracket_optimizer(\n        pipeline_space=space,\n        bracket_type=\"hyperband\",\n        eta=eta,\n        sampler=sampler,\n        sample_prior_first=sample_prior_first,\n        early_stopping_rate=None,\n        # TODO: Implement this\n        bayesian_optimization_kick_in_point=None,\n        device=None,\n        multi_objective=True,\n        mo_selector=mo_selector,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.pibo","title":"pibo","text":"<pre><code>pibo(\n    space: SearchSpace,\n    *,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    cost_aware: bool | Literal[\"log\"] = False,\n    device: device | str | None = None,\n    sample_prior_first: bool = False,\n    ignore_fidelity: bool = False\n) -&gt; BayesianOptimization\n</code></pre> <p>A modification of <code>bayesian_optimization</code> that also incorporates the use of priors in the search space.</p> <p>When to use this?</p> <p>Use this if you'd like to use bayesian optimization while also having a good idea of what good parameters look like and can specify them through the <code>prior</code> and <code>prior_confidence</code> parameters in the search space.</p> <p>Note that this incurs the same tradeoffs that bayesian optimization has.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>initial_design_size</code> <p>Number of samples used before using the surrogate model. If \"ndim\", it will use the number of parameters in the search space.</p> <p> TYPE: <code>int | Literal['ndim']</code> DEFAULT: <code>'ndim'</code> </p> <code>cost_aware</code> <p>Whether to consider reported \"cost\" from configurations in decision making. If True, the optimizer will weigh potential candidates by how much they cost, incentivising the optimizer to explore cheap, good performing configurations. This amount is modified over time. If \"log\", the cost will be log-transformed before being used.</p> <p>Cost aware</p> <p>If using <code>cost</code>, cost must be provided in the reports of the trials.</p> <p> TYPE: <code>bool | Literal['log']</code> DEFAULT: <code>False</code> </p> <code>device</code> <p>Device to use for the optimization.</p> <p> TYPE: <code>device | str | None</code> DEFAULT: <code>None</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def pibo(\n    space: SearchSpace,\n    *,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    cost_aware: bool | Literal[\"log\"] = False,\n    device: torch.device | str | None = None,\n    sample_prior_first: bool = False,\n    ignore_fidelity: bool = False,\n) -&gt; BayesianOptimization:\n    \"\"\"A modification of\n    [`bayesian_optimization`][neps.optimizers.algorithms.bayesian_optimization]\n    that also incorporates the use of priors in the search space.\n\n    !!! tip \"When to use this?\"\n\n        Use this if you'd like to use bayesian optimization while also having\n        a good idea of what good parameters look like and can specify them\n        through the `prior` and `prior_confidence` parameters in the search space.\n\n        Note that this incurs the same tradeoffs that bayesian optimization\n        has.\n\n    Args:\n        space: The search space to sample from.\n        initial_design_size: Number of samples used before using the surrogate model.\n            If \"ndim\", it will use the number of parameters in the search space.\n        cost_aware: Whether to consider reported \"cost\" from configurations in decision\n            making. If True, the optimizer will weigh potential candidates by how much\n            they cost, incentivising the optimizer to explore cheap, good performing\n            configurations. This amount is modified over time. If \"log\", the cost will be\n            log-transformed before being used.\n            !!! warning \"Cost aware\"\n\n                If using `cost`, cost must be provided in the reports of the trials.\n\n        device: Device to use for the optimization.\n        sample_prior_first: Whether to sample the prior configuration first.\n        ignore_fidelity: Whether to ignore the fidelity parameter when sampling.\n            In this case, the max fidelity is always used.\n    \"\"\"\n    if all(parameter.prior is None for parameter in space.searchables.values()):\n        logger.warning(\n            \"Warning: PiBO was called without any priors - using uniform priors on all\"\n            \" parameters.\\nConsider using Bayesian Optimization instead.\"\n        )\n    if ignore_fidelity and space.fidelity is None:\n        logger.warning(\n            \"Warning: You are using ignore_fidelity, but no fidelity is defined in the\"\n            \" search space. Consider setting ignore_fidelity to False.\"\n        )\n\n    return _bo(\n        pipeline_space=space,\n        initial_design_size=initial_design_size,\n        cost_aware=cost_aware,\n        device=device,\n        use_priors=True,\n        sample_prior_first=sample_prior_first,\n        ignore_fidelity=ignore_fidelity,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.priorband","title":"priorband","text":"<pre><code>priorband(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sample_prior_first: (\n        bool | Literal[\"highest_fidelity\"]\n    ) = False,\n    base: Literal[\n        \"successive_halving\",\n        \"hyperband\",\n        \"asha\",\n        \"async_hb\",\n    ] = \"hyperband\",\n    bayesian_optimization_kick_in_point: (\n        int | float | None\n    ) = None\n) -&gt; BracketOptimizer\n</code></pre> <p>Priorband is also a bandit-based optimization algorithm that uses a fidelity, providing a general purpose sampling extension to other algorithms. It makes better use of the prior information you provide in the search space along with the fact that you can afford to explore and take more risk at lower fidelities.</p> <p>When to use this?</p> <p>Use this when you have a good idea of what good parameters look like and can specify them through the <code>prior</code> and <code>prior_confidence</code> parameters in the search space.</p> <p>As <code>priorband</code> is flexible, you may choose between the existing tradeoffs the other algorithms provide through the use of <code>base=</code>.</p> <p>Priorband works by adjusting the sampling procedure to sample from one of the following three distributions:</p> <ul> <li>1) a uniform distribution</li> <li>2) a prior distribution</li> <li>3) a distribution around the best found configuration so far.</li> </ul> <p>By weighing the likelihood of good configurations having been sampled from each of these distribution, we can score them against each other to aid selection. We further use the fact that we can afford to explore and take more risk at lower fidelities, which is factored into the sampling procedure.</p> <p>See: openreview.net/forum?id=uoiwugtpCH&amp;noteId=xECpK2WH6k</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> <code>base</code> <p>The base algorithm to use for the bracketing.</p> <p> TYPE: <code>Literal['successive_halving', 'hyperband', 'asha', 'async_hb']</code> DEFAULT: <code>'hyperband'</code> </p> <code>bayesian_optimization_kick_in_point</code> <p>If a number <code>N</code>, after <code>N</code> * <code>maximum_fidelity</code> worth of fidelity has been evaluated, proceed with bayesian optimization when sampling a new configuration.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def priorband(\n    space: SearchSpace,\n    *,\n    eta: int = 3,\n    sample_prior_first: bool | Literal[\"highest_fidelity\"] = False,\n    base: Literal[\"successive_halving\", \"hyperband\", \"asha\", \"async_hb\"] = \"hyperband\",\n    bayesian_optimization_kick_in_point: int | float | None = None,\n) -&gt; BracketOptimizer:\n    \"\"\"Priorband is also a bandit-based optimization algorithm that uses a _fidelity_,\n    providing a general purpose sampling extension to other algorithms. It makes better\n    use of the prior information you provide in the search space along with the fact\n    that you can afford to explore and take more risk at lower fidelities.\n\n    !!! tip \"When to use this?\"\n\n        Use this when you have a good idea of what good parameters look like and\n        can specify them through the `prior` and `prior_confidence` parameters in\n        the search space.\n\n        As `priorband` is flexible, you may choose between the existing tradeoffs\n        the other algorithms provide through the use of `base=`.\n\n    Priorband works by adjusting the sampling procedure to sample from one of\n    the following three distributions:\n\n    * 1) a uniform distribution\n    * 2) a prior distribution\n    * 3) a distribution around the best found configuration so far.\n\n    By weighing the likelihood of good configurations having been sampled\n    from each of these distribution, we can score them against each other to aid\n    selection. We further use the fact that we can afford to explore and take more\n    risk at lower fidelities, which is factored into the sampling procedure.\n\n    See: https://openreview.net/forum?id=uoiwugtpCH&amp;noteId=xECpK2WH6k\n\n    Args:\n        space: The search space to sample from.\n        eta: The reduction factor used for building brackets\n        sample_prior_first: Whether to sample the prior configuration first.\n        base: The base algorithm to use for the bracketing.\n        bayesian_optimization_kick_in_point: If a number `N`, after\n            `N` * `maximum_fidelity` worth of fidelity has been evaluated,\n            proceed with bayesian optimization when sampling a new configuration.\n    \"\"\"\n    if all(parameter.prior is None for parameter in space.searchables.values()):\n        logger.warning(\n            \"Warning: No priors are defined in the search space, priorband will sample\"\n            \" uniformly. Consider using hyperband instead.\"\n        )\n    return _bracket_optimizer(\n        pipeline_space=space,\n        bracket_type=base,\n        eta=eta,\n        sampler=\"priorband\",\n        sample_prior_first=sample_prior_first,\n        early_stopping_rate=0 if base in (\"successive_halving\", \"asha\") else None,\n        bayesian_optimization_kick_in_point=bayesian_optimization_kick_in_point,\n        device=None,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.random_search","title":"random_search","text":"<pre><code>random_search(\n    pipeline_space: SearchSpace,\n    *,\n    use_priors: bool = False,\n    ignore_fidelity: (\n        bool | Literal[\"highest fidelity\"]\n    ) = False\n) -&gt; RandomSearch\n</code></pre> <p>A simple random search algorithm that samples configurations uniformly at random.</p> <p>You may also <code>use_priors=</code> to sample from a distribution centered around your defined priors.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>use_priors</code> <p>Whether to use priors when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore fidelity when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool | Literal['highest fidelity']</code> DEFAULT: <code>False</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def random_search(\n    pipeline_space: SearchSpace,\n    *,\n    use_priors: bool = False,\n    ignore_fidelity: bool | Literal[\"highest fidelity\"] = False,\n) -&gt; RandomSearch:\n    \"\"\"A simple random search algorithm that samples configurations uniformly at random.\n\n    You may also `use_priors=` to sample from a distribution centered around your defined\n    priors.\n\n    Args:\n        pipeline_space: The search space to sample from.\n        use_priors: Whether to use priors when sampling.\n        ignore_fidelity: Whether to ignore fidelity when sampling.\n            In this case, the max fidelity is always used.\n    \"\"\"\n    assert ignore_fidelity in (\n        True,\n        False,\n        \"highest fidelity\",\n    ), \"ignore_fidelity should be either True, False or 'highest fidelity'\"\n    if not ignore_fidelity and pipeline_space.fidelity is not None:\n        raise ValueError(\n            \"Fidelities are not supported for RandomSearch. Consider setting the\"\n            \" fidelity to a constant value, or setting ignore_fidelity to True to sample\"\n            \" from it like any other parameter or 'highest fidelity' to always sample at\"\n            f\" max fidelity. Got fidelity: {pipeline_space.fidelities} \"\n        )\n    if ignore_fidelity in (True, \"highest fidelity\") and pipeline_space.fidelity is None:\n        logger.warning(\n            \"Warning: You are using ignore_fidelity, but no fidelity is defined in the\"\n            \" search space. Consider setting ignore_fidelity to False.\"\n        )\n    match ignore_fidelity:\n        case True:\n            parameters = {**pipeline_space.searchables, **pipeline_space.fidelities}\n        case False:\n            parameters = {**pipeline_space.searchables}\n        case \"highest fidelity\":\n            parameters = {**pipeline_space.searchables}\n\n    if use_priors and not any(\n        parameter.prior is not None for parameter in parameters.values()\n    ):\n        logger.warning(\n            \"Warning: You are using priors, but no priors are defined in the search\"\n            \" space. Consider setting use_priors to False.\"\n        )\n\n    if not use_priors and any(\n        parameter.prior is not None for parameter in parameters.values()\n    ):\n        priors = [\n            parameter for parameter in parameters.values() if parameter.prior is not None\n        ]\n        raise ValueError(\n            f\"To use priors, you must set use_priors=True. Got priors: {priors}\"\n        )\n\n    return RandomSearch(\n        space=pipeline_space,\n        encoder=ConfigEncoder.from_parameters(parameters),\n        sampler=(\n            Prior.from_parameters(parameters)\n            if use_priors\n            else Uniform(ndim=len(parameters))\n        ),\n    )\n</code></pre>"},{"location":"api/neps/optimizers/algorithms/#neps.optimizers.algorithms.successive_halving","title":"successive_halving","text":"<pre><code>successive_halving(\n    space: SearchSpace,\n    *,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    sample_prior_first: (\n        bool | Literal[\"highest_fidelity\"]\n    ) = False\n) -&gt; BracketOptimizer\n</code></pre> <p>A bandit-based optimization algorithm that uses a fidelity parameter to gradually invest resources into more promising configurations.</p> Fidelities? <p>A fidelity parameter lets you control how many resources to invest in a single evaluation. For example, a common one for deep-learing is <code>epochs</code>. By evaluating a model for just a few epochs, we can quickly get a sense if the model is promising or not. Only those that perform well get promoted and evaluated at a higher epoch.</p> <p>When to use this?</p> <p>When you think that the rank of N configurations at a lower fidelity correlates very well with the rank if you were to evaluate those configurations at higher fidelities.</p> <p>It does this by creating a competition between N configurations and racing them in a bracket against each other. This bracket has a series of incrementing rungs, where lower rungs indicate less resources invested. The amount of resources is related to your fidelity parameter, with the highest rung relating to the maximum of your fidelity parameter.</p> <p>Those that perform well get promoted and evaluated with more resources.</p> <pre><code># A bracket indicating the rungs and configurations.\n# Those which performed best get promoted through the rungs.\n\n|        | fidelity    | c1 | c2 | c3 | c4 | c5 | ... | cN |\n| Rung 0 | (3 epochs)  |  o |  o |  o |  o |  o | ... | o  |\n| Rung 1 | (9 epochs)  |  o |    |  o |  o |    | ... | o  |\n| Rung 2 | (27 epochs) |  o |    |    |    |    | ... |    |\n</code></pre> <p>By default, new configurations are sampled using a uniform distribution, however you can also specify to prefer sampling from around a distribution you think is more promising by setting the <code>prior</code> and the <code>prior_confidence</code> of parameters of your search space.</p> <p>You can choose between these by setting <code>sampler=\"uniform\"</code> or <code>sampler=\"prior\"</code>.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>eta</code> <p>The reduction factor used for building brackets</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>early_stopping_rate</code> <p>Determines the number of rungs in a bracket Choosing 0 creates maximal rungs given the fidelity bounds.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sampler</code> <p>The type of sampling procedure to use:</p> <ul> <li>If <code>\"uniform\"</code>, samples uniformly from the space when     it needs to sample.</li> <li>If <code>\"prior\"</code>, samples from the prior     distribution built from the <code>prior</code> and <code>prior_confidence</code>     values in the search space.</li> </ul> <p> TYPE: <code>Literal['uniform', 'prior']</code> DEFAULT: <code>'uniform'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first, and if so, should it be at the highest fidelity level.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> Source code in <code>neps\\optimizers\\algorithms.py</code> <pre><code>def successive_halving(\n    space: SearchSpace,\n    *,\n    sampler: Literal[\"uniform\", \"prior\"] = \"uniform\",\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    sample_prior_first: bool | Literal[\"highest_fidelity\"] = False,\n) -&gt; BracketOptimizer:\n    \"\"\"\n    A bandit-based optimization algorithm that uses a _fidelity_ parameter\n    to gradually invest resources into more promising configurations.\n\n    ??? tip \"Fidelities?\"\n\n        A fidelity parameter lets you control how many resources to invest in\n        a single evaluation. For example, a common one for deep-learing is\n        `epochs`. By evaluating a model for just a few epochs, we can quickly\n        get a sense if the model is promising or not. Only those that perform\n        well get _promoted_ and evaluated at a higher epoch.\n\n    !!! tip \"When to use this?\"\n\n        When you think that the rank of N configurations at a lower fidelity correlates\n        very well with the rank if you were to evaluate those configurations at higher\n        fidelities.\n\n    It does this by creating a competition between N configurations and\n    racing them in a _bracket_ against each other.\n    This _bracket_ has a series of incrementing _rungs_, where lower rungs\n    indicate less resources invested. The amount of resources is related\n    to your fidelity parameter, with the highest rung relating to the\n    maximum of your fidelity parameter.\n\n    Those that perform well get _promoted_ and evaluated with more resources.\n\n    ```\n    # A bracket indicating the rungs and configurations.\n    # Those which performed best get promoted through the rungs.\n\n    |        | fidelity    | c1 | c2 | c3 | c4 | c5 | ... | cN |\n    | Rung 0 | (3 epochs)  |  o |  o |  o |  o |  o | ... | o  |\n    | Rung 1 | (9 epochs)  |  o |    |  o |  o |    | ... | o  |\n    | Rung 2 | (27 epochs) |  o |    |    |    |    | ... |    |\n    ```\n\n    By default, new configurations are sampled using a _uniform_ distribution,\n    however you can also specify to prefer sampling from around a distribution you\n    think is more promising by setting the `prior` and the `prior_confidence`\n    of parameters of your search space.\n\n    You can choose between these by setting `#!python sampler=\"uniform\"`\n    or `#!python sampler=\"prior\"`.\n\n    Args:\n        space: The search space to sample from.\n        eta: The reduction factor used for building brackets\n        early_stopping_rate: Determines the number of rungs in a bracket\n            Choosing 0 creates maximal rungs given the fidelity bounds.\n        sampler: The type of sampling procedure to use:\n\n            * If `#!python \"uniform\"`, samples uniformly from the space when\n                it needs to sample.\n            * If `#!python \"prior\"`, samples from the prior\n                distribution built from the `prior` and `prior_confidence`\n                values in the search space.\n\n        sample_prior_first: Whether to sample the prior configuration first,\n            and if so, should it be at the highest fidelity level.\n    \"\"\"\n    return _bracket_optimizer(\n        pipeline_space=space,\n        bracket_type=\"successive_halving\",\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        sampler=sampler,\n        sample_prior_first=sample_prior_first,\n        # TODO: Implement this\n        bayesian_optimization_kick_in_point=None,\n        device=None,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/ask_and_tell/","title":"Ask and tell","text":"<p>Implements a basic Ask-and-Tell wrapper around an optimizer.</p> <p>This is a simple wrapper around an optimizer that allows you to ask for new configurations and report the results back to the optimizer, without using the NePS runtime to run the evaluation for you.</p> <p>This puts you in charge of getting new configurations, evaluating the configuration and reporting back the results, in whatever order you would prefer. For example, you could <code>ask()</code> twice to get two configuration, evaluate both configurations in parallel, and then <code>tell()</code> results back to the optimizer.</p> <pre><code>from neps import AskAndTell\n\n# Wrap an optimizer\nmy_optimizer = AskAndTell(MyOptimizer(space, ...))\n\n# Ask for a new configuration\ntrial = my_optimizer.ask()\n\n# The things you would normally get into `evaluate_pipeline`\nconfig_id = trial.config_id\nconfig = trial.config\nprevious_config_id = trial.metadata.previous_trial_id\nprevious_trial_path = trial.metadata.previous_trial_location\n\n# Evaluate the configuration\nloss = evaluate(config)\n\n# Tell the optimizer the result\nmy_optimizer.tell(config_id, loss)\n</code></pre> <p>Importantly, we expose a little more of the information that is normally hidden from you by exposing the <code>Trial</code> object. This carries most of the meta-information that is normally written to disk and stored with each evaluation.</p> <p>You can also report your own custom configurations, for example to warmstart an optimizer with previous results:</p> <pre><code>optimizer.tell_custom(\n    config_id=\"my_config_id\",  # Make sure to give it a unique id\n    config={\"a\": 1, \"b\": 2},\n    result={\"objective_to_minimize\": 0.5},  # The same as the return evaluate_pipeline\n)\n</code></pre> <p>You can provide a lot more info that normally the neps runtime would fill int for you. For a full list, please see <code>tell_custom</code>.</p> <p>Please see <code>AskFunction</code> for more information on how to implement your own optimizer.</p>"},{"location":"api/neps/optimizers/ask_and_tell/#neps.optimizers.ask_and_tell.AskAndTell","title":"AskAndTell  <code>dataclass</code>","text":"<pre><code>AskAndTell(\n    optimizer: AskFunction,\n    worker_id: str = _default_worker_name(),\n)\n</code></pre> <p>A wrapper around an optimizer that allows you to ask for new configurations.</p>"},{"location":"api/neps/optimizers/ask_and_tell/#neps.optimizers.ask_and_tell.AskAndTell.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer: AskFunction\n</code></pre> <p>The optimizer to wrap.</p>"},{"location":"api/neps/optimizers/ask_and_tell/#neps.optimizers.ask_and_tell.AskAndTell.trials","title":"trials  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trials: dict[str, Trial] = field(\n    init=False, default_factory=dict\n)\n</code></pre> <p>The trials that the optimizer is aware of, whether sampled or with a result.</p>"},{"location":"api/neps/optimizers/ask_and_tell/#neps.optimizers.ask_and_tell.AskAndTell.worker_id","title":"worker_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worker_id: str = field(default_factory=_default_worker_name)\n</code></pre> <p>The worker id used to fill out the trial metadata.</p>"},{"location":"api/neps/optimizers/ask_and_tell/#neps.optimizers.ask_and_tell.AskAndTell.ask","title":"ask","text":"<pre><code>ask(\n    *, n: int, budget_info: BudgetInfo | None = ...\n) -&gt; list[Trial]\n</code></pre><pre><code>ask(\n    *, n: None = None, budget_info: BudgetInfo | None = ...\n) -&gt; Trial\n</code></pre> <pre><code>ask(\n    *,\n    n: int | None = None,\n    budget_info: BudgetInfo | None = None\n) -&gt; Trial | list[Trial]\n</code></pre> <p>Ask the optimizer for a new configuration.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample at once.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>budget_info</code> <p>information about the budget constraints. Only required if the optimizer needs it. You have the responsibility to fill this out, which also allows you to handle it more flexibly as you need.</p> <p> TYPE: <code>BudgetInfo | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial | list[Trial]</code> <p>The sampled trial(s)</p> Source code in <code>neps\\optimizers\\ask_and_tell.py</code> <pre><code>def ask(\n    self,\n    *,\n    n: int | None = None,\n    budget_info: BudgetInfo | None = None,\n) -&gt; Trial | list[Trial]:\n    \"\"\"Ask the optimizer for a new configuration.\n\n    Args:\n        n: The number of configurations to sample at once.\n        budget_info: information about the budget constraints. Only\n            required if the optimizer needs it. You have the\n            responsibility to fill this out, which also allows\n            you to handle it more flexibly as you need.\n\n    Returns:\n        The sampled trial(s)\n    \"\"\"\n    sampled_config = self.optimizer(self.trials, budget_info, n)\n    if isinstance(sampled_config, SampledConfig):\n        _configs = [sampled_config]\n    else:\n        _configs = sampled_config\n\n    sample_time = time.time()\n    trials: list[Trial] = []\n    for sampled in _configs:\n        trial = Trial.new(\n            trial_id=sampled.id,\n            location=\"\",\n            config=sampled.config,\n            previous_trial=sampled.previous_config_id,\n            previous_trial_location=\"\",\n            time_sampled=sample_time,\n            worker_id=self.worker_id,\n        )\n\n        # This is sort of some cruft we have to include here to make\n        # it match up with what the runtime would do... oh well\n        trial.set_evaluating(\n            time_started=sample_time,\n            worker_id=self.worker_id,\n        )\n        self.trials[sampled.id] = trial\n        trials.append(trial)\n\n    if n is None:\n        return trials[0]\n\n    return trials\n</code></pre>"},{"location":"api/neps/optimizers/ask_and_tell/#neps.optimizers.ask_and_tell.AskAndTell.tell","title":"tell","text":"<pre><code>tell(\n    trial: str | Trial,\n    result: EvaluatePipelineReturn,\n    *,\n    time_end: float | None = None,\n    evaluation_duration: float | None = None,\n    traceback_str: str | None = None\n) -&gt; Trial\n</code></pre> <p>Report the result of an evaluation back to the optimizer.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial to report the result for. This can be either the trial id (a string) or the trial object itself.</p> <p> TYPE: <code>str | Trial</code> </p> <code>result</code> <p>The result of the evaluation. This can be an exception, a float, or a mapping of values, similar to that which you would return from <code>evaluate_pipeline</code> when your normally call <code>neps.run()</code>.</p> <p> TYPE: <code>EvaluatePipelineReturn</code> </p> <code>time_end</code> <p>The time the configuration was finished being evaluated. Defaults to <code>time.time()</code>. Only used as metadata.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>evaluation_duration</code> <p>The duration of the evaluation. Defaults to the difference between when it was <code>ask()</code>ed for and now. Only used as metadata</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>traceback_str</code> <p>The traceback of any error, only to fill in metadata if you need.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial</code> <p>The trial object that was updated. You can find the report generated at <code>trial.report</code>. You do not require this at any other point and the return value can safely be ignored if you wish.</p> Source code in <code>neps\\optimizers\\ask_and_tell.py</code> <pre><code>def tell(\n    self,\n    trial: str | Trial,\n    result: EvaluatePipelineReturn,\n    *,\n    time_end: float | None = None,\n    evaluation_duration: float | None = None,\n    traceback_str: str | None = None,\n) -&gt; Trial:\n    \"\"\"Report the result of an evaluation back to the optimizer.\n\n    Args:\n        trial: The trial to report the result for. This can be either\n            the trial id (a string) or the trial object itself.\n        result: The result of the evaluation. This can be an exception,\n            a float, or a mapping of values, similar to that which\n            you would return from `evaluate_pipeline` when your normally\n            call [`neps.run()`][neps.api.run].\n        time_end: The time the configuration was finished being evaluated.\n            Defaults to `time.time()`. Only used as metadata.\n        evaluation_duration: The duration of the evaluation. Defaults\n            to the difference between when it was\n            [`ask()`][neps.optimizers.ask_and_tell.AskAndTell.ask]ed\n            for and now. Only used as metadata\n        traceback_str: The traceback of any error, only to fill in\n            metadata if you need.\n\n    Returns:\n        The trial object that was updated. You can find the report\n        generated at `trial.report`. You do not require this at any\n        other point and the return value can safely be ignored if you wish.\n    \"\"\"\n    trial_id = trial if isinstance(trial, str) else trial.id\n\n    _trial = self.trials.get(trial_id)\n    if _trial is None:\n        raise ValueError(\n            f\"Unknown trial id: {trial_id}.\"\n            f\" Known trial ids: {list(self.trials.keys())}\"\n        )\n\n    parsed_result = UserResult.parse(\n        result,\n        default_objective_to_minimize_value=None,\n        default_cost_value=None,\n        default_learning_curve=None,\n    )\n    report_as: Literal[\"success\", \"crashed\"] = (\n        \"success\" if parsed_result.exception is None else \"crashed\"\n    )\n\n    _trial = self.trials[_trial.id]\n    _trial.report = _trial.set_complete(\n        report_as=report_as,\n        objective_to_minimize=parsed_result.objective_to_minimize,\n        cost=parsed_result.cost,\n        learning_curve=parsed_result.learning_curve,\n        extra=parsed_result.extra,\n        time_end=time_end if time_end is not None else time.time(),\n        evaluation_duration=evaluation_duration,\n        err=parsed_result.exception,\n        tb=traceback_str,\n    )\n    return _trial\n</code></pre>"},{"location":"api/neps/optimizers/ask_and_tell/#neps.optimizers.ask_and_tell.AskAndTell.tell_custom","title":"tell_custom","text":"<pre><code>tell_custom(\n    *,\n    config_id: str,\n    config: Mapping[str, Any],\n    result: EvaluatePipelineReturn,\n    time_sampled: float = float(\"nan\"),\n    time_started: float = float(\"nan\"),\n    time_end: float = float(\"nan\"),\n    evaluation_duration: float = float(\"nan\"),\n    previous_trial_id: str | None = None,\n    worker_id: str | None = None,\n    traceback_str: str | None = None\n) -&gt; Trial\n</code></pre> <p>Report a custom configuration and result to the optimizer.</p> <p>This is useful for warmstarting an optimizer with previous results.</p> PARAMETER DESCRIPTION <code>config_id</code> <p>The id of the configuration.</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The configuration.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>result</code> <p>The result of the evaluation. This can be an exception, a float, or a mapping of values, similar to that which you would return from <code>evaluate_pipeline</code> when your normally call <code>neps.run()</code>.</p> <p> TYPE: <code>EvaluatePipelineReturn</code> </p> <code>time_sampled</code> <p>The time the configuration was sampled. Only used as metadata.</p> <p> TYPE: <code>float</code> DEFAULT: <code>float('nan')</code> </p> <code>time_started</code> <p>The time the configuration was started to be evaluated. Only used as metadata.</p> <p> TYPE: <code>float</code> DEFAULT: <code>float('nan')</code> </p> <code>time_end</code> <p>The time the configuration was finished being evaluated. Only used as metadata.</p> <p> TYPE: <code>float</code> DEFAULT: <code>float('nan')</code> </p> <code>evaluation_duration</code> <p>The duration of the evaluation. Only used as metadata</p> <p> TYPE: <code>float</code> DEFAULT: <code>float('nan')</code> </p> <code>previous_trial_id</code> <p>The id of any previous trial that this configuration was derived from, for example, the same configuration as an earlier one but at a later epoch.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>worker_id</code> <p>The worker id that sampled this configuration, only to fill in metadata if you need.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>traceback_str</code> <p>The traceback of any error, only to fill in metadata if you need.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial</code> <p>The trial object that was created. You can find the report generated at <code>trial.report</code>. You do not require this at any other point and the return value can safely be ignored if you wish.</p> Source code in <code>neps\\optimizers\\ask_and_tell.py</code> <pre><code>def tell_custom(\n    self,\n    *,\n    config_id: str,\n    config: Mapping[str, Any],\n    result: EvaluatePipelineReturn,\n    time_sampled: float = float(\"nan\"),\n    time_started: float = float(\"nan\"),\n    time_end: float = float(\"nan\"),\n    evaluation_duration: float = float(\"nan\"),\n    previous_trial_id: str | None = None,\n    worker_id: str | None = None,\n    traceback_str: str | None = None,\n) -&gt; Trial:\n    \"\"\"Report a custom configuration and result to the optimizer.\n\n    This is useful for warmstarting an optimizer with previous results.\n\n    Args:\n        config_id: The id of the configuration.\n        config: The configuration.\n        result: The result of the evaluation. This can be an exception,\n            a float, or a mapping of values, similar to that which\n            you would return from `evaluate_pipeline` when your normally\n            call [`neps.run()`][neps.api.run].\n        time_sampled: The time the configuration was sampled.\n            Only used as metadata.\n        time_started: The time the configuration was started to be evaluated.\n            Only used as metadata.\n        time_end: The time the configuration was finished being evaluated.\n            Only used as metadata.\n        evaluation_duration: The duration of the evaluation. Only used\n            as metadata\n        previous_trial_id: The id of any previous trial that this configuration\n            was derived from, for example, the same configuration as an earlier\n            one but at a later epoch.\n        worker_id: The worker id that sampled this configuration, only to fill in\n            metadata if you need.\n        traceback_str: The traceback of any error, only to fill in\n            metadata if you need.\n\n    Returns:\n        The trial object that was created. You can find the report\n        generated at `trial.report`. You do not require this at any\n        other point and the return value can safely be ignored if you wish.\n    \"\"\"\n    if config_id in self.trials:\n        raise ValueError(f\"Config id '{config_id}' already exists!\")\n\n    if worker_id is None:\n        worker_id = self.worker_id\n\n    parsed_result = UserResult.parse(\n        result,\n        default_objective_to_minimize_value=None,\n        default_cost_value=None,\n        default_learning_curve=None,\n    )\n    report_as: Literal[\"success\", \"crashed\"] = (\n        \"success\" if parsed_result.exception is None else \"crashed\"\n    )\n\n    # Just go through the motions of the trial life-cycle\n    trial = Trial.new(\n        trial_id=config_id,\n        location=\"\",\n        config=config,\n        previous_trial=previous_trial_id,\n        previous_trial_location=\"\",\n        time_sampled=time_sampled,\n        worker_id=worker_id,\n    )\n    trial.set_evaluating(\n        time_started=time_started,\n        worker_id=worker_id,\n    )\n    trial.report = trial.set_complete(\n        report_as=report_as,\n        objective_to_minimize=parsed_result.objective_to_minimize,\n        cost=parsed_result.cost,\n        learning_curve=parsed_result.learning_curve,\n        extra=parsed_result.extra,\n        err=parsed_result.exception,\n        time_end=time_end,\n        evaluation_duration=evaluation_duration,\n        tb=traceback_str,\n    )\n    self.trials[config_id] = trial\n    return trial\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/","title":"Bayesian optimization","text":""},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization","title":"BayesianOptimization  <code>dataclass</code>","text":"<pre><code>BayesianOptimization(\n    space: SearchSpace,\n    encoder: ConfigEncoder,\n    prior: Prior | None,\n    sample_prior_first: bool,\n    cost_aware: bool | Literal[\"log\"],\n    n_initial_design: int,\n    device: device | None,\n    reference_point: tuple[float, ...] | None = None,\n)\n</code></pre> <p>Uses <code>botorch</code> as an engine for doing bayesian optimiziation.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.cost_aware","title":"cost_aware  <code>instance-attribute</code>","text":"<pre><code>cost_aware: bool | Literal['log']\n</code></pre> <p>Whether to consider the cost of configurations in decision making.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: device | None\n</code></pre> <p>The device to use for the optimization.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder: ConfigEncoder\n</code></pre> <p>The encoder to use for encoding and decoding configurations.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.n_initial_design","title":"n_initial_design  <code>instance-attribute</code>","text":"<pre><code>n_initial_design: int\n</code></pre> <p>The number of initial design samples to use before fitting the GP.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.prior","title":"prior  <code>instance-attribute</code>","text":"<pre><code>prior: Prior | None\n</code></pre> <p>The prior to use for sampling configurations and inferring their likelihood.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.reference_point","title":"reference_point  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reference_point: tuple[float, ...] | None = None\n</code></pre> <p>The reference point to use for the multi-objective optimization.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.sample_prior_first","title":"sample_prior_first  <code>instance-attribute</code>","text":"<pre><code>sample_prior_first: bool\n</code></pre> <p>Whether to sample the prior configuration first.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/#neps.optimizers.bayesian_optimization.BayesianOptimization.space","title":"space  <code>instance-attribute</code>","text":"<pre><code>space: SearchSpace\n</code></pre> <p>The search space to use.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/","title":"Bracket optimizer","text":""},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer","title":"BracketOptimizer  <code>dataclass</code>","text":"<pre><code>BracketOptimizer(\n    space: SearchSpace,\n    encoder: ConfigEncoder,\n    sample_prior_first: bool | Literal[\"highest_fidelity\"],\n    eta: int,\n    rung_to_fid: Mapping[int, int | float],\n    create_brackets: Callable[\n        [DataFrame], Sequence[Bracket] | Bracket\n    ],\n    sampler: Sampler | PriorBandSampler,\n    gp_sampler: GPSampler | None,\n    fid_min: int | float,\n    fid_max: int | float,\n    fid_name: str,\n)\n</code></pre> <p>Implements an optimizer over brackets.</p> <p>This is the main class behind algorithms like <code>\"priorband\"</code>, <code>\"successive_halving\"</code>, <code>\"asha\"</code>, <code>\"hyperband\"</code>, etc.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.create_brackets","title":"create_brackets  <code>instance-attribute</code>","text":"<pre><code>create_brackets: Callable[\n    [DataFrame], Sequence[Bracket] | Bracket\n]\n</code></pre> <p>A function that creates the brackets from the table of trials.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder: ConfigEncoder\n</code></pre> <p>The encoder to use for the pipeline space.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.eta","title":"eta  <code>instance-attribute</code>","text":"<pre><code>eta: int\n</code></pre> <p>The eta parameter for the algorithm.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.fid_max","title":"fid_max  <code>instance-attribute</code>","text":"<pre><code>fid_max: int | float\n</code></pre> <p>The maximum fidelity value.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.fid_min","title":"fid_min  <code>instance-attribute</code>","text":"<pre><code>fid_min: int | float\n</code></pre> <p>The minimum fidelity value.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.fid_name","title":"fid_name  <code>instance-attribute</code>","text":"<pre><code>fid_name: str\n</code></pre> <p>The name of the fidelity in the space.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.gp_sampler","title":"gp_sampler  <code>instance-attribute</code>","text":"<pre><code>gp_sampler: GPSampler | None\n</code></pre> <p>If set, uses a GP for sampling configurations once it's threshold for fidelity units has been reached.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.rung_to_fid","title":"rung_to_fid  <code>instance-attribute</code>","text":"<pre><code>rung_to_fid: Mapping[int, int | float]\n</code></pre> <p>The mapping from rung to fidelity value.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.sample_prior_first","title":"sample_prior_first  <code>instance-attribute</code>","text":"<pre><code>sample_prior_first: bool | Literal['highest_fidelity']\n</code></pre> <p>Whether or not to sample the prior first.</p> <p>If set to <code>\"highest_fidelity\"</code>, the prior will be sampled at the highest fidelity, otherwise at the lowest fidelity.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler: Sampler | PriorBandSampler\n</code></pre> <p>The sampler used to generate new trials.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.BracketOptimizer.space","title":"space  <code>instance-attribute</code>","text":"<pre><code>space: SearchSpace\n</code></pre> <p>The pipeline space to optimize over.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler","title":"GPSampler  <code>dataclass</code>","text":"<pre><code>GPSampler(\n    parameters: Mapping[str, Parameter],\n    encoder: ConfigEncoder,\n    threshold: float,\n    two_stage_batch_sample_size: int,\n    fidelity_name: str,\n    fidelity_max: int | float,\n    device: device | None,\n)\n</code></pre> <p>See the following reference.</p> <p>PriorBand Appendix E.4, Model extensions, openreview.net/attachment?id=uoiwugtpCH&amp;name=supplementary_material</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: device | None\n</code></pre> <p>The device to use for the GP optimization.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder: ConfigEncoder\n</code></pre> <p>The encoder to use for encoding and decoding configurations.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.fidelity_max","title":"fidelity_max  <code>instance-attribute</code>","text":"<pre><code>fidelity_max: int | float\n</code></pre> <p>The maximum fidelity value.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.fidelity_name","title":"fidelity_name  <code>instance-attribute</code>","text":"<pre><code>fidelity_name: str\n</code></pre> <p>The name of the fidelity in the space.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: Mapping[str, Parameter]\n</code></pre> <p>The parameters to use.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold: float\n</code></pre> <p>The threshold at which to switch to the Bayesian optimizer.</p> <p>This is calculated in the following way: * 1 <code>fid_unit</code> is equal to <code>fid_max</code>. * The minimum fidelity is equal to <code>fid_min / fid_max</code>. * BO Sampling kicks in after <code>threshold</code> units of <code>fit_unit</code> have been used.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.two_stage_batch_sample_size","title":"two_stage_batch_sample_size  <code>instance-attribute</code>","text":"<pre><code>two_stage_batch_sample_size: int\n</code></pre> <p>When fitting a GP jointly across all fidelitys, we do a two stage acquisition.</p> <p>For simplicity in writing, lets assume <code>two_stage_batch_sample_size</code> is 10.</p> <p>In the first stage, we acquire from the GP, the <code>10</code> configurations that are predicted to be best at that target fidelity. We then use another expected improvement on these <code>10</code> configurations, but with their fidelity set to the maximum, essentially to predict which one of those <code>10</code> configurations will be best at the maximum fidelity.</p>"},{"location":"api/neps/optimizers/bracket_optimizer/#neps.optimizers.bracket_optimizer.GPSampler.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n    target_fidelity: int | float,\n) -&gt; dict[str, Any]\n</code></pre> <p>Samples a configuration using the GP model.</p> <p>Please see parameter descriptions in the class docstring for more.</p> Source code in <code>neps\\optimizers\\bracket_optimizer.py</code> <pre><code>def sample_config(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n    target_fidelity: int | float,\n) -&gt; dict[str, Any]:\n    \"\"\"Samples a configuration using the GP model.\n\n    Please see parameter descriptions in the class docstring for more.\n    \"\"\"\n    assert budget_info is None, \"cost-aware (using budget_info) not supported yet.\"\n    # fit the GP model using all trials, using fidelity as a dimension.\n    # Get to top 10 configurations for acquisition fixed at fidelity Z\n    # Switch those configurations to be at fidelity z_max and take the best.\n    # y_max for EI is taken to be the best value seen so far, across all fidelity\n    data, _ = encode_trials_for_gp(\n        trials,\n        self.parameters,\n        encoder=self.encoder,\n        device=self.device,\n    )\n    gp = make_default_single_obj_gp(x=data.x, y=data.y, encoder=self.encoder)\n\n    with disable_warnings(NumericalWarning):\n        acqf = qLogNoisyExpectedImprovement(\n            model=gp,\n            X_baseline=data.x,\n            # Unfortunatly, there's no option to indicate that we minimize\n            # the AcqFunction so we need to do some kind of transformation.\n            # https://github.com/pytorch/botorch/issues/2316#issuecomment-2085964607\n            objective=LinearMCObjective(\n                weights=torch.tensor([-1.0], device=self.device)\n            ),\n            X_pending=data.x_pending,\n            prune_baseline=True,\n        )\n\n    # When it's max fidelity, we can just sample the best configuration we find,\n    # as we do not need to do the two step procedure.\n    requires_two_step = target_fidelity != self.fidelity_max\n    N = 1 if requires_two_step else self.two_stage_batch_sample_size\n\n    candidates = fit_and_acquire_from_gp(\n        gp=gp,\n        encoder=self.encoder,\n        x_train=data.x,\n        n_candidates_required=N,\n        acquisition=acqf,\n        # Ensure we fix that acquisition happens at target fidelity\n        fixed_acq_features={self.fidelity_name: target_fidelity},\n        # NOTE: We don't support any cost aware or prior based GP stuff here\n        # TODO: Theoretically, we could. Check out the implementation of\n        # `BayesianOptimization` for more details\n        prior=None,\n        pibo_exp_term=None,\n        costs=None,\n        cost_percentage_used=None,\n        costs_on_log_scale=False,\n        hide_warnings=True,\n    )\n    assert len(candidates) == N\n\n    # We bail out here, as we already acquired over max fidelity.\n    if not requires_two_step:\n        config = self.encoder.decode_one(candidates[0])\n        assert config[self.fidelity_name] == target_fidelity, (\n            f\"Expected the target fidelity to be {target_fidelity}, \"\n            f\"but got {config[self.fidelity_name]} for config: {config}\"\n        )\n        return config\n\n    # Next, we set those N configurations to be at the max fidelity\n    # Decode, set max fidelity, and encode again (TODO: Could do directly on tensors)\n    configs = self.encoder.decode(candidates)\n    fid_max_configs = [{**c, self.fidelity_name: self.fidelity_max} for c in configs]\n    encoded_fix_max_configs = self.encoder.encode(fid_max_configs)\n\n    ys = acqf(encoded_fix_max_configs)\n    idx_max = torch.argmax(ys)\n    config = configs[idx_max]\n    config.update({self.fidelity_name: target_fidelity})\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/grid_search/","title":"Grid search","text":""},{"location":"api/neps/optimizers/grid_search/#neps.optimizers.grid_search.GridSearch","title":"GridSearch  <code>dataclass</code>","text":"<pre><code>GridSearch(configs_list: list[dict[str, Any]])\n</code></pre> <p>Evaluates a fixed list of configurations in order.</p>"},{"location":"api/neps/optimizers/grid_search/#neps.optimizers.grid_search.GridSearch.configs_list","title":"configs_list  <code>instance-attribute</code>","text":"<pre><code>configs_list: list[dict[str, Any]]\n</code></pre> <p>The list of configurations to evaluate.</p>"},{"location":"api/neps/optimizers/ifbo/","title":"Ifbo","text":""},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO","title":"IFBO  <code>dataclass</code>","text":"<pre><code>IFBO(\n    space: SearchSpace,\n    encoder: ConfigEncoder,\n    sample_prior_first: bool,\n    prior: Prior | None,\n    n_initial_design: int,\n    device: device | None,\n    ftpfn: FTPFNSurrogate,\n    n_fidelity_bins: int,\n)\n</code></pre> <p>The ifBO optimizer.</p> <ul> <li>Paper: openreview.net/forum?id=VyoY3Wh9Wd</li> <li>Github: github.com/automl/ifBO/tree/main</li> </ul>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: device | None\n</code></pre> <p>The device to use for the optimizer.</p>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder: ConfigEncoder\n</code></pre> <p>The encoder to use for the pipeline space.</p>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.ftpfn","title":"ftpfn  <code>instance-attribute</code>","text":"<pre><code>ftpfn: FTPFNSurrogate\n</code></pre> <p>The FTPFN surrogate to use.</p>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.n_fidelity_bins","title":"n_fidelity_bins  <code>instance-attribute</code>","text":"<pre><code>n_fidelity_bins: int\n</code></pre> <p>The number of bins to divide the fidelity domain into.</p> <p>Each one will be treated as an individual fidelity level.</p>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.n_initial_design","title":"n_initial_design  <code>instance-attribute</code>","text":"<pre><code>n_initial_design: int\n</code></pre> <p>The number of initial designs to sample.</p>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.prior","title":"prior  <code>instance-attribute</code>","text":"<pre><code>prior: Prior | None\n</code></pre> <p>The prior to use for sampling the pipeline space.</p>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.sample_prior_first","title":"sample_prior_first  <code>instance-attribute</code>","text":"<pre><code>sample_prior_first: bool\n</code></pre> <p>Whether to sample the prior first.</p>"},{"location":"api/neps/optimizers/ifbo/#neps.optimizers.ifbo.IFBO.space","title":"space  <code>instance-attribute</code>","text":"<pre><code>space: SearchSpace\n</code></pre> <p>The entire search space for the pipeline.</p>"},{"location":"api/neps/optimizers/neps_algorithms/","title":"Neps algorithms","text":""},{"location":"api/neps/optimizers/neps_algorithms/#neps.optimizers.neps_algorithms--neps-algorithms","title":"NePS Algorithms","text":"<p>This module provides implementations of various NePS algorithms for optimizing pipeline spaces.</p>"},{"location":"api/neps/optimizers/neps_algorithms/#neps.optimizers.neps_algorithms.neps_complex_random_search","title":"neps_complex_random_search","text":"<pre><code>neps_complex_random_search(\n    pipeline: Pipeline, *_args: Any, **_kwargs: Any\n) -&gt; NePSComplexRandomSearch\n</code></pre> <p>A complex random search algorithm that samples configurations uniformly at random, but allows for more complex sampling strategies.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The search space to sample from.</p> <p> TYPE: <code>Pipeline</code> </p> Source code in <code>neps\\optimizers\\neps_algorithms.py</code> <pre><code>def neps_complex_random_search(\n    pipeline: Pipeline,\n    *_args: Any,\n    **_kwargs: Any,\n) -&gt; NePSComplexRandomSearch:\n    \"\"\"A complex random search algorithm that samples configurations uniformly at random,\n    but allows for more complex sampling strategies.\n\n    Args:\n        pipeline: The search space to sample from.\n    \"\"\"\n\n    return NePSComplexRandomSearch(\n        pipeline=pipeline,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/neps_algorithms/#neps.optimizers.neps_algorithms.neps_priorband","title":"neps_priorband","text":"<pre><code>neps_priorband(\n    space: Pipeline,\n    *,\n    eta: int = 3,\n    sample_prior_first: (\n        bool | Literal[\"highest_fidelity\"]\n    ) = False,\n    base: Literal[\n        \"successive_halving\",\n        \"hyperband\",\n        \"asha\",\n        \"async_hb\",\n    ] = \"hyperband\"\n) -&gt; _NePSBracketOptimizer\n</code></pre> <p>Create a PriorBand optimizer for the given pipeline space.</p> PARAMETER DESCRIPTION <code>space</code> <p>The pipeline space to optimize over.</p> <p> TYPE: <code>Pipeline</code> </p> <code>eta</code> <p>The eta parameter for the algorithm.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior first. If set to <code>\"highest_fidelity\"</code>, the prior will be sampled at the highest fidelity, otherwise at the lowest fidelity.</p> <p> TYPE: <code>bool | Literal['highest_fidelity']</code> DEFAULT: <code>False</code> </p> <code>base</code> <p>The type of bracket optimizer to use. One of: - \"successive_halving\" - \"hyperband\" - \"asha\" - \"async_hb\"</p> <p> TYPE: <code>Literal['successive_halving', 'hyperband', 'asha', 'async_hb']</code> DEFAULT: <code>'hyperband'</code> </p> <p>Returns:     An instance of _BracketOptimizer configured for PriorBand sampling.</p> Source code in <code>neps\\optimizers\\neps_algorithms.py</code> <pre><code>def neps_priorband(\n    space: Pipeline,\n    *,\n    eta: int = 3,\n    sample_prior_first: bool | Literal[\"highest_fidelity\"] = False,\n    base: Literal[\"successive_halving\", \"hyperband\", \"asha\", \"async_hb\"] = \"hyperband\",\n) -&gt; _NePSBracketOptimizer:\n    \"\"\"Create a PriorBand optimizer for the given pipeline space.\n\n    Args:\n        space: The pipeline space to optimize over.\n        eta: The eta parameter for the algorithm.\n        sample_prior_first: Whether to sample the prior first.\n            If set to `\"highest_fidelity\"`, the prior will be sampled at the\n            highest fidelity, otherwise at the lowest fidelity.\n        base: The type of bracket optimizer to use. One of:\n            - \"successive_halving\"\n            - \"hyperband\"\n            - \"asha\"\n            - \"async_hb\"\n    Returns:\n        An instance of _BracketOptimizer configured for PriorBand sampling.\n    \"\"\"\n    return _neps_bracket_optimizer(\n        pipeline_space=space,\n        bracket_type=base,\n        eta=eta,\n        sampler=\"priorband\",\n        sample_prior_first=sample_prior_first,\n        early_stopping_rate=0 if base in (\"successive_halving\", \"asha\") else None,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/neps_algorithms/#neps.optimizers.neps_algorithms.neps_random_search","title":"neps_random_search","text":"<pre><code>neps_random_search(\n    pipeline: Pipeline, *_args: Any, **_kwargs: Any\n) -&gt; NePSRandomSearch\n</code></pre> <p>A simple random search algorithm that samples configurations uniformly at random.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The search space to sample from.</p> <p> TYPE: <code>Pipeline</code> </p> Source code in <code>neps\\optimizers\\neps_algorithms.py</code> <pre><code>def neps_random_search(\n    pipeline: Pipeline,\n    *_args: Any,\n    **_kwargs: Any,\n) -&gt; NePSRandomSearch:\n    \"\"\"A simple random search algorithm that samples configurations uniformly at random.\n\n    Args:\n        pipeline: The search space to sample from.\n    \"\"\"\n\n    return NePSRandomSearch(\n        pipeline=pipeline,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/neps_bracket_optimizer/","title":"Neps bracket optimizer","text":"<p>This module provides multi-fidelity optimizers for NePS spaces. It implements a bracket-based optimization strategy that samples configurations from a prior band, allowing for efficient exploration of the search space. It supports different bracket types such as successive halving, hyperband, ASHA, and async hyperband, and can sample configurations at different fidelity levels.</p>"},{"location":"api/neps/optimizers/neps_priorband/","title":"Neps priorband","text":"<p>PriorBand Sampler for NePS Optimizers. This sampler implements the PriorBand algorithm, which is a sampling strategy that combines prior knowledge with random sampling to efficiently explore the search space. It uses a combination of prior sampling, incumbent mutation, and random sampling based on the fidelity bounds and SH bracket.</p>"},{"location":"api/neps/optimizers/neps_priorband/#neps.optimizers.neps_priorband.NePSPriorBandSampler","title":"NePSPriorBandSampler  <code>dataclass</code>","text":"<pre><code>NePSPriorBandSampler(\n    space: Pipeline,\n    eta: int,\n    early_stopping_rate: int,\n    fid_bounds: tuple[int, int] | tuple[float, float],\n)\n</code></pre> <p>Implement a sampler based on PriorBand.</p>"},{"location":"api/neps/optimizers/neps_priorband/#neps.optimizers.neps_priorband.NePSPriorBandSampler.early_stopping_rate","title":"early_stopping_rate  <code>instance-attribute</code>","text":"<pre><code>early_stopping_rate: int\n</code></pre> <p>The early stopping rate to use for the SH bracket.</p>"},{"location":"api/neps/optimizers/neps_priorband/#neps.optimizers.neps_priorband.NePSPriorBandSampler.eta","title":"eta  <code>instance-attribute</code>","text":"<pre><code>eta: int\n</code></pre> <p>The eta value to use for the SH bracket.</p>"},{"location":"api/neps/optimizers/neps_priorband/#neps.optimizers.neps_priorband.NePSPriorBandSampler.fid_bounds","title":"fid_bounds  <code>instance-attribute</code>","text":"<pre><code>fid_bounds: tuple[int, int] | tuple[float, float]\n</code></pre> <p>The fidelity bounds.</p>"},{"location":"api/neps/optimizers/neps_priorband/#neps.optimizers.neps_priorband.NePSPriorBandSampler.space","title":"space  <code>instance-attribute</code>","text":"<pre><code>space: Pipeline\n</code></pre> <p>The pipeline space to optimize over.</p>"},{"location":"api/neps/optimizers/neps_priorband/#neps.optimizers.neps_priorband.NePSPriorBandSampler.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    table: DataFrame, rung: int\n) -&gt; dict[str, Any]\n</code></pre> <p>Sample a configuration based on the PriorBand algorithm.</p> PARAMETER DESCRIPTION <code>table</code> <p>The table containing the configurations and their performance.</p> <p> TYPE: <code>DataFrame</code> </p> <code>rung</code> <p>The current rung of the optimization.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>dict[str, Any]: A sampled configuration.</p> Source code in <code>neps\\optimizers\\neps_priorband.py</code> <pre><code>def sample_config(self, table: pd.DataFrame, rung: int) -&gt; dict[str, Any]:\n    \"\"\"Sample a configuration based on the PriorBand algorithm.\n\n    Args:\n        table (pd.DataFrame): The table containing the configurations and their\n            performance.\n        rung (int): The current rung of the optimization.\n\n    Returns:\n        dict[str, Any]: A sampled configuration.\n    \"\"\"\n    rung_to_fid, rung_sizes = brackets.calculate_sh_rungs(\n        bounds=self.fid_bounds,\n        eta=self.eta,\n        early_stopping_rate=self.early_stopping_rate,\n    )\n    max_rung = max(rung_sizes)\n\n    # Below we will follow the \"geometric\" spacing\n    w_random = 1 / (1 + self.eta**rung)\n    w_prior = 1 - w_random\n\n    completed: pd.DataFrame = table[table[\"perf\"].notna()]  # type: ignore\n\n    # To see if we activate incumbent sampling, we check:\n    # 1) We have at least one fully complete run\n    # 2) We have spent at least one full SH bracket worth of fidelity\n    # 3) There is at least one rung with eta evaluations to get the top 1/eta configs\n    completed_rungs = completed.index.get_level_values(\"rung\")\n    one_complete_run_at_max_rung = (completed_rungs == max_rung).any()\n\n    # For SH bracket cost, we include the fact we can continue runs,\n    # i.e. resources for rung 2 discounts the cost of evaluating to rung 1,\n    # only counting the difference in fidelity cost between rung 2 and rung 1.\n    cost_per_rung = {\n        i: rung_to_fid[i] - rung_to_fid.get(i - 1, 0) for i in rung_to_fid\n    }\n\n    cost_of_one_sh_bracket = sum(rung_sizes[r] * cost_per_rung[r] for r in rung_sizes)\n    current_cost_used = sum(r * cost_per_rung[r] for r in completed_rungs)\n    spent_one_sh_bracket_worth_of_fidelity = (\n        current_cost_used &gt;= cost_of_one_sh_bracket\n    )\n\n    # Check that there is at least rung with `eta` evaluations\n    rung_counts = completed.groupby(\"rung\").size()\n    any_rung_with_eta_evals = (rung_counts == self.eta).any()\n\n    # If the conditions are not met, we sample from the prior or randomly depending on\n    # the geometrically distributed prior and uniform weights\n    if (\n        one_complete_run_at_max_rung is False\n        or spent_one_sh_bracket_worth_of_fidelity is False\n        or any_rung_with_eta_evals is False\n    ):\n        policy = np.random.choice([\"prior\", \"random\"], p=[w_prior, w_random])\n        match policy:\n            case \"prior\":\n                return self._sample_prior()\n            case \"random\":\n                return self._sample_random()\n            case _:\n                raise RuntimeError(f\"Unknown policy: {policy}\")\n\n    # Otherwise, we now further split the `prior` weight into `(prior, inc)`\n\n    # 1. Select the top `1//eta` percent of configs at the highest rung supporting it\n    rungs_with_at_least_eta = rung_counts[rung_counts &gt;= self.eta].index  # type: ignore\n    rung_table: pd.DataFrame = completed[  # type: ignore\n        completed.index.get_level_values(\"rung\") == rungs_with_at_least_eta.max()\n    ]\n\n    K = len(rung_table) // self.eta\n    rung_table.nsmallest(K, columns=[\"perf\"])[\"config\"].tolist()\n\n    # 2. Get the global incumbent\n    inc_config = completed.loc[completed[\"perf\"].idxmin()][\"config\"]\n\n    # 3. Calculate a ratio score of how likely each of the top K configs are under\n    # TODO: [lum]: Here I am simply using fixed values.\n    #  Will maybe have to come up with a way to approximate the pdf for the top\n    # configs.\n    inc_ratio = 0.9\n    prior_ratio = 0.1\n\n    # 4. And finally, we distribute the original w_prior according to this ratio\n    w_inc = w_prior * inc_ratio\n    w_prior = w_prior * prior_ratio\n    assert np.isclose(w_prior + w_inc + w_random, 1.0)\n\n    # Now we use these weights to choose which sampling distribution to sample from\n    policy = np.random.choice(\n        [\"prior\", \"inc\", \"random\"],\n        p=[w_prior, w_inc, w_random],\n    )\n    match policy:\n        case \"prior\":\n            return self._sample_prior()\n        case \"random\":\n            return self._sample_random()\n        case \"inc\":\n            assert inc_config is not None\n            return self._mutate_inc(inc_config)\n    raise RuntimeError(f\"Unknown policy: {policy}\")\n</code></pre>"},{"location":"api/neps/optimizers/neps_random_search/","title":"Neps random search","text":"<p>This module implements a simple random search optimizer for a NePS pipeline. It samples configurations randomly from the pipeline's domain and environment values.</p>"},{"location":"api/neps/optimizers/neps_random_search/#neps.optimizers.neps_random_search.NePSComplexRandomSearch","title":"NePSComplexRandomSearch  <code>dataclass</code>","text":"<pre><code>NePSComplexRandomSearch(pipeline: Pipeline)\n</code></pre> <p>A complex random search optimizer for a NePS pipeline. It samples configurations randomly from the pipeline's domain and environment values, and also performs mutations and crossovers based on previous successful trials.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline to optimize, which should be a Pipeline object.</p> <p> TYPE: <code>Pipeline</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the pipeline is not a Pipeline object.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline to optimize, which should be a Pipeline object.</p> <p> TYPE: <code>Pipeline</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the pipeline is not a Pipeline object.</p> Source code in <code>neps\\optimizers\\neps_random_search.py</code> <pre><code>def __init__(self, pipeline: Pipeline):\n    \"\"\"Initialize the ComplexRandomSearch optimizer with a pipeline.\n\n    Args:\n        pipeline: The pipeline to optimize, which should be a Pipeline object.\n\n    Raises:\n        ValueError: If the pipeline is not a Pipeline object.\n    \"\"\"\n    self._pipeline = pipeline\n\n    self._environment_values = {}\n    fidelity_attrs = self._pipeline.fidelity_attrs\n    for fidelity_name, fidelity_obj in fidelity_attrs.items():\n        self._environment_values[fidelity_name] = fidelity_obj.max_value\n\n    self._random_sampler = RandomSampler(\n        predefined_samplings={},\n    )\n    self._try_always_priors_sampler = PriorOrFallbackSampler(\n        fallback_sampler=self._random_sampler,\n        prior_use_probability=1,\n    )\n    self._sometimes_priors_sampler = PriorOrFallbackSampler(\n        fallback_sampler=self._random_sampler,\n        prior_use_probability=0.1,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/neps_random_search/#neps.optimizers.neps_random_search.NePSComplexRandomSearch.__call__","title":"__call__","text":"<pre><code>__call__(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n    n: int | None = None,\n) -&gt; SampledConfig | list[SampledConfig]\n</code></pre> <p>Sample configurations randomly from the pipeline's domain and environment values, and also perform mutations and crossovers based on previous successful trials.</p> PARAMETER DESCRIPTION <code>trials</code> <p>A mapping of trial IDs to Trial objects, representing previous trials.</p> <p> TYPE: <code>Mapping[str, Trial]</code> </p> <code>budget_info</code> <p>The budget information for the optimization process.</p> <p> TYPE: <code>BudgetInfo | None</code> </p> <code>n</code> <p>The number of configurations to sample. If None, a single configuration will be sampled.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SampledConfig | list[SampledConfig]</code> <p>A SampledConfig object or a list of SampledConfig objects, depending     on the value of n.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the pipeline is not a Pipeline object or if the trials are not a valid mapping of trial IDs to Trial objects.</p> Source code in <code>neps\\optimizers\\neps_random_search.py</code> <pre><code>def __call__(\n    self,\n    trials: Mapping[str, trial_state.Trial],\n    budget_info: optimizer_state.BudgetInfo | None,\n    n: int | None = None,\n) -&gt; optimizer.SampledConfig | list[optimizer.SampledConfig]:\n    \"\"\"Sample configurations randomly from the pipeline's domain and environment\n    values, and also perform mutations and crossovers based on previous successful\n    trials.\n\n    Args:\n        trials: A mapping of trial IDs to Trial objects, representing previous\n            trials.\n        budget_info: The budget information for the optimization process.\n        n: The number of configurations to sample. If None, a single configuration\n            will be sampled.\n\n    Returns:\n        A SampledConfig object or a list of SampledConfig objects, depending\n            on the value of n.\n\n    Raises:\n        ValueError: If the pipeline is not a Pipeline object or if the trials are\n            not a valid mapping of trial IDs to Trial objects.\n    \"\"\"\n    n_prev_trials = len(trials)\n    n_requested = 1 if n is None else n\n    return_single = n is None\n\n    random_pipelines = [\n        resolve(\n            pipeline=self._pipeline,\n            domain_sampler=self._random_sampler,\n            environment_values=self._environment_values,\n        )\n        for _ in range(n_requested * 5)\n    ]\n    sometimes_priors_pipelines = [\n        resolve(\n            pipeline=self._pipeline,\n            domain_sampler=self._sometimes_priors_sampler,\n            environment_values=self._environment_values,\n        )\n        for _ in range(n_requested * 5)\n    ]\n\n    mutated_incumbents = []\n    crossed_over_incumbents = []\n\n    successful_trials: list[Trial] = list(\n        filter(\n            lambda trial: (\n                trial.report.reported_as == trial.State.SUCCESS\n                if trial.report is not None\n                else False\n            ),\n            trials.values(),\n        )\n    )\n    if len(successful_trials) &gt; 0:\n        n_top_trials = 5\n        top_trials = heapq.nsmallest(\n            n_top_trials,\n            successful_trials,\n            key=lambda trial: (\n                float(trial.report.objective_to_minimize)\n                if trial.report\n                and isinstance(trial.report.objective_to_minimize, float)\n                else float(\"inf\")\n            ),\n        )  # Will have up to `n_top_trials` items.\n\n        # Do some mutations.\n        for top_trial in top_trials:\n            top_trial_config = top_trial.config\n\n            # Mutate by resampling around some values of the original config.\n            mutated_incumbents += [\n                resolve(\n                    pipeline=self._pipeline,\n                    domain_sampler=MutatateUsingCentersSampler(\n                        predefined_samplings=top_trial_config,\n                        n_mutations=1,\n                    ),\n                    environment_values=self._environment_values,\n                )\n                for _ in range(n_requested * 5)\n            ]\n            mutated_incumbents += [\n                resolve(\n                    pipeline=self._pipeline,\n                    domain_sampler=MutatateUsingCentersSampler(\n                        predefined_samplings=top_trial_config,\n                        n_mutations=max(\n                            1, random.randint(1, int(len(top_trial_config) / 2))\n                        ),\n                    ),\n                    environment_values=self._environment_values,\n                )\n                for _ in range(n_requested * 5)\n            ]\n\n            # Mutate by completely forgetting some values of the original config.\n            mutated_incumbents += [\n                resolve(\n                    pipeline=self._pipeline,\n                    domain_sampler=MutateByForgettingSampler(\n                        predefined_samplings=top_trial_config,\n                        n_forgets=1,\n                    ),\n                    environment_values=self._environment_values,\n                )\n                for _ in range(n_requested * 5)\n            ]\n            mutated_incumbents += [\n                resolve(\n                    pipeline=self._pipeline,\n                    domain_sampler=MutateByForgettingSampler(\n                        predefined_samplings=top_trial_config,\n                        n_forgets=max(\n                            1, random.randint(1, int(len(top_trial_config) / 2))\n                        ),\n                    ),\n                    environment_values=self._environment_values,\n                )\n                for _ in range(n_requested * 5)\n            ]\n\n        # Do some crossovers.\n        if len(top_trials) &gt; 1:\n            for _ in range(n_requested * 3):\n                trial_1, trial_2 = random.sample(top_trials, k=2)\n\n                try:\n                    crossover_sampler = CrossoverByMixingSampler(\n                        predefined_samplings_1=trial_1.config,\n                        predefined_samplings_2=trial_2.config,\n                        prefer_first_probability=0.5,\n                    )\n                except CrossoverNotPossibleError:\n                    # A crossover was not possible for them. Do nothing.\n                    pass\n                else:\n                    crossed_over_incumbents.append(\n                        resolve(\n                            pipeline=self._pipeline,\n                            domain_sampler=crossover_sampler,\n                            environment_values=self._environment_values,\n                        ),\n                    )\n\n                try:\n                    crossover_sampler = CrossoverByMixingSampler(\n                        predefined_samplings_1=trial_2.config,\n                        predefined_samplings_2=trial_1.config,\n                        prefer_first_probability=0.5,\n                    )\n                except CrossoverNotPossibleError:\n                    # A crossover was not possible for them. Do nothing.\n                    pass\n                else:\n                    crossed_over_incumbents.append(\n                        resolve(\n                            pipeline=self._pipeline,\n                            domain_sampler=crossover_sampler,\n                            environment_values=self._environment_values,\n                        ),\n                    )\n\n    all_sampled_pipelines = [\n        *random_pipelines,\n        *sometimes_priors_pipelines,\n        *mutated_incumbents,\n        *crossed_over_incumbents,\n    ]\n\n    # Here we can have a model which picks from all the sampled pipelines.\n    # Currently, we just pick randomly from them.\n    chosen_pipelines = random.sample(all_sampled_pipelines, k=n_requested)\n\n    if n_prev_trials == 0:\n        # In this case, always include the prior pipeline.\n        prior_pipeline = resolve(\n            pipeline=self._pipeline,\n            domain_sampler=self._try_always_priors_sampler,\n            environment_values=self._environment_values,\n        )\n        chosen_pipelines[0] = prior_pipeline\n\n    return _prepare_sampled_configs(chosen_pipelines, n_prev_trials, return_single)\n</code></pre>"},{"location":"api/neps/optimizers/neps_random_search/#neps.optimizers.neps_random_search.NePSRandomSearch","title":"NePSRandomSearch  <code>dataclass</code>","text":"<pre><code>NePSRandomSearch(pipeline: Pipeline)\n</code></pre> <p>A simple random search optimizer for a NePS pipeline. It samples configurations randomly from the pipeline's domain and environment values.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline to optimize, which should be a Pipeline object.</p> <p> TYPE: <code>Pipeline</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the pipeline is not a Pipeline object.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline to optimize, which should be a Pipeline object.</p> <p> TYPE: <code>Pipeline</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the pipeline is not a Pipeline object.</p> Source code in <code>neps\\optimizers\\neps_random_search.py</code> <pre><code>def __init__(self, pipeline: Pipeline):\n    \"\"\"Initialize the RandomSearch optimizer with a pipeline.\n\n    Args:\n        pipeline: The pipeline to optimize, which should be a Pipeline object.\n\n    Raises:\n        ValueError: If the pipeline is not a Pipeline object.\n    \"\"\"\n    self._pipeline = pipeline\n\n    self._environment_values = {}\n    fidelity_attrs = self._pipeline.fidelity_attrs\n    for fidelity_name, fidelity_obj in fidelity_attrs.items():\n        self._environment_values[fidelity_name] = fidelity_obj.max_value\n\n    self._random_sampler = RandomSampler(predefined_samplings={})\n</code></pre>"},{"location":"api/neps/optimizers/neps_random_search/#neps.optimizers.neps_random_search.NePSRandomSearch.__call__","title":"__call__","text":"<pre><code>__call__(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n    n: int | None = None,\n) -&gt; SampledConfig | list[SampledConfig]\n</code></pre> <p>Sample configurations randomly from the pipeline's domain and environment values.</p> PARAMETER DESCRIPTION <code>trials</code> <p>A mapping of trial IDs to Trial objects, representing previous trials.</p> <p> TYPE: <code>Mapping[str, Trial]</code> </p> <code>budget_info</code> <p>The budget information for the optimization process.</p> <p> TYPE: <code>BudgetInfo | None</code> </p> <code>n</code> <p>The number of configurations to sample. If None, a single configuration will be sampled.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SampledConfig | list[SampledConfig]</code> <p>A SampledConfig object or a list of SampledConfig objects, depending     on the value of n.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the pipeline is not a Pipeline object or if the trials are not a valid mapping of trial IDs to Trial objects.</p> Source code in <code>neps\\optimizers\\neps_random_search.py</code> <pre><code>def __call__(\n    self,\n    trials: Mapping[str, trial_state.Trial],\n    budget_info: optimizer_state.BudgetInfo | None,\n    n: int | None = None,\n) -&gt; optimizer.SampledConfig | list[optimizer.SampledConfig]:\n    \"\"\"Sample configurations randomly from the pipeline's domain and environment\n    values.\n\n    Args:\n        trials: A mapping of trial IDs to Trial objects, representing previous\n            trials.\n        budget_info: The budget information for the optimization process.\n        n: The number of configurations to sample. If None, a single configuration\n            will be sampled.\n\n    Returns:\n        A SampledConfig object or a list of SampledConfig objects, depending\n            on the value of n.\n\n    Raises:\n        ValueError: If the pipeline is not a Pipeline object or if the trials are\n            not a valid mapping of trial IDs to Trial objects.\n    \"\"\"\n    n_prev_trials = len(trials)\n    n_requested = 1 if n is None else n\n    return_single = n is None\n\n    chosen_pipelines = [\n        resolve(\n            pipeline=self._pipeline,\n            domain_sampler=self._random_sampler,\n            environment_values=self._environment_values,\n        )\n        for _ in range(n_requested)\n    ]\n\n    return _prepare_sampled_configs(chosen_pipelines, n_prev_trials, return_single)\n</code></pre>"},{"location":"api/neps/optimizers/optimizer/","title":"Optimizer","text":"<p>Optimizer interface.</p> <p>By implementing the <code>AskFunction</code> protocol, you can inject your own optimizer into the neps runtime.</p> <pre><code>class MyOpt:\n\n    def __init__(self, space: SearchSpace, ...): ...\n\n    def __call__(\n        self,\n        trials: Mapping[str, Trial],\n        budget_info: BudgetInfo | None,\n        n: int | None = None,\n    ) -&gt; SampledConfig | list[SampledConfig]: ...\n\nneps.run(..., optimizer=MyOpt)\n\n# Or with optimizer hyperparameters\nneps.run(..., optimizer=(MyOpt, {\"a\": 1, \"b\": 2}))\n</code></pre>"},{"location":"api/neps/optimizers/optimizer/#neps.optimizers.optimizer.AskFunction","title":"AskFunction","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface to implement the ask of optimizer.</p>"},{"location":"api/neps/optimizers/optimizer/#neps.optimizers.optimizer.AskFunction.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n    n: int | None = None,\n) -&gt; SampledConfig | list[SampledConfig]\n</code></pre> <p>Sample a new configuration.</p> PARAMETER DESCRIPTION <code>trials</code> <p>All of the trials that are known about.</p> <p> TYPE: <code>Mapping[str, Trial]</code> </p> <code>budget_info</code> <p>information about the budget constraints.</p> <p> TYPE: <code>BudgetInfo | None</code> </p> <code>n</code> <p>The number of configurations to sample. If you do not support sampling multiple configurations at once, you should raise a <code>ValueError</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SampledConfig | list[SampledConfig]</code> <p>The sampled configuration(s)</p> Source code in <code>neps\\optimizers\\optimizer.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n    n: int | None = None,\n) -&gt; SampledConfig | list[SampledConfig]:\n    \"\"\"Sample a new configuration.\n\n    Args:\n        trials: All of the trials that are known about.\n        budget_info: information about the budget constraints.\n        n: The number of configurations to sample. If you do not support\n            sampling multiple configurations at once, you should raise\n            a `ValueError`.\n\n    Returns:\n        The sampled configuration(s)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/optimizers/optimizer/#neps.optimizers.optimizer.OptimizerInfo","title":"OptimizerInfo","text":"<p>               Bases: <code>TypedDict</code></p> <p>Information about the optimizer, usually used for serialization.</p>"},{"location":"api/neps/optimizers/optimizer/#neps.optimizers.optimizer.OptimizerInfo.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: Mapping[str, Any]\n</code></pre> <p>Additional information about the optimizer.</p> <p>Usually this will be the keyword arguments used to initialize the optimizer.</p>"},{"location":"api/neps/optimizers/optimizer/#neps.optimizers.optimizer.OptimizerInfo.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the optimizer.</p>"},{"location":"api/neps/optimizers/priorband/","title":"Priorband","text":"<p>Implements functionallity for the priorband sampling strategy.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler","title":"PriorBandSampler  <code>dataclass</code>","text":"<pre><code>PriorBandSampler(\n    parameters: Mapping[str, Parameter],\n    encoder: ConfigEncoder,\n    mutation_rate: float,\n    mutation_std: float,\n    eta: int,\n    early_stopping_rate: int,\n    fid_bounds: tuple[int, int] | tuple[float, float],\n)\n</code></pre> <p>A Sampler implementing the PriorBand algorithm for sampling.</p> <ul> <li>openreview.net/forum?id=uoiwugtpCH&amp;noteId=xECpK2WH6k</li> </ul>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.early_stopping_rate","title":"early_stopping_rate  <code>instance-attribute</code>","text":"<pre><code>early_stopping_rate: int\n</code></pre> <p>The early stopping rate to use for the SH bracket.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder: ConfigEncoder\n</code></pre> <p>The encoder to use for encoding and decoding configurations into tensors.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.eta","title":"eta  <code>instance-attribute</code>","text":"<pre><code>eta: int\n</code></pre> <p>The eta value to use for the SH bracket.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.fid_bounds","title":"fid_bounds  <code>instance-attribute</code>","text":"<pre><code>fid_bounds: tuple[int, int] | tuple[float, float]\n</code></pre> <p>The fidelity bounds.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.mutation_rate","title":"mutation_rate  <code>instance-attribute</code>","text":"<pre><code>mutation_rate: float\n</code></pre> <p>The mutation rate to use when sampling from the incumbent distribution.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.mutation_std","title":"mutation_std  <code>instance-attribute</code>","text":"<pre><code>mutation_std: float\n</code></pre> <p>The mutation deviation to use when sampling from the incumbent distribution.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: Mapping[str, Parameter]\n</code></pre> <p>The parameters to consider.</p>"},{"location":"api/neps/optimizers/priorband/#neps.optimizers.priorband.PriorBandSampler.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    table: DataFrame, rung: int\n) -&gt; dict[str, Any]\n</code></pre> <p>Samples a configuration using the PriorBand algorithm.</p> PARAMETER DESCRIPTION <code>table</code> <p>The table of all the trials that have been run.</p> <p> TYPE: <code>DataFrame</code> </p> <code>rung</code> <p>The rung to sample for.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The sampled configuration.</p> Source code in <code>neps\\optimizers\\priorband.py</code> <pre><code>def sample_config(self, table: pd.DataFrame, rung: int) -&gt; dict[str, Any]:\n    \"\"\"Samples a configuration using the PriorBand algorithm.\n\n    Args:\n        table: The table of all the trials that have been run.\n        rung: The rung to sample for.\n\n    Returns:\n        The sampled configuration.\n    \"\"\"\n    rung_to_fid, rung_sizes = brackets.calculate_sh_rungs(\n        bounds=self.fid_bounds,\n        eta=self.eta,\n        early_stopping_rate=self.early_stopping_rate,\n    )\n    max_rung = max(rung_sizes)\n\n    prior_dist = Prior.from_parameters(self.parameters)\n\n    # Below we will follow the \"geomtric\" spacing\n    w_random = 1 / (1 + self.eta**rung)\n    w_prior = 1 - w_random\n\n    completed: pd.DataFrame = table[table[\"perf\"].notna()]  # type: ignore\n\n    # To see if we activate incumbent sampling, we check:\n    # 1) We have at least one fully complete run\n    # 2) We have spent at least one full SH bracket worth of fidelity\n    # 3) There is at least one rung with eta evaluations to get the top 1/eta configs\n    completed_rungs = completed.index.get_level_values(\"rung\")\n    one_complete_run_at_max_rung = (completed_rungs == max_rung).any()\n\n    # For SH bracket cost, we include the fact we can continue runs,\n    # i.e. resources for rung 2 discounts the cost of evaluating to rung 1,\n    # only counting the difference in fidelity cost between rung 2 and rung 1.\n    cost_per_rung = {\n        i: rung_to_fid[i] - rung_to_fid.get(i - 1, 0) for i in rung_to_fid\n    }\n\n    cost_of_one_sh_bracket = sum(rung_sizes[r] * cost_per_rung[r] for r in rung_sizes)\n    current_cost_used = sum(r * cost_per_rung[r] for r in completed_rungs)\n    spent_one_sh_bracket_worth_of_fidelity = (\n        current_cost_used &gt;= cost_of_one_sh_bracket\n    )\n\n    # Check that there is at least rung with `eta` evaluations\n    rung_counts = completed.groupby(\"rung\").size()\n    any_rung_with_eta_evals = (rung_counts == self.eta).any()\n\n    # If the conditions are not met, we sample from the prior or randomly depending on\n    # the geometrically distributed prior and uniform weights\n    if (\n        one_complete_run_at_max_rung is False\n        or spent_one_sh_bracket_worth_of_fidelity is False\n        or any_rung_with_eta_evals is False\n    ):\n        policy = np.random.choice([\"prior\", \"random\"], p=[w_prior, w_random])\n        match policy:\n            case \"prior\":\n                config = prior_dist.sample_config(to=self.encoder)\n            case \"random\":\n                _sampler = Sampler.uniform(ndim=self.encoder.ndim)\n                config = _sampler.sample_config(to=self.encoder)\n\n        return config\n\n    # Otherwise, we now further split the `prior` weight into `(prior, inc)`\n\n    # 1. Select the top `1//eta` percent of configs at the highest rung supporting it\n    rungs_with_at_least_eta = rung_counts[rung_counts &gt;= self.eta].index  # type: ignore\n    rung_table: pd.DataFrame = completed[  # type: ignore\n        completed.index.get_level_values(\"rung\") == rungs_with_at_least_eta.max()\n    ]\n\n    K = len(rung_table) // self.eta\n    top_k_configs = rung_table.nsmallest(K, columns=[\"perf\"])[\"config\"].tolist()\n\n    # 2. Get the global incumbent, and build a prior distribution around it\n    inc = completed.loc[completed[\"perf\"].idxmin()][\"config\"]\n    inc_dist = Prior.from_parameters(self.parameters, center_values=inc)\n\n    # 3. Calculate a ratio score of how likely each of the top K configs are under\n    # the prior and inc distribution, weighing them by their position in the top K\n    weights = torch.arange(K, 0, -1)\n\n    top_k_pdf_inc = inc_dist.pdf_configs(top_k_configs, frm=self.encoder)  # type: ignore\n    top_k_pdf_prior = prior_dist.pdf_configs(top_k_configs, frm=self.encoder)  # type: ignore\n\n    unnormalized_inc_score = (weights * top_k_pdf_inc).sum()\n    unnormalized_prior_score = (weights * top_k_pdf_prior).sum()\n    total_score = unnormalized_inc_score + unnormalized_prior_score\n\n    inc_ratio = float(unnormalized_inc_score / total_score)\n    prior_ratio = float(unnormalized_prior_score / total_score)\n\n    # 4. And finally, we distribute the original w_prior according to this ratio\n    w_inc = w_prior * inc_ratio\n    w_prior = w_prior * prior_ratio\n    assert np.isclose(w_prior + w_inc + w_random, 1.0)\n\n    # Now we use these weights to choose which sampling distribution to sample from\n    policy = np.random.choice(\n        [\"prior\", \"inc\", \"random\"],\n        p=[w_prior, w_inc, w_random],\n    )\n    match policy:\n        case \"prior\":\n            return prior_dist.sample_config(to=self.encoder)\n        case \"random\":\n            _sampler = Sampler.uniform(ndim=self.encoder.ndim)\n            return _sampler.sample_config(to=self.encoder)\n        case \"inc\":\n            assert inc is not None\n            return mutate_config(\n                inc,\n                parameters=self.parameters,\n                mutation_rate=self.mutation_rate,\n                std=self.mutation_std,\n                seed=None,\n            )\n\n    raise RuntimeError(f\"Unknown policy: {policy}\")\n</code></pre>"},{"location":"api/neps/optimizers/random_search/","title":"Random search","text":""},{"location":"api/neps/optimizers/random_search/#neps.optimizers.random_search.RandomSearch","title":"RandomSearch  <code>dataclass</code>","text":"<pre><code>RandomSearch(\n    space: SearchSpace,\n    encoder: ConfigEncoder,\n    sampler: Sampler,\n)\n</code></pre> <p>A simple random search optimizer.</p>"},{"location":"api/neps/optimizers/acquisition/cost_cooling/","title":"Cost cooling","text":""},{"location":"api/neps/optimizers/acquisition/pibo/","title":"Pibo","text":""},{"location":"api/neps/optimizers/acquisition/pibo/#neps.optimizers.acquisition.pibo--copyright-c-meta-platforms-inc-and-affiliates","title":"Copyright (c) Meta Platforms, Inc. and affiliates.","text":""},{"location":"api/neps/optimizers/acquisition/pibo/#neps.optimizers.acquisition.pibo--_1","title":"Pibo","text":""},{"location":"api/neps/optimizers/acquisition/pibo/#neps.optimizers.acquisition.pibo--this-source-code-is-licensed-under-the-mit-license-found-in-the","title":"This source code is licensed under the MIT license found in the","text":""},{"location":"api/neps/optimizers/acquisition/pibo/#neps.optimizers.acquisition.pibo--license-file-in-the-root-directory-of-this-source-tree","title":"LICENSE file in the root directory of this source tree.","text":"<p>Prior-Guided Acquisition Functions</p> <p>References:</p> <p>.. [Hvarfner2022]     C. Hvarfner, D. Stoll, A. Souza, M. Lindauer, F. Hutter, L. Nardi. PiBO:     Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization.     ICLR 2022.</p>"},{"location":"api/neps/optimizers/acquisition/weighted_acquisition/","title":"Weighted acquisition","text":"<p>This module provides most of the functionality we require in NePS for now, i.e., we need the ability to apply an arbitrary weight to an acquisition function.</p> <p>I spent some time understanding the meaning of the various dimensions of botorch/gpytorch.</p> <p>The two primary dimensions to consider are:</p> <ul> <li><code>d</code> - The dimensionality of the design space, i.e. how many hyperparameters.</li> <li><code>batch</code> - The number of independent evaluations to make, i.e. how many times to     evaluate the acquisition function.</li> </ul> <p>There are two extra dimensions which are special cases and need to be accounted for.</p> <ul> <li> <p><code>q</code> - Comes from the <code>qXXX</code> variants of acquisition, these will add an extra dimension     <code>q</code> to each <code>batch</code>, where instead of a <code>batch</code> representing a single config to get     the acquisition of, we might instead be getting the acquisition of 5 configs together,     representing the joint utility of evaluating these 5 configs, relative to other sets     of 5 configs. This dimension is reduced away in the final step of the acquisition     when suggesting which set of group of 5 configs to suggest.</p> </li> <li> <p><code>mc_samples</code> - Comes from the <code>SampleReducdingXXX</code> variants of acquisition, will add an     extra dimension <code>mc_samples</code> which represent the amount of Monte Carlo samples used     to estimate the acquisition. These will eventually be reduced away but are present     in the intermediate steps. These variants also seem to have <code>q</code> variants implicitly     and so you are likely to see the <code>q</code> dimension whever you see the <code>mc_samples</code>     dimension, even if it is just <code>q=1</code>.</p> </li> <li> <p><code>m</code> - The number of objectives in the multi-objective case. We will     specifically ignore this for now, however it exists as the last dimension (after <code>d</code>)     and is the first to be reduced away. They are also used in constrainted settings     which we will also ignore for now.</p> </li> </ul> <p>The most expanded tensor shape is the following, with the usual order of reduction being the following below. If you are not using a SamplingReducing variant, you will not see <code>mc_samples</code> and if you are not using a <code>q</code> variant, you will not see <code>q</code>. The simplest case then being <code>acq(tensor: batch x d)</code>.</p> <ul> <li><code>batch x q x d</code>.         reduce(..., d) = Config -&gt; Single number  (!!!Acq applies here!!!)</li> <li><code>batch x q</code>.         expand(mc_samples , ...) = MC Sampling from posterior (I think)</li> <li><code>mc_samples x batch x q</code>.         reduce(..., q) = Joint-Config-Group -&gt; Single number.</li> <li><code>mc_samples x batch</code>         reduce(mc_samples, ...) = MC-samples -&gt; statistical estimate</li> <li><code>batch</code></li> </ul> <p>Finally we get out a batch of values we can argmax over, used to index into either a single configuration or a single index into a joint-group of <code>q</code> configurations.</p> <p>Tip</p> <p>The <code>mc_samples</code> is not of concern to the <code>WeightedAcquisition</code> below, and broadcasting can be used, as a result, the <code>apply_weight</code> function only needs to be able to handle:</p> <ul> <li>(X: batch x q x d, acq_values: batch x q, acq: A) -&gt; batch x q</li> </ul> <p>If utilizing the configurations <code>X</code> for weighting, you effectively will want to reduce the <code>d</code> dimension.</p> <p>As a result of this, acquisition functions need to be able to handle arbitrary dimensions and act accordingly.</p> <p>This module mostly follows the structure of the <code>PriorGuidedAcquisitionFunction</code> which weights the acquisition function by a prior.</p> <ul> <li>botorch.org/api/_modules/botorch/acquisition/prior_guided.html#PriorGuidedAcquisitionFunction</li> </ul> <p>We use this to create a more generic <code>WeightedAcquisition</code> which follows the required structure to make new weightings easier to implement, but also to serve as an educational reference.</p>"},{"location":"api/neps/optimizers/acquisition/weighted_acquisition/#neps.optimizers.acquisition.weighted_acquisition.WeightedAcquisition","title":"WeightedAcquisition","text":"<pre><code>WeightedAcquisition(\n    acq: A,\n    apply_weight: Callable[[Tensor, Tensor, A], Tensor],\n)\n</code></pre> <p>               Bases: <code>AcquisitionFunction</code></p> <p>Class for weighting acquisition functions.</p> <p>Please see module docstring for more information.</p> PARAMETER DESCRIPTION <code>acq</code> <p>The base acquisition function.</p> <p> TYPE: <code>A</code> </p> <code>apply_weight</code> <p>A function that takes the acquisition function values, the design points and the acquisition function itself and returns the weighted acquisition function values.</p> <p>Please see the module docstring for more information on the dimensions and how to handle them.</p> <p> TYPE: <code>Callable[[Tensor, Tensor, A], Tensor]</code> </p> Source code in <code>neps\\optimizers\\acquisition\\weighted_acquisition.py</code> <pre><code>def __init__(\n    self,\n    acq: A,\n    apply_weight: Callable[[Tensor, Tensor, A], Tensor],\n) -&gt; None:\n    \"\"\"Initialize the weighted acquisition function.\n\n    Args:\n        acq: The base acquisition function.\n        apply_weight: A function that takes the acquisition function values, the\n            design points and the acquisition function itself and returns the\n            weighted acquisition function values.\n\n            Please see the module docstring for more information on the dimensions\n            and how to handle them.\n    \"\"\"\n    super().__init__(model=acq.model)\n    # NOTE: We remove the X_pending from the base acquisition function as we will get\n    # it in our own forward with `@concatenate_pending_points` and pass that forward.\n    # This avoids possible duplicates. Also important to explicitly set it to None\n    # even if it does not exist as otherwise the attribute does not exists -_-\n    if (X_pending := getattr(acq, \"X_pending\", None)) is not None:\n        acq.set_X_pending(None)\n        self.set_X_pending(X_pending)\n    else:\n        acq.set_X_pending(None)\n        self.set_X_pending(None)\n\n    self.apply_weight = apply_weight\n    self.acq = acq\n    self._log = acq._log\n</code></pre>"},{"location":"api/neps/optimizers/acquisition/weighted_acquisition/#neps.optimizers.acquisition.weighted_acquisition.WeightedAcquisition.forward","title":"forward","text":"<pre><code>forward(X: Tensor) -&gt; Tensor\n</code></pre> <p>Evaluate a weighted acquisition function on the candidate set X.</p> PARAMETER DESCRIPTION <code>X</code> <p>A tensor of size <code>batch_shape x q x d</code>-dim tensor of <code>q</code> <code>d</code>-dim design points.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor with the <code>d</code> dimension reduced away, representing the weighted acquisition function values at the given design points <code>X</code>.</p> Source code in <code>neps\\optimizers\\acquisition\\weighted_acquisition.py</code> <pre><code>@concatenate_pending_points  # type: ignore\n@t_batch_mode_transform()  # type: ignore\ndef forward(self, X: Tensor) -&gt; Tensor:\n    \"\"\"Evaluate a weighted acquisition function on the candidate set X.\n\n    Args:\n        X: A tensor of size `batch_shape x q x d`-dim tensor of `q` `d`-dim\n            design points.\n\n    Returns:\n        A tensor with the `d` dimension reduced away, representing the\n        weighted acquisition function values at the given design points `X`.\n    \"\"\"\n    if isinstance(self.acq, SampleReducingMCAcquisitionFunction):\n        # shape: mc_samples x batch x q-candidates\n        acq_values = self.acq._non_reduced_forward(X)\n        weighted_acq_values = self.apply_weight(acq_values, X, self.acq)\n        q_reduced_acq = self.acq._q_reduction(weighted_acq_values)\n        sample_reduced_acq = self.acq._sample_reduction(q_reduced_acq)\n        return sample_reduced_acq.squeeze(-1)  # type: ignore\n\n    # shape: batch x q-candidates\n    acq_values = self.acq(X).unsqueeze(-1)\n    weighted_acq_values = self.apply_weight(acq_values, X, self.acq)\n    return weighted_acq_values.squeeze(-1)  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/models/ftpfn/","title":"Ftpfn","text":""},{"location":"api/neps/optimizers/models/ftpfn/#neps.optimizers.models.ftpfn.FTPFNSurrogate","title":"FTPFNSurrogate","text":"<pre><code>FTPFNSurrogate(\n    target_path: Path | None = None,\n    version: str = \"0.0.1\",\n    device: device | None = None,\n)\n</code></pre> <p>Wrapper around the IfBO model.</p> Source code in <code>neps\\optimizers\\models\\ftpfn.py</code> <pre><code>def __init__(\n    self,\n    target_path: Path | None = None,\n    version: str = \"0.0.1\",\n    device: torch.device | None = None,\n):\n    if target_path is None:\n        # TODO: We also probably want to link this to the actual root directory\n        # or some shared directory between runs as relying on the path of the initial\n        # python invocation is likely to lead to issues somewhere.\n        # TODO: ifbo support for windows has issues with decompression\n        # We basically just do the same thing they do but manually\n        target_path = _download_workaround_for_ifbo_issue_10(target_path, version)\n\n    key = (str(target_path), version)\n    ftpfn = _CACHED_FTPFN_MODEL.get(key)\n    if ftpfn is None:\n        ftpfn = FTPFN(target_path=target_path, version=version, device=device)\n        _CACHED_FTPFN_MODEL[key] = ftpfn\n\n    self.ftpfn = ftpfn\n    self.device = self.ftpfn.device\n</code></pre>"},{"location":"api/neps/optimizers/models/ftpfn/#neps.optimizers.models.ftpfn.encode_ftpfn","title":"encode_ftpfn","text":"<pre><code>encode_ftpfn(\n    trials: Mapping[str, Trial],\n    fid: tuple[str, Integer | Float],\n    budget_domain: Domain,\n    encoder: ConfigEncoder,\n    *,\n    device: device | None = None,\n    dtype: dtype = FTPFN_DTYPE,\n    error_value: float = 0.0,\n    pending_value: float = nan\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Encode the trials into a format that the FTPFN model can understand.</p> <p>Pending trials</p> <p>For trials which do not have a loss reported yet, they are considered pending. By default this is torch.nan and we recommend fantasizing these values.</p> <p>Error values</p> <p>The FTPFN model requires that all loss values lie in the interval [0, 1]. By default, using the value of <code>error_value=0.0</code>, we encode crashed configurations as having an error value of 0.</p> PARAMETER DESCRIPTION <code>trials</code> <p>The trials to encode</p> <p> TYPE: <code>Mapping[str, Trial]</code> </p> <code>encoder</code> <p>The encoder to use</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>budget_domain</code> <p>The domain to use for the budgets of the FTPFN</p> <p> TYPE: <code>Domain</code> </p> <code>device</code> <p>The device to use</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype to use</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>FTPFN_DTYPE</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor]</code> <p>The encoded trials and their corresponding scores</p> Source code in <code>neps\\optimizers\\models\\ftpfn.py</code> <pre><code>def encode_ftpfn(\n    trials: Mapping[str, Trial],\n    fid: tuple[str, Integer | Float],\n    budget_domain: Domain,\n    encoder: ConfigEncoder,\n    *,\n    device: torch.device | None = None,\n    dtype: torch.dtype = FTPFN_DTYPE,\n    error_value: float = 0.0,\n    pending_value: float = torch.nan,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encode the trials into a format that the FTPFN model can understand.\n\n    !!! warning \"Pending trials\"\n\n        For trials which do not have a loss reported yet, they are considered pending.\n        By default this is torch.nan and we recommend fantasizing these values.\n\n    !!! warning \"Error values\"\n\n        The FTPFN model requires that all loss values lie in the interval [0, 1].\n        By default, using the value of `error_value=0.0`, we encode crashed configurations\n        as having an error value of 0.\n\n    Args:\n        trials: The trials to encode\n        encoder: The encoder to use\n        budget_domain: The domain to use for the budgets of the FTPFN\n        device: The device to use\n        dtype: The dtype to use\n\n    Returns:\n        The encoded trials and their corresponding **scores**\n    \"\"\"\n    # Select all trials which have something we can actually use for modelling\n    # The absence of a report signifies pending\n    selected = dict(trials.items())\n    fidelity_name, fidelity = fid\n\n    assert 0 &lt;= error_value &lt;= 1\n    train_configs = encoder.encode(\n        [t.config for t in selected.values()], device=device, dtype=dtype\n    )\n    ids = torch.tensor(\n        [int(config_id.split(\"_\", maxsplit=1)[0]) for config_id in selected],\n        device=device,\n        dtype=dtype,\n    )\n    # PFN uses `0` id for test configurations\n    ids = ids + 1\n\n    train_fidelities = torch.tensor(\n        [t.config[fidelity_name] for t in selected.values()],\n        device=device,\n        dtype=dtype,\n    )\n    train_max_cost_total = budget_domain.cast(\n        train_fidelities, frm=fidelity.domain, dtype=dtype\n    )\n\n    # TODO: Document that it's on the user to ensure these are already all bounded\n    # We could possibly include some bounded transform to assert this.\n    minimize_ys = torch.tensor(\n        [\n            pending_value\n            if trial.report is None\n            else (\n                error_value\n                if trial.report.objective_to_minimize is None\n                else trial.report.objective_to_minimize\n            )\n            for trial in trials.values()\n        ],\n        device=device,\n        dtype=dtype,\n    )\n    if minimize_ys.max() &gt; 1 or minimize_ys.min() &lt; 0:\n        raise RuntimeError(\n            \"ifBO requires that all loss values reported lie in the interval [0, 1]\"\n            \" but recieved loss value outside of that range!\"\n            f\"\\n{minimize_ys}\"\n        )\n    maximize_ys = 1 - minimize_ys\n    x_train = torch.cat(\n        [ids.unsqueeze(1), train_max_cost_total.unsqueeze(1), train_configs], dim=1\n    )\n    return x_train, maximize_ys\n</code></pre>"},{"location":"api/neps/optimizers/models/gp/","title":"Gp","text":"<p>Gaussian Process models for Bayesian Optimization.</p>"},{"location":"api/neps/optimizers/models/gp/#neps.optimizers.models.gp.GPEncodedData","title":"GPEncodedData  <code>dataclass</code>","text":"<pre><code>GPEncodedData(\n    x: Tensor,\n    y: Tensor,\n    cost: Tensor | None = None,\n    x_pending: Tensor | None = None,\n)\n</code></pre> <p>Tensor data of finished configurations.</p>"},{"location":"api/neps/optimizers/models/gp/#neps.optimizers.models.gp.default_categorical_kernel","title":"default_categorical_kernel","text":"<pre><code>default_categorical_kernel(\n    N: int, active_dims: tuple[int, ...] | None = None\n) -&gt; ScaleKernel\n</code></pre> <p>Default Categorical kernel for the GP.</p> Source code in <code>neps\\optimizers\\models\\gp.py</code> <pre><code>def default_categorical_kernel(\n    N: int,\n    active_dims: tuple[int, ...] | None = None,\n) -&gt; ScaleKernel:\n    \"\"\"Default Categorical kernel for the GP.\"\"\"\n    # Following BoTorches implementation of the MixedSingleTaskGP\n    return ScaleKernel(\n        CategoricalKernel(\n            ard_num_dims=N,\n            active_dims=active_dims,\n            lengthscale_constraint=gpytorch.constraints.GreaterThan(1e-6),\n        )\n    )\n</code></pre>"},{"location":"api/neps/optimizers/models/gp/#neps.optimizers.models.gp.encode_trials_for_gp","title":"encode_trials_for_gp","text":"<pre><code>encode_trials_for_gp(\n    trials: Mapping[str, Trial],\n    parameters: Mapping[str, Parameter],\n    *,\n    encoder: ConfigEncoder | None = None,\n    device: device | None = None\n) -&gt; tuple[GPEncodedData, ConfigEncoder]\n</code></pre> <p>Encode the trials for use in a GP.</p> PARAMETER DESCRIPTION <code>trials</code> <p>The trials to encode.</p> <p> TYPE: <code>Mapping[str, Trial]</code> </p> <code>encoder</code> <p>The encoder to use. If <code>None</code>, one will be created.</p> <p> TYPE: <code>ConfigEncoder | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device to use.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[GPEncodedData, ConfigEncoder]</code> <p>The encoded data and the encoder</p> Source code in <code>neps\\optimizers\\models\\gp.py</code> <pre><code>def encode_trials_for_gp(\n    trials: Mapping[str, Trial],\n    parameters: Mapping[str, Parameter],\n    *,\n    encoder: ConfigEncoder | None = None,\n    device: torch.device | None = None,\n) -&gt; tuple[GPEncodedData, ConfigEncoder]:\n    \"\"\"Encode the trials for use in a GP.\n\n    Args:\n        trials: The trials to encode.\n        encoder: The encoder to use. If `None`, one will be created.\n        device: The device to use.\n\n    Returns:\n        The encoded data and the encoder\n    \"\"\"\n    train_configs: list[Mapping[str, Any]] = []\n    train_losses: list[float] | list[Sequence[float]] = []\n    train_costs: list[float] = []\n    pending_configs: list[Mapping[str, Any]] = []\n\n    if encoder is None:\n        encoder = ConfigEncoder.from_parameters(parameters)\n\n    for trial in trials.values():\n        if trial.report is None:\n            pending_configs.append(trial.config)\n            continue\n\n        train_configs.append(trial.config)\n\n        objective_to_minimize = trial.report.objective_to_minimize\n        train_losses.append(\n            torch.nan if objective_to_minimize is None else objective_to_minimize\n        )\n\n        cost = trial.report.cost\n        train_costs.append(torch.nan if cost is None else cost)\n\n    x_train = encoder.encode(train_configs, device=device)\n    y_train = torch.tensor(train_losses, dtype=torch.float64, device=device)\n\n    # OPTIM: The issue here is that the error could be a bug, in which case\n    # if the user restarts, we don't want to too heavily penalize that area.\n    # On the flip side, if the configuration is actually what's causing the\n    # crashes, then we just want to ensure that the GP is discouraged from\n    # visiting that area. Setting to the median also ensures that the GP does\n    # not end up with a highly skewed function apprxoimation, for example,\n    # setting tiny lengthscales, to ensure it can model the sharp change\n    # in the performance around the crashed config.\n    fill_value = torch.nanmedian(y_train).item()\n    y_train = torch.nan_to_num(y_train, nan=fill_value)\n\n    cost_train = torch.tensor(train_costs, dtype=torch.float64, device=device)\n    if len(pending_configs) &gt; 0:\n        x_pending = encoder.encode(pending_configs, device=device)\n    else:\n        x_pending = None\n\n    data = GPEncodedData(x=x_train, y=y_train, cost=cost_train, x_pending=x_pending)\n    return data, encoder\n</code></pre>"},{"location":"api/neps/optimizers/models/gp/#neps.optimizers.models.gp.fit_and_acquire_from_gp","title":"fit_and_acquire_from_gp","text":"<pre><code>fit_and_acquire_from_gp(\n    *,\n    gp: SingleTaskGP,\n    x_train: Tensor,\n    encoder: ConfigEncoder,\n    acquisition: AcquisitionFunction,\n    prior: Prior | None = None,\n    pibo_exp_term: float | None = None,\n    cost_gp: SingleTaskGP | None = None,\n    costs: Tensor | None = None,\n    cost_percentage_used: float | None = None,\n    costs_on_log_scale: bool = True,\n    seed: int | None = None,\n    n_candidates_required: int | None = None,\n    num_restarts: int = 20,\n    n_initial_start_points: int = 256,\n    maximum_allowed_categorical_combinations: int = 30,\n    fixed_acq_features: dict[str, Any] | None = None,\n    acq_options: Mapping[str, Any] | None = None,\n    hide_warnings: bool = False\n) -&gt; Tensor\n</code></pre> <p>Acquire the next configuration to evaluate using a GP.</p> <p>Please see the following for:</p> <ul> <li>Making a GP to pass in:     <code>make_default_single_obj_gp()</code></li> <li>Encoding configurations:     <code>encode_trials_for_gp()</code></li> </ul> PARAMETER DESCRIPTION <code>gp</code> <p>The GP model to use.</p> <p> TYPE: <code>SingleTaskGP</code> </p> <code>x_train</code> <p>The encoded configurations that have already been evaluated</p> <p> TYPE: <code>Tensor</code> </p> <code>encoder</code> <p>The encoder used for encoding the configurations</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>acquisition</code> <p>The acquisition function to use.</p> <p>A good default is <code>qLogNoisyExpectedImprovement</code> which can handle pending configurations gracefully without fantasization.</p> <p> TYPE: <code>AcquisitionFunction</code> </p> <code>prior</code> <p>The prior to use over configurations. If this is provided, the acquisition function will be further weighted using the piBO acquisition.</p> <p> TYPE: <code>Prior | None</code> DEFAULT: <code>None</code> </p> <code>pibo_exp_term</code> <p>The exponential term for the piBO acquisition. If <code>None</code> is provided, one will be estimated.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>costs</code> <p>The costs of evaluating the configurations. If this is provided, then a secondary GP will be used to estimate the cost of a given configuration and factor into the weighting during the acquisiton of a new configuration.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>cost_percentage_used</code> <p>The percentage of the budget used so far. This is used to determine the strength of the cost cooling. Should be between 0 and 1. Must be provided if costs is provided.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>costs_on_log_scale</code> <p>Whether the costs are on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>encoder</code> <p>The encoder used for encoding the configurations</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>fixed_acq_features</code> <p>The features to fix to a certain value during acquisition.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>n_candidates_required</code> <p>The number of candidates to return. If left as <code>None</code>, only the best candidate will be returned. Otherwise a list of candidates will be returned.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>num_restarts</code> <p>The number of restarts to use during optimization.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>n_initial_start_points</code> <p>The number of initial start points to use during optimization.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>maximum_allowed_categorical_combinations</code> <p>The maximum number of categorical combinations to allow. If the number of combinations exceeds this, an error will be raised.</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>acq_options</code> <p>Additional options to pass to the botorch <code>optimizer_acqf</code> function.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>hide_warnings</code> <p>Whether to hide numerical warnings issued during GP routines.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The encoded next configuration(s) to evaluate. Use the encoder you provided to decode the configuration.</p> Source code in <code>neps\\optimizers\\models\\gp.py</code> <pre><code>def fit_and_acquire_from_gp(\n    *,\n    gp: SingleTaskGP,\n    x_train: torch.Tensor,\n    encoder: ConfigEncoder,\n    acquisition: AcquisitionFunction,\n    prior: Prior | None = None,\n    pibo_exp_term: float | None = None,\n    cost_gp: SingleTaskGP | None = None,\n    costs: torch.Tensor | None = None,\n    cost_percentage_used: float | None = None,\n    costs_on_log_scale: bool = True,\n    seed: int | None = None,\n    n_candidates_required: int | None = None,\n    num_restarts: int = 20,\n    n_initial_start_points: int = 256,\n    maximum_allowed_categorical_combinations: int = 30,\n    fixed_acq_features: dict[str, Any] | None = None,\n    acq_options: Mapping[str, Any] | None = None,\n    hide_warnings: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Acquire the next configuration to evaluate using a GP.\n\n    Please see the following for:\n\n    * Making a GP to pass in:\n        [`make_default_single_obj_gp()`][neps.optimizers.models.gp.make_default_single_obj_gp]\n    * Encoding configurations:\n        [`encode_trials_for_gp()`][neps.optimizers.models.gp.encode_trials_for_gp]\n\n    Args:\n        gp: The GP model to use.\n        x_train: The encoded configurations that have already been evaluated\n        encoder: The encoder used for encoding the configurations\n        acquisition: The acquisition function to use.\n\n            A good default is `qLogNoisyExpectedImprovement` which can\n            handle pending configurations gracefully without fantasization.\n\n        prior: The prior to use over configurations. If this is provided, the\n            acquisition function will be further weighted using the piBO acquisition.\n        pibo_exp_term: The exponential term for the piBO acquisition. If `None` is\n            provided, one will be estimated.\n        costs: The costs of evaluating the configurations. If this is provided,\n            then a secondary GP will be used to estimate the cost of a given\n            configuration and factor into the weighting during the acquisiton of a new\n            configuration.\n        cost_percentage_used: The percentage of the budget used so far. This is used to\n            determine the strength of the cost cooling. Should be between 0 and 1.\n            Must be provided if costs is provided.\n        costs_on_log_scale: Whether the costs are on a log scale.\n        encoder: The encoder used for encoding the configurations\n        seed: The seed to use.\n        fixed_acq_features: The features to fix to a certain value during acquisition.\n        n_candidates_required: The number of candidates to return. If left\n            as `None`, only the best candidate will be returned. Otherwise\n            a list of candidates will be returned.\n        num_restarts: The number of restarts to use during optimization.\n        n_initial_start_points: The number of initial start points to use during\n            optimization.\n        maximum_allowed_categorical_combinations: The maximum number of categorical\n            combinations to allow. If the number of combinations exceeds this, an error\n            will be raised.\n        acq_options: Additional options to pass to the botorch `optimizer_acqf` function.\n        hide_warnings: Whether to hide numerical warnings issued during GP routines.\n\n    Returns:\n        The encoded next configuration(s) to evaluate. Use the encoder you provided\n        to decode the configuration.\n    \"\"\"\n    if seed is not None:\n        raise NotImplementedError(\"Seed is not implemented yet for gps\")\n\n    fit_gpytorch_mll(ExactMarginalLogLikelihood(likelihood=gp.likelihood, model=gp))\n\n    if prior:\n        if pibo_exp_term is None:\n            raise ValueError(\n                \"If providing a prior, you must provide the `pibo_exp_term`.\"\n            )\n\n        acquisition = pibo_acquisition(\n            acquisition,\n            prior=prior,\n            prior_exponent=pibo_exp_term,\n            x_domain=encoder.domains,\n        )\n\n    if costs is not None:\n        if cost_percentage_used is None:\n            raise ValueError(\n                \"If providing costs, you must provide `cost_percentage_used`.\"\n            )\n\n        # We simply ignore missing costs when training the cost GP.\n        missing_costs = torch.isnan(costs)\n        if missing_costs.any():\n            raise ValueError(\n                \"Must have at least some configurations reported with a cost\"\n                \" if using costs with a GP.\"\n            )\n\n        if missing_costs.any():\n            not_missing_mask = ~missing_costs\n            x_train_cost = costs[not_missing_mask]\n            y_train_cost = x_train[not_missing_mask]\n        else:\n            x_train_cost = x_train\n            y_train_cost = costs\n\n        if costs_on_log_scale:\n            transform = ChainedOutcomeTransform(\n                log=Log(),\n                standardize=Standardize(m=1),\n            )\n        else:\n            transform = Standardize(m=1)\n\n        cost_gp = make_default_single_obj_gp(\n            x_train_cost,\n            y_train_cost,\n            encoder=encoder,\n            y_transform=transform,\n        )\n        fit_gpytorch_mll(\n            ExactMarginalLogLikelihood(likelihood=cost_gp.likelihood, model=cost_gp)\n        )\n        acquisition = cost_cooled_acq(\n            acq_fn=acquisition,\n            model=cost_gp,\n            used_max_cost_total_percentage=cost_percentage_used,\n        )\n\n    _n = n_candidates_required if n_candidates_required is not None else 1\n\n    candidates, _scores = optimize_acq(\n        acquisition,\n        encoder,\n        n_candidates_required=_n,\n        num_restarts=num_restarts,\n        n_intial_start_points=n_initial_start_points,\n        fixed_features=fixed_acq_features,\n        acq_options=acq_options,\n        maximum_allowed_categorical_combinations=maximum_allowed_categorical_combinations,\n        hide_warnings=hide_warnings,\n    )\n    return candidates\n</code></pre>"},{"location":"api/neps/optimizers/models/gp/#neps.optimizers.models.gp.make_default_single_obj_gp","title":"make_default_single_obj_gp","text":"<pre><code>make_default_single_obj_gp(\n    x: Tensor,\n    y: Tensor,\n    encoder: ConfigEncoder,\n    *,\n    y_transform: OutcomeTransform | None = None\n) -&gt; SingleTaskGP\n</code></pre> <p>Default GP for single objective optimization.</p> Source code in <code>neps\\optimizers\\models\\gp.py</code> <pre><code>def make_default_single_obj_gp(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    encoder: ConfigEncoder,\n    *,\n    y_transform: OutcomeTransform | None = None,\n) -&gt; SingleTaskGP:\n    \"\"\"Default GP for single objective optimization.\"\"\"\n    if y.ndim == 1:\n        y = y.unsqueeze(-1)\n\n    if y_transform is None:\n        y_transform = Standardize(m=1)\n\n    numerics: list[int] = []\n    categoricals: list[int] = []\n    for hp_name, transformer in encoder.transformers.items():\n        if isinstance(transformer, CategoricalToIntegerTransformer):\n            categoricals.append(encoder.index_of[hp_name])\n        else:\n            numerics.append(encoder.index_of[hp_name])\n\n    # Purely vectorial\n    if len(categoricals) == 0:\n        return SingleTaskGP(train_X=x, train_Y=y, outcome_transform=y_transform)\n\n    # Purely categorical\n    if len(numerics) == 0:\n        return SingleTaskGP(\n            train_X=x,\n            train_Y=y,\n            covar_module=default_categorical_kernel(len(categoricals)),\n            outcome_transform=y_transform,\n        )\n\n    # Mixed\n    numeric_kernel = get_covar_module_with_dim_scaled_prior(\n        ard_num_dims=len(numerics),\n        active_dims=tuple(numerics),\n    )\n    cat_kernel = default_categorical_kernel(\n        len(categoricals), active_dims=tuple(categoricals)\n    )\n\n    # WARNING: I previously tried SingleTaskMixedGp which does the following:\n    #\n    # x K((x1, c1), (x2, c2)) =\n    # x     K_cont_1(x1, x2) + K_cat_1(c1, c2) +\n    # x      K_cont_2(x1, x2) * K_cat_2(c1, c2)\n    #\n    # In a toy example with a single binary categorical which acted like F * {0, 1},\n    # the model collapsed to always predicting `0`. Causing all parameters defining F\n    # to essentially be guess at random. This is a lot more stable but likely not as\n    # good...\n    # TODO: Figure out how to improve stability of this.\n    kernel = numeric_kernel + cat_kernel\n\n    return SingleTaskGP(\n        train_X=x, train_Y=y, covar_module=kernel, outcome_transform=y_transform\n    )\n</code></pre>"},{"location":"api/neps/optimizers/models/gp/#neps.optimizers.models.gp.optimize_acq","title":"optimize_acq","text":"<pre><code>optimize_acq(\n    acq_fn: AcquisitionFunction,\n    encoder: ConfigEncoder,\n    *,\n    n_candidates_required: int = 1,\n    num_restarts: int = 20,\n    n_intial_start_points: int | None = None,\n    acq_options: Mapping[str, Any] | None = None,\n    fixed_features: dict[str, Any] | None = None,\n    maximum_allowed_categorical_combinations: int = 30,\n    hide_warnings: bool = False\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Optimize the acquisition function.</p> Source code in <code>neps\\optimizers\\models\\gp.py</code> <pre><code>def optimize_acq(\n    acq_fn: AcquisitionFunction,\n    encoder: ConfigEncoder,\n    *,\n    n_candidates_required: int = 1,\n    num_restarts: int = 20,\n    n_intial_start_points: int | None = None,\n    acq_options: Mapping[str, Any] | None = None,\n    fixed_features: dict[str, Any] | None = None,\n    maximum_allowed_categorical_combinations: int = 30,\n    hide_warnings: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Optimize the acquisition function.\"\"\"\n    warning_context = (\n        disable_warnings(NumericalWarning) if hide_warnings else nullcontext()\n    )\n    acq_options = acq_options or {}\n\n    _fixed_features: dict[int, float] = {}\n    if fixed_features is not None:\n        for name, value in fixed_features.items():\n            encoded_value = encoder.transformers[name].encode_one(value)\n            encoded_index = encoder.index_of[name]\n            _fixed_features[encoded_index] = encoded_value\n\n    lower = [domain.lower for domain in encoder.domains]\n    upper = [domain.upper for domain in encoder.domains]\n    bounds = torch.tensor([lower, upper], dtype=torch.float64)\n\n    cat_transformers = {\n        name: t\n        for name, t in encoder.transformers.items()\n        if (\n            name not in _fixed_features  # Don't include those which are fixed by caller\n            and t.domain.is_categorical  # Only include categoricals\n        )\n    }\n\n    # Proceed with regular numerical acquisition\n    if not any(cat_transformers):\n        # Small heuristic to increase the number of candidates as our\n        # dimensionality increases... we apply a cap of 4096,\n        # which occurs when len(bounds) &gt;= 8\n        # TODO: Need to investigate how num_restarts is fully used in botorch to inform.\n        if n_intial_start_points is None:\n            n_intial_start_points = min(64 * len(bounds) ** 2, 4096)\n\n        with warning_context:\n            return optimize_acqf(  # type: ignore\n                acq_function=acq_fn,\n                bounds=bounds,\n                q=n_candidates_required,\n                num_restarts=num_restarts,\n                raw_samples=n_intial_start_points,\n                fixed_features=_fixed_features,\n                **acq_options,\n            )\n\n    # We need to generate the product of all possible combinations of categoricals,\n    # first we do a sanity check\n    n_combos = reduce(\n        lambda x, y: x * y,  # type: ignore\n        [t.domain.cardinality for t in cat_transformers.values()],\n        1,\n    )\n    if n_combos &gt; maximum_allowed_categorical_combinations:\n        raise ValueError(\n            \"The number of fixed categorical dimensions is too high. \"\n            \"This will lead to an explosion in the number of possible \"\n            f\"combinations. Got: {n_combos} while the setting for the function\"\n            f\" is: {maximum_allowed_categorical_combinations=}. Consider reducing the \"\n            \"dimensions or consider encoding your categoricals in some other format.\"\n        )\n\n    # Right, now we generate all possible combinations\n    # First, just collect the possible values per cat column\n    # {hp_name: [v1, v2], hp_name2: [v1, v2, v3], ...}\n    cats: dict[int, list[float]] = {\n        encoder.index_of[name]: [\n            float(i)  # NOTE: Botorchs optim requires them to be as floats\n            for i in range(transformer.domain.cardinality)  # type: ignore\n        ]\n        for name, transformer in cat_transformers.items()\n    }\n\n    # Second, generate all possible combinations\n    fixed_cats: list[dict[int, float]]\n    if len(cats) == 1:\n        col, choice_indices = next(iter(cats.items()))\n        fixed_cats = [{col: i} for i in choice_indices]\n    else:\n        fixed_cats = [\n            dict(zip(cats.keys(), combo, strict=False))\n            for combo in product(*cats.values())\n        ]\n\n    # Make sure to include caller's fixed features if provided\n    if len(_fixed_features) &gt; 0:\n        fixed_cats = [{**cat, **_fixed_features} for cat in fixed_cats]\n\n    with warning_context:\n        # TODO: we should deterministically shuffle the fixed_categoricals\n        # as the underlying function does not.\n        return optimize_acqf_mixed(  # type: ignore\n            acq_function=acq_fn,\n            bounds=bounds,\n            num_restarts=min(num_restarts // n_combos, 2),\n            raw_samples=n_intial_start_points,\n            q=n_candidates_required,\n            fixed_features_list=fixed_cats,\n            **acq_options,\n        )\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/","title":"Brackets","text":""},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Async","title":"Async  <code>dataclass</code>","text":"<pre><code>Async(\n    rungs: list[Rung],\n    eta: int,\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n)\n</code></pre> <p>A bracket that holds a collection of rungs with no capacity constraints.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Async.eta","title":"eta  <code>instance-attribute</code>","text":"<pre><code>eta: int\n</code></pre> <p>The eta parameter used for deciding when to promote.</p> <p>When any of the top_k configs in a rung can be promoted and have not been promoted yet, they will be.</p> <p>Here <code>k = len(rung) // eta</code>.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Async.is_multi_objective","title":"is_multi_objective  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_multi_objective: bool = field(default=False)\n</code></pre> <p>Whether the BracketOptimizer is multi-objective or not.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Async.mo_selector","title":"mo_selector  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mo_selector: Literal[\"nsga2\", \"epsnet\"] = field(\n    default=\"epsnet\"\n)\n</code></pre> <p>The selector to use for multi-objective optimization.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Async.rungs","title":"rungs  <code>instance-attribute</code>","text":"<pre><code>rungs: list[Rung]\n</code></pre> <p>A list of rungs, ordered from lowest to highest.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.AsyncHyperband","title":"AsyncHyperband  <code>dataclass</code>","text":"<pre><code>AsyncHyperband(\n    asha_brackets: list[Async],\n    eta: int,\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n)\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.AsyncHyperband.asha_brackets","title":"asha_brackets  <code>instance-attribute</code>","text":"<pre><code>asha_brackets: list[Async]\n</code></pre> <p>A list of ASHA brackets, ordered from lowest to highest according to the lowest rung value in each bracket.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.AsyncHyperband.eta","title":"eta  <code>instance-attribute</code>","text":"<pre><code>eta: int\n</code></pre> <p>The eta parameter used for deciding when to promote.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.AsyncHyperband.is_multi_objective","title":"is_multi_objective  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_multi_objective: bool = field(default=False)\n</code></pre> <p>Whether the BracketOptimizer is multi-objective or not.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.AsyncHyperband.mo_selector","title":"mo_selector  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mo_selector: Literal[\"nsga2\", \"epsnet\"] = field(\n    default=\"epsnet\"\n)\n</code></pre> <p>The selector to use for multi-objective optimization.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.AsyncHyperband.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    table: DataFrame,\n    *,\n    bracket_rungs: list[list[int]],\n    eta: int,\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\"\n) -&gt; AsyncHyperband\n</code></pre> <p>Create an AsyncHyperbandBrackets from the table.</p> <p>The table should have a multi-index of (id, rung) where rung is the fidelity level of the configuration. <pre><code># Unrealistic example showing the format of the table\n(id, rung) -&gt; config, perf\n--------------------------\n0     0   |  {\"hp\": 0, ...}, 0.1\n      1   |  {\"hp\": 0, ...}, 0.2\n1     0   |  {\"hp\": 1, ...}, 0.1\n2     1   |  {\"hp\": 2, ...}, 0.3\n      2   |  {\"hp\": 2, ...}, 0.4\n3     2   |  {\"hp\": 3, ...}, 0.4\n</code></pre></p> PARAMETER DESCRIPTION <code>table</code> <p>The table of configurations to split into brackets.</p> <p> TYPE: <code>DataFrame</code> </p> <code>bracket_rungs</code> <p>A list of rungs for each bracket. Each element of the list is a list for that given bracket.</p> <p> TYPE: <code>list[list[int]]</code> </p> RETURNS DESCRIPTION <code>AsyncHyperband</code> <p>The AsyncHyperbandBrackets which have each subselected the table with the corresponding rung sizes.</p> Source code in <code>neps\\optimizers\\utils\\brackets.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    table: pd.DataFrame,\n    *,\n    bracket_rungs: list[list[int]],\n    eta: int,\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n) -&gt; AsyncHyperband:\n    \"\"\"Create an AsyncHyperbandBrackets from the table.\n\n    The table should have a multi-index of (id, rung) where rung is the\n    fidelity level of the configuration.\n    ```\n    # Unrealistic example showing the format of the table\n    (id, rung) -&gt; config, perf\n    --------------------------\n    0     0   |  {\"hp\": 0, ...}, 0.1\n          1   |  {\"hp\": 0, ...}, 0.2\n    1     0   |  {\"hp\": 1, ...}, 0.1\n    2     1   |  {\"hp\": 2, ...}, 0.3\n          2   |  {\"hp\": 2, ...}, 0.4\n    3     2   |  {\"hp\": 3, ...}, 0.4\n    ```\n\n    Args:\n        table: The table of configurations to split into brackets.\n        bracket_rungs: A list of rungs for each bracket. Each element of the list\n            is a list for that given bracket.\n\n    Returns:\n        The AsyncHyperbandBrackets which have each subselected the table with the\n        corresponding rung sizes.\n    \"\"\"\n    return AsyncHyperband(\n        asha_brackets=[\n            Async.create(\n                table=table,\n                rungs=layout,\n                eta=eta,\n                is_multi_objective=is_multi_objective,\n                mo_selector=mo_selector,\n            )\n            for layout in bracket_rungs\n        ],\n        eta=eta,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Hyperband","title":"Hyperband  <code>dataclass</code>","text":"<pre><code>Hyperband(\n    sh_brackets: list[Sync],\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n)\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Hyperband.create_repeating","title":"create_repeating  <code>classmethod</code>","text":"<pre><code>create_repeating(\n    table: DataFrame,\n    *,\n    bracket_layouts: list[dict[int, int]],\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\"\n) -&gt; list[Hyperband]\n</code></pre> <p>Create a list of brackets from the table.</p> <p>The table should have a multi-index of (id, rung) where rung is the fidelity level of the configuration.</p> <p>This method will always ensure there is at least one hyperband set of brackets, with at least one empty slot. For example, if each hyperband set of brackets houses a maximum of 9 configurations, and there are 27 total unique configurations in the table, these will be split into 3 hyperband brackets with 9 configurations + 1 additional hyperband bracket with 0 in it configurations.</p> <pre><code># Unrealistic example showing the format of the table\n(id, rung) -&gt; config, perf\n--------------------------\n0     0   |  {\"hp\": 0, ...}, 0.1\n      1   |  {\"hp\": 0, ...}, 0.2\n1     0   |  {\"hp\": 1, ...}, 0.1\n2     1   |  {\"hp\": 2, ...}, 0.3\n      2   |  {\"hp\": 2, ...}, 0.4\n3     2   |  {\"hp\": 3, ...}, 0.4\n</code></pre> PARAMETER DESCRIPTION <code>table</code> <p>The table of configurations to split into brackets.</p> <p> TYPE: <code>DataFrame</code> </p> <code>bracket_layouts</code> <p>A mapping of rung to the capacity of that rung.</p> <p> TYPE: <code>list[dict[int, int]]</code> </p> RETURNS DESCRIPTION <code>list[Hyperband]</code> <p>HyperbandBrackets which have each subselected the table with the corresponding rung sizes.</p> Source code in <code>neps\\optimizers\\utils\\brackets.py</code> <pre><code>@classmethod\ndef create_repeating(\n    cls,\n    table: pd.DataFrame,\n    *,\n    bracket_layouts: list[dict[int, int]],\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n) -&gt; list[Hyperband]:\n    \"\"\"Create a list of brackets from the table.\n\n    The table should have a multi-index of (id, rung) where rung is the\n    fidelity level of the configuration.\n\n    This method will always ensure there is at least one hyperband set of brackets,\n    with at least one empty slot. For example, if each hyperband set of brackets\n    houses a maximum of 9 configurations, and there are 27 total unique configurations\n    in the table, these will be split into 3 hyperband brackets with 9 configurations\n    + 1 additional hyperband bracket with 0 in it configurations.\n\n    ```\n    # Unrealistic example showing the format of the table\n    (id, rung) -&gt; config, perf\n    --------------------------\n    0     0   |  {\"hp\": 0, ...}, 0.1\n          1   |  {\"hp\": 0, ...}, 0.2\n    1     0   |  {\"hp\": 1, ...}, 0.1\n    2     1   |  {\"hp\": 2, ...}, 0.3\n          2   |  {\"hp\": 2, ...}, 0.4\n    3     2   |  {\"hp\": 3, ...}, 0.4\n    ```\n\n    Args:\n        table: The table of configurations to split into brackets.\n        bracket_layouts: A mapping of rung to the capacity of that rung.\n\n    Returns:\n        HyperbandBrackets which have each subselected the table with the\n        corresponding rung sizes.\n    \"\"\"\n    all_ids = table.index.get_level_values(\"id\").unique()\n\n    # Split the ids into N hyperband brackets of size K.\n    # K is sum of number of configurations in the lowest rung of each SH bracket\n    #\n    # For example:\n    # &gt; bracket_layouts = [\n    # &gt;   {0: 81, 1: 27, 2: 9, 3: 3, 4: 1},\n    # &gt;   {1: 27, 2: 9, 3: 3, 4: 1},\n    # &gt;   {2: 9, 3: 3, 4: 1},\n    # &gt;   ...\n    # &gt; ]\n    #\n    # Corresponds to:\n    # bracket1 - [rung_0: 81, rung_1: 27, rung_2: 9, rung_3: 3, rung_4: 1]\n    # bracket2 - [rung_1: 27, rung_2: 9, rung_3: 3, rung_4: 1]\n    # bracket3 - [rung_2: 9, rung_3: 3, rung_4: 1]\n    # ...\n    # &gt; K = 81 + 27 + 9 + ...\n    #\n    bottom_rung_sizes = [sh[min(sh.keys())] for sh in bracket_layouts]\n    K = sum(bottom_rung_sizes)\n    N = max(len(all_ids) // K + 1, 1)\n\n    hb_id_slices: list[Index] = [all_ids[i * K : (i + 1) * K] for i in range(N)]\n\n    # Used if there is nothing for one of the rungs\n    empty_slice = table.loc[[]]\n\n    # Now for each of our HB brackets, we need to split them into the SH brackets\n    hb_brackets: list[list[Sync]] = []\n\n    offsets = np.cumsum([0, *bottom_rung_sizes])\n    for hb_ids in hb_id_slices:\n        # Split the ids into each of the respective brackets, e.g. [81, 27, 9, ...]\n        ids_for_each_bracket = [hb_ids[s:e] for s, e in pairwise(offsets)]\n\n        # Select the data for each of the configs allocated to these sh_brackets\n        data_for_each_bracket = [table.loc[_ids] for _ids in ids_for_each_bracket]\n\n        # Create the bracket\n        sh_brackets: list[Sync] = []\n        for data_for_bracket, layout in zip(\n            data_for_each_bracket,\n            bracket_layouts,\n            strict=True,\n        ):\n            rung_data = dict(iter(data_for_bracket.groupby(level=\"rung\", sort=False)))\n            bracket = Sync(\n                rungs=[\n                    Rung(\n                        value=rung,\n                        capacity=capacity,\n                        table=rung_data.get(rung, empty_slice),\n                    )\n                    for rung, capacity in layout.items()\n                ],\n                is_multi_objective=is_multi_objective,\n                mo_selector=mo_selector,\n            )\n            sh_brackets.append(bracket)\n\n        hb_brackets.append(sh_brackets)\n\n    return [cls(sh_brackets=sh_brackets) for sh_brackets in hb_brackets]\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung","title":"Rung  <code>dataclass</code>","text":"<pre><code>Rung(value: int, table: DataFrame, capacity: int | None)\n</code></pre> <p>               Bases: <code>Sized</code></p> <p>A rung in a bracket</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung.capacity","title":"capacity  <code>instance-attribute</code>","text":"<pre><code>capacity: int | None\n</code></pre> <p>The capacity of the rung, if any.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung.table","title":"table  <code>instance-attribute</code>","text":"<pre><code>table: DataFrame\n</code></pre> <p>The slice of the table that constitutes this rung.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: int\n</code></pre> <p>The value of a rung, used to determine order between rungs.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung.epsnet_selector","title":"epsnet_selector","text":"<pre><code>epsnet_selector(\n    *, k: int, contenders: DataFrame\n) -&gt; DataFrame\n</code></pre> <p>Selects the best configurations based on epsilon-net sorting strategy. Uses Epsilon-net based sorting from SyneTune.</p> Source code in <code>neps\\optimizers\\utils\\brackets.py</code> <pre><code>def epsnet_selector(\n    self,\n    *,\n    k: int,\n    contenders: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Selects the best configurations based on epsilon-net sorting strategy.\n    Uses Epsilon-net based sorting from SyneTune.\n    \"\"\"\n    from neps.optimizers.utils.multiobjective.epsnet import nondominated_sort\n\n    mo_costs = np.vstack(contenders[\"perf\"].values)\n    indices = nondominated_sort(\n        X=mo_costs,\n        max_items=k,\n    )\n    return contenders.iloc[indices]\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung.mo_selector","title":"mo_selector","text":"<pre><code>mo_selector(\n    *,\n    selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n    contenders: DataFrame | None = None,\n    k: int\n) -&gt; DataFrame\n</code></pre> <p>Replaces top_k in single objective Bracket Optimizers with a multi-objective selector, which selects the best configurations based on the Pareto front. Page 4, Algorithm 2, Line 11: <code>mo_selector</code> in the MO-ASHA paper: arxiv.org/pdf/2106.12639</p> Source code in <code>neps\\optimizers\\utils\\brackets.py</code> <pre><code>def mo_selector(\n    self,\n    *,\n    selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n    contenders: pd.DataFrame | None = None,\n    k: int,\n) -&gt; pd.DataFrame:\n    \"\"\"Replaces top_k in single objective Bracket Optimizers\n    with a multi-objective selector, which selects the best\n    configurations based on the Pareto front.\n    Page 4, Algorithm 2, Line 11: `mo_selector` in the MO-ASHA paper:\n    https://arxiv.org/pdf/2106.12639\n    \"\"\"\n    if contenders is None:\n        contenders = self.table\n    match selector:\n        case \"nsga2\":\n            raise NotImplementedError(\n                \"NSGA2 selector is not implemented yet. Please use epsnet.\"\n            )\n        case \"epsnet\":\n            return self.epsnet_selector(\n                k=k,\n                contenders=contenders,\n            )\n        case _:\n            raise ValueError(\n                f\"Unknown selector {selector}, please use either nsga2 or epsnet\"\n            )\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung.nsga2_selector","title":"nsga2_selector","text":"<pre><code>nsga2_selector(k: int) -&gt; DataFrame\n</code></pre> <p>Selects the best configurations based on NSGA2 algorithm. Uses Non-dominated sorting and Crowding distance from Pymoo.</p> Source code in <code>neps\\optimizers\\utils\\brackets.py</code> <pre><code>def nsga2_selector(\n    self,\n    k: int,\n) -&gt; pd.DataFrame:\n    \"\"\"Selects the best configurations based on NSGA2 algorithm.\n    Uses Non-dominated sorting and Crowding distance from Pymoo.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Rung.pareto_promotion_sync","title":"pareto_promotion_sync","text":"<pre><code>pareto_promotion_sync(\n    *,\n    k: int,\n    exclude: Sequence[Hashable] = [],\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\"\n) -&gt; tuple[int, dict[str, Any], float] | None\n</code></pre> <p>Selects the best configurations based on Pareto front for sync bracket optimizers.</p> Source code in <code>neps\\optimizers\\utils\\brackets.py</code> <pre><code>def pareto_promotion_sync(\n    self,\n    *,\n    k: int,\n    exclude: Sequence[Hashable] = [],\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n) -&gt; tuple[int, dict[str, Any], float] | None:\n    \"\"\"Selects the best configurations based on Pareto front for\n    sync bracket optimizers.\n    \"\"\"\n    if exclude:\n        contenders = self.table.drop(exclude)\n        if contenders.empty:\n            return None\n    else:\n        contenders = self.table\n    _df = self.mo_selector(\n        selector=mo_selector,\n        contenders=contenders,\n        k=k,\n    )\n    _idx, _rung = _df.index[0]\n    row = _df.loc[(_idx, _rung)]\n    config = dict(row[\"config\"])\n    perf = row[\"perf\"]\n    return _idx, config, perf\n</code></pre>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Sync","title":"Sync  <code>dataclass</code>","text":"<pre><code>Sync(\n    rungs: list[Rung],\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n)\n</code></pre> <p>A bracket that holds a collection of rungs with a capacity constraint.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Sync.is_multi_objective","title":"is_multi_objective  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_multi_objective: bool = field(default=False)\n</code></pre> <p>Whether the BracketOptimizer is multi-objective or not.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Sync.mo_selector","title":"mo_selector  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mo_selector: Literal[\"nsga2\", \"epsnet\"] = field(\n    default=\"epsnet\"\n)\n</code></pre> <p>The selector to use for multi-objective optimization.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Sync.rungs","title":"rungs  <code>instance-attribute</code>","text":"<pre><code>rungs: list[Rung]\n</code></pre> <p>A list of unique rungs, ordered from lowest to highest. The must have a capacity set.</p>"},{"location":"api/neps/optimizers/utils/brackets/#neps.optimizers.utils.brackets.Sync.create_repeating","title":"create_repeating  <code>classmethod</code>","text":"<pre><code>create_repeating(\n    table: DataFrame,\n    *,\n    rung_sizes: dict[int, int],\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\"\n) -&gt; list[Sync]\n</code></pre> <p>Create a list of brackets from the table.</p> <p>The table should have a multi-index of (id, rung) where rung is the fidelity level of the configuration.</p> <p>This method will always ensure there is at least one bracket, with at least one empty slot. For example, if each bracket houses a maximum of 9 configurations, and there are 27 total unique configurations in the table, these will be split into 3 brackets with 9 configurations + 1 additional bracket with 0 in it configurations.</p> <pre><code># Unrealistic example showing the format of the table\n(id, rung) -&gt; config, perf\n--------------------------\n0     0   |  {\"hp\": 0, ...}, 0.1\n      1   |  {\"hp\": 0, ...}, 0.2\n1     0   |  {\"hp\": 1, ...}, 0.1\n2     1   |  {\"hp\": 2, ...}, 0.3\n      2   |  {\"hp\": 2, ...}, 0.4\n3     2   |  {\"hp\": 3, ...}, 0.4\n</code></pre> PARAMETER DESCRIPTION <code>table</code> <p>The table of configurations to split into brackets.</p> <p> TYPE: <code>DataFrame</code> </p> <code>rung_sizes</code> <p>A mapping of rung to the capacity of that rung.</p> <p> TYPE: <code>dict[int, int]</code> </p> RETURNS DESCRIPTION <code>list[Sync]</code> <p>Brackets which have each subselected the table with the corresponding rung sizes.</p> Source code in <code>neps\\optimizers\\utils\\brackets.py</code> <pre><code>@classmethod\ndef create_repeating(\n    cls,\n    table: pd.DataFrame,\n    *,\n    rung_sizes: dict[int, int],\n    is_multi_objective: bool = False,\n    mo_selector: Literal[\"nsga2\", \"epsnet\"] = \"epsnet\",\n) -&gt; list[Sync]:\n    \"\"\"Create a list of brackets from the table.\n\n    The table should have a multi-index of (id, rung) where rung is the\n    fidelity level of the configuration.\n\n    This method will always ensure there is at least one bracket, with at least one\n    empty slot. For example, if each bracket houses a maximum of 9 configurations,\n    and there are 27 total unique configurations in the table, these will be split\n    into 3 brackets with 9 configurations + 1 additional bracket with 0 in it\n    configurations.\n\n    ```\n    # Unrealistic example showing the format of the table\n    (id, rung) -&gt; config, perf\n    --------------------------\n    0     0   |  {\"hp\": 0, ...}, 0.1\n          1   |  {\"hp\": 0, ...}, 0.2\n    1     0   |  {\"hp\": 1, ...}, 0.1\n    2     1   |  {\"hp\": 2, ...}, 0.3\n          2   |  {\"hp\": 2, ...}, 0.4\n    3     2   |  {\"hp\": 3, ...}, 0.4\n    ```\n\n    Args:\n        table: The table of configurations to split into brackets.\n        rung_sizes: A mapping of rung to the capacity of that rung.\n\n    Returns:\n        Brackets which have each subselected the table with the corresponding rung\n        sizes.\n    \"\"\"\n    # Split the trials by their unique_id, taking batches of K at a time, which will\n    # gives us N = len(unique_is) / K brackets in total.\n    #\n    # Here, unique_id referes to the `1` in config_1_0 i.e. id = 1, rung = 0\n    #\n    #      1  2  3  4  5  6  7  8  9   10 11 12 13 14 15 16 17 18   ...\n    #     |           bracket1       |       bracket 2            | ... |\n\n    # K is the number of configurations in the lowest rung, which is how many unique\n    # ids are needed to fill a single bracket.\n    K = rung_sizes[min(rung_sizes)]\n\n    # N is the number of brackets we need to create to accomodate all the unique ids.\n    # First we need all of the unique ids.\n    uniq_ids = table.index.get_level_values(\"id\").unique()\n\n    # * Why (len(uniq_ids) + K) // K?\n    #   * If we have `1` unique id, then 1 // K == 0 while (K + 1) // K == 1\n    N = (len(uniq_ids) + K) // K\n\n    # Now we take the unique ids and split them into batches of size K\n    bracket_id_slices: list[Index] = [uniq_ids[i * K : (i + 1) * K] for i in range(N)]\n\n    # And now select the data for each of the unique_ids in the bracket\n    bracket_datas = [\n        table.loc[bracket_unique_ids] for bracket_unique_ids in bracket_id_slices\n    ]\n\n    # This will give us a list of dictionaries, where each element `n` of the\n    # list is on of the `N` brackets, and the dictionary at element `n` maps\n    # from a rung, to the slice of the data for that rung.\n    all_N_bracket_datas = [\n        dict(iter(d.groupby(level=\"rung\", sort=False))) for d in bracket_datas\n    ]\n\n    # Used if there is nothing for one of the rungs\n    empty_slice = table.loc[[]]\n\n    return [\n        Sync(\n            rungs=[\n                Rung(rung, bracket_data.get(rung, empty_slice), capacity)\n                for rung, capacity in rung_sizes.items()\n            ],\n            is_multi_objective=is_multi_objective,\n            mo_selector=mo_selector,\n        )\n        for bracket_data in all_N_bracket_datas\n    ]\n</code></pre>"},{"location":"api/neps/optimizers/utils/grid/","title":"Grid","text":""},{"location":"api/neps/optimizers/utils/grid/#neps.optimizers.utils.grid.make_grid","title":"make_grid","text":"<pre><code>make_grid(\n    space: SearchSpace,\n    *,\n    size_per_numerical_hp: int = 10,\n    ignore_fidelity: bool = True\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Get a grid of configurations from the search space.</p> <p>For <code>Float</code> and <code>Integer</code> the parameter <code>size_per_numerical_hp=</code> is used to determine a grid.</p> <p>For <code>Categorical</code> hyperparameters, we include all the choices in the grid.</p> <p>For <code>Constant</code> hyperparameters, we include the constant value in the grid.</p> PARAMETER DESCRIPTION <code>size_per_numerical_hp</code> <p>The size of the grid for each numerical hyperparameter.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations from the search space.</p> Source code in <code>neps\\optimizers\\utils\\grid.py</code> <pre><code>def make_grid(\n    space: SearchSpace,\n    *,\n    size_per_numerical_hp: int = 10,\n    ignore_fidelity: bool = True,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Get a grid of configurations from the search space.\n\n    For [`Float`][neps.space.Float] and [`Integer`][neps.space.Integer]\n    the parameter `size_per_numerical_hp=` is used to determine a grid.\n\n    For [`Categorical`][neps.space.Categorical]\n    hyperparameters, we include all the choices in the grid.\n\n    For [`Constant`][neps.space.Constant] hyperparameters,\n    we include the constant value in the grid.\n\n    Args:\n        size_per_numerical_hp: The size of the grid for each numerical hyperparameter.\n\n    Returns:\n        A list of configurations from the search space.\n    \"\"\"\n    param_ranges: dict[str, list[Any]] = {}\n    for name, hp in space.items():\n        match hp:\n            case Categorical():\n                param_ranges[name] = list(hp.choices)\n            case Constant():\n                param_ranges[name] = [hp.value]\n            case Integer() | Float():\n                if hp.is_fidelity and ignore_fidelity:\n                    param_ranges[name] = [hp.upper]\n                    continue\n\n                if hp.domain.cardinality is None:\n                    steps = size_per_numerical_hp\n                else:\n                    steps = min(size_per_numerical_hp, hp.domain.cardinality)\n\n                xs = torch.linspace(0, 1, steps=steps)\n                numeric_values = hp.domain.cast(xs, frm=Domain.unit_float())\n                uniq_values = torch.unique(numeric_values).tolist()\n                param_ranges[name] = uniq_values\n            case _:\n                raise NotImplementedError(f\"Unknown Parameter type: {type(hp)}\\n{hp}\")\n    values = product(*param_ranges.values())\n    keys = list(space.keys())\n\n    return [dict(zip(keys, p, strict=False)) for p in values]\n</code></pre>"},{"location":"api/neps/optimizers/utils/initial_design/","title":"Initial design","text":""},{"location":"api/neps/optimizers/utils/initial_design/#neps.optimizers.utils.initial_design.make_initial_design","title":"make_initial_design","text":"<pre><code>make_initial_design(\n    *,\n    parameters: Mapping[str, Parameter],\n    encoder: ConfigEncoder,\n    sampler: Literal[\"sobol\", \"prior\", \"uniform\"] | Sampler,\n    sample_size: int | Literal[\"ndim\"] | None = \"ndim\",\n    sample_prior_first: bool = True,\n    seed: Generator | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Generate the initial design of the optimization process.</p> PARAMETER DESCRIPTION <code>encoder</code> <p>The encoder to use for encoding/decoding configurations.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>sampler</code> <p>The sampler to use for the initial design.</p> <p>If set to \"sobol\", a Sobol sequence will be used. If set to \"uniform\", a uniform random sampler will be used. If set to \"prior\", a prior sampler will be used, based on the defaults,     and confidence scores of the hyperparameters. If set to a custom sampler, the sampler will be used directly.</p> <p> TYPE: <code>Literal['sobol', 'prior', 'uniform'] | Sampler</code> </p> <code>sample_size</code> <p>The number of configurations to sample.</p> <p>If \"ndim\", the number of configs will be equal to the number of dimensions. If None, no configurations will be sampled.</p> <p> TYPE: <code>int | Literal['ndim'] | None</code> DEFAULT: <code>'ndim'</code> </p> <code>sample_prior_first</code> <p>Whether to sample the prior configuration first.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>seed</code> <p>The seed to use for the random number generation.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\optimizers\\utils\\initial_design.py</code> <pre><code>def make_initial_design(\n    *,\n    parameters: Mapping[str, Parameter],\n    encoder: ConfigEncoder,\n    sampler: Literal[\"sobol\", \"prior\", \"uniform\"] | Sampler,\n    sample_size: int | Literal[\"ndim\"] | None = \"ndim\",\n    sample_prior_first: bool = True,\n    seed: torch.Generator | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Generate the initial design of the optimization process.\n\n    Args:\n        encoder: The encoder to use for encoding/decoding configurations.\n        sampler: The sampler to use for the initial design.\n\n            If set to \"sobol\", a Sobol sequence will be used.\n            If set to \"uniform\", a uniform random sampler will be used.\n            If set to \"prior\", a prior sampler will be used, based on the defaults,\n                and confidence scores of the hyperparameters.\n            If set to a custom sampler, the sampler will be used directly.\n\n        sample_size:\n            The number of configurations to sample.\n\n            If \"ndim\", the number of configs will be equal to the number of dimensions.\n            If None, no configurations will be sampled.\n\n        sample_prior_first: Whether to sample the prior configuration first.\n        seed: The seed to use for the random number generation.\n\n    \"\"\"\n    configs: list[dict[str, Any]] = []\n    if sample_prior_first:\n        configs.append(\n            {\n                name: p.prior if p.prior is not None else p.center\n                for name, p in parameters.items()\n            }\n        )\n\n    ndims = len(parameters)\n    if sample_size == \"ndim\":\n        sample_size = ndims\n    elif sample_size is not None and not sample_size &gt; 0:\n        raise ValueError(\n            \"The sample size should be a positive integer if passing an int.\"\n        )\n\n    if sample_size is not None:\n        match sampler:\n            case \"sobol\":\n                sampler = Sampler.sobol(ndim=ndims)\n            case \"uniform\":\n                sampler = Sampler.uniform(ndim=ndims)\n            case \"prior\":\n                sampler = Prior.from_parameters(parameters)\n            case _:\n                pass\n\n        encoded_configs = sampler.sample(sample_size * 2, to=encoder.domains, seed=seed)\n        uniq_x = torch.unique(encoded_configs, dim=0)\n        sample_configs = encoder.decode(uniq_x[:sample_size])\n        configs.extend(sample_configs)\n\n    return configs\n</code></pre>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/","title":"Epsnet","text":"<p>Implements an epsilon-net based multi-objective sort. Source: github.com/syne-tune/syne-tune/blob/main/syne_tune/optimizer/schedulers/multiobjective/non_dominated_priority.py Proposed in the paper: arxiv.org/pdf/2106.12639</p>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.compute_epsilon_net","title":"compute_epsilon_net","text":"<pre><code>compute_epsilon_net(\n    X: ndarray, dim: int | None = None\n) -&gt; ndarray\n</code></pre> <p>Outputs an order of the items in the provided array such that the items are spaced well. This means that after choosing a seed item, the next item is chosen to be the farthest from the seed item. The third item is then chosen to maximize the distance to the existing points and so on.</p> <p>This algorithm is taken from \"Nearest-Neighbor Searching and Metric Space Dimensions\" (Clarkson, 2005, p.17).</p>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.compute_epsilon_net--parameters","title":"Parameters","text":"<p>X: np.ndarray [N, D]     The items to sparsify where N is the number of items and D their dimensionality. dim: Optional[int], default: None     The index of the dimension which to use to choose the seed item.     If <code>None</code>, an item is chosen at random, otherwise the item with the     lowest value in the specified dimension is used.</p>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.compute_epsilon_net--returns","title":"Returns","text":"<p>np.ndarray [N]     A list of item indices, defining a sparsified order of the items.</p> Source code in <code>neps\\optimizers\\utils\\multiobjective\\epsnet.py</code> <pre><code>def compute_epsilon_net(X: np.ndarray, dim: int | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Outputs an order of the items in the provided array such that the items are\n    spaced well. This means that after choosing a seed item, the next item is\n    chosen to be the farthest from the seed item. The third item is then chosen\n    to maximize the distance to the existing points and so on.\n\n    This algorithm is taken from \"Nearest-Neighbor Searching and Metric Space Dimensions\"\n    (Clarkson, 2005, p.17).\n\n    Parameters\n    ----------\n    X: np.ndarray [N, D]\n        The items to sparsify where N is the number of items and D their dimensionality.\n    dim: Optional[int], default: None\n        The index of the dimension which to use to choose the seed item.\n        If ``None``, an item is chosen at random, otherwise the item with the\n        lowest value in the specified dimension is used.\n\n    Returns\n    -------\n    np.ndarray [N]\n        A list of item indices, defining a sparsified order of the items.\n    \"\"\"\n    indices = set(range(X.shape[0]))\n\n    # Choose the seed item according to dim\n    if dim is None:\n        initial_index = np.random.choice(X.shape[0])\n    else:\n        initial_index = np.argmin(X, axis=0)[dim]\n\n    # Initialize the order\n    order = [initial_index]\n    indices.remove(initial_index)\n\n    # Iterate until all models have been chosen\n    while indices:\n        # Get the distance to all items that have already been chosen\n        ordered_indices = list(indices)\n        diff = X[ordered_indices][:, None, :].repeat(len(order), axis=1) - X[order]\n        min_distances = np.linalg.norm(diff, axis=-1).min(-1)\n\n        # Then, choose the one with the maximum distance to all points\n        choice = ordered_indices[min_distances.argmax()]\n        order.append(choice)\n        indices.remove(choice)\n\n    # convert argsort indices to rank\n    ranks = np.empty(len(order), dtype=int)\n    for rank, i in enumerate(order):\n        ranks[i] = rank\n    return np.array(ranks)\n</code></pre>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.nondominated_sort","title":"nondominated_sort","text":"<pre><code>nondominated_sort(\n    X: ndarray,\n    dim: int | None = None,\n    max_items: int | None = None,\n    *,\n    flatten: bool = True\n) -&gt; list[int] | list[list[int]]\n</code></pre> <p>Performs a multi-objective sort by iteratively computing the Pareto front and sparsifying the items within the Pareto front. This is a non-dominated sort leveraging an epsilon-net.</p>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.nondominated_sort--parameters","title":"Parameters","text":"<p>X: np.ndarray [N, D]     The multi-dimensional items to sort. dim: Optional[int], default: None     The feature (metric) to prefer when ranking items within the Pareto front.     If <code>None</code>, items are chosen randomly. max_items: Optional[int], default: None     The maximum number of items that should be returned.     When this is <code>None</code>, all items are sorted. flatten: bool, default: True     Whether to flatten the resulting array.</p>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.nondominated_sort--returns","title":"Returns","text":"<p>Union[List[int], List[List[int]]]     The indices of the sorted items, either globally or within each of the     Pareto front depending on the value of <code>flatten</code>.</p> Source code in <code>neps\\optimizers\\utils\\multiobjective\\epsnet.py</code> <pre><code>def nondominated_sort(\n    X: np.ndarray,\n    dim: int | None = None,\n    max_items: int | None = None,\n    *,\n    flatten: bool = True,\n) -&gt; list[int] | list[list[int]]:\n    \"\"\"\n    Performs a multi-objective sort by iteratively computing the Pareto front\n    and sparsifying the items within the Pareto front. This is a non-dominated sort\n    leveraging an epsilon-net.\n\n    Parameters\n    ----------\n    X: np.ndarray [N, D]\n        The multi-dimensional items to sort.\n    dim: Optional[int], default: None\n        The feature (metric) to prefer when ranking items within the Pareto front.\n        If ``None``, items are chosen randomly.\n    max_items: Optional[int], default: None\n        The maximum number of items that should be returned.\n        When this is ``None``, all items are sorted.\n    flatten: bool, default: True\n        Whether to flatten the resulting array.\n\n    Returns\n    -------\n    Union[List[int], List[List[int]]]\n        The indices of the sorted items, either globally or within each of the\n        Pareto front depending on the value of ``flatten``.\n    \"\"\"\n    remaining = np.arange(X.shape[0])\n    indices = []\n    num_items = 0\n\n    # Iterate until max_items are reached or there are no items left\n    while remaining.size &gt; 0 and (max_items is None or num_items &lt; max_items):\n        # Compute the Pareto front and sort the items within\n        pareto_mask = pareto_efficient(X[remaining])\n        pareto_front = remaining[pareto_mask]\n        pareto_order = compute_epsilon_net(X[pareto_front], dim=dim)\n\n        # Add order to the indices\n        indices.append(pareto_front[pareto_order].tolist())\n        num_items += len(pareto_front)\n\n        # Remove items in the Pareto front from the remaining items\n        remaining = remaining[~pareto_mask]\n\n    # Restrict the number of items returned and optionally flatten\n    if max_items is not None:\n        limit = max_items - sum(len(x) for x in indices[:-1])\n        indices[-1] = indices[-1][:limit]\n        if not indices[-1]:\n            indices = indices[:-1]\n\n    if flatten:\n        return [i for ix in indices for i in ix]\n    return indices\n</code></pre>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.pareto_efficient","title":"pareto_efficient","text":"<pre><code>pareto_efficient(X: ndarray) -&gt; ndarray\n</code></pre> <p>Evaluates for each allocation in the provided array whether it is Pareto efficient. The costs are assumed to be improved by lowering them (eg lower is better).</p>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.pareto_efficient--parameters","title":"Parameters","text":"<p>X: np.ndarray [N, D]     The allocations to check where N is the number of allocations and D the number of     costs per allocation.</p>"},{"location":"api/neps/optimizers/utils/multiobjective/epsnet/#neps.optimizers.utils.multiobjective.epsnet.pareto_efficient--returns","title":"Returns","text":"<p>np.ndarray [N]     A boolean array, indicating for each allocation whether it is Pareto efficient.</p> Source code in <code>neps\\optimizers\\utils\\multiobjective\\epsnet.py</code> <pre><code>def pareto_efficient(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates for each allocation in the provided array whether it is Pareto efficient.\n    The costs are assumed to be improved by lowering them (eg lower is better).\n\n    Parameters\n    ----------\n    X: np.ndarray [N, D]\n        The allocations to check where N is the number of allocations and D the number of\n        costs per allocation.\n\n    Returns\n    -------\n    np.ndarray [N]\n        A boolean array, indicating for each allocation whether it is Pareto efficient.\n    \"\"\"\n    # First, we assume that all allocations are Pareto efficient, i.e. not dominated\n    mask = np.ones(X.shape[0], dtype=bool)\n    # Then, we iterate over all allocations A and check which are dominated by then\n    # current allocation A. If it is, we don't need to check it against another\n    # allocation.\n    for i, allocation in enumerate(X):\n        # Only consider allocation if it hasn't been dominated yet\n        if mask[i]:\n            # An allocation is dominated by A if all costs are equal or lower\n            # and at least one cost is strictly lower. Using that definition,\n            # A cannot be dominated by itself.\n            dominated = np.all(allocation &lt;= X[mask], axis=1) * np.any(\n                allocation &lt; X[mask], axis=1\n            )\n            mask[mask] = ~dominated\n\n    return mask\n</code></pre>"},{"location":"api/neps/plot/plot/","title":"Plot","text":"<p>Plot results of a neural pipeline search run.</p>"},{"location":"api/neps/plot/plot/#neps.plot.plot.plot","title":"plot","text":"<pre><code>plot(\n    root_directory: str | Path,\n    *,\n    scientific_mode: bool = False,\n    key_to_extract: str | None = None,\n    benchmarks: list[str] | None = None,\n    algorithms: list[str] | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n    x_range: tuple | None = None,\n    log_x: bool = False,\n    log_y: bool = True,\n    filename: str = \"incumbent_trajectory\",\n    extension: str = \"png\",\n    dpi: int = 100\n) -&gt; None\n</code></pre> <p>Plot results of a neural pipeline search run.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The directory with neps results (see below).</p> <p> TYPE: <code>str | Path</code> </p> <code>scientific_mode</code> <p>If true, plot from a tree-structured root_directory: benchmark={}/algorithm={}/seed={}</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>key_to_extract</code> <p>The metric to be used on the x-axis (if active, make sure evaluate_pipeline returns the metric in the info_dict)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>benchmarks</code> <p>List of benchmarks to plot</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>algorithms</code> <p>List of algorithms to plot</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>consider_continuations</code> <p>If true, toggle calculation of continuation costs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_workers</code> <p>Number of parallel processes of neps.run</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>x_range</code> <p>Bound x-axis (e.g. 1 10)</p> <p> TYPE: <code>tuple | None</code> DEFAULT: <code>None</code> </p> <code>log_x</code> <p>If true, toggle logarithmic scale on the x-axis</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>log_y</code> <p>If true, toggle logarithmic scale on the y-axis</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>filename</code> <p>Filename</p> <p> TYPE: <code>str</code> DEFAULT: <code>'incumbent_trajectory'</code> </p> <code>extension</code> <p>Image format</p> <p> TYPE: <code>str</code> DEFAULT: <code>'png'</code> </p> <code>dpi</code> <p>Image resolution</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the data to be plotted is not present.</p> Source code in <code>neps\\plot\\plot.py</code> <pre><code>def plot(  # noqa: C901, PLR0913\n    root_directory: str | Path,\n    *,\n    scientific_mode: bool = False,\n    key_to_extract: str | None = None,\n    benchmarks: list[str] | None = None,\n    algorithms: list[str] | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n    x_range: tuple | None = None,\n    log_x: bool = False,\n    log_y: bool = True,\n    filename: str = \"incumbent_trajectory\",\n    extension: str = \"png\",\n    dpi: int = 100,\n) -&gt; None:\n    \"\"\"Plot results of a neural pipeline search run.\n\n    Args:\n        root_directory: The directory with neps results (see below).\n        scientific_mode: If true, plot from a tree-structured root_directory:\n            benchmark={}/algorithm={}/seed={}\n        key_to_extract: The metric to be used on the x-axis\n            (if active, make sure evaluate_pipeline returns the metric in the info_dict)\n        benchmarks: List of benchmarks to plot\n        algorithms: List of algorithms to plot\n        consider_continuations: If true, toggle calculation of continuation costs\n        n_workers: Number of parallel processes of neps.run\n        x_range: Bound x-axis (e.g. 1 10)\n        log_x: If true, toggle logarithmic scale on the x-axis\n        log_y: If true, toggle logarithmic scale on the y-axis\n        filename: Filename\n        extension: Image format\n        dpi: Image resolution\n\n    Raises:\n        FileNotFoundError: If the data to be plotted is not present.\n    \"\"\"\n    logger = logging.getLogger(\"neps\")\n    logger.info(f\"Starting neps.plot using working directory {root_directory}\")\n\n    if benchmarks is None:\n        benchmarks = [\"example\"]\n    if algorithms is None:\n        algorithms = [\"neps\"]\n\n    logger.info(\n        f\"Processing {len(benchmarks)} benchmark(s) and {len(algorithms)} algorithm(s)...\"\n    )\n\n    ncols = 1 if len(benchmarks) == 1 else 2\n    nrows = np.ceil(len(benchmarks) / ncols).astype(int)\n\n    fig, axs = _get_fig_and_axs(nrows=nrows, ncols=ncols)\n\n    base_path = Path(root_directory)\n\n    for benchmark_idx, benchmark in enumerate(benchmarks):\n        if scientific_mode:\n            _base_path = base_path / f\"benchmark={benchmark}\"\n            if not _base_path.is_dir():\n                raise FileNotFoundError(\n                    errno.ENOENT, os.strerror(errno.ENOENT), _base_path\n                )\n        else:\n            _base_path = None\n\n        for algorithm in algorithms:\n            seeds = [None]\n            if _base_path is not None:\n                assert scientific_mode\n                _path = _base_path / f\"algorithm={algorithm}\"\n                if not _path.is_dir():\n                    raise FileNotFoundError(\n                        errno.ENOENT, os.strerror(errno.ENOENT), _path\n                    )\n\n                seeds = sorted(os.listdir(_path))  # type: ignore\n            else:\n                _path = None\n\n            incumbents = []\n            costs = []\n            max_costs = []\n            for seed in seeds:\n                incumbent, cost, max_cost = process_seed(\n                    path=_path if _path is not None else base_path,\n                    seed=seed,\n                    key_to_extract=key_to_extract,\n                    consider_continuations=consider_continuations,\n                    n_workers=n_workers,\n                )\n                incumbents.append(incumbent)\n                costs.append(cost)\n                max_costs.append(max_cost)\n\n            is_last_row = benchmark_idx &gt;= (nrows - 1) * ncols\n            is_first_column = benchmark_idx % ncols == 0\n            xlabel = \"Evaluations\" if key_to_extract is None else key_to_extract.upper()\n            _plot_incumbent(\n                ax=_map_axs(\n                    axs,\n                    benchmark_idx,\n                    len(benchmarks),\n                    ncols,\n                ),\n                x=costs,\n                y=incumbents,\n                scale_x=max(max_costs) if key_to_extract == \"fidelity\" else None,\n                title=benchmark if scientific_mode else None,\n                xlabel=xlabel if is_last_row else None,\n                ylabel=\"Best error\" if is_first_column else None,\n                log_x=log_x,\n                log_y=log_y,\n                x_range=x_range,\n                label=algorithm,\n            )\n\n    if scientific_mode:\n        _set_legend(\n            fig,\n            axs,\n            benchmarks=benchmarks,\n            algorithms=algorithms,\n            nrows=nrows,\n            ncols=ncols,\n        )\n    _save_fig(fig, output_dir=base_path, filename=filename, extension=extension, dpi=dpi)\n    logger.info(f\"Saved to '{base_path}/{filename}.{extension}'\")\n</code></pre>"},{"location":"api/neps/plot/plot3D/","title":"plot3D","text":"<p>Plot a 3D landscape of learning curves for a given run.</p>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D","title":"Plotter3D  <code>dataclass</code>","text":"<pre><code>Plotter3D(\n    objective_to_minimize_key: str = \"Objective to minimize\",\n    fidelity_key: str = \"epochs\",\n    run_path: str | Path | None = None,\n    scatter: bool = True,\n    footnote: bool = True,\n    alpha: float = 0.9,\n    scatter_size: float | int = 3,\n    bck_color_2d: tuple[float, float, float] = (\n        0.8,\n        0.82,\n        0.8,\n    ),\n    view_angle: tuple[float, float] = (15, -70),\n)\n</code></pre> <p>Plot a 3d landscape of learning curves for a given run.</p>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_color","title":"get_color  <code>staticmethod</code>","text":"<pre><code>get_color(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the color values for the plot.</p> Source code in <code>neps\\plot\\plot3D.py</code> <pre><code>@staticmethod\ndef get_color(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the color values for the plot.\"\"\"\n    return df.index.to_numpy()  # type: ignore\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_x","title":"get_x  <code>staticmethod</code>","text":"<pre><code>get_x(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the x-axis values for the plot.</p> Source code in <code>neps\\plot\\plot3D.py</code> <pre><code>@staticmethod\ndef get_x(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the x-axis values for the plot.\"\"\"\n    return df[\"epochID\"].to_numpy()  # type: ignore\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_y","title":"get_y  <code>staticmethod</code>","text":"<pre><code>get_y(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the y-axis values for the plot.</p> Source code in <code>neps\\plot\\plot3D.py</code> <pre><code>@staticmethod\ndef get_y(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the y-axis values for the plot.\"\"\"\n    y_ = df[\"configID\"].to_numpy()\n    return np.ones_like(y_) * y_[0]  # type: ignore\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_z","title":"get_z  <code>staticmethod</code>","text":"<pre><code>get_z(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the z-axis values for the plot.</p> Source code in <code>neps\\plot\\plot3D.py</code> <pre><code>@staticmethod\ndef get_z(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the z-axis values for the plot.\"\"\"\n    return df[\"objective_to_minimize\"].to_numpy()  # type: ignore\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.plot3D","title":"plot3D","text":"<pre><code>plot3D(\n    data: DataFrame | None = None,\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None\n</code></pre> <p>Plot the 3D landscape of learning curves.</p> Source code in <code>neps\\plot\\plot3D.py</code> <pre><code>def plot3D(  # noqa: N802, PLR0915\n    self,\n    data: pd.DataFrame | None = None,\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None:\n    \"\"\"Plot the 3D landscape of learning curves.\"\"\"\n    data = self.prep_df(data)\n\n    # Create the figure and the axes for the plot\n    fig, (ax3D, ax, cax) = plt.subplots(\n        1, 3, figsize=(12, 5), width_ratios=(20, 20, 1)\n    )\n\n    # remove a 2D axis and replace with a 3D projection one\n    ax3D.remove()\n    ax3D = fig.add_subplot(131, projection=\"3d\")\n\n    # Create the normalizer to normalize the color values\n    norm = Normalize(self.get_color(data).min(), self.get_color(data).max())\n\n    # Counters to keep track of the configurations run for only a single fidelity\n    n_lines = 0\n    n_points = 0\n\n    data_groups = data.groupby(\"configID\", sort=False)\n\n    for idx, (_configID, data_) in enumerate(data_groups):\n        x = self.get_x(data_)\n        y = self.get_y(data_)\n        z = self.get_z(data_)\n\n        y = np.ones_like(y) * idx\n        color = self.get_color(data_)\n\n        if len(x) &lt; 2:\n            n_points += 1\n            if self.scatter:\n                # 3D points\n                ax3D.scatter(\n                    y,\n                    z,\n                    s=self.scatter_size,\n                    zs=0,\n                    zdir=\"x\",\n                    c=color,\n                    cmap=\"RdYlBu_r\",\n                    norm=norm,\n                    alpha=self.alpha * 0.8,\n                )\n                # 2D points\n                ax.scatter(\n                    x,\n                    z,\n                    s=self.scatter_size,\n                    c=color,\n                    cmap=\"RdYlBu_r\",\n                    norm=norm,\n                    alpha=self.alpha * 0.8,\n                )\n        else:\n            n_lines += 1\n\n            # Plot 3D\n            # Get segments for all lines\n            points3D = np.array([x, y, z]).T.reshape(-1, 1, 3)\n            segments3D = np.concatenate([points3D[:-1], points3D[1:]], axis=1)\n\n            # Construct lines from segments\n            lc3D = Line3DCollection(\n                segments3D,  # type: ignore\n                cmap=\"RdYlBu_r\",\n                norm=norm,\n                alpha=self.alpha,\n            )\n            lc3D.set_array(color)\n\n            # Draw lines\n            ax3D.add_collection3d(lc3D)  # type: ignore\n\n            # Plot 2D\n            # Get segments for all lines\n            points = np.array([x, z]).T.reshape(-1, 1, 2)\n            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n\n            # Construct lines from segments\n            lc = LineCollection(\n                segments,  # type: ignore\n                cmap=\"RdYlBu_r\",\n                norm=norm,\n                alpha=self.alpha,  # type: ignore\n            )\n            lc.set_array(color)\n\n            # Draw lines\n            ax.add_collection(lc)\n\n    assert self.objective_to_minimize_range is not None\n    assert self.epochs_range is not None\n\n    ax3D.axes.set_xlim3d(left=self.epochs_range[0], right=self.epochs_range[1])  # type: ignore\n    ax3D.axes.set_ylim3d(bottom=0, top=data_groups.ngroups)  # type: ignore\n    ax3D.axes.set_zlim3d(  # type: ignore\n        bottom=self.objective_to_minimize_range[0],\n        top=self.objective_to_minimize_range[1],\n    )  # type: ignore\n\n    ax3D.set_xlabel(\"Epochs\")\n    ax3D.set_ylabel(\"Iteration sampled\")\n    ax3D.set_zlabel(f\"{self.objective_to_minimize_key}\")  # type: ignore\n\n    # set view angle\n    ax3D.view_init(elev=self.view_angle[0], azim=self.view_angle[1])  # type: ignore\n\n    ax.autoscale_view()\n    ax.set_xlabel(self.fidelity_key)\n    ax.set_ylabel(f\"{self.objective_to_minimize_key}\")\n    ax.set_facecolor(self.bck_color_2d)\n    fig.suptitle(\"ifBO run\")\n\n    if self.footnote:\n        fig.text(\n            0.01,\n            0.02,\n            f\"Total {n_lines + n_points} configs evaluated; for multiple budgets: \"\n            f\"{n_lines}, for single budget: {n_points}\",\n            ha=\"left\",\n            va=\"bottom\",\n            fontsize=10,\n        )\n\n    plt.colorbar(\n        cm.ScalarMappable(norm=norm, cmap=\"RdYlBu_r\"),\n        cax=cax,\n        label=\"Iteration\",\n        use_gridspec=True,\n        alpha=self.alpha,\n    )\n    fig.tight_layout()\n\n    self.save(save_path, filename)\n    plt.close(fig)\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.prep_df","title":"prep_df","text":"<pre><code>prep_df(df: DataFrame | None = None) -&gt; DataFrame\n</code></pre> <p>Prepare the dataframe for plotting.</p> Source code in <code>neps\\plot\\plot3D.py</code> <pre><code>def prep_df(self, df: pd.DataFrame | None = None) -&gt; pd.DataFrame:\n    \"\"\"Prepare the dataframe for plotting.\"\"\"\n    df = self.df if df is None else df\n\n    _fid_key = f\"config.{self.fidelity_key}\"\n    self.objective_to_minimize_range = (\n        df[\"objective_to_minimize\"].min(),\n        df[\"objective_to_minimize\"].max(),\n    )  # type: ignore\n    self.epochs_range = (df[_fid_key].min(), df[_fid_key].max())  # type: ignore\n\n    split_values = np.array([[*index.split(\"_\")] for index in df.index])\n    df[[\"configID\", \"epochID\"]] = split_values\n    df.configID = df.configID.astype(int)\n    df.epochID = df.epochID.astype(int)\n    if df.epochID.min() == 0:\n        df.epochID += 1\n\n    # indices become sampling order\n    time_cols = [\"time_started\", \"time_end\"]\n    return df.sort_values(by=time_cols).reset_index(drop=True)\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.save","title":"save","text":"<pre><code>save(\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None\n</code></pre> <p>Save the plot to a file.</p> Source code in <code>neps\\plot\\plot3D.py</code> <pre><code>def save(\n    self,\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None:\n    \"\"\"Save the plot to a file.\"\"\"\n    path = save_path if save_path is not None else self.run_path\n    assert path is not None\n\n    run_path = Path(path)\n    run_path.mkdir(parents=True, exist_ok=True)\n    assert run_path.is_dir()\n    plot_path = run_path / f\"Plot3D_{filename}.png\"\n    plt.savefig(plot_path, bbox_inches=\"tight\")\n    print(f\"Saved plot to {plot_path}\")  # noqa: T201\n</code></pre>"},{"location":"api/neps/plot/plotting/","title":"Plotting","text":"<p>Plotting functions for incumbent trajectory plots.</p>"},{"location":"api/neps/plot/read_results/","title":"Read results","text":"<p>Utility functions for reading and processing results.</p>"},{"location":"api/neps/plot/read_results/#neps.plot.read_results.process_seed","title":"process_seed","text":"<pre><code>process_seed(\n    *,\n    path: str | Path,\n    seed: str | int | None,\n    key_to_extract: str | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1\n) -&gt; tuple[list[float], list[float], float]\n</code></pre> <p>Reads and processes data per seed.</p> Source code in <code>neps\\plot\\read_results.py</code> <pre><code>def process_seed(\n    *,\n    path: str | Path,\n    seed: str | int | None,\n    key_to_extract: str | None = None,  # noqa: ARG001\n    consider_continuations: bool = False,  # noqa: ARG001\n    n_workers: int = 1,  # noqa: ARG001\n) -&gt; tuple[list[float], list[float], float]:\n    \"\"\"Reads and processes data per seed.\"\"\"\n    path = Path(path)\n    if seed is not None:\n        path = path / str(seed) / \"neps_root_directory\"\n\n    _fulldf, _summary = neps.status(path, print_summary=False)\n    raise NotImplementedError(\n        \"I'm sorry, I broke this. We now dump all the information neps has available\"\n        \" into the above dataframe `fulldf`.\"\n    )\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/","title":"Tensorboard eval","text":"<p>The tblogger module provides a simplified interface for logging to TensorBoard.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger","title":"tblogger","text":"<p>The tblogger class provides a simplified interface for logging to tensorboard.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.ConfigWriter","title":"ConfigWriter  <code>staticmethod</code>","text":"<pre><code>ConfigWriter(\n    *, write_summary_incumbent: bool = True\n) -&gt; SummaryWriter\n</code></pre> <p>Creates and returns a TensorBoard SummaryWriter configured to write logs to the appropriate directory for NePS.</p> PARAMETER DESCRIPTION <code>write_summary_incumbent</code> <p>Determines whether to write summaries                             for the incumbent configurations.                             Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>SummaryWriter</code> <p>An instance of TensorBoard SummaryWriter pointing to the             designated NePS directory.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>neps\\plot\\tensorboard_eval.py</code> <pre><code>@staticmethod\ndef ConfigWriter(*, write_summary_incumbent: bool = True) -&gt; SummaryWriter:  # noqa: N802\n    \"\"\"Creates and returns a TensorBoard SummaryWriter configured to write logs\n    to the appropriate directory for NePS.\n\n    Args:\n        write_summary_incumbent (bool): Determines whether to write summaries\n                                        for the incumbent configurations.\n                                        Defaults to True.\n\n    Returns:\n        SummaryWriter: An instance of TensorBoard SummaryWriter pointing to the\n                    designated NePS directory.\n    \"\"\"\n    tblogger.write_incumbent = write_summary_incumbent\n    tblogger._initiate_internal_configurations()\n    # This code runs only once per config, to assign that config a config_writer.\n    if (\n        tblogger.config_previous_directory is None\n        and tblogger.config_working_directory is not None\n    ):\n        # If no fidelities are there yet, define the writer via the config_id\n        tblogger.config_id = str(tblogger.config_working_directory).rsplit(\n            \"/\", maxsplit=1\n        )[-1]\n        tblogger.config_writer = SummaryWriter(\n            tblogger.config_working_directory / \"tbevents\"\n        )\n        return tblogger.config_writer\n\n    # Searching for the initial directory where tensorboard events are stored.\n    if tblogger.config_working_directory is not None:\n        init_dir = get_initial_directory(\n            pipeline_directory=tblogger.config_working_directory\n        )\n        tblogger.config_id = str(init_dir).rsplit(\"/\", maxsplit=1)[-1]\n        if (init_dir / \"tbevents\").exists():\n            tblogger.config_writer = SummaryWriter(init_dir / \"tbevents\")\n            return tblogger.config_writer\n\n        raise FileNotFoundError(\n            \"'tbevents' was not found in the initial directory of the configuration.\"\n        )\n    return None\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.WriteIncumbent","title":"WriteIncumbent  <code>staticmethod</code>","text":"<pre><code>WriteIncumbent() -&gt; None\n</code></pre> <p>Allows for writing the incumbent of the current search.</p> Source code in <code>neps\\plot\\tensorboard_eval.py</code> <pre><code>@staticmethod\ndef WriteIncumbent() -&gt; None:  # noqa: N802\n    \"\"\"Allows for writing the incumbent of the current search.\"\"\"\n    tblogger._initiate_internal_configurations()\n    tblogger.write_incumbent = True\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.end_of_config","title":"end_of_config  <code>staticmethod</code>","text":"<pre><code>end_of_config(trial: Trial) -&gt; None\n</code></pre> <p>Closes the writer.</p> Source code in <code>neps\\plot\\tensorboard_eval.py</code> <pre><code>@staticmethod\ndef end_of_config(trial: Trial) -&gt; None:  # noqa: ARG004\n    \"\"\"Closes the writer.\"\"\"\n    if tblogger.config_writer:\n        # Close and reset previous config writers for consistent logging.\n        # Prevent conflicts by reinitializing writers when logging ongoing.\n        tblogger.config_writer.close()\n        tblogger.config_writer = None\n\n    if tblogger.write_incumbent:\n        tblogger._tracking_incumbent_api()\n</code></pre>"},{"location":"api/neps/sampling/distributions/","title":"Distributions","text":"<p>Custom distributions for NEPS.</p>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.TorchDistributionWithDomain","title":"TorchDistributionWithDomain  <code>dataclass</code>","text":"<pre><code>TorchDistributionWithDomain(\n    distribution: Distribution, domain: Domain\n)\n</code></pre> <p>A torch distribution with an associated domain it samples over.</p>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.TruncatedNormal","title":"TruncatedNormal","text":"<pre><code>TruncatedNormal(\n    loc: float | Tensor,\n    scale: float | Tensor,\n    a: float | Tensor,\n    b: float | Tensor,\n    validate_args: bool | None = None,\n    device: device | None = None,\n)\n</code></pre> <p>               Bases: <code>TruncatedStandardNormal</code></p> <p>Truncated Normal distribution.</p> <p>people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf</p> PARAMETER DESCRIPTION <code>loc</code> <p>The mean of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>scale</code> <p>The std of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>a</code> <p>The lower bound of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>b</code> <p>The upper bound of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>validate_args</code> <p>Whether to validate input.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to use.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\sampling\\distributions.py</code> <pre><code>def __init__(\n    self,\n    loc: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    a: float | torch.Tensor,\n    b: float | torch.Tensor,\n    validate_args: bool | None = None,\n    device: torch.device | None = None,\n):\n    \"\"\"Initialize a truncated standard normal distribution.\n\n    Args:\n        loc: The mean of the distribution.\n        scale: The std of the distribution.\n        a: The lower bound of the distribution.\n        b: The upper bound of the distribution.\n        validate_args: Whether to validate input.\n        device: Device to use.\n    \"\"\"\n    scale = torch.as_tensor(scale, device=device)\n    scale = scale.clamp_min(self.eps)\n\n    self.loc, self.scale, a, b = broadcast_all(loc, scale, a, b)\n    a = a.to(device)  # type: ignore\n    b = b.to(device)  # type: ignore\n    self._non_std_a = a\n    self._non_std_b = b\n    a = (a - self.loc) / self.scale\n    b = (b - self.loc) / self.scale\n    super().__init__(a, b, validate_args=validate_args)  # type: ignore\n    self._log_scale = self.scale.log()\n    self._mean = self._mean * self.scale + self.loc\n    self._variance = self._variance * self.scale**2\n    self._entropy += self._log_scale\n</code></pre>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.TruncatedStandardNormal","title":"TruncatedStandardNormal","text":"<pre><code>TruncatedStandardNormal(\n    a: Tensor,\n    b: Tensor,\n    validate_args: bool | None = None,\n    device: device | None = None,\n)\n</code></pre> <p>               Bases: <code>Distribution</code></p> <p>Truncated Standard Normal distribution.</p> <p>Source: people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf</p> PARAMETER DESCRIPTION <code>a</code> <p>Lower truncation bound.</p> <p> TYPE: <code>Tensor</code> </p> <code>b</code> <p>Upper truncation bound.</p> <p> TYPE: <code>Tensor</code> </p> <code>validate_args</code> <p>Whether to validate input.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to use.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\sampling\\distributions.py</code> <pre><code>def __init__(\n    self,\n    a: torch.Tensor,\n    b: torch.Tensor,\n    validate_args: bool | None = None,\n    device: torch.device | None = None,\n):\n    \"\"\"Initialize a truncated standard normal distribution.\n\n    Args:\n        a: Lower truncation bound.\n        b: Upper truncation bound.\n        validate_args: Whether to validate input.\n        device: Device to use.\n    \"\"\"\n    self.a, self.b = broadcast_all(a, b)\n    self.a = self.a.to(device)\n    self.b = self.b.to(device)\n\n    if isinstance(a, Number) and isinstance(b, Number):\n        batch_shape = torch.Size()\n    else:\n        batch_shape = self.a.size()\n\n    super().__init__(batch_shape, validate_args=validate_args)\n\n    if self.a.dtype != self.b.dtype:\n        raise ValueError(\"Truncation bounds types are different\")\n\n    if any((self.a &gt;= self.b).view(-1).tolist()):\n        raise ValueError(\"Incorrect truncation range\")\n\n    eps = self.eps\n    self._dtype_min_gt_0 = eps\n    self._dtype_max_lt_1 = 1 - eps\n    self._little_phi_a = self._little_phi(self.a)\n    self._little_phi_b = self._little_phi(self.b)\n    self._big_phi_a = self._big_phi(self.a)\n    self._big_phi_b = self._big_phi(self.b)\n    self._Z = (self._big_phi_b - self._big_phi_a).clamp(eps, 1 - eps)\n    self._log_Z = self._Z.log()\n    little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n    little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n    self._lpbb_m_lpaa_d_Z = (\n        self._little_phi_b * little_phi_coeff_b\n        - self._little_phi_a * little_phi_coeff_a\n    ) / self._Z\n    self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n    self._variance = (\n        1\n        - self._lpbb_m_lpaa_d_Z\n        - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n    )\n    self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n</code></pre>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.UniformWithUpperBound","title":"UniformWithUpperBound","text":"<p>               Bases: <code>Uniform</code></p> <p>Uniform distribution with upper bound inclusive.</p> <p>This is mostly a hack because torch's version of Uniform does not include the upper bound which only causes a problem when considering the log_prob. Otherwise the upper bound works with every other method.</p>"},{"location":"api/neps/sampling/priors/","title":"Priors","text":"<p>Priors for search spaces.</p> <p>Loosely speaking, they are joint distributions over multiple independent variables, i.e. each column of a tensor is assumed to be independent and can be acted on independently.</p> <p>See the class doc description of <code>Prior</code> for more details.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior","title":"CenteredPrior  <code>dataclass</code>","text":"<pre><code>CenteredPrior(\n    distributions: list[TorchDistributionWithDomain],\n)\n</code></pre> <p>               Bases: <code>Prior</code></p> <p>A prior that is centered around a given value with a given confidence.</p> <p>This prior is useful for creating priors for search spaces where the values are centered around a given value with a given confidence level.</p> <p>You can use a <code>torch.distribution.Uniform</code> for any values which do not have a center and confidence level, i.e. no prior information.</p> <p>You can create this class more easily using <code>Prior.from_parameters()</code> to use the <code>.prior</code> values of the parameters in a search space.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.distributions","title":"distributions  <code>instance-attribute</code>","text":"<pre><code>distributions: list[TorchDistributionWithDomain]\n</code></pre> <p>Distributions along with the corresponding domains they sample from.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.from_domains_and_centers","title":"from_domains_and_centers  <code>classmethod</code>","text":"<pre><code>from_domains_and_centers(\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: device | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior for a given list of domains.</p> <p>This is a lower level version of <code>from_parameters()</code> which requires a full specification of the domains and the centers.</p> <p>Will use a <code>TruncatedNormal</code> distribution for all parameters, except those who have a domain marked with <code>is_categorical=True</code>, using a <code>Categorical</code> distribution instead. If the center for a given domain is <code>None</code>, a uniform prior will be used instead.</p> <p>For non-categoricals, this will be interpreted as the mean and std <code>(1 - confidence)</code> for a truncnorm. For categorical values, the center will contain a probability mass of <code>confidence</code> with the remaining <code>(1 - confidence)</code> probability mass distributed uniformly amongest the other choices.</p> <p>The order of the items in <code>domains</code> matters and should align with any tensors that you will use to evaluate from the prior. I.e. the first domain in <code>domains</code> will be the first column of a tensor that this prior can be used on.</p> PARAMETER DESCRIPTION <code>domains</code> <p>domains over which to have a centered prior.</p> <p> TYPE: <code>Iterable[Domain] | ConfigEncoder</code> </p> <code>centers</code> <p>centers for the priors, i.e. the mode of the prior for that domain, along with the confidence of that mode, which get's re-interpreted as the std of the truncnorm or the probability mass for the categorical.</p> <p>If <code>None</code>, a uniform prior will be used.</p> <p>Warning</p> <p>The values contained in centers should be contained within the domain. All confidence levels should be within the <code>[0, 1]</code> range.</p> <p> TYPE: <code>Iterable[None | tuple[int | float, float]]</code> </p> <code>device</code> <p>Device to place the tensors on for distributions.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>A prior for the search space.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef from_domains_and_centers(\n    cls,\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: torch.device | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior for a given list of domains.\n\n    This is a lower level version of\n    [`from_parameters()`][neps.sampling.Prior.from_parameters] which\n    requires a full specification of the domains and the centers.\n\n    Will use a `TruncatedNormal` distribution for all parameters,\n    except those who have a domain marked with `is_categorical=True`,\n    using a `Categorical` distribution instead.\n    If the center for a given domain is `None`, a uniform prior\n    will be used instead.\n\n    For non-categoricals, this will be interpreted as the mean and\n    std `(1 - confidence)` for a truncnorm. For categorical values,\n    the _center_ will contain a probability mass of `confidence` with\n    the remaining `(1 - confidence)` probability mass distributed uniformly\n    amongest the other choices.\n\n    The order of the items in `domains` matters and should align\n    with any tensors that you will use to evaluate from the prior.\n    I.e. the first domain in `domains` will be the first column\n    of a tensor that this prior can be used on.\n\n    Args:\n        domains: domains over which to have a centered prior.\n        centers: centers for the priors, i.e. the mode of the prior for that\n            domain, along with the confidence of that mode, which get's\n            re-interpreted as the std of the truncnorm or the probability\n            mass for the categorical.\n\n            If `None`, a uniform prior will be used.\n\n            !!! warning\n\n                The values contained in centers should be contained within the\n                domain. All confidence levels should be within the `[0, 1]` range.\n\n        device: Device to place the tensors on for distributions.\n\n    Returns:\n        A prior for the search space.\n    \"\"\"\n    match domains:\n        case ConfigEncoder():\n            domains = domains.domains\n        case _:\n            domains = list(domains)\n\n    distributions: list[TorchDistributionWithDomain] = []\n    for domain, center_conf in zip(domains, centers, strict=True):\n        # If the center is None, we use a uniform distribution. We try to match\n        # the distributions to all be unit uniform as it can speed up sampling when\n        # consistentaly the same. This still works for categoricals\n        if center_conf is None:\n            if domain.is_categorical:\n                # Uniform categorical\n                n_cats = domain.cardinality\n                assert n_cats is not None\n                dist = TorchDistributionWithDomain(\n                    distribution=torch.distributions.Categorical(\n                        probs=torch.ones(n_cats, device=device) / n_cats,\n                        validate_args=False,\n                    ),\n                    domain=domain,\n                )\n                distributions.append(dist)\n            else:\n                distributions.append(UNIT_UNIFORM_DIST)\n\n            continue\n\n        center, conf = center_conf\n        assert 0 &lt;= conf &lt;= 1\n\n        # If categorical, treat it as a weighted distribution over integers\n        if domain.is_categorical:\n            domain_as_ints = domain.as_integer_domain()\n            assert domain_as_ints.cardinality is not None\n\n            weight_for_choice = conf\n            remaining_weight = 1 - weight_for_choice\n\n            distributed_weight = remaining_weight / (domain_as_ints.cardinality - 1)\n            weights = torch.full(\n                (domain_as_ints.cardinality,),\n                distributed_weight,\n                device=device,\n                dtype=torch.float64,\n            )\n            center_index = domain_as_ints.cast_one(center, frm=domain)\n            weights[int(center_index)] = conf\n\n            dist = TorchDistributionWithDomain(\n                distribution=torch.distributions.Categorical(\n                    probs=weights, validate_args=False\n                ),\n                domain=domain,\n            )\n            distributions.append(dist)\n            continue\n\n        # Otherwise, we use a continuous truncnorm\n        unit_center = domain.to_unit_one(center)\n        scale = torch.tensor(1 - conf, device=device, dtype=torch.float64)\n        a = torch.tensor(0.0, device=device, dtype=torch.float64)\n        b = torch.tensor(1.0, device=device, dtype=torch.float64)\n        dist = TorchDistributionWithDomain(\n            distribution=TruncatedNormal(\n                loc=unit_center,\n                scale=scale,\n                a=a,\n                b=b,\n                device=device,\n                validate_args=False,\n            ),\n            domain=Domain.unit_float(),\n        )\n        distributions.append(dist)\n\n    return CenteredPrior(distributions=distributions)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior distribution from dict of parameters.</p> PARAMETER DESCRIPTION <code>parameters</code> <p>The parameters to createa a prior from. Will look at the <code>.prior</code> and <code>.prior_confidence</code> of the parameters to create a truncated normal.</p> <p>Any parameters that do not have a <code>.prior</code> will be covered by a uniform distribution.</p> <p> TYPE: <code>Mapping[str, Categorical | Float | Integer]</code> </p> <code>center_values</code> <p>Any values that should be used instead of the parameter's <code>.prior</code>.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>confidence_values</code> <p>Any additional values that should be used for determining the strength of the prior. Values should be between 0 and 1. Overwrites whatever is set by default in the <code>.prior-confidence</code>.</p> <p> TYPE: <code>Mapping[str, float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>The prior distribution</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior distribution from dict of parameters.\n\n    Args:\n        parameters: The parameters to createa a prior from. Will look\n            at the `.prior` and `.prior_confidence` of the parameters\n            to create a truncated normal.\n\n            Any parameters that do not have a `.prior` will be covered by\n            a uniform distribution.\n        center_values: Any values that should be used instead of the\n            parameter's `.prior`.\n        confidence_values: Any additional values that should be\n            used for determining the strength of the prior. Values should\n            be between 0 and 1. Overwrites whatever is set by default in\n            the `.prior-confidence`.\n\n    Returns:\n        The prior distribution\n    \"\"\"\n    _mapping = {\"low\": 0.25, \"medium\": 0.5, \"high\": 0.75}\n\n    center_values = center_values or {}\n    confidence_values = confidence_values or {}\n    domains: list[Domain] = []\n    centers: list[tuple[Any, float] | None] = []\n\n    for name, hp in parameters.items():\n        domains.append(hp.domain)\n\n        default = center_values.get(name, hp.prior)\n        if default is None:\n            centers.append(None)\n            continue\n\n        confidence_score = confidence_values.get(name, _mapping[hp.prior_confidence])\n        center = hp.choices.index(default) if isinstance(hp, Categorical) else default\n        centers.append((center, confidence_score))\n\n    return Prior.from_domains_and_centers(domains=domains, centers=centers)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.pdf","title":"pdf","text":"<pre><code>pdf(\n    x: Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>def pdf(\n    self, x: torch.Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.sampling.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return torch.exp(self.log_pdf(x, frm=frm))\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.pdf_configs","title":"pdf_configs","text":"<pre><code>pdf_configs(\n    x: list[dict[str, Any]], *, frm: ConfigEncoder\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>def pdf_configs(self, x: list[dict[str, Any]], *, frm: ConfigEncoder) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.sampling.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return self.pdf(frm.encode(x), frm=frm)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>See <code>sample_configs()</code>.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_config(\n    self,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"See [`sample_configs()`][neps.sampling.Sampler.sample_configs].\"\"\"\n    return self.sample_configs(1, to, seed=seed, include=include)[0]\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.sample_configs","title":"sample_configs","text":"<pre><code>sample_configs(\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Sample configurations directly into a search space.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample.</p> <p> TYPE: <code>int</code> </p> <code>to</code> <p>The encoding to sample into.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Additional values to include in the configuration.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_configs(\n    self,\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Sample configurations directly into a search space.\n\n    Args:\n        n: The number of configurations to sample.\n        to: The encoding to sample into.\n        seed: The seed generator.\n        include: Additional values to include in the configuration.\n\n    Returns:\n        A list of configurations.\n    \"\"\"\n    tensors = self.sample(n, to=to, seed=seed)\n    configs = to.decode(tensors)\n    if include is None:\n        return configs\n    return [{**config, **include} for config in configs]\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ncols: int) -&gt; Uniform\n</code></pre> <p>Create a uniform prior for a given list of domains.</p> PARAMETER DESCRIPTION <code>ncols</code> <p>The number of columns in the tensor to sample.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef uniform(cls, ncols: int) -&gt; Uniform:\n    \"\"\"Create a uniform prior for a given list of domains.\n\n    Args:\n        ncols: The number of columns in the tensor to sample.\n    \"\"\"\n    return Uniform(ndim=ncols)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior","title":"Prior","text":"<p>               Bases: <code>Sampler</code></p> <p>A protocol for priors over search spaces.</p> <p>Extends from the <code>Sampler</code> protocol.</p> <p>At it's core, the two methods that need to be implemented are <code>log_pdf</code> and <code>sample</code>. The <code>log_pdf</code> method should return the log probability of a given tensor of samples under its distribution. The <code>sample</code> method should return a tensor of samples from distribution.</p> <p>All values given to the <code>log_pdf</code> and the ones returned from the <code>sample</code> method are assumed to be in the value domain of the prior, i.e. the <code>.domains</code> attribute.</p> <p>Warning</p> <p>The domain in which samples are actually drawn from not necessarily need to match that of the value domain. For example, the <code>Uniform</code> class uses a unit uniform distribution to sample from the unit interval before converting samples to the value domain.</p> <p>As a result, the <code>log_pdf</code> and <code>pdf</code> method may not give the same values as you might expect for a distribution over the value domain.</p> <p>For example, consider a value domain <code>[0, 1e9]</code>. You might expect the <code>pdf</code> to be <code>1e-9</code> (1 / 1e9) for any given value inside the domain. However, since the <code>Uniform</code> samples from the unit interval, the <code>pdf</code> will actually be <code>1</code> (1 / 1) for any value inside the domain.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.ncols","title":"ncols  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ncols: int\n</code></pre> <p>The number of columns in the samples produced by this sampler.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.from_domains_and_centers","title":"from_domains_and_centers  <code>classmethod</code>","text":"<pre><code>from_domains_and_centers(\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: device | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior for a given list of domains.</p> <p>This is a lower level version of <code>from_parameters()</code> which requires a full specification of the domains and the centers.</p> <p>Will use a <code>TruncatedNormal</code> distribution for all parameters, except those who have a domain marked with <code>is_categorical=True</code>, using a <code>Categorical</code> distribution instead. If the center for a given domain is <code>None</code>, a uniform prior will be used instead.</p> <p>For non-categoricals, this will be interpreted as the mean and std <code>(1 - confidence)</code> for a truncnorm. For categorical values, the center will contain a probability mass of <code>confidence</code> with the remaining <code>(1 - confidence)</code> probability mass distributed uniformly amongest the other choices.</p> <p>The order of the items in <code>domains</code> matters and should align with any tensors that you will use to evaluate from the prior. I.e. the first domain in <code>domains</code> will be the first column of a tensor that this prior can be used on.</p> PARAMETER DESCRIPTION <code>domains</code> <p>domains over which to have a centered prior.</p> <p> TYPE: <code>Iterable[Domain] | ConfigEncoder</code> </p> <code>centers</code> <p>centers for the priors, i.e. the mode of the prior for that domain, along with the confidence of that mode, which get's re-interpreted as the std of the truncnorm or the probability mass for the categorical.</p> <p>If <code>None</code>, a uniform prior will be used.</p> <p>Warning</p> <p>The values contained in centers should be contained within the domain. All confidence levels should be within the <code>[0, 1]</code> range.</p> <p> TYPE: <code>Iterable[None | tuple[int | float, float]]</code> </p> <code>device</code> <p>Device to place the tensors on for distributions.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>A prior for the search space.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef from_domains_and_centers(\n    cls,\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: torch.device | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior for a given list of domains.\n\n    This is a lower level version of\n    [`from_parameters()`][neps.sampling.Prior.from_parameters] which\n    requires a full specification of the domains and the centers.\n\n    Will use a `TruncatedNormal` distribution for all parameters,\n    except those who have a domain marked with `is_categorical=True`,\n    using a `Categorical` distribution instead.\n    If the center for a given domain is `None`, a uniform prior\n    will be used instead.\n\n    For non-categoricals, this will be interpreted as the mean and\n    std `(1 - confidence)` for a truncnorm. For categorical values,\n    the _center_ will contain a probability mass of `confidence` with\n    the remaining `(1 - confidence)` probability mass distributed uniformly\n    amongest the other choices.\n\n    The order of the items in `domains` matters and should align\n    with any tensors that you will use to evaluate from the prior.\n    I.e. the first domain in `domains` will be the first column\n    of a tensor that this prior can be used on.\n\n    Args:\n        domains: domains over which to have a centered prior.\n        centers: centers for the priors, i.e. the mode of the prior for that\n            domain, along with the confidence of that mode, which get's\n            re-interpreted as the std of the truncnorm or the probability\n            mass for the categorical.\n\n            If `None`, a uniform prior will be used.\n\n            !!! warning\n\n                The values contained in centers should be contained within the\n                domain. All confidence levels should be within the `[0, 1]` range.\n\n        device: Device to place the tensors on for distributions.\n\n    Returns:\n        A prior for the search space.\n    \"\"\"\n    match domains:\n        case ConfigEncoder():\n            domains = domains.domains\n        case _:\n            domains = list(domains)\n\n    distributions: list[TorchDistributionWithDomain] = []\n    for domain, center_conf in zip(domains, centers, strict=True):\n        # If the center is None, we use a uniform distribution. We try to match\n        # the distributions to all be unit uniform as it can speed up sampling when\n        # consistentaly the same. This still works for categoricals\n        if center_conf is None:\n            if domain.is_categorical:\n                # Uniform categorical\n                n_cats = domain.cardinality\n                assert n_cats is not None\n                dist = TorchDistributionWithDomain(\n                    distribution=torch.distributions.Categorical(\n                        probs=torch.ones(n_cats, device=device) / n_cats,\n                        validate_args=False,\n                    ),\n                    domain=domain,\n                )\n                distributions.append(dist)\n            else:\n                distributions.append(UNIT_UNIFORM_DIST)\n\n            continue\n\n        center, conf = center_conf\n        assert 0 &lt;= conf &lt;= 1\n\n        # If categorical, treat it as a weighted distribution over integers\n        if domain.is_categorical:\n            domain_as_ints = domain.as_integer_domain()\n            assert domain_as_ints.cardinality is not None\n\n            weight_for_choice = conf\n            remaining_weight = 1 - weight_for_choice\n\n            distributed_weight = remaining_weight / (domain_as_ints.cardinality - 1)\n            weights = torch.full(\n                (domain_as_ints.cardinality,),\n                distributed_weight,\n                device=device,\n                dtype=torch.float64,\n            )\n            center_index = domain_as_ints.cast_one(center, frm=domain)\n            weights[int(center_index)] = conf\n\n            dist = TorchDistributionWithDomain(\n                distribution=torch.distributions.Categorical(\n                    probs=weights, validate_args=False\n                ),\n                domain=domain,\n            )\n            distributions.append(dist)\n            continue\n\n        # Otherwise, we use a continuous truncnorm\n        unit_center = domain.to_unit_one(center)\n        scale = torch.tensor(1 - conf, device=device, dtype=torch.float64)\n        a = torch.tensor(0.0, device=device, dtype=torch.float64)\n        b = torch.tensor(1.0, device=device, dtype=torch.float64)\n        dist = TorchDistributionWithDomain(\n            distribution=TruncatedNormal(\n                loc=unit_center,\n                scale=scale,\n                a=a,\n                b=b,\n                device=device,\n                validate_args=False,\n            ),\n            domain=Domain.unit_float(),\n        )\n        distributions.append(dist)\n\n    return CenteredPrior(distributions=distributions)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior distribution from dict of parameters.</p> PARAMETER DESCRIPTION <code>parameters</code> <p>The parameters to createa a prior from. Will look at the <code>.prior</code> and <code>.prior_confidence</code> of the parameters to create a truncated normal.</p> <p>Any parameters that do not have a <code>.prior</code> will be covered by a uniform distribution.</p> <p> TYPE: <code>Mapping[str, Categorical | Float | Integer]</code> </p> <code>center_values</code> <p>Any values that should be used instead of the parameter's <code>.prior</code>.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>confidence_values</code> <p>Any additional values that should be used for determining the strength of the prior. Values should be between 0 and 1. Overwrites whatever is set by default in the <code>.prior-confidence</code>.</p> <p> TYPE: <code>Mapping[str, float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>The prior distribution</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior distribution from dict of parameters.\n\n    Args:\n        parameters: The parameters to createa a prior from. Will look\n            at the `.prior` and `.prior_confidence` of the parameters\n            to create a truncated normal.\n\n            Any parameters that do not have a `.prior` will be covered by\n            a uniform distribution.\n        center_values: Any values that should be used instead of the\n            parameter's `.prior`.\n        confidence_values: Any additional values that should be\n            used for determining the strength of the prior. Values should\n            be between 0 and 1. Overwrites whatever is set by default in\n            the `.prior-confidence`.\n\n    Returns:\n        The prior distribution\n    \"\"\"\n    _mapping = {\"low\": 0.25, \"medium\": 0.5, \"high\": 0.75}\n\n    center_values = center_values or {}\n    confidence_values = confidence_values or {}\n    domains: list[Domain] = []\n    centers: list[tuple[Any, float] | None] = []\n\n    for name, hp in parameters.items():\n        domains.append(hp.domain)\n\n        default = center_values.get(name, hp.prior)\n        if default is None:\n            centers.append(None)\n            continue\n\n        confidence_score = confidence_values.get(name, _mapping[hp.prior_confidence])\n        center = hp.choices.index(default) if isinstance(hp, Categorical) else default\n        centers.append((center, confidence_score))\n\n    return Prior.from_domains_and_centers(domains=domains, centers=centers)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.log_pdf","title":"log_pdf  <code>abstractmethod</code>","text":"<pre><code>log_pdf(\n    x: Tensor, *, frm: ConfigEncoder | list[Domain] | Domain\n) -&gt; Tensor\n</code></pre> <p>Compute the log pdf of values in <code>x</code> under a prior.</p> <p>The last dimenion of <code>x</code> is assumed to be independent, such that the log pdf of the entire tensor is the sum of the log pdf of each element in that dimension.</p> <p>For example, if <code>x</code> is of shape <code>(n_samples, n_dims)</code>, then the you will be given back a tensor of shape <code>(n_samples,)</code> with the each entry being the log pdf of the corresponding sample.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of shape (..., n_dims) In the case of a 1D tensor, the shape is assumed to be (n_dims,)</p> <p> TYPE: <code>Tensor</code> </p> <code>frm</code> <p>The domain of the values in <code>x</code>. If a single domain, then all the values are assumed to be from that domain, otherwise each column <code>n_dims</code> in (n_samples, n_dims) is from the corresponding domain. If a <code>ConfigEncoder</code> is passed in, it will just take it's domains for use.</p> <p> TYPE: <code>ConfigEncoder | list[Domain] | Domain</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of shape (...,), with the last dimension reduced out. In the case that only single dimensional tensor is passed, the returns value is a scalar.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@abstractmethod\ndef log_pdf(\n    self,\n    x: torch.Tensor,\n    *,\n    frm: ConfigEncoder | list[Domain] | Domain,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the log pdf of values in `x` under a prior.\n\n    The last dimenion of `x` is assumed to be independent, such that the\n    log pdf of the entire tensor is the sum of the log\n    pdf of each element in that dimension.\n\n    For example, if `x` is of shape `(n_samples, n_dims)`, then the\n    you will be given back a tensor of shape `(n_samples,)` with the\n    each entry being the log pdf of the corresponding sample.\n\n    Args:\n        x: Tensor of shape (..., n_dims)\n            In the case of a 1D tensor, the shape is assumed to be (n_dims,)\n        frm: The domain of the values in `x`. If a single domain, then all the\n            values are assumed to be from that domain, otherwise each column\n            `n_dims` in (n_samples, n_dims) is from the corresponding domain.\n            If a `ConfigEncoder` is passed in, it will just take it's domains\n            for use.\n\n    Returns:\n        Tensor of shape (...,), with the last dimension reduced out. In the\n        case that only single dimensional tensor is passed, the returns value\n        is a scalar.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.pdf","title":"pdf","text":"<pre><code>pdf(\n    x: Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>def pdf(\n    self, x: torch.Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.sampling.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return torch.exp(self.log_pdf(x, frm=frm))\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.pdf_configs","title":"pdf_configs","text":"<pre><code>pdf_configs(\n    x: list[dict[str, Any]], *, frm: ConfigEncoder\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>def pdf_configs(self, x: list[dict[str, Any]], *, frm: ConfigEncoder) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.sampling.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return self.pdf(frm.encode(x), frm=frm)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(\n    n: int | Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample <code>n</code> points and convert them to the given domain.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of points to sample. If a torch.Size, an additional dimension will be added with <code>.ncols</code>. For example, if <code>n = 5</code>, the output will be <code>(5, ncols)</code>. If <code>n = (5, 3)</code>, the output will be <code>(5, 3, ncols)</code>.</p> <p> TYPE: <code>int | Size</code> </p> <code>to</code> <p>If a single domain, <code>.ncols</code> columns will be produced form that one domain. If a list of domains, then it must have the same length as the number of columns, with each column being in the corresponding domain.</p> <p> TYPE: <code>Domain | list[Domain] | ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the output tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device to cast the samples to.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor of (n, ndim) points sampled cast to the given domain.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    n: int | torch.Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample `n` points and convert them to the given domain.\n\n    Args:\n        n: The number of points to sample. If a torch.Size, an additional dimension\n            will be added with [`.ncols`][neps.sampling.Sampler.ncols].\n            For example, if `n = 5`, the output will be `(5, ncols)`. If\n            `n = (5, 3)`, the output will be `(5, 3, ncols)`.\n        to: If a single domain, `.ncols` columns will be produced form that one\n            domain. If a list of domains, then it must have the same length as the\n            number of columns, with each column being in the corresponding domain.\n        seed: The seed generator\n        dtype: The dtype of the output tensor.\n        device: The device to cast the samples to.\n\n    Returns:\n        A tensor of (n, ndim) points sampled cast to the given domain.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>See <code>sample_configs()</code>.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_config(\n    self,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"See [`sample_configs()`][neps.sampling.Sampler.sample_configs].\"\"\"\n    return self.sample_configs(1, to, seed=seed, include=include)[0]\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.sample_configs","title":"sample_configs","text":"<pre><code>sample_configs(\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Sample configurations directly into a search space.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample.</p> <p> TYPE: <code>int</code> </p> <code>to</code> <p>The encoding to sample into.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Additional values to include in the configuration.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_configs(\n    self,\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Sample configurations directly into a search space.\n\n    Args:\n        n: The number of configurations to sample.\n        to: The encoding to sample into.\n        seed: The seed generator.\n        include: Additional values to include in the configuration.\n\n    Returns:\n        A list of configurations.\n    \"\"\"\n    tensors = self.sample(n, to=to, seed=seed)\n    configs = to.decode(tensors)\n    if include is None:\n        return configs\n    return [{**config, **include} for config in configs]\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ncols: int) -&gt; Uniform\n</code></pre> <p>Create a uniform prior for a given list of domains.</p> PARAMETER DESCRIPTION <code>ncols</code> <p>The number of columns in the tensor to sample.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef uniform(cls, ncols: int) -&gt; Uniform:\n    \"\"\"Create a uniform prior for a given list of domains.\n\n    Args:\n        ncols: The number of columns in the tensor to sample.\n    \"\"\"\n    return Uniform(ndim=ncols)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform","title":"Uniform  <code>dataclass</code>","text":"<pre><code>Uniform(ndim: int)\n</code></pre> <p>               Bases: <code>Prior</code></p> <p>A prior that is uniform over a given domain.</p> <p>Uses a UnitUniform under the hood before converting to the value domain.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.ndim","title":"ndim  <code>instance-attribute</code>","text":"<pre><code>ndim: int\n</code></pre> <p>The number of columns in the tensor to sample from.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.from_domains_and_centers","title":"from_domains_and_centers  <code>classmethod</code>","text":"<pre><code>from_domains_and_centers(\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: device | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior for a given list of domains.</p> <p>This is a lower level version of <code>from_parameters()</code> which requires a full specification of the domains and the centers.</p> <p>Will use a <code>TruncatedNormal</code> distribution for all parameters, except those who have a domain marked with <code>is_categorical=True</code>, using a <code>Categorical</code> distribution instead. If the center for a given domain is <code>None</code>, a uniform prior will be used instead.</p> <p>For non-categoricals, this will be interpreted as the mean and std <code>(1 - confidence)</code> for a truncnorm. For categorical values, the center will contain a probability mass of <code>confidence</code> with the remaining <code>(1 - confidence)</code> probability mass distributed uniformly amongest the other choices.</p> <p>The order of the items in <code>domains</code> matters and should align with any tensors that you will use to evaluate from the prior. I.e. the first domain in <code>domains</code> will be the first column of a tensor that this prior can be used on.</p> PARAMETER DESCRIPTION <code>domains</code> <p>domains over which to have a centered prior.</p> <p> TYPE: <code>Iterable[Domain] | ConfigEncoder</code> </p> <code>centers</code> <p>centers for the priors, i.e. the mode of the prior for that domain, along with the confidence of that mode, which get's re-interpreted as the std of the truncnorm or the probability mass for the categorical.</p> <p>If <code>None</code>, a uniform prior will be used.</p> <p>Warning</p> <p>The values contained in centers should be contained within the domain. All confidence levels should be within the <code>[0, 1]</code> range.</p> <p> TYPE: <code>Iterable[None | tuple[int | float, float]]</code> </p> <code>device</code> <p>Device to place the tensors on for distributions.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>A prior for the search space.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef from_domains_and_centers(\n    cls,\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: torch.device | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior for a given list of domains.\n\n    This is a lower level version of\n    [`from_parameters()`][neps.sampling.Prior.from_parameters] which\n    requires a full specification of the domains and the centers.\n\n    Will use a `TruncatedNormal` distribution for all parameters,\n    except those who have a domain marked with `is_categorical=True`,\n    using a `Categorical` distribution instead.\n    If the center for a given domain is `None`, a uniform prior\n    will be used instead.\n\n    For non-categoricals, this will be interpreted as the mean and\n    std `(1 - confidence)` for a truncnorm. For categorical values,\n    the _center_ will contain a probability mass of `confidence` with\n    the remaining `(1 - confidence)` probability mass distributed uniformly\n    amongest the other choices.\n\n    The order of the items in `domains` matters and should align\n    with any tensors that you will use to evaluate from the prior.\n    I.e. the first domain in `domains` will be the first column\n    of a tensor that this prior can be used on.\n\n    Args:\n        domains: domains over which to have a centered prior.\n        centers: centers for the priors, i.e. the mode of the prior for that\n            domain, along with the confidence of that mode, which get's\n            re-interpreted as the std of the truncnorm or the probability\n            mass for the categorical.\n\n            If `None`, a uniform prior will be used.\n\n            !!! warning\n\n                The values contained in centers should be contained within the\n                domain. All confidence levels should be within the `[0, 1]` range.\n\n        device: Device to place the tensors on for distributions.\n\n    Returns:\n        A prior for the search space.\n    \"\"\"\n    match domains:\n        case ConfigEncoder():\n            domains = domains.domains\n        case _:\n            domains = list(domains)\n\n    distributions: list[TorchDistributionWithDomain] = []\n    for domain, center_conf in zip(domains, centers, strict=True):\n        # If the center is None, we use a uniform distribution. We try to match\n        # the distributions to all be unit uniform as it can speed up sampling when\n        # consistentaly the same. This still works for categoricals\n        if center_conf is None:\n            if domain.is_categorical:\n                # Uniform categorical\n                n_cats = domain.cardinality\n                assert n_cats is not None\n                dist = TorchDistributionWithDomain(\n                    distribution=torch.distributions.Categorical(\n                        probs=torch.ones(n_cats, device=device) / n_cats,\n                        validate_args=False,\n                    ),\n                    domain=domain,\n                )\n                distributions.append(dist)\n            else:\n                distributions.append(UNIT_UNIFORM_DIST)\n\n            continue\n\n        center, conf = center_conf\n        assert 0 &lt;= conf &lt;= 1\n\n        # If categorical, treat it as a weighted distribution over integers\n        if domain.is_categorical:\n            domain_as_ints = domain.as_integer_domain()\n            assert domain_as_ints.cardinality is not None\n\n            weight_for_choice = conf\n            remaining_weight = 1 - weight_for_choice\n\n            distributed_weight = remaining_weight / (domain_as_ints.cardinality - 1)\n            weights = torch.full(\n                (domain_as_ints.cardinality,),\n                distributed_weight,\n                device=device,\n                dtype=torch.float64,\n            )\n            center_index = domain_as_ints.cast_one(center, frm=domain)\n            weights[int(center_index)] = conf\n\n            dist = TorchDistributionWithDomain(\n                distribution=torch.distributions.Categorical(\n                    probs=weights, validate_args=False\n                ),\n                domain=domain,\n            )\n            distributions.append(dist)\n            continue\n\n        # Otherwise, we use a continuous truncnorm\n        unit_center = domain.to_unit_one(center)\n        scale = torch.tensor(1 - conf, device=device, dtype=torch.float64)\n        a = torch.tensor(0.0, device=device, dtype=torch.float64)\n        b = torch.tensor(1.0, device=device, dtype=torch.float64)\n        dist = TorchDistributionWithDomain(\n            distribution=TruncatedNormal(\n                loc=unit_center,\n                scale=scale,\n                a=a,\n                b=b,\n                device=device,\n                validate_args=False,\n            ),\n            domain=Domain.unit_float(),\n        )\n        distributions.append(dist)\n\n    return CenteredPrior(distributions=distributions)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior distribution from dict of parameters.</p> PARAMETER DESCRIPTION <code>parameters</code> <p>The parameters to createa a prior from. Will look at the <code>.prior</code> and <code>.prior_confidence</code> of the parameters to create a truncated normal.</p> <p>Any parameters that do not have a <code>.prior</code> will be covered by a uniform distribution.</p> <p> TYPE: <code>Mapping[str, Categorical | Float | Integer]</code> </p> <code>center_values</code> <p>Any values that should be used instead of the parameter's <code>.prior</code>.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>confidence_values</code> <p>Any additional values that should be used for determining the strength of the prior. Values should be between 0 and 1. Overwrites whatever is set by default in the <code>.prior-confidence</code>.</p> <p> TYPE: <code>Mapping[str, float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>The prior distribution</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior distribution from dict of parameters.\n\n    Args:\n        parameters: The parameters to createa a prior from. Will look\n            at the `.prior` and `.prior_confidence` of the parameters\n            to create a truncated normal.\n\n            Any parameters that do not have a `.prior` will be covered by\n            a uniform distribution.\n        center_values: Any values that should be used instead of the\n            parameter's `.prior`.\n        confidence_values: Any additional values that should be\n            used for determining the strength of the prior. Values should\n            be between 0 and 1. Overwrites whatever is set by default in\n            the `.prior-confidence`.\n\n    Returns:\n        The prior distribution\n    \"\"\"\n    _mapping = {\"low\": 0.25, \"medium\": 0.5, \"high\": 0.75}\n\n    center_values = center_values or {}\n    confidence_values = confidence_values or {}\n    domains: list[Domain] = []\n    centers: list[tuple[Any, float] | None] = []\n\n    for name, hp in parameters.items():\n        domains.append(hp.domain)\n\n        default = center_values.get(name, hp.prior)\n        if default is None:\n            centers.append(None)\n            continue\n\n        confidence_score = confidence_values.get(name, _mapping[hp.prior_confidence])\n        center = hp.choices.index(default) if isinstance(hp, Categorical) else default\n        centers.append((center, confidence_score))\n\n    return Prior.from_domains_and_centers(domains=domains, centers=centers)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.pdf","title":"pdf","text":"<pre><code>pdf(\n    x: Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>def pdf(\n    self, x: torch.Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.sampling.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return torch.exp(self.log_pdf(x, frm=frm))\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.pdf_configs","title":"pdf_configs","text":"<pre><code>pdf_configs(\n    x: list[dict[str, Any]], *, frm: ConfigEncoder\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>def pdf_configs(self, x: list[dict[str, Any]], *, frm: ConfigEncoder) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.sampling.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return self.pdf(frm.encode(x), frm=frm)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>See <code>sample_configs()</code>.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_config(\n    self,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"See [`sample_configs()`][neps.sampling.Sampler.sample_configs].\"\"\"\n    return self.sample_configs(1, to, seed=seed, include=include)[0]\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.sample_configs","title":"sample_configs","text":"<pre><code>sample_configs(\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Sample configurations directly into a search space.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample.</p> <p> TYPE: <code>int</code> </p> <code>to</code> <p>The encoding to sample into.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Additional values to include in the configuration.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_configs(\n    self,\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Sample configurations directly into a search space.\n\n    Args:\n        n: The number of configurations to sample.\n        to: The encoding to sample into.\n        seed: The seed generator.\n        include: Additional values to include in the configuration.\n\n    Returns:\n        A list of configurations.\n    \"\"\"\n    tensors = self.sample(n, to=to, seed=seed)\n    configs = to.decode(tensors)\n    if include is None:\n        return configs\n    return [{**config, **include} for config in configs]\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Uniform.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ncols: int) -&gt; Uniform\n</code></pre> <p>Create a uniform prior for a given list of domains.</p> PARAMETER DESCRIPTION <code>ncols</code> <p>The number of columns in the tensor to sample.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps\\sampling\\priors.py</code> <pre><code>@classmethod\ndef uniform(cls, ncols: int) -&gt; Uniform:\n    \"\"\"Create a uniform prior for a given list of domains.\n\n    Args:\n        ncols: The number of columns in the tensor to sample.\n    \"\"\"\n    return Uniform(ndim=ncols)\n</code></pre>"},{"location":"api/neps/sampling/samplers/","title":"Samplers","text":"<p>Samplers for generating points in a search space.</p> <p>These are similar to <code>Prior</code> objects, but they do not necessarily have an easily definable pdf.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler","title":"BorderSampler  <code>dataclass</code>","text":"<pre><code>BorderSampler(ndim: int)\n</code></pre> <p>               Bases: <code>Sampler</code></p> <p>A sampler that samples from the border of a hypercube.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.n_possible","title":"n_possible  <code>property</code>","text":"<pre><code>n_possible: int\n</code></pre> <p>The amount of possible border configurations.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>See <code>sample_configs()</code>.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_config(\n    self,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"See [`sample_configs()`][neps.sampling.Sampler.sample_configs].\"\"\"\n    return self.sample_configs(1, to, seed=seed, include=include)[0]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.sample_configs","title":"sample_configs","text":"<pre><code>sample_configs(\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Sample configurations directly into a search space.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample.</p> <p> TYPE: <code>int</code> </p> <code>to</code> <p>The encoding to sample into.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Additional values to include in the configuration.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_configs(\n    self,\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Sample configurations directly into a search space.\n\n    Args:\n        n: The number of configurations to sample.\n        to: The encoding to sample into.\n        seed: The seed generator.\n        include: Additional values to include in the configuration.\n\n    Returns:\n        A list of configurations.\n    \"\"\"\n    tensors = self.sample(n, to=to, seed=seed)\n    configs = to.decode(tensors)\n    if include is None:\n        return configs\n    return [{**config, **include} for config in configs]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; Uniform\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Uniform</code> <p>A uniform sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; Uniform:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import Uniform\n\n    return Uniform(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler","title":"Sampler","text":"<p>               Bases: <code>ABC</code></p> <p>A protocol for sampling tensors and vonerting them to a given domain.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.ncols","title":"ncols  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ncols: int\n</code></pre> <p>The number of columns in the samples produced by this sampler.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(\n    n: int | Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample <code>n</code> points and convert them to the given domain.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of points to sample. If a torch.Size, an additional dimension will be added with <code>.ncols</code>. For example, if <code>n = 5</code>, the output will be <code>(5, ncols)</code>. If <code>n = (5, 3)</code>, the output will be <code>(5, 3, ncols)</code>.</p> <p> TYPE: <code>int | Size</code> </p> <code>to</code> <p>If a single domain, <code>.ncols</code> columns will be produced form that one domain. If a list of domains, then it must have the same length as the number of columns, with each column being in the corresponding domain.</p> <p> TYPE: <code>Domain | list[Domain] | ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the output tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device to cast the samples to.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor of (n, ndim) points sampled cast to the given domain.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    n: int | torch.Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample `n` points and convert them to the given domain.\n\n    Args:\n        n: The number of points to sample. If a torch.Size, an additional dimension\n            will be added with [`.ncols`][neps.sampling.Sampler.ncols].\n            For example, if `n = 5`, the output will be `(5, ncols)`. If\n            `n = (5, 3)`, the output will be `(5, 3, ncols)`.\n        to: If a single domain, `.ncols` columns will be produced form that one\n            domain. If a list of domains, then it must have the same length as the\n            number of columns, with each column being in the corresponding domain.\n        seed: The seed generator\n        dtype: The dtype of the output tensor.\n        device: The device to cast the samples to.\n\n    Returns:\n        A tensor of (n, ndim) points sampled cast to the given domain.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>See <code>sample_configs()</code>.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_config(\n    self,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"See [`sample_configs()`][neps.sampling.Sampler.sample_configs].\"\"\"\n    return self.sample_configs(1, to, seed=seed, include=include)[0]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.sample_configs","title":"sample_configs","text":"<pre><code>sample_configs(\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Sample configurations directly into a search space.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample.</p> <p> TYPE: <code>int</code> </p> <code>to</code> <p>The encoding to sample into.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Additional values to include in the configuration.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_configs(\n    self,\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Sample configurations directly into a search space.\n\n    Args:\n        n: The number of configurations to sample.\n        to: The encoding to sample into.\n        seed: The seed generator.\n        include: Additional values to include in the configuration.\n\n    Returns:\n        A list of configurations.\n    \"\"\"\n    tensors = self.sample(n, to=to, seed=seed)\n    configs = to.decode(tensors)\n    if include is None:\n        return configs\n    return [{**config, **include} for config in configs]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; Uniform\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Uniform</code> <p>A uniform sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; Uniform:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import Uniform\n\n    return Uniform(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol","title":"Sobol  <code>dataclass</code>","text":"<pre><code>Sobol(ndim: int, scramble: bool = True)\n</code></pre> <p>               Bases: <code>Sampler</code></p> <p>Sample from a Sobol sequence.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.ndim","title":"ndim  <code>instance-attribute</code>","text":"<pre><code>ndim: int\n</code></pre> <p>The number of dimensions to sample for.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.scramble","title":"scramble  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scramble: bool = True\n</code></pre> <p>Whether to scramble the Sobol sequence.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>See <code>sample_configs()</code>.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_config(\n    self,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"See [`sample_configs()`][neps.sampling.Sampler.sample_configs].\"\"\"\n    return self.sample_configs(1, to, seed=seed, include=include)[0]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.sample_configs","title":"sample_configs","text":"<pre><code>sample_configs(\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Sample configurations directly into a search space.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample.</p> <p> TYPE: <code>int</code> </p> <code>to</code> <p>The encoding to sample into.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Additional values to include in the configuration.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_configs(\n    self,\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Sample configurations directly into a search space.\n\n    Args:\n        n: The number of configurations to sample.\n        to: The encoding to sample into.\n        seed: The seed generator.\n        include: Additional values to include in the configuration.\n\n    Returns:\n        A list of configurations.\n    \"\"\"\n    tensors = self.sample(n, to=to, seed=seed)\n    configs = to.decode(tensors)\n    if include is None:\n        return configs\n    return [{**config, **include} for config in configs]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; Uniform\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Uniform</code> <p>A uniform sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; Uniform:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import Uniform\n\n    return Uniform(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler","title":"WeightedSampler  <code>dataclass</code>","text":"<pre><code>WeightedSampler(\n    samplers: Sequence[Sampler], weights: Sequence[float]\n)\n</code></pre> <p>               Bases: <code>Sampler</code></p> <p>A sampler that samples from a weighted combination of samplers.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.sampler_probabilities","title":"sampler_probabilities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sampler_probabilities: Tensor = field(\n    init=False, repr=False\n)\n</code></pre> <p>The probabilities for each sampler. Normalized weights.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.samplers","title":"samplers  <code>instance-attribute</code>","text":"<pre><code>samplers: Sequence[Sampler]\n</code></pre> <p>The samplers to sample from.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights: Sequence[float]\n</code></pre> <p>The weights for each sampler.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>See <code>sample_configs()</code>.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_config(\n    self,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"See [`sample_configs()`][neps.sampling.Sampler.sample_configs].\"\"\"\n    return self.sample_configs(1, to, seed=seed, include=include)[0]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.sample_configs","title":"sample_configs","text":"<pre><code>sample_configs(\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: Generator | None = None,\n    include: Mapping[str, Any] | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Sample configurations directly into a search space.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configurations to sample.</p> <p> TYPE: <code>int</code> </p> <code>to</code> <p>The encoding to sample into.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Additional values to include in the configuration.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>def sample_configs(\n    self,\n    n: int,\n    to: ConfigEncoder,\n    *,\n    seed: torch.Generator | None = None,\n    include: Mapping[str, Any] | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Sample configurations directly into a search space.\n\n    Args:\n        n: The number of configurations to sample.\n        to: The encoding to sample into.\n        seed: The seed generator.\n        include: Additional values to include in the configuration.\n\n    Returns:\n        A list of configurations.\n    \"\"\"\n    tensors = self.sample(n, to=to, seed=seed)\n    configs = to.decode(tensors)\n    if include is None:\n        return configs\n    return [{**config, **include} for config in configs]\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; Uniform\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Uniform</code> <p>A uniform sampler.</p> Source code in <code>neps\\sampling\\samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; Uniform:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import Uniform\n\n    return Uniform(ndim=ndim)\n</code></pre>"},{"location":"api/neps/space/domain/","title":"Domain","text":"<p>A class representing a domain, a range for a value + properties.</p> <p>Some properties include:</p> <ul> <li>The lower and upper bounds of the domain.</li> <li>Whether the domain is a log domain.</li> <li>Whether the domain is float/int.</li> <li>Whether the domain is split into bins.</li> </ul> <p>With that, the primary method of a domain is to be able to <code>cast()</code> a tensor of values from one to domain to another, e.g. <code>values_a = domain_a.cast(values_b, frm=domain_b)</code>.</p> <p>This can be used to convert float samples to integers, integers to log space, etc.</p> <p>The core method to do so is to be able to cast <code>to_unit()</code> which takes values to a unit interval [0, 1], and then to be able to cast values in [0, 1] to the new domain with <code>from_unit()</code>.</p> <p>There are some shortcuts implemented in <code>cast</code>, such as skipping going through the unit interval if the domains are the same, as no transformation is needed.</p> <p>The primary methods for creating a domain are</p> <ul> <li><code>Domain.floating(l, u, ...)</code> -     Used for modelling float ranges</li> <li><code>Domain.integer(l, u, ...)</code> -     Used for modelling integer ranges</li> <li><code>Domain.indices(n)</code> -     Primarly used to model categorical choices</li> </ul> <p>If you have a tensor of values, where each column corresponds to a different domain, you can take a look at <code>Domain.translate()</code></p> <p>If you need a unit-interval domain, please use the <code>Domain.unit_float()</code>.</p>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain","title":"Domain  <code>dataclass</code>","text":"<pre><code>Domain(\n    lower: V,\n    upper: V,\n    round: bool,\n    log_bounds: tuple[float, float] | None = None,\n    bins: int | None = None,\n    is_categorical: bool = False,\n)\n</code></pre> <p>               Bases: <code>Generic[V]</code></p> <p>A domain for a value.</p> <p>The primary methods for creating a domain are</p> <ul> <li><code>Domain.floating(l, u, ...)</code> -     Used for modelling float ranges</li> <li><code>Domain.integer(l, u, ...)</code> -     Used for modelling integer ranges</li> <li><code>Domain.indices(n)</code> -     Primarly used to model categorical choices</li> </ul>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.bins","title":"bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bins: int | None = None\n</code></pre> <p>The number of discrete bins to split the domain into.</p> <p>Includes both endpoints of the domain and values are rounded to the nearest bin value.</p>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.is_categorical","title":"is_categorical  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_categorical: bool = False\n</code></pre> <p>Whether the domain is representing a categorical.</p> <p>The domain does not use this information directly, but it can be useful for external classes that consume Domain objects. This can only be set to <code>True</code> if the <code>cardinality</code> of the domain is finite, i.e. <code>bins</code> is not <code>None</code> OR <code>round</code> is <code>True</code> or the boundaries are both integers.</p>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.log_bounds","title":"log_bounds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_bounds: tuple[float, float] | None = None\n</code></pre> <p>The log bounds of the domain, if the domain is in log space.</p>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.lower","title":"lower  <code>instance-attribute</code>","text":"<pre><code>lower: V\n</code></pre> <p>The lower bound of the domain.</p>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.round","title":"round  <code>instance-attribute</code>","text":"<pre><code>round: bool\n</code></pre> <p>Whether to round the values to the nearest integer.</p>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.upper","title":"upper  <code>instance-attribute</code>","text":"<pre><code>upper: V\n</code></pre> <p>The upper bound of the domain.</p>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.as_integer_domain","title":"as_integer_domain","text":"<pre><code>as_integer_domain() -&gt; Domain\n</code></pre> <p>Get the integer version of this domain.</p> <p>Warning</p> <p>This is only possible if this domain has a finite cardinality</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>def as_integer_domain(self) -&gt; Domain:\n    \"\"\"Get the integer version of this domain.\n\n    !!! warning\n\n        This is only possible if this domain has a finite cardinality\n    \"\"\"\n    if self.cardinality is None:\n        raise ValueError(\n            \"Cannot get integer representation of this domain as its\"\n            \" cardinality is non-finite.\"\n        )\n\n    return Domain.indices(self.cardinality, is_categorical=self.is_categorical)\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.cast","title":"cast","text":"<pre><code>cast(\n    x: Tensor, frm: Domain, *, dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Cast a tensor of values frm the domain <code>frm</code> to this domain.</p> <p>If you need to cast a tensor of mixed domains, use <code>Domain.translate()</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of values in the <code>frm</code> domain to cast to this domain.</p> <p> TYPE: <code>Tensor</code> </p> <code>frm</code> <p>The domain to cast from.</p> <p> TYPE: <code>Domain</code> </p> <code>dtype</code> <p>The dtype to convert to</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Same shape tensor with the values cast to this domain.</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>def cast(self, x: Tensor, frm: Domain, *, dtype: torch.dtype | None = None) -&gt; Tensor:\n    \"\"\"Cast a tensor of values frm the domain `frm` to this domain.\n\n    If you need to cast a tensor of mixed domains, use\n    [`Domain.translate()`][neps.space.domain.Domain.translate].\n\n    Args:\n        x: Tensor of values in the `frm` domain to cast to this domain.\n        frm: The domain to cast from.\n        dtype: The dtype to convert to\n\n    Returns:\n        Same shape tensor with the values cast to this domain.\n    \"\"\"\n    dtype = dtype or self.preffered_dtype\n    # NOTE: In general, we should always be able to go through the unit interval\n    # [0, 1] to be able to transform between domains. However sometimes we can\n    # bypass some steps, dependant on the domains, hence the ugliness...\n\n    # Shortcut 1. (Same Domain)\n    # We can shortcut out going through normalized space if all the boundaries and\n    # they live on the same scale. However, if their bins don't line up, we will\n    # have to go through unit space to figure out the bins\n    same_bounds = self.lower == frm.lower and self.upper == frm.upper\n    same_log_bounds = self.log_bounds == frm.log_bounds\n    same_cardinality = self.cardinality == frm.cardinality\n    if same_bounds and same_log_bounds and same_cardinality:\n        if self.round:\n            x = torch.round(x)\n        return x.type(dtype)\n\n    # Shortcut 2. (From normalized)\n    # The domain we are coming from is already normalized, we only need to lift\n    if frm.is_unit_float:\n        return self.from_unit(x, dtype=dtype)  # type: ignore\n\n    # Shortcut 3. (Log lift)\n    # We can also shortcut out if the only diffrence is that we are coming frm the\n    # log bounds of this domain. We dont care if where we came from was binned or not,\n    # we just lift it up with `np.exp` and round if needed\n    if (self.lower, self.upper) == frm.log_bounds and self.cardinality is None:\n        x = torch.exp(x)\n        if self.round:\n            x = torch.round(x)\n        return x.type(dtype)\n\n    # Otherwise, through the unit interval we go\n    lift = self.from_unit(frm.to_unit(x), dtype=dtype)\n    return lift  # noqa: RET504\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.cast_one","title":"cast_one","text":"<pre><code>cast_one(x: float | int, frm: Domain) -&gt; V\n</code></pre> <p>Cast a single value from the domain <code>frm</code> to this domain.</p> PARAMETER DESCRIPTION <code>x</code> <p>Value in the <code>frm</code> domain to cast to this domain.</p> <p> TYPE: <code>float | int</code> </p> <code>frm</code> <p>The domain to cast from.</p> <p> TYPE: <code>Domain</code> </p> RETURNS DESCRIPTION <code>V</code> <p>Value cast to this domain.</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>def cast_one(self, x: float | int, frm: Domain) -&gt; V:\n    \"\"\"Cast a single value from the domain `frm` to this domain.\n\n    Args:\n        x: Value in the `frm` domain to cast to this domain.\n        frm: The domain to cast from.\n\n    Returns:\n        Value cast to this domain.\n    \"\"\"\n    return self.cast(torch.tensor(x), frm=frm).item()  # type: ignore\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.floating","title":"floating  <code>classmethod</code>","text":"<pre><code>floating(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False\n) -&gt; Domain[float]\n</code></pre> <p>Create a domain for a range of float values.</p> PARAMETER DESCRIPTION <code>lower</code> <p>The lower bound of the domain.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>The upper bound of the domain.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>Whether the domain is in log space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>bins</code> <p>The number of discrete bins to split the domain into.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>is_categorical</code> <p>Whether the domain is representing a categorical.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Domain[float]</code> <p>A domain for a range of float values.</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>@classmethod\ndef floating(\n    cls,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False,\n) -&gt; Domain[float]:\n    \"\"\"Create a domain for a range of float values.\n\n    Args:\n        lower: The lower bound of the domain.\n        upper: The upper bound of the domain.\n        log: Whether the domain is in log space.\n        bins: The number of discrete bins to split the domain into.\n        is_categorical: Whether the domain is representing a categorical.\n\n    Returns:\n        A domain for a range of float values.\n    \"\"\"\n    return Domain(\n        lower=float(lower),\n        upper=float(upper),\n        log_bounds=(math.log(lower), math.log(upper)) if log else None,\n        bins=bins,\n        round=False,\n        is_categorical=is_categorical,\n    )\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.from_unit","title":"from_unit","text":"<pre><code>from_unit(\n    x: Tensor, *, dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Transform a tensor of values from the unit interval [0, 1] to this domain.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of values in the unit interval [0, 1] to convert.</p> <p> TYPE: <code>Tensor</code> </p> <code>dtype</code> <p>The dtype to convert to</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Same shape tensor with the lifted into this domain.</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>def from_unit(self, x: Tensor, *, dtype: torch.dtype | None = None) -&gt; Tensor:\n    \"\"\"Transform a tensor of values from the unit interval [0, 1] to this domain.\n\n    Args:\n        x: A tensor of values in the unit interval [0, 1] to convert.\n        dtype: The dtype to convert to\n\n    Returns:\n        Same shape tensor with the lifted into this domain.\n    \"\"\"\n    dtype = dtype or self.preffered_dtype\n    if self.is_unit_float:\n        return x.to(dtype)\n\n    q = self.cardinality\n    if q is not None:\n        quantization_levels = torch.floor(x * q).clip(0, q - 1)\n        x = quantization_levels / (q - 1)\n\n    # Now we scale to the new domain\n    if self.log_bounds is not None:\n        lower, upper = self.log_bounds\n        x = x * (upper - lower) + lower\n        x = torch.exp(x)\n    else:\n        lower, upper = self.lower, self.upper\n        x = x * (upper - lower) + lower\n\n    if self.round:\n        x = torch.round(x)\n\n    if (x &gt; upper).any():\n        import warnings\n\n        warnings.warn(  # noqa: B028\n            \"Decoded value is above the upper bound of the domain. \"\n            \"Clipping to the upper bound. \"\n            \"This is likely due floating point precision in `torch.exp(x)` \"\n            \"with torch.float64.\"\n        )\n        x = torch.clip(x, max=self.upper)\n\n    return x.type(dtype)\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.indices","title":"indices  <code>classmethod</code>","text":"<pre><code>indices(\n    n: int, *, is_categorical: bool = False\n) -&gt; Domain[int]\n</code></pre> <p>Create a domain for a range of indices.</p> <p>Like range based functions this domain is inclusive of the lower bound and exclusive of the upper bound.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices.</p> <p> TYPE: <code>int</code> </p> <code>is_categorical</code> <p>Whether the domain is representing a categorical.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Domain[int]</code> <p>A domain for a range of indices.</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>@classmethod\ndef indices(cls, n: int, *, is_categorical: bool = False) -&gt; Domain[int]:\n    \"\"\"Create a domain for a range of indices.\n\n    Like range based functions this domain is inclusive of the lower bound\n    and exclusive of the upper bound.\n\n    Args:\n        n: The number of indices.\n        is_categorical: Whether the domain is representing a categorical.\n\n    Returns:\n        A domain for a range of indices.\n    \"\"\"\n    return Domain.integer(0, n - 1, is_categorical=is_categorical)\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.integer","title":"integer  <code>classmethod</code>","text":"<pre><code>integer(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False\n) -&gt; Domain[int]\n</code></pre> <p>Create a domain for a range of integer values.</p> PARAMETER DESCRIPTION <code>lower</code> <p>The lower bound of the domain.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>The upper bound of the domain (inclusive).</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>Whether the domain is in log space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>bins</code> <p>The number of discrete bins to split the domain into.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>is_categorical</code> <p>Whether the domain is representing a categorical.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Domain[int]</code> <p>A domain for a range of integer values.</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>@classmethod\ndef integer(\n    cls,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False,\n) -&gt; Domain[int]:\n    \"\"\"Create a domain for a range of integer values.\n\n    Args:\n        lower: The lower bound of the domain.\n        upper: The upper bound of the domain (inclusive).\n        log: Whether the domain is in log space.\n        bins: The number of discrete bins to split the domain into.\n        is_categorical: Whether the domain is representing a categorical.\n\n    Returns:\n        A domain for a range of integer values.\n    \"\"\"\n    return Domain(\n        lower=int(round(lower)),\n        upper=int(round(upper)),\n        log_bounds=(math.log(lower), math.log(upper)) if log else None,\n        round=True,\n        bins=bins,\n        is_categorical=is_categorical,\n    )\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.to_unit","title":"to_unit","text":"<pre><code>to_unit(x: Tensor, *, dtype: dtype | None = None) -&gt; Tensor\n</code></pre> <p>Transform a tensor of values from this domain to the unit interval [0, 1].</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of values in this domain to convert.</p> <p> TYPE: <code>Tensor</code> </p> <code>dtype</code> <p>The dtype to convert to</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Same shape tensor with the values normalized to the unit interval [0, 1].</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>def to_unit(self, x: Tensor, *, dtype: torch.dtype | None = None) -&gt; Tensor:\n    \"\"\"Transform a tensor of values from this domain to the unit interval [0, 1].\n\n    Args:\n        x: Tensor of values in this domain to convert.\n        dtype: The dtype to convert to\n\n    Returns:\n        Same shape tensor with the values normalized to the unit interval [0, 1].\n    \"\"\"\n    if dtype is None:\n        dtype = torch.float64\n    elif not dtype.is_floating_point:\n        raise ValueError(f\"Unit interval only allows floating dtypes, got {dtype}.\")\n\n    q = self.cardinality\n    if self.is_unit_float and q is None:\n        return x.to(dtype)\n\n    if self.log_bounds is not None:\n        x = torch.log(x)\n        lower, upper = self.log_bounds\n    else:\n        lower, upper = self.lower, self.upper\n\n    x = (x - lower) / (upper - lower)\n\n    if q is not None:\n        quantization_levels = torch.floor(x * q).clip(0, q - 1)\n        x = quantization_levels / (q - 1)\n\n    return x.type(dtype)\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.to_unit_one","title":"to_unit_one","text":"<pre><code>to_unit_one(x: float | int) -&gt; float\n</code></pre> <p>Transform a single value from this domain to the unit interval [0, 1].</p> PARAMETER DESCRIPTION <code>x</code> <p>Value in this domain to convert.</p> <p> TYPE: <code>float | int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Value normalized to the unit interval [0, 1].</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>def to_unit_one(self, x: float | int) -&gt; float:\n    \"\"\"Transform a single value from this domain to the unit interval [0, 1].\n\n    Args:\n        x: Value in this domain to convert.\n\n    Returns:\n        Value normalized to the unit interval [0, 1].\n    \"\"\"\n    return self.to_unit(torch.tensor(x)).item()\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.translate","title":"translate  <code>classmethod</code>","text":"<pre><code>translate(\n    x: Tensor,\n    frm: Domain | Iterable[Domain] | ConfigEncoder,\n    to: Domain | Iterable[Domain] | ConfigEncoder,\n    *,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Cast a tensor of mixed domains to a new set of mixed domains.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of shape (..., n_dims) with each dim <code>i</code> corresponding to the domain <code>frm[i]</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>frm</code> <p>List of domains to cast from. If list, must be length of <code>n_dims</code>, otherwise we assume the single domain provided is the one to be used across all dimensions.</p> <p> TYPE: <code>Domain | Iterable[Domain] | ConfigEncoder</code> </p> <code>to</code> <p>List of domains to cast to. If list, must be length as <code>n_dims</code>, otherwise we assume the single domain provided is the one to be used across all dimensions.</p> <p> TYPE: <code>Domain | Iterable[Domain] | ConfigEncoder</code> </p> <code>dtype</code> <p>The dtype of the converted tensor</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of the same shape as <code>x</code> with the last dimension casted     from the domain <code>frm[i]</code> to the domain <code>to[i]</code>.</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>@classmethod\ndef translate(\n    cls,\n    x: Tensor,\n    frm: Domain | Iterable[Domain] | ConfigEncoder,\n    to: Domain | Iterable[Domain] | ConfigEncoder,\n    *,\n    dtype: torch.dtype | None = None,\n) -&gt; Tensor:\n    \"\"\"Cast a tensor of mixed domains to a new set of mixed domains.\n\n    Args:\n        x: Tensor of shape (..., n_dims) with each dim `i` corresponding\n            to the domain `frm[i]`.\n        frm: List of domains to cast from. If list, must be length of `n_dims`,\n            otherwise we assume the single domain provided is the one to be used\n            across all dimensions.\n        to: List of domains to cast to. If list, must be length as `n_dims`,\n            otherwise we assume the single domain provided is the one to be used\n            across all dimensions.\n        dtype: The dtype of the converted tensor\n\n    Returns:\n        Tensor of the same shape as `x` with the last dimension casted\n            from the domain `frm[i]` to the domain `to[i]`.\n    \"\"\"\n    if x.ndim == 0:\n        raise ValueError(\"Expected a tensor with at least one dimension.\")\n\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n\n    ndims = x.shape[-1]\n\n    # If both are not a list, we can just cast the whole tensor\n    if isinstance(frm, Domain) and isinstance(to, Domain):\n        return to.cast(x, frm=frm, dtype=dtype)\n\n    from neps.space.encoding import ConfigEncoder\n\n    frm = (\n        [frm] * ndims\n        if isinstance(frm, Domain)\n        else (frm.domains if isinstance(frm, ConfigEncoder) else list(frm))\n    )\n    to = (\n        [to] * ndims\n        if isinstance(to, Domain)\n        else (to.domains if isinstance(to, ConfigEncoder) else list(to))\n    )\n\n    if len(frm) != ndims:\n        raise ValueError(\n            \"The number of domains in `frm` must match the number of tensors\"\n            \" if provided as a list.\"\n            f\" Expected {ndims} from last dimension of {x.shape}, got {len(frm)}.\"\n        )\n\n    if len(to) != ndims:\n        raise ValueError(\n            \"The number of domains in `to` must match the number of tensors\"\n            \" if provided as a list.\"\n            f\" Expected {ndims} from last dimension of {x.shape=}, got {len(to)}.\"\n        )\n\n    out = torch.empty_like(x, dtype=dtype)\n    for i, (f, t) in enumerate(zip(frm, to, strict=False)):\n        out[..., i] = t.cast(x[..., i], frm=f, dtype=dtype)\n\n    return out\n</code></pre>"},{"location":"api/neps/space/domain/#neps.space.domain.Domain.unit_float","title":"unit_float  <code>classmethod</code>","text":"<pre><code>unit_float() -&gt; Domain[float]\n</code></pre> <p>Get a domain for the unit interval [0, 1].</p> Source code in <code>neps\\space\\domain.py</code> <pre><code>@classmethod\ndef unit_float(cls) -&gt; Domain[float]:\n    \"\"\"Get a domain for the unit interval [0, 1].\"\"\"\n    return UNIT_FLOAT_DOMAIN\n</code></pre>"},{"location":"api/neps/space/encoding/","title":"Encoding","text":"<p>Encoding of hyperparameter configurations into tensors.</p> <p>For the most part, you can just use <code>ConfigEncoder.from_parameters()</code> to create an encoder over a list of hyperparameters.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.CategoricalToIntegerTransformer","title":"CategoricalToIntegerTransformer  <code>dataclass</code>","text":"<pre><code>CategoricalToIntegerTransformer(choices: Sequence[Any])\n</code></pre> <p>               Bases: <code>TensorTransformer[int]</code></p> <p>A transformer that encodes categorical values into integers.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.CategoricalToIntegerTransformer.encode_one","title":"encode_one","text":"<pre><code>encode_one(\n    x: Any,\n    *,\n    dtype: dtype | None = None,\n    device: device | None = None\n) -&gt; V\n</code></pre> <p>Encode a single hyperparameter value into a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A single hyperparameter value.</p> <p> TYPE: <code>Any</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>V</code> <p>The encoded tensor.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def encode_one(\n    self,\n    x: Any,\n    *,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; V:\n    \"\"\"Encode a single hyperparameter value into a tensor.\n\n    Args:\n        x: A single hyperparameter value.\n        dtype: The dtype of the tensor.\n        device: The device of the tensor.\n\n    Returns:\n        The encoded tensor.\n    \"\"\"\n    return self.encode([x], dtype=dtype, device=device).item()  # type: ignore\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.CategoricalToUnitNorm","title":"CategoricalToUnitNorm  <code>dataclass</code>","text":"<pre><code>CategoricalToUnitNorm(choices: Sequence[Any])\n</code></pre> <p>               Bases: <code>TensorTransformer[float]</code></p> <p>A transformer that encodes categorical values into a unit normalized tensor.</p> <p>If there are <code>n</code> choices, the tensor will have <code>n</code> bins between <code>0</code> and <code>1</code>.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.CategoricalToUnitNorm.encode_one","title":"encode_one","text":"<pre><code>encode_one(\n    x: Any,\n    *,\n    dtype: dtype | None = None,\n    device: device | None = None\n) -&gt; V\n</code></pre> <p>Encode a single hyperparameter value into a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A single hyperparameter value.</p> <p> TYPE: <code>Any</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>V</code> <p>The encoded tensor.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def encode_one(\n    self,\n    x: Any,\n    *,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; V:\n    \"\"\"Encode a single hyperparameter value into a tensor.\n\n    Args:\n        x: A single hyperparameter value.\n        dtype: The dtype of the tensor.\n        device: The device of the tensor.\n\n    Returns:\n        The encoded tensor.\n    \"\"\"\n    return self.encode([x], dtype=dtype, device=device).item()  # type: ignore\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder","title":"ConfigEncoder  <code>dataclass</code>","text":"<pre><code>ConfigEncoder(\n    transformers: dict[str, TensorTransformer],\n    constants: Mapping[str, Any] = dict(),\n)\n</code></pre> <p>An encoder for hyperparameter configurations.</p> <p>This class is used to encode and decode hyperparameter configurations into tensors and back. It's main uses currently are to support surrogate models that require tensors.</p> <p>The primary methods/properties to be aware of are: * <code>from_parameters()</code>: Create a     default encoder over a list of hyperparameters. Please see the method docs for     more details on how it encodes different types of hyperparameters. * [<code>encode()</code>]]neps.space.encoding.ConfigEncoder.encode]: Encode a list of     configurations into a single tensor using the transforms of the encoder. * <code>decode()</code>: Decode a 2d tensor     of length <code>N</code> into a list of <code>N</code> configurations. * <code>domains</code>: The     <code>Domain</code> that each hyperparameter is encoded     into. This is useful in combination with classes like     <code>Sampler</code>,     <code>Prior</code>, and     <code>TorchDistributionWithDomain</code>,     which require knowledge of the     domains of each column for the tensor, for example, to sample values directly     into the encoded space, getting log probabilities of the encoded values. * <code>ndim</code>: The number of columns     in the encoded tensor, useful for initializing some <code>Sampler</code>s.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder.domains","title":"domains  <code>property</code>","text":"<pre><code>domains: list[Domain]\n</code></pre> <p>The domains of the encoded hyperparameters.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>The number of columns in the encoded tensor.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder.decode","title":"decode","text":"<pre><code>decode(x: Tensor) -&gt; list[dict[str, Any]]\n</code></pre> <p>Decode a tensor of hyperparameter configurations into a list of configurations.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of shape <code>(N, ncols)</code> containing the encoded configurations.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of <code>N</code> configurations, including any constants that were included when creating the encoder.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; list[dict[str, Any]]:\n    \"\"\"Decode a tensor of hyperparameter configurations into a list of configurations.\n\n    Args:\n        x: A tensor of shape `(N, ncols)` containing the encoded configurations.\n\n    Returns:\n        A list of `N` configurations, including any constants that were included\n        when creating the encoder.\n    \"\"\"\n    values: dict[str, list[Any]] = {}\n    N = len(x)\n    for hp_name, transformer in self.transformers.items():\n        lookup = self.index_of[hp_name]\n        tensor = x[:, lookup]\n        values[hp_name] = transformer.decode(tensor)\n\n    constants = {name: [v] * N for name, v in self.constants.items()}\n    values.update(constants)\n\n    keys = list(values.keys())\n    return [\n        dict(zip(keys, vals, strict=False))\n        for vals in zip(*values.values(), strict=False)\n    ]\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder.decode_one","title":"decode_one","text":"<pre><code>decode_one(x: Tensor) -&gt; dict[str, Any]\n</code></pre> <p>Decode a tensor representing one configuration into a dict.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def decode_one(self, x: torch.Tensor) -&gt; dict[str, Any]:\n    \"\"\"Decode a tensor representing one configuration into a dict.\"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n    return self.decode(x)[0]\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder.encode","title":"encode","text":"<pre><code>encode(\n    x: Sequence[Mapping[str, Any]],\n    *,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Encode a list of hyperparameter configurations into a tensor.</p> <p>Constants</p> <p>Constants included in configurations will not be encoded into the tensor, but are included when decoding.</p> <p>Parameters with no transformers</p> <p>Any parameters in the configurations, whos key is not in <code>self.transformers</code>, will be ignored.</p> PARAMETER DESCRIPTION <code>x</code> <p>A list of hyperparameter configurations.</p> <p> TYPE: <code>Sequence[Mapping[str, Any]]</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor of shape <code>(len(x), ncols)</code> containing the encoded configurations.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def encode(\n    self,\n    x: Sequence[Mapping[str, Any]],\n    *,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Encode a list of hyperparameter configurations into a tensor.\n\n    !!! warning \"Constants\"\n\n        Constants included in configurations will not be encoded into the tensor,\n        but are included when decoding.\n\n    !!! warning \"Parameters with no transformers\"\n\n        Any parameters in the configurations, whos key is not in\n        `self.transformers`, will be ignored.\n\n    Args:\n        x: A list of hyperparameter configurations.\n        device: The device of the tensor.\n        dtype: The dtype of the tensor.\n\n    Returns:\n        A tensor of shape `(len(x), ncols)` containing the encoded configurations.\n    \"\"\"\n    dtype = torch.float64 if dtype is None else dtype\n    width = len(self.transformers)\n    buffer = torch.empty((len(x), width), dtype=dtype, device=device)\n\n    for hp_name, transformer in self.transformers.items():\n        values = [conf[hp_name] for conf in x]\n        lookup = self.index_of[hp_name]\n\n        # Encode directly into buffer\n        transformer.encode(\n            values,\n            out=buffer[:, lookup],\n            dtype=dtype,\n            device=device,\n        )\n\n    return buffer\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Parameter],\n    *,\n    custom_transformers: (\n        Mapping[str, TensorTransformer] | None\n    ) = None\n) -&gt; ConfigEncoder\n</code></pre> <p>Create a default encoder over a list of hyperparameters.</p> <p>This method creates a default encoder over a list of hyperparameters. It automatically creates transformers for each hyperparameter based on its type.</p> <p>The transformers are as follows:</p> <ul> <li><code>Float</code> and <code>Integer</code> are normalized to the unit interval.</li> <li><code>Categorical</code> is transformed into an integer.</li> </ul> PARAMETER DESCRIPTION <code>parameters</code> <p>The parameters to build an encoder for</p> <p> TYPE: <code>Mapping[str, Parameter]</code> </p> <code>custom_transformers</code> <p>A mapping of hyperparameter names to custom transformers to use</p> <p> TYPE: <code>Mapping[str, TensorTransformer] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ConfigEncoder</code> <p>A <code>ConfigEncoder</code> instance</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Parameter],\n    *,\n    custom_transformers: Mapping[str, TensorTransformer] | None = None,\n) -&gt; ConfigEncoder:\n    \"\"\"Create a default encoder over a list of hyperparameters.\n\n    This method creates a default encoder over a list of hyperparameters. It\n    automatically creates transformers for each hyperparameter based on its type.\n\n    The transformers are as follows:\n\n    * `Float` and `Integer` are normalized to the unit interval.\n    * `Categorical` is transformed into an integer.\n\n    Args:\n        parameters: The parameters to build an encoder for\n        custom_transformers: A mapping of hyperparameter names\n            to custom transformers to use\n\n    Returns:\n        A `ConfigEncoder` instance\n    \"\"\"\n    custom = custom_transformers or {}\n    transformers: dict[str, TensorTransformer] = {}\n    for name, hp in parameters.items():\n        if name in custom:\n            transformers[name] = custom[name]\n            continue\n\n        match hp:\n            case Float() | Integer():\n                transformers[name] = MinMaxNormalizer(hp.domain)  # type: ignore\n            case Categorical():\n                transformers[name] = CategoricalToIntegerTransformer(hp.choices)\n            case _:\n                raise ValueError(f\"Unsupported parameter type: {type(hp)}.\")\n\n    return cls(transformers)\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.ConfigEncoder.pdist","title":"pdist","text":"<pre><code>pdist(\n    x: Tensor,\n    *,\n    numerical_ord: int = 2,\n    categorical_ord: int = 0,\n    dtype: dtype = float64,\n    square_form: bool = False\n) -&gt; Tensor\n</code></pre> <p>Compute the pairwise distance between rows of a tensor.</p> <p>Will sum the results of the numerical and categorical distances. The encoding will be normalized such that all numericals lie within the unit cube, and categoricals will by default, have a <code>p=0</code> norm, which is equivalent to the Hamming distance.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of shape <code>(N, ncols)</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>numerical_ord</code> <p>The order of the norm to use for the numerical columns.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>categorical_ord</code> <p>The order of the norm to use for the categorical columns.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>dtype</code> <p>The dtype of the output tensor.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float64</code> </p> <code>square_form</code> <p>If <code>True</code>, the output will be a square matrix of shape <code>(N, N)</code>. If <code>False</code>, the output will be a single dim tensor of shape <code>1/2 * N * (N - 1)</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The distances, shaped according to <code>square_form</code>.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def pdist(\n    self,\n    x: torch.Tensor,\n    *,\n    numerical_ord: int = 2,\n    categorical_ord: int = 0,\n    dtype: torch.dtype = torch.float64,\n    square_form: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pairwise distance between rows of a tensor.\n\n    Will sum the results of the numerical and categorical distances.\n    The encoding will be normalized such that all numericals lie within the unit\n    cube, and categoricals will by default, have a `p=0` norm, which is equivalent\n    to the Hamming distance.\n\n    Args:\n        x: A tensor of shape `(N, ncols)`.\n        numerical_ord: The order of the norm to use for the numerical columns.\n        categorical_ord: The order of the norm to use for the categorical columns.\n        dtype: The dtype of the output tensor.\n        square_form: If `True`, the output will be a square matrix of shape\n            `(N, N)`. If `False`, the output will be a single dim tensor of shape\n            `1/2 * N * (N - 1)`.\n\n    Returns:\n        The distances, shaped according to `square_form`.\n    \"\"\"\n    dists: torch.Tensor | None = None\n    if self.numerical_slice is not None:\n        # Ensure they are all within the unit cube\n        numericals = Domain.translate(\n            x[..., self.numerical_slice],\n            frm=self.numerical_domains,\n            to=Domain.unit_float(),\n        )\n\n        dists = torch.nn.functional.pdist(numericals, p=numerical_ord)\n\n    if self.categorical_slice is not None:\n        cat_dists = torch.nn.functional.pdist(\n            x[..., self.categorical_slice],\n            p=categorical_ord,\n        )\n        dists = cat_dists if dists is None else (dists + cat_dists)\n\n    if dists is None:\n        raise ValueError(\"No columns to compute distances on.\")\n\n    if not square_form:\n        return dists\n\n    # Turn the single dimensional vector into a square matrix\n    N = len(x)\n    sq = torch.zeros((N, N), dtype=dtype)\n    row_ix, col_ix = torch.triu_indices(N, N, offset=1)\n    sq[row_ix, col_ix] = dists\n    sq[col_ix, row_ix] = dists\n    return sq\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.MinMaxNormalizer","title":"MinMaxNormalizer  <code>dataclass</code>","text":"<pre><code>MinMaxNormalizer(\n    original_domain: Domain[V], bins: int | None = None\n)\n</code></pre> <p>               Bases: <code>TensorTransformer[float]</code>, <code>Generic[V]</code></p> <p>A transformer that normalizes values to the unit interval.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.MinMaxNormalizer.encode_one","title":"encode_one","text":"<pre><code>encode_one(\n    x: Any,\n    *,\n    dtype: dtype | None = None,\n    device: device | None = None\n) -&gt; V\n</code></pre> <p>Encode a single hyperparameter value into a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A single hyperparameter value.</p> <p> TYPE: <code>Any</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>V</code> <p>The encoded tensor.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def encode_one(\n    self,\n    x: Any,\n    *,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; V:\n    \"\"\"Encode a single hyperparameter value into a tensor.\n\n    Args:\n        x: A single hyperparameter value.\n        dtype: The dtype of the tensor.\n        device: The device of the tensor.\n\n    Returns:\n        The encoded tensor.\n    \"\"\"\n    return self.encode([x], dtype=dtype, device=device).item()  # type: ignore\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.TensorTransformer","title":"TensorTransformer","text":"<p>               Bases: <code>Protocol[V]</code></p> <p>A protocol for encoding and decoding hyperparameter values into tensors.</p>"},{"location":"api/neps/space/encoding/#neps.space.encoding.TensorTransformer.decode","title":"decode","text":"<pre><code>decode(x: Tensor) -&gt; list[Any]\n</code></pre> <p>Decode a tensor of hyperparameter values into a sequence of values.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of hyperparameter values.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>list[Any]</code> <p>A sequence of hyperparameter values.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; list[Any]:\n    \"\"\"Decode a tensor of hyperparameter values into a sequence of values.\n\n    Args:\n        x: A tensor of hyperparameter values.\n\n    Returns:\n        A sequence of hyperparameter values.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.TensorTransformer.encode","title":"encode","text":"<pre><code>encode(\n    x: Sequence[Any],\n    *,\n    out: Tensor | None = None,\n    dtype: dtype | None = None,\n    device: device | None = None\n) -&gt; Tensor\n</code></pre> <p>Encode a sequence of hyperparameter values into a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A sequence of hyperparameter values.</p> <p> TYPE: <code>Sequence[Any]</code> </p> <code>out</code> <p>An optional tensor to write the encoded values to.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The encoded tensor.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def encode(\n    self,\n    x: Sequence[Any],\n    *,\n    out: torch.Tensor | None = None,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Encode a sequence of hyperparameter values into a tensor.\n\n    Args:\n        x: A sequence of hyperparameter values.\n        out: An optional tensor to write the encoded values to.\n        dtype: The dtype of the tensor.\n        device: The device of the tensor.\n\n    Returns:\n        The encoded tensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/space/encoding/#neps.space.encoding.TensorTransformer.encode_one","title":"encode_one","text":"<pre><code>encode_one(\n    x: Any,\n    *,\n    dtype: dtype | None = None,\n    device: device | None = None\n) -&gt; V\n</code></pre> <p>Encode a single hyperparameter value into a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A single hyperparameter value.</p> <p> TYPE: <code>Any</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>V</code> <p>The encoded tensor.</p> Source code in <code>neps\\space\\encoding.py</code> <pre><code>def encode_one(\n    self,\n    x: Any,\n    *,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; V:\n    \"\"\"Encode a single hyperparameter value into a tensor.\n\n    Args:\n        x: A single hyperparameter value.\n        dtype: The dtype of the tensor.\n        device: The device of the tensor.\n\n    Returns:\n        The encoded tensor.\n    \"\"\"\n    return self.encode([x], dtype=dtype, device=device).item()  # type: ignore\n</code></pre>"},{"location":"api/neps/space/parameters/","title":"Parameters","text":"<p>A module of all the parameters for the search space.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Parameter","title":"Parameter  <code>module-attribute</code>","text":"<pre><code>Parameter: TypeAlias = Float | Integer | Categorical\n</code></pre> <p>A type alias for all the parameter types.</p> <ul> <li><code>Float</code></li> <li><code>Integer</code></li> <li><code>Categorical</code></li> </ul> <p>A <code>Constant</code> is not included as it does not change value.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Categorical","title":"Categorical  <code>dataclass</code>","text":"<pre><code>Categorical(\n    choices: list[float | int | str],\n    prior: float | int | str | None = None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\",\n)\n</code></pre> <p>A list of unordered choices for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters that can take on a discrete set of unordered values. For example, the <code>optimizer</code> hyperparameter in a neural network search space can be a <code>Categorical</code> with choices like <code>[\"adam\", \"sgd\", \"rmsprop\"]</code>.</p> <pre><code>import neps\n\noptimizer_choice = neps.Categorical(\n    [\"adam\", \"sgd\", \"rmsprop\"],\n    prior=\"adam\"\n)\n</code></pre>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Categorical.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: float | int | str = field(init=False)\n</code></pre> <p>The center value of the categorical hyperparameter.</p> <p>As there is no natural center for a categorical parameter, this is the first value in the choices list.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Categorical.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: list[float | int | str]\n</code></pre> <p>The list of choices for the categorical hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Categorical.prior","title":"prior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior: float | int | str | None = None\n</code></pre> <p>The default value for the categorical hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Categorical.prior_confidence","title":"prior_confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior_confidence: Literal['low', 'medium', 'high'] = 'low'\n</code></pre> <p>Confidence score for the prior value when considering prior based optimization.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Constant","title":"Constant  <code>dataclass</code>","text":"<pre><code>Constant(value: Any)\n</code></pre> <p>A constant value for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters with values that should not change during optimization.</p> <p>For example, the <code>batch_size</code> hyperparameter in a neural network search space can be a <code>Constant</code> with a value of <code>32</code>.</p> <pre><code>import neps\n\nbatch_size = neps.Constant(32)\n</code></pre>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Constant.center","title":"center  <code>property</code>","text":"<pre><code>center: Any\n</code></pre> <p>The center of the hyperparameter.</p> <p>Warning</p> <p>There is no real center of a constant value, hence we take this to be the value itself.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float","title":"Float  <code>dataclass</code>","text":"<pre><code>Float(\n    lower: float,\n    upper: float,\n    log: bool = False,\n    prior: float | None = None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\",\n    is_fidelity: bool = False,\n)\n</code></pre> <p>A float value for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters with continuous float values, optionally specifying if it exists on a log scale.</p> <p>For example, <code>l2_norm</code> could be a value in <code>(0.1)</code>, while the <code>learning_rate</code> hyperparameter in a neural network search space can be a <code>Float</code> with a range of <code>(0.0001, 0.1)</code> but on a log scale.</p> <pre><code>import neps\n\nl2_norm = neps.Float(0, 1)\nlearning_rate = neps.Float(1e-4, 1e-1, log=True)\n</code></pre>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: float = field(init=False)\n</code></pre> <p>The center value of the numerical hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float.is_fidelity","title":"is_fidelity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_fidelity: bool = False\n</code></pre> <p>Whether the hyperparameter is fidelity.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float.log","title":"log  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log: bool = False\n</code></pre> <p>Whether the hyperparameter is in log space.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float.lower","title":"lower  <code>instance-attribute</code>","text":"<pre><code>lower: float\n</code></pre> <p>The lower bound of the numerical hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float.prior","title":"prior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior: float | None = None\n</code></pre> <p>Prior value for the hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float.prior_confidence","title":"prior_confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior_confidence: Literal['low', 'medium', 'high'] = 'low'\n</code></pre> <p>Confidence score for the prior value when considering prior based optimization.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Float.upper","title":"upper  <code>instance-attribute</code>","text":"<pre><code>upper: float\n</code></pre> <p>The upper bound of the numerical hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer","title":"Integer  <code>dataclass</code>","text":"<pre><code>Integer(\n    lower: int,\n    upper: int,\n    log: bool = False,\n    prior: int | None = None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\",\n    is_fidelity: bool = False,\n)\n</code></pre> <p>An integer value for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters with continuous integer values, optionally specifying f it exists on a log scale.</p> <p>For example, <code>batch_size</code> could be a value in <code>(32, 128)</code>, while the <code>num_layers</code> hyperparameter in a neural network search space can be a <code>Integer</code> with a range of <code>(1, 1000)</code> but on a log scale.</p> <pre><code>import neps\n\nbatch_size = neps.Integer(32, 128)\nnum_layers = neps.Integer(1, 1000, log=True)\n</code></pre>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: int = field(init=False)\n</code></pre> <p>The center value of the numerical hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer.is_fidelity","title":"is_fidelity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_fidelity: bool = False\n</code></pre> <p>Whether the hyperparameter is fidelity.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer.log","title":"log  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log: bool = False\n</code></pre> <p>Whether the hyperparameter is in log space.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer.lower","title":"lower  <code>instance-attribute</code>","text":"<pre><code>lower: int\n</code></pre> <p>The lower bound of the numerical hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer.prior","title":"prior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior: int | None = None\n</code></pre> <p>Prior value for the hyperparameter.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer.prior_confidence","title":"prior_confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior_confidence: Literal['low', 'medium', 'high'] = 'low'\n</code></pre> <p>Confidence score for the prior value when considering prior based optimization.</p>"},{"location":"api/neps/space/parameters/#neps.space.parameters.Integer.upper","title":"upper  <code>instance-attribute</code>","text":"<pre><code>upper: int\n</code></pre> <p>The upper bound of the numerical hyperparameter.</p>"},{"location":"api/neps/space/parsing/","title":"Parsing","text":"<p>This module contains functions for parsing search spaces.</p>"},{"location":"api/neps/space/parsing/#neps.space.parsing.as_parameter","title":"as_parameter","text":"<pre><code>as_parameter(\n    details: SerializedParameter,\n) -&gt; Parameter | Constant\n</code></pre> <p>Deduces the parameter type from details.</p> PARAMETER DESCRIPTION <code>details</code> <p>A dictionary containing parameter specifications or a direct value (string, integer, or float).</p> <p> TYPE: <code>SerializedParameter</code> </p> RETURNS DESCRIPTION <code>Parameter | Constant</code> <p>The deduced parameter type ('int', 'float', 'categorical', or 'constant').</p> RAISES DESCRIPTION <code>TypeError</code> <p>If the type cannot be deduced or the details don't align with expected     constraints.</p> Source code in <code>neps\\space\\parsing.py</code> <pre><code>def as_parameter(  # noqa: C901, PLR0911, PLR0912\n    details: SerializedParameter,\n) -&gt; Parameter | Constant:\n    \"\"\"Deduces the parameter type from details.\n\n    Args:\n        details: A dictionary containing parameter specifications or\n            a direct value (string, integer, or float).\n\n    Returns:\n        The deduced parameter type ('int', 'float', 'categorical', or 'constant').\n\n    Raises:\n        TypeError: If the type cannot be deduced or the details don't align with expected\n                constraints.\n    \"\"\"\n    match details:\n        # Constant\n        case str() | int() | float():\n            val = scientific_parse(details)\n            return Constant(val)\n\n        # Bounds of float or int\n        case tuple((x, y)):\n            _x = scientific_parse(x)\n            _y = scientific_parse(y)\n            match (_x, _y):\n                case (int(), int()):\n                    return Integer(_x, _y)\n                case (float(), float()):\n                    return Float(_x, _y)\n                case _:\n                    raise ValueError(\n                        f\"Expected both 'int' or 'float' for bounds but got {type(_x)=}\"\n                        f\" and {type(_y)=}.\"\n                    )\n        # Matches any sequence of length 2. We could have the issue that the user\n        # deserializes a yaml tuple pair which gets converted to a list.\n        # We interpret this as bounds if:\n        # 1. There are 2 elements\n        # 2. Both elements are co-ercible to the same number type\n        # 3. They are ordered\n        case (x, y):  # 1.\n            _x = scientific_parse(x)\n            _y = scientific_parse(y)\n            match (_x, _y):\n                case (int(), int()) if _x &lt;= _y:  # 2./3.\n                    return Integer(_x, _y)\n                case (float(), float()) if _x &lt;= _y:  # 2./3.\n                    return Float(_x, _y)\n\n                # Error case:\n                # We do have two numbers, but of different types. This could\n                # be user error so rather than guess, we raise an error.\n                case (int(), float()) | (float(), int()):\n                    raise ValueError(\n                        f\"Got a mix of a float and an int with {details=},\"\n                        \" tried to interpret these as bounds but found them to be\"\n                        \" different types.\"\n                        \"\\nIf you wanted to specify a categorical, i.e. a discrete\"\n                        f\" choices between the values {x=} and {y=}, then you can use\"\n                        \" the more verbose syntax of specifying 'type: cat'.\"\n                        \"\\nIf you did intend to specify bounds, then ensure that\"\n                        \" the values are both of the same type.\"\n                    )\n                # At least one of them is a string, so we treat is as categorical.\n                case _:\n                    return Categorical(choices=[_x, _y])\n\n        ## Categorical list of choices (tuple is reserved for bounds)\n        case Sequence() if not isinstance(details, tuple):\n            # It's unlikely that if we find an element that can be converted to\n            # scientific notation that we wouldn't want to do so, for example,\n            # when specifying a grid. Hence, we map over the list and convert\n            # what we can\n            details = [scientific_parse(d) for d in details]\n            return Categorical(details)\n\n        # Categorical dict declartion\n        case {\"choices\": choices, **rest}:\n            _type = rest.pop(\"type\", None)\n            if _type is not None and _type not in (\"cat\", \"categorical\"):\n                raise ValueError(f\"Unrecognized type '{_type}' with 'choices' set.\")\n\n            # See note above about scientific notation elements\n            choices = [scientific_parse(c) for c in choices]\n            return Categorical(choices, **rest)  # type: ignore\n\n        # Constant dict declartion\n        case {\"value\": v, **_rest}:\n            _type = _rest.pop(\"type\", None)\n            if _type is not None and _type not in (\"const\", \"constant\"):\n                raise ValueError(\n                    f\"Unrecognized type '{_type}' with 'value' set,\"\n                    f\" which indicates to treat value `{v}` a constant.\"\n                )\n\n            return Constant(v, **_rest)  # type: ignore\n\n        # Bounds dict declartion\n        case {\"lower\": l, \"upper\": u, **rest}:\n            _x = scientific_parse(l)\n            _y = scientific_parse(u)\n\n            _type = rest.pop(\"type\", None)\n            match _type:\n                case \"int\" | \"integer\":\n                    return Integer(_x, _y, **rest)  # type: ignore\n                case \"float\" | \"floating\":\n                    return Float(_x, _y, **rest)  # type: ignore\n                case None:\n                    match (_x, _y):\n                        case (int(), int()):\n                            return Integer(_x, _y, **rest)  # type: ignore\n                        case (float(), float()):\n                            return Float(_x, _y, **rest)  # type: ignore\n                        case _:\n                            raise ValueError(\n                                \"Expected both 'int' or 'float' for bounds but\"\n                                f\" got {type(_x)=} and {type(_y)=}.\"\n                            )\n                case _:\n                    raise ValueError(\n                        f\"Unrecognized type '{_type}' with both a 'lower'\"\n                        \" and 'upper' set.\"\n                    )\n        case _:\n            raise ValueError(\n                f\"Unable to deduce parameter with details '{details}'.\"\n                \" Please see our documentation for details.\"\n            )\n</code></pre>"},{"location":"api/neps/space/parsing/#neps.space.parsing.convert_configspace","title":"convert_configspace","text":"<pre><code>convert_configspace(\n    configspace: ConfigurationSpace,\n) -&gt; SearchSpace\n</code></pre> <p>Constructs a <code>SearchSpace</code> from a <code>ConfigurationSpace</code>.</p> PARAMETER DESCRIPTION <code>configspace</code> <p>The configuration space to construct the pipeline space from.</p> <p> TYPE: <code>ConfigurationSpace</code> </p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>A dictionary where keys are parameter names and values are parameter objects.</p> Source code in <code>neps\\space\\parsing.py</code> <pre><code>def convert_configspace(configspace: ConfigurationSpace) -&gt; SearchSpace:\n    \"\"\"Constructs a [`SearchSpace`][neps.space.SearchSpace]\n    from a [`ConfigurationSpace`](https://automl.github.io/ConfigSpace/latest/).\n\n    Args:\n        configspace: The configuration space to construct the pipeline space from.\n\n    Returns:\n        A dictionary where keys are parameter names and values are parameter objects.\n    \"\"\"\n    import ConfigSpace as CS\n\n    space: dict[str, Parameter | Constant] = {}\n    if any(configspace.conditions) or any(configspace.forbidden_clauses):\n        raise NotImplementedError(\n            \"The ConfigurationSpace has conditions or forbidden clauses, \"\n            \"which are not supported by neps.\"\n        )\n\n    for name, hyperparameter in configspace.items():\n        match hyperparameter:\n            case CS.Constant():\n                space[name] = Constant(value=hyperparameter.value)\n            case CS.CategoricalHyperparameter():\n                space[name] = Categorical(hyperparameter.choices)  # type: ignore\n            case CS.OrdinalHyperparameter():\n                raise ValueError(\n                    \"NePS does not support ordinals yet, please\"\n                    \" either convert it to an integer or use a\"\n                    \" categorical hyperparameter.\"\n                )\n            case CS.UniformIntegerHyperparameter():\n                space[name] = Integer(\n                    lower=hyperparameter.lower,\n                    upper=hyperparameter.upper,\n                    log=hyperparameter.log,\n                    prior=None,\n                )\n            case CS.UniformFloatHyperparameter():\n                space[name] = Float(\n                    lower=hyperparameter.lower,\n                    upper=hyperparameter.upper,\n                    log=hyperparameter.log,\n                    prior=None,\n                )\n\n            case CS.NormalFloatHyperparameter():\n                warnings.warn(\n                    \"NormalFloatHyperparameter is detected as a prior for NePS\"\n                    \" and will will consider it as a 'medium' prior_confidence.\"\n                    \" If you wish to silence this warning, please manually\"\n                    \" convert your ConfigurationSpace to a SearchSpace.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                space[name] = Float(\n                    lower=hyperparameter.lower,\n                    upper=hyperparameter.upper,\n                    log=hyperparameter.log,\n                    prior=hyperparameter.mu,\n                )\n            case CS.NormalIntegerHyperparameter():\n                warnings.warn(\n                    \"NormalIntegerHyperparameter is detected as a prior for NePS\"\n                    \" and will will consider it as a 'medium' prior_confidence.\"\n                    \" If you wish to silence this warning, please manually\"\n                    \" convert your ConfigurationSpace to a SearchSpace.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                space[name] = Integer(\n                    lower=hyperparameter.lower,\n                    upper=hyperparameter.upper,\n                    log=hyperparameter.log,\n                    prior=int(hyperparameter.mu),\n                )\n            case _:\n                raise ValueError(f\"Unknown hyperparameter type {hyperparameter}\")\n\n    return SearchSpace(space)\n</code></pre>"},{"location":"api/neps/space/parsing/#neps.space.parsing.convert_mapping","title":"convert_mapping","text":"<pre><code>convert_mapping(\n    pipeline_space: Mapping[str, Any],\n) -&gt; SearchSpace\n</code></pre> <p>Converts a dictionary to a SearchSpace object.</p> Source code in <code>neps\\space\\parsing.py</code> <pre><code>def convert_mapping(pipeline_space: Mapping[str, Any]) -&gt; SearchSpace:\n    \"\"\"Converts a dictionary to a SearchSpace object.\"\"\"\n    parameters: dict[str, Parameter | Constant] = {}\n    for name, details in pipeline_space.items():\n        match details:\n            case Float() | Integer() | Categorical() | Constant():\n                parameters[name] = dataclasses.replace(details)  # copy\n            case str() | int() | float() | Mapping():\n                try:\n                    parameters[name] = as_parameter(details)\n                except (TypeError, ValueError) as e:\n                    raise ValueError(f\"Error parsing parameter '{name}'\") from e\n            case None:\n                parameters[name] = Constant(None)\n            case _:\n                raise ValueError(\n                    f\"Unrecognized parameter type '{type(details)}' for '{name}'.\"\n                )\n\n    return SearchSpace(parameters)\n</code></pre>"},{"location":"api/neps/space/parsing/#neps.space.parsing.convert_to_space","title":"convert_to_space","text":"<pre><code>convert_to_space(\n    space: (\n        Mapping[str, dict | str | int | float | Parameter]\n        | SearchSpace\n        | ConfigurationSpace\n        | Pipeline\n    ),\n) -&gt; SearchSpace | Pipeline\n</code></pre> <p>Converts a search space to a SearchSpace object.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to convert.</p> <p> TYPE: <code>Mapping[str, dict | str | int | float | Parameter] | SearchSpace | ConfigurationSpace | Pipeline</code> </p> RETURNS DESCRIPTION <code>SearchSpace | Pipeline</code> <p>The SearchSpace object representing the search space.</p> Source code in <code>neps\\space\\parsing.py</code> <pre><code>def convert_to_space(\n    space: (\n        Mapping[str, dict | str | int | float | Parameter]\n        | SearchSpace\n        | ConfigurationSpace\n        | Pipeline\n    ),\n) -&gt; SearchSpace | Pipeline:\n    \"\"\"Converts a search space to a SearchSpace object.\n\n    Args:\n        space: The search space to convert.\n\n    Returns:\n        The SearchSpace object representing the search space.\n    \"\"\"\n    # We quickly check ConfigSpace because it inherits from Mapping\n    try:\n        from ConfigSpace import ConfigurationSpace\n\n        if isinstance(space, ConfigurationSpace):\n            return convert_configspace(space)\n    except ImportError:\n        pass\n\n    match space:\n        case SearchSpace():\n            return space\n        case Mapping():\n            return convert_mapping(space)\n        case Pipeline():\n            return space\n        case _:\n            raise ValueError(\n                f\"Unsupported type '{type(space)}' for conversion to SearchSpace.\"\n            )\n</code></pre>"},{"location":"api/neps/space/parsing/#neps.space.parsing.scientific_parse","title":"scientific_parse","text":"<pre><code>scientific_parse(\n    value: str | int | float,\n) -&gt; str | int | float\n</code></pre> <p>Parse a value that may be scientific notation.</p> Source code in <code>neps\\space\\parsing.py</code> <pre><code>def scientific_parse(value: str | int | float) -&gt; str | int | float:\n    \"\"\"Parse a value that may be scientific notation.\"\"\"\n    if not isinstance(value, str):\n        return value\n\n    value_no_space = value.replace(\" \", \"\")\n    is_scientific = re.match(E_NOTATION_PATTERN, value_no_space)\n\n    if not is_scientific:\n        return value\n\n    # We know there's an 'e' in the string,\n    # Now we need to check if its an integer or float\n    # `int` wont parse scientific notation so we first cast to float\n    # and see if it's the same as the int cast\n    float_val = float(value_no_space)\n    int_val = int(float_val)\n    if float_val == int_val:\n        return int_val\n\n    return float_val\n</code></pre>"},{"location":"api/neps/space/search_space/","title":"Search space","text":"<p>Contains the <code>SearchSpace</code> class which contains the hyperparameters for the search space, as well as any fidelities and constants.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace","title":"SearchSpace  <code>dataclass</code>","text":"<pre><code>SearchSpace(\n    elements: Mapping[str, Parameter | Constant] = dict(),\n)\n</code></pre> <p>               Bases: <code>Mapping[str, Parameter | Constant]</code></p> <p>A container for parameters.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace.categoricals","title":"categoricals  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>categoricals: Mapping[str, Categorical] = field(init=False)\n</code></pre> <p>The categorical hyperparameters in the search space.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace.constants","title":"constants  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constants: Mapping[str, Any] = field(\n    init=False, default_factory=dict\n)\n</code></pre> <p>The constants in the search space.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace.elements","title":"elements  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>elements: Mapping[str, Parameter | Constant] = field(\n    default_factory=dict\n)\n</code></pre> <p>All items in the search space.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Integer | Float] = field(\n    init=False\n)\n</code></pre> <p>The fidelities in the search space.</p> <p>Currently no optimizer supports multiple fidelities but it is defined here incase.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace.fidelity","title":"fidelity  <code>property</code>","text":"<pre><code>fidelity: tuple[str, Float | Integer] | None\n</code></pre> <p>The fidelity parameter for the search space.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace.numerical","title":"numerical  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>numerical: Mapping[str, Integer | Float] = field(init=False)\n</code></pre> <p>The numerical hyperparameters in the search space.</p> <p>Note</p> <p>This does not include fidelities.</p>"},{"location":"api/neps/space/search_space/#neps.space.search_space.SearchSpace.searchables","title":"searchables  <code>property</code>","text":"<pre><code>searchables: Mapping[str, Parameter]\n</code></pre> <p>The hyperparameters that can be searched over.</p> <p>Note</p> <p>This does not include either constants or fidelities.</p>"},{"location":"api/neps/space/neps_spaces/config_string/","title":"Config string","text":"<p>This module provides functionality to unwrap and wrap configuration strings used in NePS spaces. It defines the <code>UnwrappedConfigStringPart</code> data class to represent parts of the unwrapped configuration string and provides functions to unwrap a configuration string into these parts and to wrap unwrapped parts back into a configuration string.</p>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.ConfigString","title":"ConfigString","text":"<pre><code>ConfigString(config_string: str)\n</code></pre> <p>A class representing a configuration string in NePS spaces. It provides methods to unwrap the configuration string into structured parts, retrieve the maximum hierarchy level, and get a representation of the configuration at a specific hierarchy level.</p> PARAMETER DESCRIPTION <code>config_string</code> <p>The configuration string to be wrapped.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the config_string is None or empty.</p> Source code in <code>neps\\space\\neps_spaces\\config_string.py</code> <pre><code>def __init__(self, config_string: str) -&gt; None:\n    \"\"\"Initialize the ConfigString with a given configuration string.\n\n    Args:\n        config_string: The configuration string to be wrapped.\n\n    Raises:\n        ValueError: If the config_string is None or empty.\n    \"\"\"\n    if config_string is None or len(config_string) == 0:\n        raise ValueError(f\"Invalid config string: {config_string}\")\n    self.config_string = config_string\n\n    # The fields below are needed for lazy and cached evaluation.\n    # In python 3.8+ can be replaced by `cached_property`\n    self._unwrapped: tuple[UnwrappedConfigStringPart, ...] | None = None\n    self._max_hierarchy_level: int | None = None\n\n    # a cache for the different hierarchy levels of this config string\n    self._at_hierarchy_level_cache: dict[int, ConfigString] = {}\n</code></pre>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.ConfigString.max_hierarchy_level","title":"max_hierarchy_level  <code>property</code>","text":"<pre><code>max_hierarchy_level: int\n</code></pre> <p>Get the maximum hierarchy level of the configuration string.</p> RETURNS DESCRIPTION <code>int</code> <p>The maximum hierarchy level of the configuration string.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the maximum hierarchy level is invalid.</p>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.ConfigString.unwrapped","title":"unwrapped  <code>property</code>","text":"<pre><code>unwrapped: tuple[UnwrappedConfigStringPart, ...]\n</code></pre> <p>Get the unwrapped representation of the configuration string.</p> RETURNS DESCRIPTION <code>tuple[UnwrappedConfigStringPart, ...]</code> <p>A tuple of UnwrappedConfigStringPart objects representing the unwrapped config.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If there is an error unwrapping the config string.</p>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.ConfigString.at_hierarchy_level","title":"at_hierarchy_level","text":"<pre><code>at_hierarchy_level(level: int) -&gt; ConfigString\n</code></pre> <p>Get the configuration string at a specific hierarchy level.</p> PARAMETER DESCRIPTION <code>level</code> <p>The hierarchy level to retrieve the configuration string for.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ConfigString</code> <p>A ConfigString object representing the configuration at the specified hierarchy level.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the level is invalid (0 or out of bounds).</p> Source code in <code>neps\\space\\neps_spaces\\config_string.py</code> <pre><code>def at_hierarchy_level(self, level: int) -&gt; ConfigString:\n    \"\"\"Get the configuration string at a specific hierarchy level.\n\n    Args:\n        level: The hierarchy level to retrieve the configuration string for.\n\n    Returns:\n        A ConfigString object representing the configuration at the specified\n        hierarchy level.\n\n    Raises:\n        ValueError: If the level is invalid (0 or out of bounds).\n    \"\"\"\n    if level == 0:\n        raise ValueError(f\"Invalid value for `level`. Received level == 0: {level}\")\n    if level &gt; self.max_hierarchy_level:\n        raise ValueError(\n            \"Invalid value for `level`. \"\n            + f\"level&gt;max_hierarchy_level: {level}&gt;{self.max_hierarchy_level}\"\n        )\n    if level &lt; -self.max_hierarchy_level:\n        raise ValueError(\n            \"Invalid value for `level`. \"\n            + f\"level&lt;-max_hierarchy_level: {level}&lt;-{self.max_hierarchy_level}\"\n        )\n\n    if level &lt; 0:\n        # for example for level=-1, when max_hierarchy_level=7, new level is 7\n        # for example for level=-3, when max_hierarchy_level=7, new level is 5\n        level = self.max_hierarchy_level + (level + 1)\n\n    if level in self._at_hierarchy_level_cache:\n        return self._at_hierarchy_level_cache[level]\n\n    config_string_at_hierarchy_level = wrap_config_into_string(\n        unwrapped_config=self.unwrapped, max_level=level\n    )\n    config_at_hierarchy_level = ConfigString(config_string_at_hierarchy_level)\n    self._at_hierarchy_level_cache[level] = config_at_hierarchy_level\n\n    return self._at_hierarchy_level_cache[level]\n</code></pre>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.ConfigString.pretty_format","title":"pretty_format","text":"<pre><code>pretty_format() -&gt; str\n</code></pre> <p>Get a pretty formatted string representation of the configuration string.</p> RETURNS DESCRIPTION <code>str</code> <p>A string representation of the configuration string with indentation based on the hierarchy level of each part.</p> Source code in <code>neps\\space\\neps_spaces\\config_string.py</code> <pre><code>def pretty_format(self) -&gt; str:\n    \"\"\"Get a pretty formatted string representation of the configuration string.\n\n    Returns:\n        A string representation of the configuration string with indentation\n        based on the hierarchy level of each part.\n    \"\"\"\n    format_str_with_kwargs = (\n        \"{indent}{item.level:0&gt;2d} :: {item.operator} {item.hyperparameters}\"\n    )\n    format_str_no_kwargs = \"{indent}{item.level:0&gt;2d} :: {item.operator}\"\n    lines = [self.config_string]\n    for item in self.unwrapped:\n        if item.hyperparameters not in {\"{}\", \"\"}:\n            line = format_str_with_kwargs.format(item=item, indent=\"\\t\" * item.level)\n        else:\n            line = format_str_no_kwargs.format(item=item, indent=\"\\t\" * item.level)\n        lines.append(line)\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.UnwrappedConfigStringPart","title":"UnwrappedConfigStringPart  <code>dataclass</code>","text":"<pre><code>UnwrappedConfigStringPart(\n    level: int,\n    opening_index: int,\n    operator: str | Callable[..., Any],\n    hyperparameters: str,\n    operands: str,\n)\n</code></pre> <p>A data class representing a part of an unwrapped configuration string.</p> PARAMETER DESCRIPTION <code>level</code> <p>The hierarchy level of this part in the configuration string.</p> <p> TYPE: <code>int</code> </p> <code>opening_index</code> <p>The index of the opening parenthesis in the original string.</p> <p> TYPE: <code>int</code> </p> <code>operator</code> <p>The operator of this part, which is the first word in the parenthesis.</p> <p> TYPE: <code>str | Callable[..., Any]</code> </p> <code>hyperparameters</code> <p>The hyperparameters of this part, if any, enclosed in curly braces.</p> <p> TYPE: <code>str</code> </p> <code>operands</code> <p>The operands of this part, which are the remaining content in the parenthesis.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.unwrap_config_string","title":"unwrap_config_string  <code>cached</code>","text":"<pre><code>unwrap_config_string(\n    config_string: str,\n) -&gt; tuple[UnwrappedConfigStringPart, ...]\n</code></pre> <p>For a given config string, gets the parenthetic contents of it and uses them to construct objects of type <code>UnwrappedConfigStringPart</code>. First unwraps a given parenthesised config_string into parts. Then it converts these parts into objects with structured information.</p> PARAMETER DESCRIPTION <code>config_string</code> <p>The configuration string to be unwrapped.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple[UnwrappedConfigStringPart, ...]</code> <p>A tuple of <code>UnwrappedConfigStringPart</code> objects representing the unwrapped configuration string.</p> Source code in <code>neps\\space\\neps_spaces\\config_string.py</code> <pre><code>@functools.lru_cache(maxsize=2000)\ndef unwrap_config_string(config_string: str) -&gt; tuple[UnwrappedConfigStringPart, ...]:\n    \"\"\"For a given config string, gets the parenthetic contents of it\n    and uses them to construct objects of type `UnwrappedConfigStringPart`.\n    First unwraps a given parenthesised config_string into parts.\n    Then it converts these parts into objects with structured information.\n\n    Args:\n        config_string: The configuration string to be unwrapped.\n\n    Returns:\n        A tuple of `UnwrappedConfigStringPart` objects representing the unwrapped\n        configuration string.\n    \"\"\"\n    # A workaround needed since in the existing configurations\n    #  generated by previous methods, e.g. the `resBlock resBlock` and `resBlock` items\n    #  occur without wrapping parenthesis, differently from other items.\n    # Wrap them appropriately in parentheses here and in the inverse process.\n    # For example 'id' comes in two forms: 'id id' and 'Ops id',\n    #  only the 'id id' variant should be replaced.\n    replacements = [\n        (\"resBlock\", True),\n        (\"id\", False),\n    ]\n    for op, replace_individual in replacements:\n        config_string = config_string.replace(f\"{op} {op}\", \"__TMP_PLACEHOLDER___\")\n        if replace_individual:\n            config_string = config_string.replace(f\"{op}\", f\"({op})\")\n        config_string = config_string.replace(\"__TMP_PLACEHOLDER___\", f\"({op} {op})\")\n\n    result = []\n\n    stack = []\n    opening_counter = 0\n    for current_char_index, current_char in enumerate(config_string):\n        if current_char == \"(\":\n            stack.append((current_char_index, opening_counter))\n            opening_counter += 1\n        elif current_char == \")\":\n            assert stack, f\"Found ')' with no matching '('. Index: {current_char_index}\"\n\n            start_char_index, opening_index = stack.pop()\n            level = len(stack) + 1  # start level counting from 1 and not 0\n\n            value_single = config_string[start_char_index + 1 : current_char_index]\n            value = value_single.split(\" (\", maxsplit=1)\n            operator = value[0]\n            operands = \"(\" + value[1] if len(value) &gt; 1 else \"\"\n\n            if \" {\" in operator:\n                operator, hyperparameters = operator.split(\" {\")\n                hyperparameters = \"{\" + hyperparameters\n            else:\n                hyperparameters = \"{}\"\n\n            item = UnwrappedConfigStringPart(\n                level=level,\n                opening_index=opening_index,\n                operator=operator,\n                hyperparameters=hyperparameters,\n                operands=operands,\n            )\n            result.append(item)\n\n    assert not stack, f\"For '(' found no matching ')': Index: {stack[0][0]}\"\n    return tuple(sorted(result, key=lambda x: x.opening_index))\n</code></pre>"},{"location":"api/neps/space/neps_spaces/config_string/#neps.space.neps_spaces.config_string.wrap_config_into_string","title":"wrap_config_into_string","text":"<pre><code>wrap_config_into_string(\n    unwrapped_config: tuple[UnwrappedConfigStringPart, ...],\n    max_level: int | None = None,\n) -&gt; str\n</code></pre> <p>For a given unwrapped config, returns the string representing it.</p> PARAMETER DESCRIPTION <code>unwrapped_config</code> <p>The unwrapped config</p> <p> TYPE: <code>tuple[UnwrappedConfigStringPart, ...]</code> </p> <code>max_level</code> <p>An optional int telling which is the maximal considered level. Bigger levels are ignored.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The string representation of the unwrapped config.</p> Source code in <code>neps\\space\\neps_spaces\\config_string.py</code> <pre><code>def wrap_config_into_string(\n    unwrapped_config: tuple[UnwrappedConfigStringPart, ...],\n    max_level: int | None = None,\n) -&gt; str:\n    \"\"\"For a given unwrapped config, returns the string representing it.\n\n    Args:\n        unwrapped_config: The unwrapped config\n        max_level: An optional int telling which is the maximal considered level.\n            Bigger levels are ignored.\n\n    Returns:\n        The string representation of the unwrapped config.\n    \"\"\"\n    result = []\n    current_level = 0\n    for item in unwrapped_config:\n        if max_level is not None and item.level &gt; max_level:\n            continue\n\n        if item.level &gt; current_level:\n            if item.hyperparameters not in (\"{}\", \"\"):\n                value = \" (\" + str(item.operator) + \" \" + item.hyperparameters\n            else:\n                value = \" (\" + str(item.operator)\n        elif item.level &lt; current_level:\n            value = \")\" * (current_level - item.level + 1) + \" (\" + str(item.operator)\n        else:\n            value = \") (\" + str(item.operator)\n        current_level = item.level\n        result.append(value)\n    result.append(\")\" * current_level)\n\n    result_string = \"\".join(result).strip()\n\n    # A workaround needed since in the existing configurations\n    #  generated by previous methods, e.g. the `resBlock resBlock` and `resBlock` items\n    #  occur without wrapping parenthesis, differently from other items.\n    # Wrap them appropriately in parentheses here and in the inverse process.\n    # For example 'id' comes in two forms: 'id id' and 'Ops id',\n    #  only the 'id id' variant should be replaced.\n    replacements = [\n        (\"resBlock\", True),\n        (\"id\", False),\n    ]\n    for op, replace_individual in replacements:\n        result_string = result_string.replace(f\"({op} {op})\", \"__TMP_PLACEHOLDER___\")\n        if replace_individual:\n            result_string = result_string.replace(f\"({op})\", f\"{op}\")\n        result_string = result_string.replace(\"__TMP_PLACEHOLDER___\", f\"{op} {op}\")\n\n    return result_string\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/","title":"Neps space","text":"<p>This module provides functionality for resolving NePS spaces, including sampling from domains, resolving pipelines, and handling various resolvable objects.</p>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.NepsCompatConverter","title":"NepsCompatConverter","text":"<p>A class to convert between NePS configurations and NEPS-compatible configurations. It provides methods to convert a SamplingResolutionContext to a NEPS-compatible config and to convert a NEPS-compatible config back to a SamplingResolutionContext.</p>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.NepsCompatConverter.from_neps_config","title":"from_neps_config  <code>classmethod</code>","text":"<pre><code>from_neps_config(\n    config: Mapping[str, Any],\n) -&gt; _FromNepsConfigResult\n</code></pre> <p>Convert a NEPS-compatible config to a SamplingResolutionContext.</p> PARAMETER DESCRIPTION <code>config</code> <p>A mapping of NEPS-compatible configuration keys to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>_FromNepsConfigResult</code> <p>A _FromNepsConfigResult containing predefined samplings,     environment values, and extra kwargs.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the config is not a valid NEPS-compatible config.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>@classmethod\ndef from_neps_config(\n    cls,\n    config: Mapping[str, Any],\n) -&gt; _FromNepsConfigResult:\n    \"\"\"Convert a NEPS-compatible config to a SamplingResolutionContext.\n\n    Args:\n        config: A mapping of NEPS-compatible configuration keys to their values.\n\n    Returns:\n        A _FromNepsConfigResult containing predefined samplings,\n            environment values, and extra kwargs.\n\n    Raises:\n        ValueError: If the config is not a valid NEPS-compatible config.\n    \"\"\"\n    predefined_samplings = {}\n    environment_values = {}\n    extra_kwargs = {}\n\n    for name, value in config.items():\n        if name.startswith(cls._SAMPLING_PREFIX):\n            sampling_path = name[cls._SAMPLING_PREFIX_LEN :]\n            predefined_samplings[sampling_path] = value\n        elif name.startswith(cls._ENVIRONMENT_PREFIX):\n            env_name = name[cls._ENVIRONMENT_PREFIX_LEN :]\n            environment_values[env_name] = value\n        else:\n            extra_kwargs[name] = value\n\n    return cls._FromNepsConfigResult(\n        predefined_samplings=predefined_samplings,\n        environment_values=environment_values,\n        extra_kwargs=extra_kwargs,\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.NepsCompatConverter.to_neps_config","title":"to_neps_config  <code>classmethod</code>","text":"<pre><code>to_neps_config(\n    resolution_context: SamplingResolutionContext,\n) -&gt; Mapping[str, Any]\n</code></pre> <p>Convert a SamplingResolutionContext to a NEPS-compatible config.</p> PARAMETER DESCRIPTION <code>resolution_context</code> <p>The SamplingResolutionContext to convert.</p> <p> TYPE: <code>SamplingResolutionContext</code> </p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of NEPS-compatible configuration keys to their values.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the resolution_context is not a SamplingResolutionContext.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>@classmethod\ndef to_neps_config(\n    cls,\n    resolution_context: SamplingResolutionContext,\n) -&gt; Mapping[str, Any]:\n    \"\"\"Convert a SamplingResolutionContext to a NEPS-compatible config.\n\n    Args:\n        resolution_context: The SamplingResolutionContext to convert.\n\n    Returns:\n        A mapping of NEPS-compatible configuration keys to their values.\n\n    Raises:\n        ValueError: If the resolution_context is not a SamplingResolutionContext.\n    \"\"\"\n    config: dict[str, Any] = {}\n\n    samplings_made = resolution_context.samplings_made\n    for sampling_path, value in samplings_made.items():\n        config[f\"{cls._SAMPLING_PREFIX}{sampling_path}\"] = value\n\n    environment_values = resolution_context.environment_values\n    for env_name, value in environment_values.items():\n        config[f\"{cls._ENVIRONMENT_PREFIX}{env_name}\"] = value\n\n    return config\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext","title":"SamplingResolutionContext","text":"<pre><code>SamplingResolutionContext(\n    *,\n    resolution_root: Resolvable,\n    domain_sampler: DomainSampler,\n    environment_values: Mapping[str, Any]\n)\n</code></pre> <p>A context for resolving samplings in a NePS space. It manages the resolution root, domain sampler, environment values, and keeps track of samplings made and resolved objects.</p> PARAMETER DESCRIPTION <code>resolution_root</code> <p>The root of the resolution, which should be a Resolvable object.</p> <p> TYPE: <code>Resolvable</code> </p> <code>domain_sampler</code> <p>The DomainSampler to use for sampling from Domain objects.</p> <p> TYPE: <code>DomainSampler</code> </p> <code>environment_values</code> <p>A mapping of environment values that are fixed and not related to samplings. These values can be used in the resolution process.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the resolution_root is not a Resolvable, or if the domain_sampler is not a DomainSampler, or if the environment_values is not a Mapping.</p> PARAMETER DESCRIPTION <code>resolution_root</code> <p>The root of the resolution, which should be a Resolvable object.</p> <p> TYPE: <code>Resolvable</code> </p> <code>domain_sampler</code> <p>The DomainSampler to use for sampling from Domain objects.</p> <p> TYPE: <code>DomainSampler</code> </p> <code>environment_values</code> <p>A mapping of environment values that are fixed and not related to samplings. These values can be used in the resolution process.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the resolution_root is not a Resolvable, or if the domain_sampler is not a DomainSampler, or if the environment_values is not a Mapping.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def __init__(\n    self,\n    *,\n    resolution_root: Resolvable,\n    domain_sampler: DomainSampler,\n    environment_values: Mapping[str, Any],\n):\n    \"\"\"Initialize the SamplingResolutionContext with a resolution root, domain\n    sampler, and environment values.\n\n    Args:\n        resolution_root: The root of the resolution, which should be a Resolvable\n            object.\n        domain_sampler: The DomainSampler to use for sampling from Domain objects.\n        environment_values: A mapping of environment values that are fixed and not\n            related to samplings. These values can be used in the resolution process.\n\n    Raises:\n        ValueError: If the resolution_root is not a Resolvable, or if the\n            domain_sampler is not a DomainSampler, or if the environment_values is\n            not a Mapping.\n    \"\"\"\n    if not isinstance(resolution_root, Resolvable):\n        raise ValueError(\n            \"The received `resolution_root` is not a Resolvable:\"\n            f\" {resolution_root!r}.\"\n        )\n\n    if not isinstance(domain_sampler, DomainSampler):\n        raise ValueError(\n            \"The received `domain_sampler` is not a DomainSampler:\"\n            f\" {domain_sampler!r}.\"\n        )\n\n    if not isinstance(environment_values, Mapping):\n        raise ValueError(\n            \"The received `environment_values` is not a Mapping:\"\n            f\" {environment_values!r}.\"\n        )\n\n    # `_resolution_root` stores the root of the resolution.\n    self._resolution_root: Resolvable = resolution_root\n\n    # `_domain_sampler` stores the object responsible for sampling from Domain\n    # objects.\n    self._domain_sampler = domain_sampler\n\n    # # `_environment_values` stores fixed values from outside.\n    # # They are not related to samplings and can not be mutated or similar.\n    self._environment_values = environment_values\n\n    # `_samplings_made` stores the values we have sampled\n    # and can be used later in case we want to redo a resolving.\n    self._samplings_made: dict[str, Any] = {}\n\n    # `_resolved_objects` stores the intermediate values to make re-use possible.\n    self._resolved_objects: dict[Any, Any] = {}\n\n    # `_current_path_parts` stores the current path we are resolving.\n    self._current_path_parts: list[str] = []\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.environment_values","title":"environment_values  <code>property</code>","text":"<pre><code>environment_values: Mapping[str, Any]\n</code></pre> <p>Get the environment values that are fixed and not related to samplings.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of environment variable names to their values.</p>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.resolution_root","title":"resolution_root  <code>property</code>","text":"<pre><code>resolution_root: Resolvable\n</code></pre> <p>Get the root of the resolution.</p> RETURNS DESCRIPTION <code>Resolvable</code> <p>The root of the resolution, which should be a Resolvable object.</p>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.samplings_made","title":"samplings_made  <code>property</code>","text":"<pre><code>samplings_made: Mapping[str, Any]\n</code></pre> <p>Get the samplings made during the resolution process.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of paths to sampled values.</p>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.add_resolved","title":"add_resolved","text":"<pre><code>add_resolved(original: Any, resolved: Any) -&gt; None\n</code></pre> <p>Add a resolved object to the context.</p> PARAMETER DESCRIPTION <code>original</code> <p>The original object that was resolved.</p> <p> TYPE: <code>Any</code> </p> <code>resolved</code> <p>The resolved value of the original object.</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the original object was already resolved or if it is a Resampled.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def add_resolved(self, original: Any, resolved: Any) -&gt; None:\n    \"\"\"Add a resolved object to the context.\n\n    Args:\n        original: The original object that was resolved.\n        resolved: The resolved value of the original object.\n\n    Raises:\n        ValueError: If the original object was already resolved or if it is a\n            Resampled.\n    \"\"\"\n    if self.was_already_resolved(original):\n        raise ValueError(\n            f\"Original object has already been resolved: {original!r}. \"\n            + \"\\nIf you are doing resampling by name, \"\n            + \"make sure you are not forgetting to request resampling also for\"\n            \" related objects.\" + \"\\nOtherwise it could lead to infinite recursion.\"\n        )\n    if isinstance(original, Resampled):\n        raise ValueError(\n            f\"Attempting to add a Resampled object to resolved values: {original!r}.\"\n        )\n    self._resolved_objects[original] = resolved\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.get_resolved","title":"get_resolved","text":"<pre><code>get_resolved(obj: Any) -&gt; Any\n</code></pre> <p>Get the resolved value for the given object.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object for which to get the resolved value.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The resolved value of the object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the object was not already resolved in the context.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def get_resolved(self, obj: Any) -&gt; Any:\n    \"\"\"Get the resolved value for the given object.\n\n    Args:\n        obj: The object for which to get the resolved value.\n\n    Returns:\n        The resolved value of the object.\n\n    Raises:\n        ValueError: If the object was not already resolved in the context.\n    \"\"\"\n    try:\n        return self._resolved_objects[obj]\n    except KeyError as err:\n        raise ValueError(\n            f\"Given object was not already resolved. Please check first: {obj!r}\"\n        ) from err\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.get_value_from_environment","title":"get_value_from_environment","text":"<pre><code>get_value_from_environment(var_name: str) -&gt; Any\n</code></pre> <p>Get a value from the environment variables.</p> PARAMETER DESCRIPTION <code>var_name</code> <p>The name of the environment variable to get the value from.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The value of the environment variable.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the environment variable is not found in the context.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def get_value_from_environment(self, var_name: str) -&gt; Any:\n    \"\"\"Get a value from the environment variables.\n\n    Args:\n        var_name: The name of the environment variable to get the value from.\n\n    Returns:\n        The value of the environment variable.\n\n    Raises:\n        ValueError: If the environment variable is not found in the context.\n    \"\"\"\n    try:\n        return self._environment_values[var_name]\n    except KeyError as err:\n        raise ValueError(\n            f\"No value is available for the environment variable {var_name!r}.\"\n        ) from err\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.resolving","title":"resolving","text":"<pre><code>resolving(_obj: Any, name: str) -&gt; Generator[None]\n</code></pre> <p>Context manager for resolving an object in the current resolution context.</p> PARAMETER DESCRIPTION <code>_obj</code> <p>The object being resolved, can be any type.</p> <p> TYPE: <code>Any</code> </p> <code>name</code> <p>The name of the object being resolved, used for debugging.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the name is not a valid string.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>@contextlib.contextmanager\ndef resolving(self, _obj: Any, name: str) -&gt; Generator[None]:\n    \"\"\"Context manager for resolving an object in the current resolution context.\n\n    Args:\n        _obj: The object being resolved, can be any type.\n        name: The name of the object being resolved, used for debugging.\n\n    Raises:\n        ValueError: If the name is not a valid string.\n    \"\"\"\n    if not name or not isinstance(name, str):\n        raise ValueError(\n            f\"Given name for what we are resolving is invalid: {name!r}.\"\n        )\n\n    # It is possible that the received object has already been resolved.\n    # That is expected and is okay, so no check is made for it.\n    # For example, in the case of a Resampled we can receive the same object again.\n\n    self._current_path_parts.append(name)\n    try:\n        yield\n    finally:\n        self._current_path_parts.pop()\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.sample_from","title":"sample_from","text":"<pre><code>sample_from(domain_obj: Domain) -&gt; Any\n</code></pre> <p>Sample a value from the given domain object.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object from which to sample a value.</p> <p> TYPE: <code>Domain</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The sampled value from the domain object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain object was already resolved or if the path has already been sampled from.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def sample_from(self, domain_obj: Domain) -&gt; Any:\n    \"\"\"Sample a value from the given domain object.\n\n    Args:\n        domain_obj: The domain object from which to sample a value.\n\n    Returns:\n        The sampled value from the domain object.\n\n    Raises:\n        ValueError: If the domain object was already resolved or if the path\n            has already been sampled from.\n    \"\"\"\n    # Each `domain_obj` is only ever sampled from once.\n    # This is okay and the expected behavior.\n    # For each `domain_obj`, its sampled value is either directly stored itself,\n    # or is used in some other Resolvable.\n    # In both cases that sampled value is cached for later uses,\n    # and so the `domain_obj` will not be re-sampled from again.\n    if self.was_already_resolved(domain_obj):\n        raise ValueError(\n            \"We have already sampled a value for the given domain object:\"\n            f\" {domain_obj!r}.\" + \"\\nThis should not be happening.\"\n        )\n\n    # The range compatibility identifier is there to make sure when we say\n    # the path matches, that the range for the value we are looking up also matches.\n    domain_obj_type_name = type(domain_obj).__name__.lower()\n    range_compatibility_identifier = domain_obj.range_compatibility_identifier\n    domain_obj_identifier = (\n        f\"{domain_obj_type_name}__{range_compatibility_identifier}\"\n    )\n\n    current_path = \".\".join(self._current_path_parts)\n    current_path += \"::\" + domain_obj_identifier\n\n    if current_path in self._samplings_made:\n        # We have already sampled a value for this path. This should not happen.\n        # Every time we sample a domain, it should have its own different path.\n        raise ValueError(\n            f\"We have already sampled a value for the current path: {current_path!r}.\"\n            + \"\\nThis should not be happening.\"\n        )\n\n    sampled_value = self._domain_sampler(\n        domain_obj=domain_obj,\n        current_path=current_path,\n    )\n\n    self._samplings_made[current_path] = sampled_value\n    return self._samplings_made[current_path]\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolutionContext.was_already_resolved","title":"was_already_resolved","text":"<pre><code>was_already_resolved(obj: Any) -&gt; bool\n</code></pre> <p>Check if the given object was already resolved in the current context.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to check if it was already resolved.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the object was already resolved, False otherwise.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def was_already_resolved(self, obj: Any) -&gt; bool:\n    \"\"\"Check if the given object was already resolved in the current context.\n\n    Args:\n        obj: The object to check if it was already resolved.\n\n    Returns:\n        True if the object was already resolved, False otherwise.\n    \"\"\"\n    return obj in self._resolved_objects\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolver","title":"SamplingResolver","text":"<p>A class responsible for resolving samplings in a NePS space. It uses a SamplingResolutionContext to manage the resolution process, and a DomainSampler to sample values from Domain objects.</p>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.SamplingResolver.__call__","title":"__call__","text":"<pre><code>__call__(\n    obj: Resolvable,\n    domain_sampler: DomainSampler,\n    environment_values: Mapping[str, Any],\n) -&gt; tuple[Resolvable, SamplingResolutionContext]\n</code></pre> <p>Resolve the given object in the context of the provided domain sampler and environment values.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The Resolvable object to resolve.</p> <p> TYPE: <code>Resolvable</code> </p> <code>domain_sampler</code> <p>The DomainSampler to use for sampling from Domain objects.</p> <p> TYPE: <code>DomainSampler</code> </p> <code>environment_values</code> <p>A mapping of environment values that are fixed and not related to samplings.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>tuple[Resolvable, SamplingResolutionContext]</code> <p>A tuple containing the resolved object and the     SamplingResolutionContext.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the object is not a Resolvable, or if the domain_sampler is not a DomainSampler, or if the environment_values is not a Mapping.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def __call__(\n    self,\n    obj: Resolvable,\n    domain_sampler: DomainSampler,\n    environment_values: Mapping[str, Any],\n) -&gt; tuple[Resolvable, SamplingResolutionContext]:\n    \"\"\"Resolve the given object in the context of the provided domain sampler and\n    environment values.\n\n    Args:\n        obj: The Resolvable object to resolve.\n        domain_sampler: The DomainSampler to use for sampling from Domain objects.\n        environment_values: A mapping of environment values that are fixed and not\n            related to samplings.\n\n    Returns:\n        A tuple containing the resolved object and the\n            SamplingResolutionContext.\n\n    Raises:\n        ValueError: If the object is not a Resolvable, or if the domain_sampler\n            is not a DomainSampler, or if the environment_values is not a Mapping.\n    \"\"\"\n    context = SamplingResolutionContext(\n        resolution_root=obj,\n        domain_sampler=domain_sampler,\n        environment_values=environment_values,\n    )\n    return self._resolve(obj, \"Resolvable\", context), context\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.adjust_evaluation_pipeline_for_neps_space","title":"adjust_evaluation_pipeline_for_neps_space","text":"<pre><code>adjust_evaluation_pipeline_for_neps_space(\n    evaluation_pipeline: Callable,\n    pipeline_space: P,\n    operation_converter: Callable[\n        [Operation], Any\n    ] = convert_operation_to_callable,\n) -&gt; Callable | str\n</code></pre> <p>Adjust the evaluation pipeline to work with a NePS space. This function wraps the evaluation pipeline to sample from the NePS space and convert the sampled pipeline to a format compatible with the evaluation pipeline.</p> PARAMETER DESCRIPTION <code>evaluation_pipeline</code> <p>The evaluation pipeline to adjust.</p> <p> TYPE: <code>Callable</code> </p> <code>pipeline_space</code> <p>The NePS pipeline space to sample from.</p> <p> TYPE: <code>P</code> </p> <code>operation_converter</code> <p>A callable to convert Operation objects to a format compatible with the evaluation pipeline.</p> <p> TYPE: <code>Callable[[Operation], Any]</code> DEFAULT: <code>convert_operation_to_callable</code> </p> RETURNS DESCRIPTION <code>Callable | str</code> <p>A wrapped evaluation pipeline that samples from the NePS space.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the evaluation_pipeline is not callable or if the pipeline_space is not a Pipeline object.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def adjust_evaluation_pipeline_for_neps_space(\n    evaluation_pipeline: Callable,\n    pipeline_space: P,\n    operation_converter: Callable[[Operation], Any] = convert_operation_to_callable,\n) -&gt; Callable | str:\n    \"\"\"Adjust the evaluation pipeline to work with a NePS space.\n    This function wraps the evaluation pipeline to sample from the NePS space\n    and convert the sampled pipeline to a format compatible with the evaluation pipeline.\n\n    Args:\n        evaluation_pipeline: The evaluation pipeline to adjust.\n        pipeline_space: The NePS pipeline space to sample from.\n        operation_converter: A callable to convert Operation objects to a format\n            compatible with the evaluation pipeline.\n\n    Returns:\n        A wrapped evaluation pipeline that samples from the NePS space.\n\n    Raises:\n        ValueError: If the evaluation_pipeline is not callable or if the\n            pipeline_space is not a Pipeline object.\n    \"\"\"\n\n    @functools.wraps(evaluation_pipeline)\n    def inner(*args: Any, **kwargs: Any) -&gt; Any:\n        # `kwargs` can contain other things not related to\n        # the samplings to make or to environment values.\n        # That is not an issue. Those items will be passed through.\n\n        sampled_pipeline_data = NepsCompatConverter.from_neps_config(config=kwargs)\n\n        sampled_pipeline, _resolution_context = resolve(\n            pipeline=pipeline_space,\n            domain_sampler=OnlyPredefinedValuesSampler(\n                predefined_samplings=sampled_pipeline_data.predefined_samplings,\n            ),\n            environment_values=sampled_pipeline_data.environment_values,\n        )\n\n        config = dict(**sampled_pipeline.get_attrs())\n\n        for name, value in config.items():\n            if isinstance(value, Operation):\n                config[name] = operation_converter(value)\n\n        # So that we still pass the kwargs not related to the config,\n        # start with the extra kwargs we passed to the converter.\n        new_kwargs = dict(**sampled_pipeline_data.extra_kwargs)\n        # Then add all the kwargs from the config.\n        new_kwargs.update(config)\n\n        return evaluation_pipeline(*args, **new_kwargs)\n\n    return inner\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.convert_operation_to_callable","title":"convert_operation_to_callable","text":"<pre><code>convert_operation_to_callable(\n    operation: Operation,\n) -&gt; Callable\n</code></pre> <p>Convert an Operation to a callable that can be executed.</p> PARAMETER DESCRIPTION <code>operation</code> <p>The Operation to convert.</p> <p> TYPE: <code>Operation</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>A callable that represents the operation.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the operation is not a valid Operation object.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def convert_operation_to_callable(operation: Operation) -&gt; Callable:\n    \"\"\"Convert an Operation to a callable that can be executed.\n\n    Args:\n        operation: The Operation to convert.\n\n    Returns:\n        A callable that represents the operation.\n\n    Raises:\n        ValueError: If the operation is not a valid Operation object.\n    \"\"\"\n    operator = cast(Callable, operation.operator)\n\n    operation_args = []\n    for arg in operation.args:\n        operation_args.append(\n            convert_operation_to_callable(arg) if isinstance(arg, Operation) else arg\n        )\n\n    operation_kwargs = {}\n    for kwarg_name, kwarg_value in operation.kwargs.items():\n        operation_kwargs[kwarg_name] = (\n            convert_operation_to_callable(kwarg_value)\n            if isinstance(kwarg_value, Operation)\n            else kwarg_value\n        )\n\n    return cast(Callable, operator(*operation_args, **operation_kwargs))\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.convert_operation_to_string","title":"convert_operation_to_string","text":"<pre><code>convert_operation_to_string(operation: Operation) -&gt; str\n</code></pre> <p>Convert an Operation to a string representation.</p> PARAMETER DESCRIPTION <code>operation</code> <p>The Operation to convert.</p> <p> TYPE: <code>Operation</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string representation of the operation.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the operation is not a valid Operation object.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def convert_operation_to_string(operation: Operation) -&gt; str:\n    \"\"\"Convert an Operation to a string representation.\n\n    Args:\n        operation: The Operation to convert.\n\n    Returns:\n        A string representation of the operation.\n\n    Raises:\n        ValueError: If the operation is not a valid Operation object.\n    \"\"\"\n    unwrapped_config = tuple(_operation_to_unwrapped_config(operation))\n    return config_string.wrap_config_into_string(unwrapped_config)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/neps_space/#neps.space.neps_spaces.neps_space.resolve","title":"resolve","text":"<pre><code>resolve(\n    pipeline: P,\n    domain_sampler: DomainSampler | None = None,\n    environment_values: Mapping[str, Any] | None = None,\n) -&gt; tuple[P, SamplingResolutionContext]\n</code></pre> <p>Resolve a NePS pipeline with the given domain sampler and environment values.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline to resolve, which should be a Pipeline object.</p> <p> TYPE: <code>P</code> </p> <code>domain_sampler</code> <p>The DomainSampler to use for sampling from Domain objects. If None, a RandomSampler with no predefined values will be used.</p> <p> TYPE: <code>DomainSampler | None</code> DEFAULT: <code>None</code> </p> <code>environment_values</code> <p>A mapping of environment variable names to their values. If None, an empty mapping will be used.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[P, SamplingResolutionContext]</code> <p>A tuple containing the resolved pipeline and the SamplingResolutionContext.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the pipeline is not a Pipeline object or if the domain_sampler is not a DomainSampler or if the environment_values is not a Mapping.</p> Source code in <code>neps\\space\\neps_spaces\\neps_space.py</code> <pre><code>def resolve(\n    pipeline: P,\n    domain_sampler: DomainSampler | None = None,\n    environment_values: Mapping[str, Any] | None = None,\n) -&gt; tuple[P, SamplingResolutionContext]:\n    \"\"\"Resolve a NePS pipeline with the given domain sampler and environment values.\n\n    Args:\n        pipeline: The pipeline to resolve, which should be a Pipeline object.\n        domain_sampler: The DomainSampler to use for sampling from Domain objects.\n            If None, a RandomSampler with no predefined values will be used.\n        environment_values: A mapping of environment variable names to their values.\n            If None, an empty mapping will be used.\n\n    Returns:\n        A tuple containing the resolved pipeline and the SamplingResolutionContext.\n\n    Raises:\n        ValueError: If the pipeline is not a Pipeline object or if the domain_sampler\n            is not a DomainSampler or if the environment_values is not a Mapping.\n    \"\"\"\n    if domain_sampler is None:\n        # By default, use a random sampler with no predefined values.\n        domain_sampler = RandomSampler(predefined_samplings={})\n\n    if environment_values is None:\n        # By default, have no environment values.\n        environment_values = {}\n\n    sampling_resolver = SamplingResolver()\n    resolved_pipeline, context = sampling_resolver(\n        obj=pipeline,\n        domain_sampler=domain_sampler,\n        environment_values=environment_values,\n    )\n    return cast(P, resolved_pipeline), context\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/","title":"Parameters","text":"<p>This module defines various classes and protocols for representing and manipulating search spaces in NePS (Neural Parameter Search). It includes definitions for domains, pipelines, operations, and fidelity, as well as utilities for sampling and resolving search spaces.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical","title":"Categorical","text":"<pre><code>Categorical(\n    choices: (\n        tuple[T | Domain[T] | Resolvable | Any, ...]\n        | Domain[T]\n    ),\n    prior: int | Domain[int] | _Unset = _UNSET,\n    prior_confidence: (\n        ConfidenceLevel\n        | Literal[\"low\", \"medium\", \"high\"]\n        | _Unset\n    ) = _UNSET,\n)\n</code></pre> <p>               Bases: <code>Domain[int]</code>, <code>Generic[T]</code></p> <p>A domain representing a categorical choice from a set of options.</p> ATTRIBUTE DESCRIPTION <code>choices</code> <p>A tuple of choices or a Domain of choices.</p> <p> TYPE: <code>tuple[T | Domain[T] | Resolvable, ...] | Domain[T]</code> </p> <code>prior</code> <p>The index of the prior choice in the choices tuple.</p> <p> TYPE: <code>int</code> </p> <code>prior_confidence</code> <p>The confidence level of the prior choice.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> PARAMETER DESCRIPTION <code>choices</code> <p>A tuple of choices or a Domain of choices.</p> <p> TYPE: <code>tuple[T | Domain[T] | Resolvable | Any, ...] | Domain[T]</code> </p> <code>prior</code> <p>The index of the prior choice in the choices tuple.</p> <p> TYPE: <code>int | Domain[int] | _Unset</code> DEFAULT: <code>_UNSET</code> </p> <code>prior_confidence</code> <p>The confidence level of the prior choice.</p> <p> TYPE: <code>ConfidenceLevel | Literal['low', 'medium', 'high'] | _Unset</code> DEFAULT: <code>_UNSET</code> </p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def __init__(\n    self,\n    choices: tuple[T | Domain[T] | Resolvable | Any, ...] | Domain[T],\n    prior: int | Domain[int] | _Unset = _UNSET,\n    prior_confidence: (\n        ConfidenceLevel | Literal[\"low\", \"medium\", \"high\"] | _Unset\n    ) = _UNSET,\n):\n    \"\"\"Initialize the Categorical domain with choices and optional prior.\n\n    Args:\n        choices: A tuple of choices or a Domain of choices.\n        prior: The index of the prior choice in the choices tuple.\n        prior_confidence: The confidence level of the prior choice.\n\n    \"\"\"\n    self._choices: tuple[T | Domain[T] | Resolvable | Any, ...] | Domain[T]\n    if isinstance(choices, Sequence):\n        self._choices = tuple(choice for choice in choices)\n    else:\n        self._choices = choices\n    self._prior = prior\n    self._prior_confidence = (\n        convert_confidence_level(prior_confidence)\n        if isinstance(prior_confidence, str)\n        else prior_confidence\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.choices","title":"choices  <code>property</code>","text":"<pre><code>choices: (\n    tuple[T | Domain[T] | Resolvable, ...] | Domain[T]\n)\n</code></pre> <p>Get the choices available in the categorical domain.</p> RETURNS DESCRIPTION <code>tuple[T | Domain[T] | Resolvable, ...] | Domain[T]</code> <p>A tuple of choices or a Domain of choices.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.has_prior","title":"has_prior  <code>property</code>","text":"<pre><code>has_prior: bool\n</code></pre> <p>Check if the categorical domain has a prior defined.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the prior and prior confidence are set, False otherwise.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.max_value","title":"max_value  <code>property</code>","text":"<pre><code>max_value: int\n</code></pre> <p>Get the maximum value of the categorical domain.</p> RETURNS DESCRIPTION <code>int</code> <p>The maximum index of the choices, which is the length of the choices tuple minus one.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.min_value","title":"min_value  <code>property</code>","text":"<pre><code>min_value: int\n</code></pre> <p>Get the minimum value of the categorical domain.</p> RETURNS DESCRIPTION <code>int</code> <p>The minimum index of the choices, which is always 0.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.prior","title":"prior  <code>property</code>","text":"<pre><code>prior: int\n</code></pre> <p>Get the prior index of the categorical domain.</p> RETURNS DESCRIPTION <code>int</code> <p>The index of the prior choice in the choices tuple.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.prior_confidence","title":"prior_confidence  <code>property</code>","text":"<pre><code>prior_confidence: ConfidenceLevel\n</code></pre> <p>Get the confidence level of the prior choice.</p> RETURNS DESCRIPTION <code>ConfidenceLevel</code> <p>The confidence level of the prior choice.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.range_compatibility_identifier","title":"range_compatibility_identifier  <code>property</code>","text":"<pre><code>range_compatibility_identifier: str\n</code></pre> <p>Get a string identifier for the range compatibility of the categorical domain.</p> RETURNS DESCRIPTION <code>str</code> <p>A string representation of the number of choices in the domain.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.centered_around","title":"centered_around","text":"<pre><code>centered_around(\n    center: int, confidence: ConfidenceLevel\n) -&gt; Categorical\n</code></pre> <p>Create a new categorical domain centered around a specific choice index.</p> PARAMETER DESCRIPTION <code>center</code> <p>The index of the choice around which to center the new domain.</p> <p> TYPE: <code>int</code> </p> <code>confidence</code> <p>The confidence level for the new domain.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> <code>center</code> <p>int:</p> <p> TYPE: <code>int</code> </p> <code>confidence</code> <p>ConfidenceLevel:</p> <p> TYPE: <code>ConfidenceLevel</code> </p> RETURNS DESCRIPTION <code>Categorical</code> <p>A new Categorical instance with a range centered around the specified choice index.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the center index is out of bounds of the choices.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def centered_around(\n    self,\n    center: int,\n    confidence: ConfidenceLevel,\n) -&gt; Categorical:\n    \"\"\"Create a new categorical domain centered around a specific choice index.\n\n    Args:\n      center: The index of the choice around which to center the new domain.\n      confidence: The confidence level for the new domain.\n      center: int:\n      confidence: ConfidenceLevel:\n\n    Returns:\n      A new Categorical instance with a range centered around the specified\n      choice index.\n\n    Raises:\n      ValueError: If the center index is out of bounds of the choices.\n\n    \"\"\"\n    new_min, new_max = cast(\n        tuple[int, int],\n        _calculate_new_domain_bounds(\n            number_type=int,\n            min_value=self.min_value,\n            max_value=self.max_value,\n            center=center,\n            confidence=confidence,\n        ),\n    )\n    new_choices = cast(tuple, self._choices)[new_min : new_max + 1]\n    return Categorical(\n        choices=new_choices,\n        prior=new_choices.index(cast(tuple, self._choices)[center]),\n        prior_confidence=confidence,\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Domain[T]\n</code></pre> <p>Create a new Domain instance from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Domain[T]</code> <p>A new Domain instance with the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the attributes do not match the domain's expected structure.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Domain[T]:\n    \"\"\"Create a new Domain instance from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new Domain instance with the specified attributes.\n\n    Raises:\n        ValueError: If the attributes do not match the domain's expected structure.\n\n    \"\"\"\n    return type(self)(**attrs)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the domain as a mapping. This method collects all attributes of the domain class and instance, excluding private attributes and methods, and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the domain as a mapping.\n    This method collects all attributes of the domain class and instance,\n    excluding private attributes and methods, and returns them as a dictionary.\n\n    Returns:\n        A mapping of attribute names to their values.\n    \"\"\"\n    return {k.lstrip(\"_\"): v for k, v in vars(self).items()}\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Categorical.sample","title":"sample","text":"<pre><code>sample() -&gt; int\n</code></pre> <p>Sample a random index from the categorical choices.</p> RETURNS DESCRIPTION <code>int</code> <p>A randomly selected index from the choices tuple.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the choices are empty.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def sample(self) -&gt; int:\n    \"\"\"Sample a random index from the categorical choices.\n\n    Returns:\n      A randomly selected index from the choices tuple.\n\n    Raises:\n      ValueError: If the choices are empty.\n\n    \"\"\"\n    return int(random.randint(0, len(cast(tuple[T], self._choices)) - 1))\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.ConfidenceLevel","title":"ConfidenceLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Enum representing confidence levels for sampling.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain","title":"Domain","text":"<p>               Bases: <code>Resolvable</code>, <code>ABC</code>, <code>Generic[T]</code></p> <p>An abstract base class representing a domain in NePS spaces.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.has_prior","title":"has_prior  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>has_prior: bool\n</code></pre> <p>Check if the domain has a prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.max_value","title":"max_value  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>max_value: T\n</code></pre> <p>Get the maximum value of the domain.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.min_value","title":"min_value  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>min_value: T\n</code></pre> <p>Get the minimum value of the domain.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.prior","title":"prior  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>prior: T\n</code></pre> <p>Get the prior value of the domain. Raises ValueError if the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.prior_confidence","title":"prior_confidence  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>prior_confidence: ConfidenceLevel\n</code></pre> <p>Get the confidence level of the prior. Raises ValueError if the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.range_compatibility_identifier","title":"range_compatibility_identifier  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>range_compatibility_identifier: str\n</code></pre> <p>Get a string identifier for the range compatibility of the domain. This identifier is used to check if two domains are compatible based on their ranges.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.centered_around","title":"centered_around  <code>abstractmethod</code>","text":"<pre><code>centered_around(\n    center: T, confidence: ConfidenceLevel\n) -&gt; Domain[T]\n</code></pre> <p>Create a new domain centered around a given value with a specified confidence level.</p> PARAMETER DESCRIPTION <code>center</code> <p>The value around which to center the new domain.</p> <p> TYPE: <code>T</code> </p> <code>confidence</code> <p>The confidence level for the new domain.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> <code>center</code> <p>T:</p> <p> TYPE: <code>T</code> </p> <code>confidence</code> <p>ConfidenceLevel:</p> <p> TYPE: <code>ConfidenceLevel</code> </p> RETURNS DESCRIPTION <code>Domain[T]</code> <p>A new Domain instance that is centered around the specified value.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the center value is not within the domain's range.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>@abc.abstractmethod\ndef centered_around(\n    self,\n    center: T,\n    confidence: ConfidenceLevel,\n) -&gt; Domain[T]:\n    \"\"\"Create a new domain centered around a given value with a specified confidence\n    level.\n\n    Args:\n      center: The value around which to center the new domain.\n      confidence: The confidence level for the new domain.\n      center: T:\n      confidence: ConfidenceLevel:\n\n    Returns:\n      A new Domain instance that is centered around the specified value.\n\n    Raises:\n      ValueError: If the center value is not within the domain's range.\n\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Domain[T]\n</code></pre> <p>Create a new Domain instance from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Domain[T]</code> <p>A new Domain instance with the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the attributes do not match the domain's expected structure.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Domain[T]:\n    \"\"\"Create a new Domain instance from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new Domain instance with the specified attributes.\n\n    Raises:\n        ValueError: If the attributes do not match the domain's expected structure.\n\n    \"\"\"\n    return type(self)(**attrs)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the domain as a mapping. This method collects all attributes of the domain class and instance, excluding private attributes and methods, and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the domain as a mapping.\n    This method collects all attributes of the domain class and instance,\n    excluding private attributes and methods, and returns them as a dictionary.\n\n    Returns:\n        A mapping of attribute names to their values.\n    \"\"\"\n    return {k.lstrip(\"_\"): v for k, v in vars(self).items()}\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Domain.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample() -&gt; T\n</code></pre> <p>Sample a value from the domain. Returns a value of type T that is within the domain's range.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>@abc.abstractmethod\ndef sample(self) -&gt; T:\n    \"\"\"Sample a value from the domain.\n    Returns a value of type T that is within the domain's range.\n\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Fidelity","title":"Fidelity","text":"<pre><code>Fidelity(domain: Integer | Float)\n</code></pre> <p>               Bases: <code>Resolvable</code>, <code>Generic[T]</code></p> <p>A class representing a fidelity in a NePS space.</p> ATTRIBUTE DESCRIPTION <code>domain</code> <p>The domain of the fidelity, which can be an Integer or Float domain.</p> <p> </p> PARAMETER DESCRIPTION <code>domain</code> <p>The domain of the fidelity, which can be an Integer or Float domain.</p> <p> TYPE: <code>Integer | Float</code> </p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def __init__(self, domain: Integer | Float):\n    \"\"\"Initialize the Fidelity with a domain.\n\n    Args:\n        domain: The domain of the fidelity, which can be an Integer or Float domain.\n\n    \"\"\"\n    if domain.has_prior:\n        raise ValueError(f\"The domain of a Fidelity can not have priors: {domain!r}.\")\n    self._domain = domain\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Fidelity.max_value","title":"max_value  <code>property</code>","text":"<pre><code>max_value: int | float\n</code></pre> <p>Get the maximum value of the fidelity domain.</p> RETURNS DESCRIPTION <code>int | float</code> <p>The maximum value of the fidelity domain.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Fidelity.min_value","title":"min_value  <code>property</code>","text":"<pre><code>min_value: int | float\n</code></pre> <p>Get the minimum value of the fidelity domain.</p> RETURNS DESCRIPTION <code>int | float</code> <p>The minimum value of the fidelity domain.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Fidelity.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Fidelity\n</code></pre> <p>Create a new Fidelity instance from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Fidelity</code> <p>A new Fidelity instance with the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the fidelity has no domain defined.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Fidelity:  # noqa: ARG002\n    \"\"\"Create a new Fidelity instance from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new Fidelity instance with the specified attributes.\n\n    Raises:\n        ValueError: If the fidelity has no domain defined.\n\n    \"\"\"\n    raise ValueError(\"For a Fidelity object there is nothing to resolve.\")\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Fidelity.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the fidelity as a mapping. This method collects all attributes of the fidelity class and instance, excluding private attributes and methods, and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the fidelity has no domain defined.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the fidelity as a mapping.\n    This method collects all attributes of the fidelity class and instance,\n    excluding private attributes and methods, and returns them as a dictionary.\n\n    Returns:\n      A mapping of attribute names to their values.\n\n    Raises:\n      ValueError: If the fidelity has no domain defined.\n\n    \"\"\"\n    raise ValueError(\"For a Fidelity object there is nothing to resolve.\")\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float","title":"Float","text":"<pre><code>Float(\n    min_value: float,\n    max_value: float,\n    log: bool = False,\n    prior: float | _Unset = _UNSET,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"]\n        | ConfidenceLevel\n        | _Unset\n    ) = _UNSET,\n)\n</code></pre> <p>               Bases: <code>Domain[float]</code></p> <p>A domain representing a continuous range of floating-point values.</p> ATTRIBUTE DESCRIPTION <code>min_value</code> <p>The minimum value of the domain.</p> <p> TYPE: <code>float</code> </p> <code>max_value</code> <p>The maximum value of the domain.</p> <p> TYPE: <code>float</code> </p> <code>log</code> <p>Whether to sample values on a logarithmic scale.</p> <p> TYPE: <code>float</code> </p> <code>prior</code> <p>The prior value for the domain, if any.</p> <p> TYPE: <code>float</code> </p> <code>prior_confidence</code> <p>The confidence level of the prior value.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> PARAMETER DESCRIPTION <code>min_value</code> <p>The minimum value of the domain.</p> <p> TYPE: <code>float</code> </p> <code>max_value</code> <p>The maximum value of the domain.</p> <p> TYPE: <code>float</code> </p> <code>log</code> <p>Whether to sample values on a logarithmic scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>prior</code> <p>The prior value for the domain, if any.</p> <p> TYPE: <code>float | _Unset</code> DEFAULT: <code>_UNSET</code> </p> <code>prior_confidence</code> <p>The confidence level of the prior value.</p> <p> TYPE: <code>Literal['low', 'medium', 'high'] | ConfidenceLevel | _Unset</code> DEFAULT: <code>_UNSET</code> </p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def __init__(\n    self,\n    min_value: float,\n    max_value: float,\n    log: bool = False,  # noqa: FBT001, FBT002\n    prior: float | _Unset = _UNSET,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | ConfidenceLevel | _Unset\n    ) = _UNSET,\n):\n    \"\"\"Initialize the Float domain with min and max values, and optional prior.\n\n    Args:\n        min_value: The minimum value of the domain.\n        max_value: The maximum value of the domain.\n        log: Whether to sample values on a logarithmic scale.\n        prior: The prior value for the domain, if any.\n        prior_confidence: The confidence level of the prior value.\n\n    \"\"\"\n    self._min_value = min_value\n    self._max_value = max_value\n    self._log = log\n    self._prior = prior\n    self._prior_confidence = (\n        convert_confidence_level(prior_confidence)\n        if isinstance(prior_confidence, str)\n        else prior_confidence\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.has_prior","title":"has_prior  <code>property</code>","text":"<pre><code>has_prior: bool\n</code></pre> <p>Check if the floating-point domain has a prior defined.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the prior and prior confidence are set, False otherwise.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.max_value","title":"max_value  <code>property</code>","text":"<pre><code>max_value: float\n</code></pre> <p>Get the maximum value of the floating-point domain.</p> RETURNS DESCRIPTION <code>float</code> <p>The maximum value of the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If min_value is greater than max_value.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.min_value","title":"min_value  <code>property</code>","text":"<pre><code>min_value: float\n</code></pre> <p>Get the minimum value of the floating-point domain.</p> RETURNS DESCRIPTION <code>float</code> <p>The minimum value of the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If min_value is greater than max_value.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.prior","title":"prior  <code>property</code>","text":"<pre><code>prior: float\n</code></pre> <p>Get the prior value of the floating-point domain.</p> RETURNS DESCRIPTION <code>float</code> <p>The prior value of the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.prior_confidence","title":"prior_confidence  <code>property</code>","text":"<pre><code>prior_confidence: ConfidenceLevel\n</code></pre> <p>Get the confidence level of the prior value.</p> RETURNS DESCRIPTION <code>ConfidenceLevel</code> <p>The confidence level of the prior value.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.range_compatibility_identifier","title":"range_compatibility_identifier  <code>property</code>","text":"<pre><code>range_compatibility_identifier: str\n</code></pre> <p>Get a string identifier for the range compatibility of the floating-point domain.</p> RETURNS DESCRIPTION <code>str</code> <p>A string representation of the minimum and maximum values, and whether the domain is logarithmic.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.centered_around","title":"centered_around","text":"<pre><code>centered_around(\n    center: float, confidence: ConfidenceLevel\n) -&gt; Float\n</code></pre> <p>Create a new floating-point domain centered around a specific value.</p> PARAMETER DESCRIPTION <code>center</code> <p>The value around which to center the new domain.</p> <p> TYPE: <code>float</code> </p> <code>confidence</code> <p>The confidence level for the new domain.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> <code>center</code> <p>float:</p> <p> TYPE: <code>float</code> </p> <code>confidence</code> <p>ConfidenceLevel:</p> <p> TYPE: <code>ConfidenceLevel</code> </p> RETURNS DESCRIPTION <code>Float</code> <p>A new Float instance that is centered around the specified value.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the center value is not within the domain's range.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def centered_around(\n    self,\n    center: float,\n    confidence: ConfidenceLevel,\n) -&gt; Float:\n    \"\"\"Create a new floating-point domain centered around a specific value.\n\n    Args:\n        center: The value around which to center the new domain.\n        confidence: The confidence level for the new domain.\n        center: float:\n        confidence: ConfidenceLevel:\n\n    Returns:\n        A new Float instance that is centered around the specified value.\n\n    Raises:\n        ValueError: If the center value is not within the domain's range.\n\n    \"\"\"\n    new_min, new_max = _calculate_new_domain_bounds(\n        number_type=float,\n        min_value=self.min_value,\n        max_value=self.max_value,\n        center=center,\n        confidence=confidence,\n    )\n    return Float(\n        min_value=new_min,\n        max_value=new_max,\n        log=self._log,\n        prior=center,\n        prior_confidence=confidence,\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Domain[T]\n</code></pre> <p>Create a new Domain instance from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Domain[T]</code> <p>A new Domain instance with the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the attributes do not match the domain's expected structure.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Domain[T]:\n    \"\"\"Create a new Domain instance from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new Domain instance with the specified attributes.\n\n    Raises:\n        ValueError: If the attributes do not match the domain's expected structure.\n\n    \"\"\"\n    return type(self)(**attrs)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the domain as a mapping. This method collects all attributes of the domain class and instance, excluding private attributes and methods, and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the domain as a mapping.\n    This method collects all attributes of the domain class and instance,\n    excluding private attributes and methods, and returns them as a dictionary.\n\n    Returns:\n        A mapping of attribute names to their values.\n    \"\"\"\n    return {k.lstrip(\"_\"): v for k, v in vars(self).items()}\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Float.sample","title":"sample","text":"<pre><code>sample() -&gt; float\n</code></pre> <p>Sample a random floating-point value from the domain.</p> RETURNS DESCRIPTION <code>float</code> <p>A randomly selected floating-point value within the domain's range.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If min_value is greater than max_value.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def sample(self) -&gt; float:\n    \"\"\"Sample a random floating-point value from the domain.\n\n    Returns:\n      A randomly selected floating-point value within the domain's range.\n\n    Raises:\n      ValueError: If min_value is greater than max_value.\n\n    \"\"\"\n    if self._log:\n        log_min = math.log(self._min_value)\n        log_max = math.log(self._max_value)\n        return float(math.exp(random.uniform(log_min, log_max)))\n    return float(random.uniform(self._min_value, self._max_value))\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer","title":"Integer","text":"<pre><code>Integer(\n    min_value: int,\n    max_value: int,\n    log: bool = False,\n    prior: int | _Unset = _UNSET,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"]\n        | ConfidenceLevel\n        | _Unset\n    ) = _UNSET,\n)\n</code></pre> <p>               Bases: <code>Domain[int]</code></p> <p>A domain representing a range of integer values.</p> ATTRIBUTE DESCRIPTION <code>min_value</code> <p>The minimum value of the domain.</p> <p> TYPE: <code>int</code> </p> <code>max_value</code> <p>The maximum value of the domain.</p> <p> TYPE: <code>int</code> </p> <code>log</code> <p>Whether to sample values on a logarithmic scale.</p> <p> TYPE: <code>int</code> </p> <code>prior</code> <p>The prior value for the domain, if any.</p> <p> TYPE: <code>int</code> </p> <code>prior_confidence</code> <p>The confidence level of the prior value.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> PARAMETER DESCRIPTION <code>min_value</code> <p>The minimum value of the domain.</p> <p> TYPE: <code>int</code> </p> <code>max_value</code> <p>The maximum value of the domain.</p> <p> TYPE: <code>int</code> </p> <code>log</code> <p>Whether to sample values on a logarithmic scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>prior</code> <p>The prior value for the domain, if any.</p> <p> TYPE: <code>int | _Unset</code> DEFAULT: <code>_UNSET</code> </p> <code>prior_confidence</code> <p>The confidence level of the prior value.</p> <p> TYPE: <code>Literal['low', 'medium', 'high'] | ConfidenceLevel | _Unset</code> DEFAULT: <code>_UNSET</code> </p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def __init__(\n    self,\n    min_value: int,\n    max_value: int,\n    log: bool = False,  # noqa: FBT001, FBT002\n    prior: int | _Unset = _UNSET,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | ConfidenceLevel | _Unset\n    ) = _UNSET,\n):\n    \"\"\"Initialize the Integer domain with min and max values, and optional prior.\n\n    Args:\n        min_value: The minimum value of the domain.\n        max_value: The maximum value of the domain.\n        log: Whether to sample values on a logarithmic scale.\n        prior: The prior value for the domain, if any.\n        prior_confidence: The confidence level of the prior value.\n    \"\"\"\n    self._min_value = min_value\n    self._max_value = max_value\n    self._log = log\n    self._prior = prior\n    self._prior_confidence = (\n        convert_confidence_level(prior_confidence)\n        if isinstance(prior_confidence, str)\n        else prior_confidence\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.has_prior","title":"has_prior  <code>property</code>","text":"<pre><code>has_prior: bool\n</code></pre> <p>Check if the integer domain has a prior defined.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the prior and prior confidence are set, False otherwise.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.max_value","title":"max_value  <code>property</code>","text":"<pre><code>max_value: int\n</code></pre> <p>Get the maximum value of the integer domain.</p> RETURNS DESCRIPTION <code>int</code> <p>The maximum value of the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If min_value is greater than max_value.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.min_value","title":"min_value  <code>property</code>","text":"<pre><code>min_value: int\n</code></pre> <p>Get the minimum value of the integer domain.</p> RETURNS DESCRIPTION <code>int</code> <p>The minimum value of the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If min_value is greater than max_value.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.prior","title":"prior  <code>property</code>","text":"<pre><code>prior: int\n</code></pre> <p>Get the prior value of the integer domain.</p> RETURNS DESCRIPTION <code>int</code> <p>The prior value of the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.prior_confidence","title":"prior_confidence  <code>property</code>","text":"<pre><code>prior_confidence: ConfidenceLevel\n</code></pre> <p>Get the confidence level of the prior value.</p> RETURNS DESCRIPTION <code>ConfidenceLevel</code> <p>The confidence level of the prior value.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain has no prior defined.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.range_compatibility_identifier","title":"range_compatibility_identifier  <code>property</code>","text":"<pre><code>range_compatibility_identifier: str\n</code></pre> <p>Get a string identifier for the range compatibility of the integer domain.</p> RETURNS DESCRIPTION <code>str</code> <p>A string representation of the minimum and maximum values, and whether the domain is logarithmic.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.centered_around","title":"centered_around","text":"<pre><code>centered_around(\n    center: int, confidence: ConfidenceLevel\n) -&gt; Integer\n</code></pre> <p>Create a new integer domain centered around a specific value.</p> PARAMETER DESCRIPTION <code>center</code> <p>The value around which to center the new domain.</p> <p> TYPE: <code>int</code> </p> <code>confidence</code> <p>The confidence level for the new domain.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> <code>center</code> <p>int:</p> <p> TYPE: <code>int</code> </p> <code>confidence</code> <p>ConfidenceLevel:</p> <p> TYPE: <code>ConfidenceLevel</code> </p> RETURNS DESCRIPTION <code>Integer</code> <p>A new Integer instance that is centered around the specified value.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the center value is not within the domain's range.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def centered_around(\n    self,\n    center: int,\n    confidence: ConfidenceLevel,\n) -&gt; Integer:\n    \"\"\"Create a new integer domain centered around a specific value.\n\n    Args:\n        center: The value around which to center the new domain.\n        confidence: The confidence level for the new domain.\n        center: int:\n        confidence: ConfidenceLevel:\n\n    Returns:\n        A new Integer instance that is centered around the specified value.\n\n    Raises:\n        ValueError: If the center value is not within the domain's range.\n\n    \"\"\"\n    new_min, new_max = cast(\n        tuple[int, int],\n        _calculate_new_domain_bounds(\n            number_type=int,\n            min_value=self.min_value,\n            max_value=self.max_value,\n            center=center,\n            confidence=confidence,\n        ),\n    )\n    return Integer(\n        min_value=new_min,\n        max_value=new_max,\n        log=self._log,\n        prior=center,\n        prior_confidence=confidence,\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Domain[T]\n</code></pre> <p>Create a new Domain instance from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Domain[T]</code> <p>A new Domain instance with the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the attributes do not match the domain's expected structure.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Domain[T]:\n    \"\"\"Create a new Domain instance from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new Domain instance with the specified attributes.\n\n    Raises:\n        ValueError: If the attributes do not match the domain's expected structure.\n\n    \"\"\"\n    return type(self)(**attrs)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the domain as a mapping. This method collects all attributes of the domain class and instance, excluding private attributes and methods, and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the domain as a mapping.\n    This method collects all attributes of the domain class and instance,\n    excluding private attributes and methods, and returns them as a dictionary.\n\n    Returns:\n        A mapping of attribute names to their values.\n    \"\"\"\n    return {k.lstrip(\"_\"): v for k, v in vars(self).items()}\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Integer.sample","title":"sample","text":"<pre><code>sample() -&gt; int\n</code></pre> <p>Sample a random integer value from the domain.</p> RETURNS DESCRIPTION <code>int</code> <p>A randomly selected integer value within the domain's range.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If the domain is set to sample on a logarithmic scale, as this is not implemented yet.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def sample(self) -&gt; int:\n    \"\"\"Sample a random integer value from the domain.\n\n    Returns:\n        A randomly selected integer value within the domain's range.\n\n    Raises:\n        NotImplementedError: If the domain is set to sample on a logarithmic\n            scale, as this is not implemented yet.\n\n    \"\"\"\n    if self._log:\n        raise NotImplementedError(\"TODO.\")\n    return int(random.randint(self._min_value, self._max_value))\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Operation","title":"Operation","text":"<pre><code>Operation(\n    operator: Callable | str,\n    args: Sequence[Any] | Resolvable | None = None,\n    kwargs: Mapping[str, Any] | Resolvable | None = None,\n)\n</code></pre> <p>               Bases: <code>Resolvable</code></p> <p>A class representing an operation in a NePS space.</p> ATTRIBUTE DESCRIPTION <code>operator</code> <p>The operator to be used in the operation, can be a callable or a string.</p> <p> TYPE: <code>Callable | str</code> </p> <code>args</code> <p>A sequence of arguments to be passed to the operator.</p> <p> TYPE: <code>tuple[Any, ...]</code> </p> <code>kwargs</code> <p>A mapping of keyword arguments to be passed to the operator.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> PARAMETER DESCRIPTION <code>operator</code> <p>The operator to be used in the operation, can be a callable or a string.</p> <p> TYPE: <code>Callable | str</code> </p> <code>args</code> <p>A sequence of arguments to be passed to the operator.</p> <p> TYPE: <code>Sequence[Any] | Resolvable | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>A mapping of keyword arguments to be passed to the operator.</p> <p> TYPE: <code>Mapping[str, Any] | Resolvable | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def __init__(\n    self,\n    operator: Callable | str,\n    args: Sequence[Any] | Resolvable | None = None,\n    kwargs: Mapping[str, Any] | Resolvable | None = None,\n):\n    \"\"\"Initialize the Operation with an operator, arguments, and keyword arguments.\n\n    Args:\n        operator: The operator to be used in the operation, can be a callable or a\n            string.\n        args: A sequence of arguments to be passed to the operator.\n        kwargs: A mapping of keyword arguments to be passed to the operator.\n\n    \"\"\"\n    self._operator = operator\n\n    self._args: tuple[Any, ...] | Resolvable\n    if not isinstance(args, Resolvable):\n        self._args = tuple(args) if args else ()\n    else:\n        self._args = args\n\n    self._kwargs: Mapping[str, Any] | Resolvable\n    if not isinstance(kwargs, Resolvable):\n        self._kwargs = kwargs if kwargs else {}\n    else:\n        self._kwargs = kwargs\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Operation.args","title":"args  <code>property</code>","text":"<pre><code>args: tuple[Any, ...]\n</code></pre> <p>Get the arguments of the operation.</p> RETURNS DESCRIPTION <code>tuple[Any, ...]</code> <p>A tuple of arguments to be passed to the operator.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the args are not a tuple or Resolvable.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Operation.kwargs","title":"kwargs  <code>property</code>","text":"<pre><code>kwargs: Mapping[str, Any]\n</code></pre> <p>Get the keyword arguments of the operation.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of keyword arguments to be passed to the operator.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the kwargs are not a mapping or Resolvable.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Operation.operator","title":"operator  <code>property</code>","text":"<pre><code>operator: Callable | str\n</code></pre> <p>Get the operator of the operation.</p> RETURNS DESCRIPTION <code>Callable | str</code> <p>The operator, which can be a callable or a string.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the operator is not callable or a string.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Operation.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Operation\n</code></pre> <p>Create a new Operation instance from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Operation</code> <p>A new Operation instance with the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the attributes do not match the operation's expected structure.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Operation:\n    \"\"\"Create a new Operation instance from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new Operation instance with the specified attributes.\n\n    Raises:\n        ValueError: If the attributes do not match the operation's expected structure.\n\n    \"\"\"\n    # TODO: [lum] simplify this. We know the fields. Maybe other places too.\n    final_attrs: dict[str, Any] = {}\n    for name, value in attrs.items():\n        if \"{\" in name and \"}\" in name:\n            base, key = name.split(\"{\")\n            key = key.rstrip(\"}\")\n            final_attrs.setdefault(base, {})[key] = value\n        elif \"[\" in name and \"]\" in name:\n            base, idx_str = name.split(\"[\")\n            idx = int(idx_str.rstrip(\"]\"))\n            final_attrs.setdefault(base, []).insert(idx, value)\n        else:\n            final_attrs[name] = value\n    return type(self)(**final_attrs)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Operation.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the operation as a mapping. This method collects all attributes of the operation class and instance, excluding private attributes and methods, and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the operation as a mapping.\n    This method collects all attributes of the operation class and instance,\n    excluding private attributes and methods, and returns them as a dictionary.\n\n    Returns:\n        A mapping of attribute names to their values.\n\n    \"\"\"\n    # TODO: [lum] simplify this. We know the fields. Maybe other places too.\n    result: dict[str, Any] = {}\n    for name, value in vars(self).items():\n        stripped_name = name.lstrip(\"_\")\n        if isinstance(value, dict):\n            for k, v in value.items():\n                # Multiple {{}} needed to escape surrounding '{' and '}'.\n                result[f\"{stripped_name}{{{k}}}\"] = v\n        elif isinstance(value, tuple):\n            for i, v in enumerate(value):\n                result[f\"{stripped_name}[{i}]\"] = v\n        else:\n            result[stripped_name] = value\n    return result\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Pipeline","title":"Pipeline","text":"<p>               Bases: <code>Resolvable</code></p> <p>A class representing a pipeline in NePS spaces.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Pipeline.fidelity_attrs","title":"fidelity_attrs  <code>property</code>","text":"<pre><code>fidelity_attrs: Mapping[str, Fidelity]\n</code></pre> <p>Get the fidelity attributes of the pipeline. Fidelity attributes are special attributes that represent the fidelity of the pipeline.</p> RETURNS DESCRIPTION <code>Mapping[str, Fidelity]</code> <p>A mapping of attribute names to Fidelity objects.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Pipeline.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Pipeline\n</code></pre> <p>Create a new Pipeline instance from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>A new Pipeline instance with the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the attributes do not match the pipeline's expected structure.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Pipeline:\n    \"\"\"Create a new Pipeline instance from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n\n    Returns:\n        A new Pipeline instance with the specified attributes.\n\n    Raises:\n        ValueError: If the attributes do not match the pipeline's expected structure.\n    \"\"\"\n    new_pipeline = Pipeline()\n    for name, value in attrs.items():\n        setattr(new_pipeline, name, value)\n    return new_pipeline\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Pipeline.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the pipeline as a mapping. This method collects all attributes of the pipeline class and instance, excluding private attributes and methods, and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the pipeline as a mapping.\n    This method collects all attributes of the pipeline class and instance,\n    excluding private attributes and methods, and returns them as a dictionary.\n\n    Returns:\n        A mapping of attribute names to their values.\n    \"\"\"\n    attrs = {}\n\n    for attr_name, attr_value in vars(self.__class__).items():\n        if attr_name.startswith(\"_\") or callable(attr_value):\n            continue\n        attrs[attr_name] = attr_value\n\n    for attr_name, attr_value in vars(self).items():\n        if attr_name.startswith(\"_\") or callable(attr_value):\n            continue\n        attrs[attr_name] = attr_value\n\n    properties_to_ignore = (\"fidelity_attrs\",)\n    for property_to_ignore in properties_to_ignore:\n        attrs.pop(property_to_ignore, None)\n\n    return attrs\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resampled","title":"Resampled","text":"<pre><code>Resampled(source: Resolvable | str)\n</code></pre> <p>               Bases: <code>Resolvable</code></p> <p>A class representing a resampling operation in a NePS space.</p> ATTRIBUTE DESCRIPTION <code>source</code> <p>The source of the resampling, which can be a resolvable object or a string.</p> <p> TYPE: <code>Resolvable | str</code> </p> PARAMETER DESCRIPTION <code>source</code> <p>The source of the resampling, can be a resolvable object or a string.</p> <p> TYPE: <code>Resolvable | str</code> </p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def __init__(self, source: Resolvable | str):\n    \"\"\"Initialize the Resampled object with a source.\n\n    Args:\n        source: The source of the resampling, can be a resolvable object or a string.\n    \"\"\"\n    self._source = source\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resampled.is_resampling_by_name","title":"is_resampling_by_name  <code>property</code>","text":"<pre><code>is_resampling_by_name: bool\n</code></pre> <p>Check if the resampling is by name.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the source is a string, indicating a resampling by name, False if the source is a resolvable object.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resampled.source","title":"source  <code>property</code>","text":"<pre><code>source: Resolvable | str\n</code></pre> <p>Get the source of the resampling.</p> RETURNS DESCRIPTION <code>Resolvable | str</code> <p>The source of the resampling, which can be a resolvable object or a string</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resampled.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Resolvable\n</code></pre> <p>Create a new resolvable object from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Resolvable</code> <p>A new resolvable object created from the specified attributes.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the resampling is by name or the source is not resolvable.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Resolvable:\n    \"\"\"Create a new resolvable object from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new resolvable object created from the specified attributes.\n\n    Raises:\n        ValueError: If the resampling is by name or the source is not resolvable.\n\n    \"\"\"\n    if self.is_resampling_by_name:\n        raise ValueError(\n            \"This is a resampling by name, can't create object for it:\"\n            f\" {self.source!r}.\"\n        )\n    if not isinstance(self._source, Resolvable):\n        raise ValueError(\n            f\"Source should be a resolvable object. Is: {self._source!r}.\"\n        )\n    return self._source.from_attrs(attrs)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resampled.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the resampling source as a mapping.</p> RETURNS DESCRIPTION <code>Mapping[str, Any]</code> <p>A mapping of attribute names to their values.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the resampling is by name or the source is not resolvable.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the resampling source as a mapping.\n\n    Returns:\n      A mapping of attribute names to their values.\n\n    Raises:\n      ValueError: If the resampling is by name or the source is not resolvable.\n\n    \"\"\"\n    if self.is_resampling_by_name:\n        raise ValueError(\n            f\"This is a resampling by name, can't get attrs from it: {self.source!r}.\"\n        )\n    if not isinstance(self._source, Resolvable):\n        raise ValueError(\n            f\"Source should be a resolvable object. Is: {self._source!r}.\"\n        )\n    return self._source.get_attrs()\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resolvable","title":"Resolvable","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for objects that can be resolved into attributes.</p>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resolvable.from_attrs","title":"from_attrs","text":"<pre><code>from_attrs(attrs: Mapping[str, Any]) -&gt; Resolvable\n</code></pre> <p>Create a new resolvable object from the given attributes.</p> PARAMETER DESCRIPTION <code>attrs</code> <p>A mapping of attribute names to their values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Resolvable</code> <p>A new resolvable object with the specified attributes.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def from_attrs(self, attrs: Mapping[str, Any]) -&gt; Resolvable:\n    \"\"\"Create a new resolvable object from the given attributes.\n\n    Args:\n        attrs: A mapping of attribute names to their values.\n\n    Returns:\n        A new resolvable object with the specified attributes.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.Resolvable.get_attrs","title":"get_attrs","text":"<pre><code>get_attrs() -&gt; Mapping[str, Any]\n</code></pre> <p>Get the attributes of the resolvable object as a mapping.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def get_attrs(self) -&gt; Mapping[str, Any]:\n    \"\"\"Get the attributes of the resolvable object as a mapping.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.convert_confidence_level","title":"convert_confidence_level","text":"<pre><code>convert_confidence_level(\n    confidence: str,\n) -&gt; ConfidenceLevel\n</code></pre> <p>Convert a string representation of confidence level to ConfidenceLevel enum.</p> PARAMETER DESCRIPTION <code>confidence</code> <p>A string representing the confidence level, e.g., \"low\", \"medium\", \"high\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ConfidenceLevel</code> <p>The corresponding ConfidenceLevel enum value.</p> <p> TYPE: <code>ConfidenceLevel</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the input string does not match any of the defined confidence levels.</p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def convert_confidence_level(confidence: str) -&gt; ConfidenceLevel:\n    \"\"\"Convert a string representation of confidence level to ConfidenceLevel enum.\n\n    Args:\n        confidence: A string representing the confidence level, e.g., \"low\", \"medium\",\n            \"high\".\n\n    Returns:\n        ConfidenceLevel: The corresponding ConfidenceLevel enum value.\n\n    Raises:\n        ValueError: If the input string does not match any of the defined confidence\n            levels.\n    \"\"\"\n    try:\n        return ConfidenceLevel[confidence.upper()]\n    except KeyError as e:\n        raise ValueError(f\"Invalid confidence level: {confidence}\") from e\n</code></pre>"},{"location":"api/neps/space/neps_spaces/parameters/#neps.space.neps_spaces.parameters.resolvable_is_fully_resolved","title":"resolvable_is_fully_resolved","text":"<pre><code>resolvable_is_fully_resolved(\n    resolvable: Resolvable,\n) -&gt; bool\n</code></pre> <p>Check if a resolvable object is fully resolved. A resolvable object is considered fully resolved if all its attributes are either not instances of Resolvable or are themselves fully resolved.</p> PARAMETER DESCRIPTION <code>resolvable</code> <p>Resolvable:</p> <p> TYPE: <code>Resolvable</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the resolvable object is fully resolved, False otherwise.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps\\space\\neps_spaces\\parameters.py</code> <pre><code>def resolvable_is_fully_resolved(resolvable: Resolvable) -&gt; bool:\n    \"\"\"Check if a resolvable object is fully resolved.\n    A resolvable object is considered fully resolved if all its attributes are either\n    not instances of Resolvable or are themselves fully resolved.\n\n    Args:\n      resolvable: Resolvable:\n\n    Returns:\n        bool: True if the resolvable object is fully resolved, False otherwise.\n    \"\"\"\n    attr_objects = resolvable.get_attrs().values()\n    return all(\n        not isinstance(obj, Resolvable) or resolvable_is_fully_resolved(obj)\n        for obj in attr_objects\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/","title":"Sampling","text":"<p>This module defines various samplers for NEPS spaces, allowing for different sampling strategies such as predefined values, random sampling, and mutation-based sampling.</p>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.CrossoverByMixingSampler","title":"CrossoverByMixingSampler","text":"<pre><code>CrossoverByMixingSampler(\n    predefined_samplings_1: Mapping[str, Any],\n    predefined_samplings_2: Mapping[str, Any],\n    prefer_first_probability: float,\n)\n</code></pre> <p>               Bases: <code>DomainSampler</code></p> <p>A sampler that performs a crossover operation by mixing two sets of predefined samplings. It combines the predefined samplings from two sources, allowing for a probability-based selection of values from either source.</p> PARAMETER DESCRIPTION <code>predefined_samplings_1</code> <p>The first set of predefined samplings.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>predefined_samplings_2</code> <p>The second set of predefined samplings.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>prefer_first_probability</code> <p>The probability of preferring values from the first set over the second set when both have values for the same path. This should be a float between 0 and 1, where 0 means always prefer the second set and 1 means always prefer the first set.</p> <p> TYPE: <code>float</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If prefer_first_probability is not between 0 and 1.</p> <code>CrossoverNotPossibleError</code> <p>If no crossovers were made between the two sets of predefined samplings.</p> PARAMETER DESCRIPTION <code>predefined_samplings_1</code> <p>The first set of predefined samplings.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>predefined_samplings_2</code> <p>The second set of predefined samplings.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>prefer_first_probability</code> <p>The probability of preferring values from the first set over the second set when both have values for the same path. This should be a float between 0 and 1, where 0 means always prefer the second set and 1 means always prefer the first set.</p> <p> TYPE: <code>float</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If prefer_first_probability is not between 0 and 1.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __init__(\n    self,\n    predefined_samplings_1: Mapping[str, Any],\n    predefined_samplings_2: Mapping[str, Any],\n    prefer_first_probability: float,\n):\n    \"\"\"Initialize the sampler with two sets of predefined samplings and a preference\n    probability for the first set.\n\n    Args:\n        predefined_samplings_1: The first set of predefined samplings.\n        predefined_samplings_2: The second set of predefined samplings.\n        prefer_first_probability: The probability of preferring values from the\n            first set over the second set when both have values for the same path.\n            This should be a float between 0 and 1, where 0 means always prefer the\n            second set and 1 means always prefer the first set.\n\n    Raises:\n        ValueError: If prefer_first_probability is not between 0 and 1.\n    \"\"\"\n    if not isinstance(prefer_first_probability, float) or not (\n        0 &lt;= prefer_first_probability &lt;= 1\n    ):\n        raise ValueError(\n            \"Invalid value for `prefer_first_probability`:\"\n            f\" {prefer_first_probability!r}.\"\n        )\n\n    (\n        made_any_crossovers,\n        crossed_over_samplings_to_make,\n    ) = _crossover_samplings_to_make_by_mixing(\n        predefined_samplings_1=predefined_samplings_1,\n        predefined_samplings_2=predefined_samplings_2,\n        prefer_first_probability=prefer_first_probability,\n    )\n\n    if not made_any_crossovers:\n        raise CrossoverNotPossibleError(\"No crossovers were made.\")\n\n    self._random_sampler = RandomSampler(\n        predefined_samplings=crossed_over_samplings_to_make,\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.CrossoverByMixingSampler.__call__","title":"__call__","text":"<pre><code>__call__(\n    *, domain_obj: Domain[T], current_path: str\n) -&gt; T\n</code></pre> <p>Sample a value from the crossed-over predefined samplings or the domain.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object from which to sample.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>current_path</code> <p>The path for which to sample a value.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A sampled value, either from the crossed-over predefined samplings or     from the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the current path is not in the crossed-over predefined samplings and the domain does not have a prior defined.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __call__(\n    self,\n    *,\n    domain_obj: Domain[T],\n    current_path: str,\n) -&gt; T:\n    \"\"\"Sample a value from the crossed-over predefined samplings or the domain.\n\n    Args:\n        domain_obj: The domain object from which to sample.\n        current_path: The path for which to sample a value.\n\n    Returns:\n        A sampled value, either from the crossed-over predefined samplings or\n            from the domain.\n\n    Raises:\n        ValueError: If the current path is not in the crossed-over predefined\n            samplings and the domain does not have a prior defined.\n    \"\"\"\n    return self._random_sampler(domain_obj=domain_obj, current_path=current_path)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.CrossoverNotPossibleError","title":"CrossoverNotPossibleError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a crossover operation is not possible.</p>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.DomainSampler","title":"DomainSampler","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for domain samplers that can sample from a given domain.</p>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.DomainSampler.__call__","title":"__call__","text":"<pre><code>__call__(\n    *, domain_obj: Domain[T], current_path: str\n) -&gt; T\n</code></pre> <p>Sample a value from the given domain.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object to sample from.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>current_path</code> <p>The current path in the resolution context.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A sampled value of type T from the domain.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If the method is not implemented.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __call__(\n    self,\n    *,\n    domain_obj: Domain[T],\n    current_path: str,\n) -&gt; T:\n    \"\"\"Sample a value from the given domain.\n\n    Args:\n        domain_obj: The domain object to sample from.\n        current_path: The current path in the resolution context.\n\n    Returns:\n        A sampled value of type T from the domain.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.MutatateUsingCentersSampler","title":"MutatateUsingCentersSampler","text":"<pre><code>MutatateUsingCentersSampler(\n    predefined_samplings: Mapping[str, Any],\n    n_mutations: int,\n)\n</code></pre> <p>               Bases: <code>DomainSampler</code></p> <p>A sampler that mutates predefined samplings by forgetting a certain number of them, but still uses the original values as centers for sampling.</p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>n_mutations</code> <p>The number of predefined samplings to mutate. This should be an integer greater than 0 and less than or equal to the number of predefined samplings.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If n_mutations is not a valid integer or if it exceeds the number of predefined samplings.</p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>n_mutations</code> <p>The number of predefined samplings to mutate. This should be an integer greater than 0 and less than or equal to the number of predefined samplings.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If n_mutations is not a valid integer or if it exceeds the number of predefined samplings.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __init__(\n    self,\n    predefined_samplings: Mapping[str, Any],\n    n_mutations: int,\n):\n    \"\"\"Initialize the sampler with predefined samplings and a number of mutations.\n\n    Args:\n        predefined_samplings: A mapping of paths to predefined values.\n        n_mutations: The number of predefined samplings to mutate.\n            This should be an integer greater than 0 and less than or equal to the\n            number of predefined samplings.\n\n    Raises:\n        ValueError: If n_mutations is not a valid integer or if it exceeds\n            the number of predefined samplings.\n    \"\"\"\n    if (\n        not isinstance(n_mutations, int)\n        or n_mutations &lt;= 0\n        or n_mutations &gt; len(predefined_samplings)\n    ):\n        raise ValueError(f\"Invalid value for `n_mutations`: {n_mutations!r}.\")\n\n    self._kept_samplings_to_make = _mutate_samplings_to_make_by_forgetting(\n        samplings_to_make=predefined_samplings,\n        n_forgets=n_mutations,\n    )\n\n    # Still remember the original choices. We'll use them as centers later.\n    self._original_samplings_to_make = predefined_samplings\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.MutatateUsingCentersSampler.__call__","title":"__call__","text":"<pre><code>__call__(\n    *, domain_obj: Domain[T], current_path: str\n) -&gt; T\n</code></pre> <p>Sample a value from the predefined samplings or the domain, using original values as centers if the current path is not in the kept samplings.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object from which to sample.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>current_path</code> <p>The path for which to sample a value.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A sampled value, either from the kept samplings or from the domain,     using the original values as centers if necessary.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the current path is not in the kept samplings and the domain does not have a prior defined.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __call__(\n    self,\n    *,\n    domain_obj: Domain[T],\n    current_path: str,\n) -&gt; T:\n    \"\"\"Sample a value from the predefined samplings or the domain, using original\n    values as centers if the current path is not in the kept samplings.\n\n    Args:\n        domain_obj: The domain object from which to sample.\n        current_path: The path for which to sample a value.\n\n    Returns:\n        A sampled value, either from the kept samplings or from the domain,\n            using the original values as centers if necessary.\n\n    Raises:\n        ValueError: If the current path is not in the kept samplings and the\n            domain does not have a prior defined.\n    \"\"\"\n    if current_path not in self._kept_samplings_to_make:\n        # For this path we either have forgotten the value or we never had it.\n        if current_path in self._original_samplings_to_make:\n            # We had a value for this path originally, use it as a center.\n            original_value = self._original_samplings_to_make[current_path]\n            sampled_value = domain_obj.centered_around(\n                center=original_value,\n                confidence=ConfidenceLevel.HIGH,\n            ).sample()\n        else:\n            # We never had a value for this path, we can only sample from the domain.\n            sampled_value = domain_obj.sample()\n    else:\n        # For this path we have chosen to keep the original value.\n        sampled_value = cast(T, self._kept_samplings_to_make[current_path])\n\n    return sampled_value\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.MutateByForgettingSampler","title":"MutateByForgettingSampler","text":"<pre><code>MutateByForgettingSampler(\n    predefined_samplings: Mapping[str, Any], n_forgets: int\n)\n</code></pre> <p>               Bases: <code>DomainSampler</code></p> <p>A sampler that mutates predefined samplings by forgetting a certain number of them. It randomly selects a number of predefined samplings to forget and returns a new sampler that only uses the remaining samplings.</p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>n_forgets</code> <p>The number of predefined samplings to forget. This should be an integer greater than 0 and less than or equal to the number of predefined samplings.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If n_forgets is not a valid integer or if it exceeds the number of predefined samplings.</p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>n_forgets</code> <p>The number of predefined samplings to forget. This should be an integer greater than 0 and less than or equal to the number of predefined samplings.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If n_forgets is not a valid integer or if it exceeds the number of predefined samplings.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __init__(\n    self,\n    predefined_samplings: Mapping[str, Any],\n    n_forgets: int,\n):\n    \"\"\"Initialize the sampler with predefined samplings and a number of forgets.\n\n    Args:\n        predefined_samplings: A mapping of paths to predefined values.\n        n_forgets: The number of predefined samplings to forget.\n            This should be an integer greater than 0 and less than or equal to the\n            number of predefined samplings.\n\n    Raises:\n        ValueError: If n_forgets is not a valid integer or if it exceeds the\n            number of predefined samplings.\n    \"\"\"\n    if (\n        not isinstance(n_forgets, int)\n        or n_forgets &lt;= 0\n        or n_forgets &gt; len(predefined_samplings)\n    ):\n        raise ValueError(f\"Invalid value for `n_forgets`: {n_forgets!r}.\")\n\n    mutated_samplings_to_make = _mutate_samplings_to_make_by_forgetting(\n        samplings_to_make=predefined_samplings,\n        n_forgets=n_forgets,\n    )\n\n    self._random_sampler = RandomSampler(\n        predefined_samplings=mutated_samplings_to_make,\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.MutateByForgettingSampler.__call__","title":"__call__","text":"<pre><code>__call__(\n    *, domain_obj: Domain[T], current_path: str\n) -&gt; T\n</code></pre> <p>Sample a value from the mutated predefined samplings or the domain.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object from which to sample.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>current_path</code> <p>The path for which to sample a value.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A sampled value, either from the mutated predefined samplings or from     the domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the current path is not in the mutated predefined samplings and the domain does not have a prior defined.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __call__(\n    self,\n    *,\n    domain_obj: Domain[T],\n    current_path: str,\n) -&gt; T:\n    \"\"\"Sample a value from the mutated predefined samplings or the domain.\n\n    Args:\n        domain_obj: The domain object from which to sample.\n        current_path: The path for which to sample a value.\n\n    Returns:\n        A sampled value, either from the mutated predefined samplings or from\n            the domain.\n\n    Raises:\n        ValueError: If the current path is not in the mutated predefined\n            samplings and the domain does not have a prior defined.\n    \"\"\"\n    return self._random_sampler(domain_obj=domain_obj, current_path=current_path)\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.OnlyPredefinedValuesSampler","title":"OnlyPredefinedValuesSampler","text":"<pre><code>OnlyPredefinedValuesSampler(\n    predefined_samplings: Mapping[str, Any],\n)\n</code></pre> <p>               Bases: <code>DomainSampler</code></p> <p>A sampler that only returns predefined values for a given path. If the path is not found in the predefined values, it raises a ValueError.</p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If predefined_samplings is empty.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __init__(\n    self,\n    predefined_samplings: Mapping[str, Any],\n):\n    \"\"\"Initialize the sampler with predefined samplings.\n\n    Args:\n        predefined_samplings: A mapping of paths to predefined values.\n\n    Raises:\n        ValueError: If predefined_samplings is empty.\n    \"\"\"\n    self._predefined_samplings = predefined_samplings\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.OnlyPredefinedValuesSampler.__call__","title":"__call__","text":"<pre><code>__call__(\n    *, domain_obj: Domain[T], current_path: str\n) -&gt; T\n</code></pre> <p>Sample a value from the predefined samplings for the given path.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object, not used in this sampler.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>current_path</code> <p>The path for which to sample a value.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The predefined value for the given path.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the current path is not in the predefined samplings.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __call__(\n    self,\n    *,\n    domain_obj: Domain[T],  # noqa: ARG002\n    current_path: str,\n) -&gt; T:\n    \"\"\"Sample a value from the predefined samplings for the given path.\n\n    Args:\n        domain_obj: The domain object, not used in this sampler.\n        current_path: The path for which to sample a value.\n\n    Returns:\n        The predefined value for the given path.\n\n    Raises:\n        ValueError: If the current path is not in the predefined samplings.\n    \"\"\"\n    if current_path not in self._predefined_samplings:\n        raise ValueError(f\"No predefined value for path: {current_path!r}.\")\n    return cast(T, self._predefined_samplings[current_path])\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.PriorOrFallbackSampler","title":"PriorOrFallbackSampler","text":"<pre><code>PriorOrFallbackSampler(\n    fallback_sampler: DomainSampler,\n    prior_use_probability: float,\n)\n</code></pre> <p>               Bases: <code>DomainSampler</code></p> <p>A sampler that uses a prior value if available, otherwise falls back to another sampler.</p> PARAMETER DESCRIPTION <code>fallback_sampler</code> <p>A DomainSampler to use if the prior is not available.</p> <p> TYPE: <code>DomainSampler</code> </p> <code>prior_use_probability</code> <p>The probability of using the prior value when available. This should be a float between 0 and 1, where 0 means never use the prior and 1 means always use it.</p> <p> TYPE: <code>float</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the prior_use_probability is not between 0 and 1.</p> PARAMETER DESCRIPTION <code>fallback_sampler</code> <p>A DomainSampler to use if the prior is not available.</p> <p> TYPE: <code>DomainSampler</code> </p> <code>prior_use_probability</code> <p>The probability of using the prior value when available. This should be a float between 0 and 1, where 0 means never use the prior and 1 means always use it.</p> <p> TYPE: <code>float</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the prior_use_probability is not between 0 and 1.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __init__(\n    self,\n    fallback_sampler: DomainSampler,\n    prior_use_probability: float,\n):\n    \"\"\"Initialize the sampler with a fallback sampler and a prior use probability.\n\n    Args:\n        fallback_sampler: A DomainSampler to use if the prior is not available.\n        prior_use_probability: The probability of using the prior value when\n            available. This should be a float between 0 and 1, where 0 means never\n            use the prior and 1 means always use it.\n\n    Raises:\n        ValueError: If the prior_use_probability is not between 0 and 1.\n    \"\"\"\n    if not 0 &lt;= prior_use_probability &lt;= 1:\n        raise ValueError(\n            \"The given `prior_use_probability` value is out of range:\"\n            f\" {prior_use_probability!r}.\"\n        )\n\n    self._fallback_sampler = fallback_sampler\n    self._prior_use_probability = prior_use_probability\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.PriorOrFallbackSampler.__call__","title":"__call__","text":"<pre><code>__call__(\n    *, domain_obj: Domain[T], current_path: str\n) -&gt; T\n</code></pre> <p>Sample a value from the domain, using the prior if available and according to the prior use probability.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object from which to sample.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>current_path</code> <p>The path for which to sample a value.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A sampled value, either from the prior or from the fallback sampler.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the domain does not have a prior defined and the fallback sampler is not provided.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __call__(\n    self,\n    *,\n    domain_obj: Domain[T],\n    current_path: str,\n) -&gt; T:\n    \"\"\"Sample a value from the domain, using the prior if available and according to\n    the prior use probability.\n\n    Args:\n        domain_obj: The domain object from which to sample.\n        current_path: The path for which to sample a value.\n\n    Returns:\n        A sampled value, either from the prior or from the fallback sampler.\n\n    Raises:\n        ValueError: If the domain does not have a prior defined and the fallback\n            sampler is not provided.\n    \"\"\"\n    use_prior = random.choices(\n        (True, False),\n        weights=(self._prior_use_probability, 1 - self._prior_use_probability),\n        k=1,\n    )[0]\n    if domain_obj.has_prior and use_prior:\n        return domain_obj.prior\n    return self._fallback_sampler(\n        domain_obj=domain_obj,\n        current_path=current_path,\n    )\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.RandomSampler","title":"RandomSampler","text":"<pre><code>RandomSampler(predefined_samplings: Mapping[str, Any])\n</code></pre> <p>               Bases: <code>DomainSampler</code></p> <p>A sampler that randomly samples from a predefined set of values. If the current path is not in the predefined values, it samples from the domain.</p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values. This sampler will use these values if available, otherwise it will sample from the domain.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> PARAMETER DESCRIPTION <code>predefined_samplings</code> <p>A mapping of paths to predefined values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If predefined_samplings is empty.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __init__(\n    self,\n    predefined_samplings: Mapping[str, Any],\n):\n    \"\"\"Initialize the sampler with predefined samplings.\n\n    Args:\n        predefined_samplings: A mapping of paths to predefined values.\n\n    Raises:\n        ValueError: If predefined_samplings is empty.\n    \"\"\"\n    self._predefined_samplings = predefined_samplings\n</code></pre>"},{"location":"api/neps/space/neps_spaces/sampling/#neps.space.neps_spaces.sampling.RandomSampler.__call__","title":"__call__","text":"<pre><code>__call__(\n    *, domain_obj: Domain[T], current_path: str\n) -&gt; T\n</code></pre> <p>Sample a value from the predefined samplings or the domain.</p> PARAMETER DESCRIPTION <code>domain_obj</code> <p>The domain object from which to sample.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>current_path</code> <p>The path for which to sample a value.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A sampled value, either from the predefined samplings or from the     domain.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the current path is not in the predefined samplings and the domain does not have a prior defined.</p> Source code in <code>neps\\space\\neps_spaces\\sampling.py</code> <pre><code>def __call__(\n    self,\n    *,\n    domain_obj: Domain[T],\n    current_path: str,\n) -&gt; T:\n    \"\"\"Sample a value from the predefined samplings or the domain.\n\n    Args:\n        domain_obj: The domain object from which to sample.\n        current_path: The path for which to sample a value.\n\n    Returns:\n        A sampled value, either from the predefined samplings or from the\n            domain.\n\n    Raises:\n        ValueError: If the current path is not in the predefined samplings and\n            the domain does not have a prior defined.\n    \"\"\"\n    if current_path not in self._predefined_samplings:\n        sampled_value = domain_obj.sample()\n    else:\n        sampled_value = cast(T, self._predefined_samplings[current_path])\n    return sampled_value\n</code></pre>"},{"location":"api/neps/state/err_dump/","title":"Err dump","text":"<p>Error dump for serializing errors.</p> <p>This resource is used to store errors that can be serialized and deserialized, such that they can be shared between workers.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump","title":"ErrDump  <code>dataclass</code>","text":"<pre><code>ErrDump(\n    SerializableTrialError: ClassVar = SerializableTrialError,\n    errs: list[SerializableTrialError] = list(),\n)\n</code></pre> <p>A collection of errors that can be serialized and deserialized.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump.append","title":"append","text":"<pre><code>append(err: SerializableTrialError) -&gt; None\n</code></pre> <p>Append the an error to the reported errors.</p> Source code in <code>neps\\state\\err_dump.py</code> <pre><code>def append(self, err: SerializableTrialError) -&gt; None:\n    \"\"\"Append the an error to the reported errors.\"\"\"\n    return self.errs.append(err)\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump.empty","title":"empty","text":"<pre><code>empty() -&gt; bool\n</code></pre> <p>Check if the queue is empty.</p> Source code in <code>neps\\state\\err_dump.py</code> <pre><code>def empty(self) -&gt; bool:\n    \"\"\"Check if the queue is empty.\"\"\"\n    return not self.errs\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump.latest_err_as_raisable","title":"latest_err_as_raisable","text":"<pre><code>latest_err_as_raisable() -&gt; SerializedError | None\n</code></pre> <p>Get the latest error.</p> Source code in <code>neps\\state\\err_dump.py</code> <pre><code>def latest_err_as_raisable(self) -&gt; SerializedError | None:\n    \"\"\"Get the latest error.\"\"\"\n    if self.errs:\n        return self.errs[-1].as_raisable()  # type: ignore\n    return None\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError","title":"SerializableTrialError  <code>dataclass</code>","text":"<pre><code>SerializableTrialError(\n    trial_id: str,\n    worker_id: str,\n    err_type: str,\n    err: str,\n    tb: str | None,\n)\n</code></pre> <p>Error information for a trial.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.err","title":"err  <code>instance-attribute</code>","text":"<pre><code>err: str\n</code></pre> <p>The error msg.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.err_type","title":"err_type  <code>instance-attribute</code>","text":"<pre><code>err_type: str\n</code></pre> <p>The type of the error.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.tb","title":"tb  <code>instance-attribute</code>","text":"<pre><code>tb: str | None\n</code></pre> <p>The traceback of the error.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.trial_id","title":"trial_id  <code>instance-attribute</code>","text":"<pre><code>trial_id: str\n</code></pre> <p>The ID of the trial.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.worker_id","title":"worker_id  <code>instance-attribute</code>","text":"<pre><code>worker_id: str\n</code></pre> <p>The ID of the worker that evaluated the trial which caused the error.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.as_raisable","title":"as_raisable","text":"<pre><code>as_raisable() -&gt; SerializedError\n</code></pre> <p>Convert the error to a raisable error.</p> Source code in <code>neps\\state\\err_dump.py</code> <pre><code>def as_raisable(self) -&gt; SerializedError:\n    \"\"\"Convert the error to a raisable error.\"\"\"\n    return SerializedError(\n        f\"An error occurred during the evaluation of a trial '{self.trial_id}' which\"\n        f\" was evaluted by worker '{self.worker_id}'. The original error could not\"\n        \" be deserialized but had the following information:\"\n        \"\\n\"\n        f\"{self.err_type}: {self.err}\"\n        \"\\n\\n\"\n        f\"{self.tb}\"\n    )\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializedError","title":"SerializedError","text":"<p>               Bases: <code>NePSError</code></p> <p>An error the is serialized.</p>"},{"location":"api/neps/state/filebased/","title":"Filebased","text":"<p>Contains reading and writing of various aspects of NePS.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.FileLocker","title":"FileLocker  <code>dataclass</code>","text":"<pre><code>FileLocker(\n    lock_path: Path, poll: float, timeout: float | None\n)\n</code></pre> <p>File-based locker using <code>portalocker</code>.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.FileLocker.lock","title":"lock","text":"<pre><code>lock(*, worker_id: str | None = None) -&gt; Iterator[None]\n</code></pre> <p>Lock the file.</p> PARAMETER DESCRIPTION <code>worker_id</code> <p>The id of the worker trying to acquire the lock.</p> <p>Used for debug messaging purposes.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\state\\filebased.py</code> <pre><code>@contextmanager\ndef lock(self, *, worker_id: str | None = None) -&gt; Iterator[None]:\n    \"\"\"Lock the file.\n\n    Args:\n        worker_id: The id of the worker trying to acquire the lock.\n\n            Used for debug messaging purposes.\n    \"\"\"\n    try:\n        with self._lock:\n            if worker_id is not None:\n                logger.debug(\n                    \"Worker %s acquired lock on %s at %s\",\n                    worker_id,\n                    self.lock_path,\n                    time.time(),\n                )\n\n            yield\n    except pl.exceptions.LockException as e:\n        raise pl.exceptions.LockException(\n            f\"Failed to acquire lock after timeout of {self.timeout} seconds.\"\n            \" This most likely indicates that another process has crashed while\"\n            \" holding the lock.\"\n            f\"\\n\\nLock path: {self.lock_path}\"\n            \"\\n\\nIf you belive this is not the case, you can set some of these\"\n            \" environment variables to increase the timeout:\"\n            f\"\\n\\n{pprint.pformat(ENV_VARS_USED)}\"\n        ) from e\n    finally:\n        if worker_id is not None:\n            with contextlib.suppress(Exception):\n                logger.debug(\n                    \"Worker %s released lock on %s at %s\",\n                    worker_id,\n                    self.lock_path,\n                    time.time(),\n                )\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterErrDump","title":"ReaderWriterErrDump  <code>dataclass</code>","text":"<pre><code>ReaderWriterErrDump()\n</code></pre> <p>ReaderWriter for shared error lists.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterErrDump.read","title":"read  <code>classmethod</code>","text":"<pre><code>read(path: Path) -&gt; ErrDump\n</code></pre> <p>Read an error dump from a file.</p> Source code in <code>neps\\state\\filebased.py</code> <pre><code>@classmethod\ndef read(cls, path: Path) -&gt; ErrDump:\n    \"\"\"Read an error dump from a file.\"\"\"\n    if not path.exists():\n        return ErrDump([])\n\n    with path.open(\"r\") as f:\n        data = [json.loads(line) for line in f]\n\n    return ErrDump([ErrDump.SerializableTrialError(**d) for d in data])\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterErrDump.write","title":"write  <code>classmethod</code>","text":"<pre><code>write(err_dump: ErrDump, path: Path) -&gt; None\n</code></pre> <p>Write an error dump to a file.</p> Source code in <code>neps\\state\\filebased.py</code> <pre><code>@classmethod\ndef write(cls, err_dump: ErrDump, path: Path) -&gt; None:\n    \"\"\"Write an error dump to a file.\"\"\"\n    with path.open(\"w\") as f:\n        lines = [json.dumps(asdict(trial_err)) for trial_err in err_dump.errs]\n        f.write(\"\\n\".join(lines))\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterTrial","title":"ReaderWriterTrial  <code>dataclass</code>","text":"<pre><code>ReaderWriterTrial()\n</code></pre> <p>ReaderWriter for Trial objects.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterTrial.read","title":"read  <code>classmethod</code>","text":"<pre><code>read(directory: Path) -&gt; Trial\n</code></pre> <p>Read a trial from a directory.</p> Source code in <code>neps\\state\\filebased.py</code> <pre><code>@classmethod\ndef read(cls, directory: Path) -&gt; Trial:\n    \"\"\"Read a trial from a directory.\"\"\"\n    config_path = directory / cls.CONFIG_FILENAME\n    metadata_path = directory / cls.METADATA_FILENAME\n    report_path = directory / cls.REPORT_FILENAME\n\n    with metadata_path.open(\"r\") as f:\n        metadata = json.load(f)\n\n    metadata[\"state\"] = Trial.State(metadata[\"state\"])\n\n    return Trial(\n        config=deserialize(config_path, file_format=CONFIG_SERIALIZE_FORMAT),\n        metadata=Trial.MetaData(**metadata),\n        report=(\n            Trial.Report(\n                **deserialize(report_path, file_format=CONFIG_SERIALIZE_FORMAT),\n            )\n            if report_path.exists()\n            else None\n        ),\n    )\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterTrial.write","title":"write  <code>classmethod</code>","text":"<pre><code>write(\n    trial: Trial,\n    directory: Path,\n    *,\n    hints: (\n        Iterable[TrialWriteHint] | TrialWriteHint | None\n    ) = None\n) -&gt; None\n</code></pre> <p>Write a trial to a directory.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial to write.</p> <p> TYPE: <code>Trial</code> </p> <code>directory</code> <p>The directory to write the trial to.</p> <p> TYPE: <code>Path</code> </p> <code>hints</code> <p>What to write. If None, write everything.</p> <p> TYPE: <code>Iterable[TrialWriteHint] | TrialWriteHint | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\state\\filebased.py</code> <pre><code>@classmethod\ndef write(  # noqa: C901, PLR0912\n    cls,\n    trial: Trial,\n    directory: Path,\n    *,\n    hints: Iterable[TrialWriteHint] | TrialWriteHint | None = None,\n) -&gt; None:\n    \"\"\"Write a trial to a directory.\n\n    Args:\n        trial: The trial to write.\n        directory: The directory to write the trial to.\n        hints: What to write. If None, write everything.\n    \"\"\"\n    config_path = directory / cls.CONFIG_FILENAME\n    metadata_path = directory / cls.METADATA_FILENAME\n\n    if isinstance(hints, str):\n        match hints:\n            case \"config\":\n                serialize(\n                    trial.config,\n                    config_path,\n                    check_serialized=False,\n                    file_format=CONFIG_SERIALIZE_FORMAT,\n                )\n            case \"metadata\":\n                data = asdict(trial.metadata)\n                data[\"state\"] = data[\"state\"].value\n                with metadata_path.open(\"w\") as f:\n                    json.dump(data, f)\n\n                if trial.metadata.previous_trial_id is not None:\n                    previous_trial_path = directory / cls.PREVIOUS_TRIAL_ID_FILENAME\n                    previous_trial_path.write_text(trial.metadata.previous_trial_id)\n            case \"report\":\n                if trial.report is None:\n                    raise ValueError(\n                        \"Cannot write report 'hint' when report is None.\"\n                    )\n\n                report_path = directory / cls.REPORT_FILENAME\n                _report = asdict(trial.report)\n                if (err := _report.get(\"err\")) is not None:\n                    _report[\"err\"] = str(err)\n\n                serialize(\n                    _report,\n                    report_path,\n                    check_serialized=False,\n                    file_format=CONFIG_SERIALIZE_FORMAT,\n                )\n            case _:\n                raise ValueError(f\"Invalid hint: {hints}\")\n    elif isinstance(hints, Iterable):\n        for hint in hints:\n            cls.write(trial, directory, hints=hint)  # type: ignore\n    elif hints is None:\n        # We don't know, write everything\n        cls.write(trial, directory, hints=[\"config\", \"metadata\"])\n\n        if trial.report is not None:\n            cls.write(trial, directory, hints=\"report\")\n    else:\n        raise ValueError(f\"Invalid hint: {hints}\")\n</code></pre>"},{"location":"api/neps/state/neps_state/","title":"Neps state","text":"<p>The main state object that holds all the shared state objects.</p> <p>This object is used to interact with the shared state objects in a safe atomic manner, such that each worker can create an identical NePSState and interact with it without having to worry about locking or out-dated information.</p> <p>For an actual instantiation of this object, see <code>create_or_load_filebased_neps_state()</code>.</p>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState","title":"NePSState  <code>dataclass</code>","text":"<pre><code>NePSState(\n    path: Path,\n    _trial_lock: FileLocker,\n    _trial_repo: TrialRepo,\n    _optimizer_lock: FileLocker,\n    _optimizer_info_path: Path,\n    _optimizer_info: OptimizerInfo,\n    _optimizer_state_path: Path,\n    _optimizer_state: OptimizationState,\n    _err_lock: FileLocker,\n    _shared_errors_path: Path,\n    _shared_errors: ErrDump,\n)\n</code></pre> <p>The main state object that holds all the shared state objects.</p>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.all_trial_ids","title":"all_trial_ids","text":"<pre><code>all_trial_ids() -&gt; list[str]\n</code></pre> <p>Get all the trial ids.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def all_trial_ids(self) -&gt; list[str]:\n    \"\"\"Get all the trial ids.\"\"\"\n    return self._trial_repo.list_trial_ids()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.create_or_load","title":"create_or_load  <code>classmethod</code>","text":"<pre><code>create_or_load(\n    path: Path,\n    *,\n    load_only: bool = False,\n    optimizer_info: OptimizerInfo | None = None,\n    optimizer_state: OptimizationState | None = None\n) -&gt; NePSState\n</code></pre> <p>Create a new NePSState in a directory or load the existing one if it already exists, depending on the argument.</p> <p>Warning</p> <p>We check that the optimizer info in the NePSState on disk matches the one that is passed. However we do not lock this check so it is possible that if two processes try to create a NePSState at the same time, both with different optimizer infos, that one will fail to create the NePSState. This is a limitation of the current design.</p> <p>In principal, we could allow multiple optimizers to be run and share the same set of trials.</p> PARAMETER DESCRIPTION <code>path</code> <p>The directory to create the state in.</p> <p> TYPE: <code>Path</code> </p> <code>load_only</code> <p>If True, only load the state and do not create a new one.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>optimizer_info</code> <p>The optimizer info to use.</p> <p> TYPE: <code>OptimizerInfo | None</code> DEFAULT: <code>None</code> </p> <code>optimizer_state</code> <p>The optimizer state to use.</p> <p> TYPE: <code>OptimizationState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NePSState</code> <p>The NePSState.</p> RAISES DESCRIPTION <code>NePSError</code> <p>If the optimizer info on disk does not match the one provided.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>@classmethod\ndef create_or_load(\n    cls,\n    path: Path,\n    *,\n    load_only: bool = False,\n    optimizer_info: OptimizerInfo | None = None,\n    optimizer_state: OptimizationState | None = None,\n) -&gt; NePSState:\n    \"\"\"Create a new NePSState in a directory or load the existing one\n    if it already exists, depending on the argument.\n\n    !!! warning\n\n        We check that the optimizer info in the NePSState on disk matches\n        the one that is passed. However we do not lock this check so it\n        is possible that if two processes try to create a NePSState at the\n        same time, both with different optimizer infos, that one will fail\n        to create the NePSState. This is a limitation of the current design.\n\n        In principal, we could allow multiple optimizers to be run and share\n        the same set of trials.\n\n    Args:\n        path: The directory to create the state in.\n        load_only: If True, only load the state and do not create a new one.\n        optimizer_info: The optimizer info to use.\n        optimizer_state: The optimizer state to use.\n\n    Returns:\n        The NePSState.\n\n    Raises:\n        NePSError: If the optimizer info on disk does not match the one provided.\n    \"\"\"\n    path = path.absolute().resolve()\n    is_new = not path.exists()\n    if load_only:\n        if is_new:\n            raise FileNotFoundError(f\"No NePSState found at '{path}'.\")\n    else:\n        assert optimizer_info is not None\n        assert optimizer_state is not None\n\n    path.mkdir(parents=True, exist_ok=True)\n    config_dir = path / \"configs\"\n    config_dir.mkdir(parents=True, exist_ok=True)\n\n    optimizer_info_path = path / \"optimizer_info.yaml\"\n    optimizer_state_path = path / \"optimizer_state.pkl\"\n    shared_errors_path = path / \"shared_errors.jsonl\"\n\n    # We have to do one bit of sanity checking to ensure that the optimzier\n    # info on disk manages the one we have recieved, otherwise we are unsure which\n    # optimizer is being used.\n    # NOTE: We assume that we do not have to worry about a race condition\n    # here where we have two different NePSState objects with two different optimizer\n    # infos trying to be created at the same time. This avoids the need to lock to\n    # check the optimizer info. If this assumption changes, then we would have\n    # to first lock before we do this check\n    if not is_new:\n        existing_info = _deserialize_optimizer_info(optimizer_info_path)\n        if not load_only and existing_info != optimizer_info:\n            raise NePSError(\n                \"The optimizer info on disk does not match the one provided.\"\n                f\"\\nOn disk: {existing_info}\\nProvided: {optimizer_info}\"\n                f\"\\n\\nLoaded the one on disk from {path}.\"\n            )\n        with optimizer_state_path.open(\"rb\") as f:\n            optimizer_state = pickle.load(f)  # noqa: S301\n\n        optimizer_info = existing_info\n        error_dump = ReaderWriterErrDump.read(shared_errors_path)\n    else:\n        assert optimizer_info is not None\n        assert optimizer_state is not None\n\n        serialize(optimizer_info, path=optimizer_info_path)\n        with optimizer_state_path.open(\"wb\") as f:\n            pickle.dump(optimizer_state, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        error_dump = ErrDump([])\n\n    return NePSState(\n        path=path,\n        _trial_repo=TrialRepo(config_dir),\n        # Locks,\n        _trial_lock=FileLocker(\n            lock_path=path / \".configs.lock\",\n            poll=TRIAL_FILELOCK_POLL,\n            timeout=TRIAL_FILELOCK_TIMEOUT,\n        ),\n        _optimizer_lock=FileLocker(\n            lock_path=path / \".optimizer.lock\",\n            poll=STATE_FILELOCK_POLL,\n            timeout=STATE_FILELOCK_TIMEOUT,\n        ),\n        _err_lock=FileLocker(\n            lock_path=path / \".errors.lock\",\n            poll=GLOBAL_ERR_FILELOCK_POLL,\n            timeout=GLOBAL_ERR_FILELOCK_TIMEOUT,\n        ),\n        # State\n        _optimizer_info_path=optimizer_info_path,\n        _optimizer_info=optimizer_info,\n        _optimizer_state_path=optimizer_state_path,\n        _optimizer_state=optimizer_state,  # type: ignore\n        _shared_errors_path=shared_errors_path,\n        _shared_errors=error_dump,\n    )\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_get_current_evaluating_trials","title":"lock_and_get_current_evaluating_trials","text":"<pre><code>lock_and_get_current_evaluating_trials() -&gt; list[Trial]\n</code></pre> <p>Get the current evaluating trials.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_get_current_evaluating_trials(self) -&gt; list[Trial]:\n    \"\"\"Get the current evaluating trials.\"\"\"\n    with self._trial_lock.lock():\n        trials = self._trial_repo.latest()\n        return [\n            trial\n            for trial in trials.values()\n            if trial.metadata.state == Trial.State.EVALUATING\n        ]\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_get_errors","title":"lock_and_get_errors","text":"<pre><code>lock_and_get_errors() -&gt; ErrDump\n</code></pre> <p>Get all the errors that have occurred during the optimization.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_get_errors(self) -&gt; ErrDump:\n    \"\"\"Get all the errors that have occurred during the optimization.\"\"\"\n    with self._err_lock.lock():\n        return ReaderWriterErrDump.read(self._shared_errors_path)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_get_next_pending_trial","title":"lock_and_get_next_pending_trial","text":"<pre><code>lock_and_get_next_pending_trial() -&gt; Trial | None\n</code></pre><pre><code>lock_and_get_next_pending_trial(n: int) -&gt; list[Trial]\n</code></pre> <pre><code>lock_and_get_next_pending_trial(\n    n: int | None = None,\n) -&gt; Trial | list[Trial] | None\n</code></pre> <p>Get the next pending trial.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_get_next_pending_trial(\n    self,\n    n: int | None = None,\n) -&gt; Trial | list[Trial] | None:\n    \"\"\"Get the next pending trial.\"\"\"\n    with self._trial_lock.lock():\n        trials = self._trial_repo.latest()\n        pendings = sorted(\n            [\n                trial\n                for trial in trials.values()\n                if trial.metadata.state == Trial.State.PENDING\n            ],\n            key=lambda t: t.metadata.time_sampled,\n        )\n        if n is None:\n            return pendings[0] if pendings else None\n        return pendings[:n]\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_get_optimizer_info","title":"lock_and_get_optimizer_info","text":"<pre><code>lock_and_get_optimizer_info() -&gt; OptimizerInfo\n</code></pre> <p>Get the optimizer information.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_get_optimizer_info(self) -&gt; OptimizerInfo:\n    \"\"\"Get the optimizer information.\"\"\"\n    with self._optimizer_lock.lock():\n        return _deserialize_optimizer_info(self._optimizer_info_path)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_get_optimizer_state","title":"lock_and_get_optimizer_state","text":"<pre><code>lock_and_get_optimizer_state() -&gt; OptimizationState\n</code></pre> <p>Get the optimizer state.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_get_optimizer_state(self) -&gt; OptimizationState:\n    \"\"\"Get the optimizer state.\"\"\"\n    with self._optimizer_lock.lock():  # noqa: SIM117\n        with self._optimizer_state_path.open(\"rb\") as f:\n            obj = pickle.load(f)  # noqa: S301\n            assert isinstance(obj, OptimizationState)\n            return obj\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_get_trial_by_id","title":"lock_and_get_trial_by_id","text":"<pre><code>lock_and_get_trial_by_id(trial_id: str) -&gt; Trial\n</code></pre> <p>Get a trial by its id.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_get_trial_by_id(self, trial_id: str) -&gt; Trial:\n    \"\"\"Get a trial by its id.\"\"\"\n    with self._trial_lock.lock():\n        return self._trial_repo.load_trial_from_disk(trial_id)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_read_trials","title":"lock_and_read_trials","text":"<pre><code>lock_and_read_trials() -&gt; dict[str, Trial]\n</code></pre> <p>Acquire the state lock and read the trials.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_read_trials(self) -&gt; dict[str, Trial]:\n    \"\"\"Acquire the state lock and read the trials.\"\"\"\n    with self._trial_lock.lock():\n        return self._trial_repo.latest()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_report_trial_evaluation","title":"lock_and_report_trial_evaluation","text":"<pre><code>lock_and_report_trial_evaluation(\n    trial: Trial, report: Report, *, worker_id: str\n) -&gt; None\n</code></pre> <p>Acquire the state lock and report the trial evaluation.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_report_trial_evaluation(\n    self,\n    trial: Trial,\n    report: Report,\n    *,\n    worker_id: str,\n) -&gt; None:\n    \"\"\"Acquire the state lock and report the trial evaluation.\"\"\"\n    with self._trial_lock.lock(), self._err_lock.lock():\n        self._report_trial_evaluation(trial, report, worker_id=worker_id)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.lock_and_sample_trial","title":"lock_and_sample_trial","text":"<pre><code>lock_and_sample_trial(\n    optimizer: AskFunction,\n    *,\n    worker_id: str,\n    n: None = None\n) -&gt; Trial\n</code></pre><pre><code>lock_and_sample_trial(\n    optimizer: AskFunction, *, worker_id: str, n: int\n) -&gt; list[Trial]\n</code></pre> <pre><code>lock_and_sample_trial(\n    optimizer: AskFunction,\n    *,\n    worker_id: str,\n    n: int | None = None\n) -&gt; Trial | list[Trial]\n</code></pre> <p>Acquire the state lock and sample a trial.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def lock_and_sample_trial(\n    self, optimizer: AskFunction, *, worker_id: str, n: int | None = None\n) -&gt; Trial | list[Trial]:\n    \"\"\"Acquire the state lock and sample a trial.\"\"\"\n    with self._optimizer_lock.lock():\n        with self._trial_lock.lock():\n            trials_ = self._trial_repo.latest()\n\n        trials = self._sample_trial(\n            optimizer,\n            trials=trials_,\n            worker_id=worker_id,\n            n=n,\n        )\n\n        with self._trial_lock.lock():\n            self._trial_repo.store_new_trial(trials)\n\n        return trials\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.put_updated_trial","title":"put_updated_trial","text":"<pre><code>put_updated_trial(\n    trial: Trial,\n    *,\n    hints: (\n        list[TrialWriteHint] | TrialWriteHint | None\n    ) = None\n) -&gt; None\n</code></pre> <p>Update the trial.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial to update.</p> <p> TYPE: <code>Trial</code> </p> <code>hints</code> <p>The hints to use when updating the trial. Defines what files need to be updated. If you don't know, leave <code>None</code>, this is a micro-optimization.</p> <p> TYPE: <code>list[TrialWriteHint] | TrialWriteHint | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def put_updated_trial(\n    self,\n    trial: Trial,\n    *,\n    hints: list[TrialWriteHint] | TrialWriteHint | None = None,\n) -&gt; None:\n    \"\"\"Update the trial.\n\n    Args:\n        trial: The trial to update.\n        hints: The hints to use when updating the trial. Defines what files need\n            to be updated.\n            If you don't know, leave `None`, this is a micro-optimization.\n    \"\"\"\n    with self._trial_lock.lock():\n        self._trial_repo.update_trial(trial, hints=hints)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.unsafe_retry_get_trial_by_id","title":"unsafe_retry_get_trial_by_id","text":"<pre><code>unsafe_retry_get_trial_by_id(trial_id: str) -&gt; Trial\n</code></pre> <p>Get a trial by id but use unsafe retries.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def unsafe_retry_get_trial_by_id(self, trial_id: str) -&gt; Trial:\n    \"\"\"Get a trial by id but use unsafe retries.\"\"\"\n    for _ in range(N_UNSAFE_RETRIES):\n        try:\n            return self._trial_repo.load_trial_from_disk(trial_id)\n        except TrialNotFoundError as e:\n            raise e\n        except Exception as e:  # noqa: BLE001\n            logger.warning(\n                \"Failed to get trial '%s' due to an error: %s\", trial_id, e\n            )\n            time.sleep(0.1)\n            continue\n\n    raise NePSError(\n        f\"Failed to get trial '{trial_id}' after {N_UNSAFE_RETRIES} retries.\"\n    )\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.TrialRepo","title":"TrialRepo  <code>dataclass</code>","text":"<pre><code>TrialRepo(directory: Path)\n</code></pre> <p>A repository for trials that are stored on disk.</p> <p>Warning</p> <p>This class does not implement locking and it is up to the caller to ensure there are no race conflicts.</p>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.TrialRepo.latest","title":"latest","text":"<pre><code>latest() -&gt; dict[str, Trial]\n</code></pre> <p>Get the latest trials from the cache.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def latest(self) -&gt; dict[str, Trial]:\n    \"\"\"Get the latest trials from the cache.\"\"\"\n    if not self.cache_path.exists():\n        # If we end up with no cache but there are trials on disk, we need to read in.\n        if any(path.name.startswith(\"config_\") for path in self.directory.iterdir()):\n            trial_ids = self.list_trial_ids()\n            trials = {\n                trial_id: self.load_trial_from_disk(trial_id)\n                for trial_id in trial_ids\n            }\n            pickle_bytes = pickle.dumps(trials, protocol=pickle.HIGHEST_PROTOCOL)\n            with atomic_write(self.cache_path, \"wb\") as f:\n                f.write(pickle_bytes)\n\n        return {}\n\n    return self._read_pkl_and_maybe_consolidate()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.TrialRepo.list_trial_ids","title":"list_trial_ids","text":"<pre><code>list_trial_ids() -&gt; list[str]\n</code></pre> <p>List all the trial ids on disk.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def list_trial_ids(self) -&gt; list[str]:\n    \"\"\"List all the trial ids on disk.\"\"\"\n    return [\n        config_path.name[CONFIG_PREFIX_LEN:]\n        for config_path in self.directory.iterdir()\n        if config_path.name.startswith(\"config_\") and config_path.is_dir()\n    ]\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.TrialRepo.load_trial_from_disk","title":"load_trial_from_disk","text":"<pre><code>load_trial_from_disk(trial_id: str) -&gt; Trial\n</code></pre> <p>Load a trial from disk.</p> RAISES DESCRIPTION <code>TrialNotFoundError</code> <p>If the trial is not found on disk.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def load_trial_from_disk(self, trial_id: str) -&gt; Trial:\n    \"\"\"Load a trial from disk.\n\n    Raises:\n        TrialNotFoundError: If the trial is not found on disk.\n    \"\"\"\n    config_path = self.directory / f\"config_{trial_id}\"\n    if not config_path.exists():\n        raise TrialNotFoundError(\n            f\"Trial {trial_id} not found at expected path of {config_path}.\"\n        )\n\n    return ReaderWriterTrial.read(config_path)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.TrialRepo.store_new_trial","title":"store_new_trial","text":"<pre><code>store_new_trial(trial: Trial | list[Trial]) -&gt; None\n</code></pre> <p>Write a new trial to disk.</p> RAISES DESCRIPTION <code>TrialAlreadyExistsError</code> <p>If the trial already exists on disk.</p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def store_new_trial(self, trial: Trial | list[Trial]) -&gt; None:\n    \"\"\"Write a new trial to disk.\n\n    Raises:\n        TrialAlreadyExistsError: If the trial already exists on disk.\n    \"\"\"\n    if isinstance(trial, Trial):\n        config_path = self.directory / f\"config_{trial.id}\"\n        if config_path.exists():\n            raise TrialAlreadyExistsError(trial.id, config_path)\n\n        bytes_ = pickle.dumps(trial, protocol=pickle.HIGHEST_PROTOCOL)\n        with atomic_write(self.cache_path, \"ab\") as f:\n            f.write(bytes_)\n\n        config_path.mkdir(parents=True, exist_ok=True)\n        ReaderWriterTrial.write(\n            trial,\n            self.directory / f\"config_{trial.id}\",\n            hints=[\"config\", \"metadata\"],\n        )\n    else:\n        for child_trial in trial:\n            config_path = self.directory / f\"config_{child_trial.id}\"\n            if config_path.exists():\n                raise TrialAlreadyExistsError(child_trial.id, config_path)\n            config_path.mkdir(parents=True, exist_ok=True)\n\n        bytes_ = pickle.dumps(trial, protocol=pickle.HIGHEST_PROTOCOL)\n        with atomic_write(self.cache_path, \"ab\") as f:\n            f.write(bytes_)\n\n        for child_trial in trial:\n            ReaderWriterTrial.write(\n                child_trial,\n                self.directory / f\"config_{child_trial.id}\",\n                hints=[\"config\", \"metadata\"],\n            )\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.TrialRepo.update_trial","title":"update_trial","text":"<pre><code>update_trial(\n    trial: Trial,\n    *,\n    hints: (\n        Iterable[TrialWriteHint] | TrialWriteHint | None\n    ) = (\"report\", \"metadata\")\n) -&gt; None\n</code></pre> <p>Update a trial on disk.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial to update.</p> <p> TYPE: <code>Trial</code> </p> <code>hints</code> <p>The hints to use when updating the trial. Defines what files need to be updated. If you don't know, leave <code>None</code>, this is a micro-optimization.</p> <p> TYPE: <code>Iterable[TrialWriteHint] | TrialWriteHint | None</code> DEFAULT: <code>('report', 'metadata')</code> </p> Source code in <code>neps\\state\\neps_state.py</code> <pre><code>def update_trial(\n    self,\n    trial: Trial,\n    *,\n    hints: Iterable[TrialWriteHint] | TrialWriteHint | None = (\"report\", \"metadata\"),\n) -&gt; None:\n    \"\"\"Update a trial on disk.\n\n    Args:\n        trial: The trial to update.\n        hints: The hints to use when updating the trial. Defines what files need\n            to be updated.\n            If you don't know, leave `None`, this is a micro-optimization.\n    \"\"\"\n    bytes_ = pickle.dumps(trial, protocol=pickle.HIGHEST_PROTOCOL)\n    with atomic_write(self.cache_path, \"ab\") as f:\n        f.write(bytes_)\n\n    ReaderWriterTrial.write(trial, self.directory / f\"config_{trial.id}\", hints=hints)\n</code></pre>"},{"location":"api/neps/state/optimizer/","title":"Optimizer","text":"<p>Optimizer state and info dataclasses.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.BudgetInfo","title":"BudgetInfo  <code>dataclass</code>","text":"<pre><code>BudgetInfo(\n    max_cost_total: float | None = None,\n    used_cost_budget: float = 0.0,\n    max_evaluations: int | None = None,\n    used_evaluations: int = 0,\n)\n</code></pre> <p>Information about the budget of an optimizer.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.BudgetInfo.clone","title":"clone","text":"<pre><code>clone() -&gt; BudgetInfo\n</code></pre> <p>Create a copy of the budget info.</p> Source code in <code>neps\\state\\optimizer.py</code> <pre><code>def clone(self) -&gt; BudgetInfo:\n    \"\"\"Create a copy of the budget info.\"\"\"\n    return replace(self)\n</code></pre>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizationState","title":"OptimizationState  <code>dataclass</code>","text":"<pre><code>OptimizationState(\n    budget: BudgetInfo | None,\n    seed_snapshot: SeedSnapshot,\n    shared_state: dict[str, Any] | None,\n)\n</code></pre> <p>The current state of an optimizer.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizationState.budget","title":"budget  <code>instance-attribute</code>","text":"<pre><code>budget: BudgetInfo | None\n</code></pre> <p>Information regarind the budget used by the optimization trajectory.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizationState.seed_snapshot","title":"seed_snapshot  <code>instance-attribute</code>","text":"<pre><code>seed_snapshot: SeedSnapshot\n</code></pre> <p>The state of the random number generators at the time of the last sample.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizationState.shared_state","title":"shared_state  <code>instance-attribute</code>","text":"<pre><code>shared_state: dict[str, Any] | None\n</code></pre> <p>Any information the optimizer wants to store between calls to sample and post evaluations.</p> <p>For example, an optimizer may wish to store running totals here or various other bits of information that may be expensive to recompute.</p> <p>Right now there's no support for tensors/arrays and almost no optimizer uses this feature. Only cost-cooling uses information out of <code>.budget</code>.</p> <p>Please reach out to @eddiebergman if you have a use case for this so we can make it more robust.</p>"},{"location":"api/neps/state/pipeline_eval/","title":"Pipeline eval","text":"<p>Encapsulating the call to the users <code>evaluate_pipeline</code>.</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.EvaluatePipelineReturn","title":"EvaluatePipelineReturn  <code>module-attribute</code>","text":"<pre><code>EvaluatePipelineReturn: TypeAlias = (\n    Exception\n    | float\n    | Sequence[float]\n    | UserResultDict\n    | dict\n)\n</code></pre> <p>The type of things <code>evaluate_pipeline</code> can return.</p> <ul> <li><code>float</code>: Interpreted as just the objective value to minimize.</li> <li><code>Sequence[float]</code>: Interpreted as multiobjective optimization with these values.     Order is important.</li> <li><code>Exception</code>: The evaluation failed.</li> <li><code>Mapping[str, Any]</code>: A dict which follows the layout of     <code>UserResultDict</code>.</li> </ul>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResult","title":"UserResult  <code>dataclass</code>","text":"<pre><code>UserResult(\n    objective_to_minimize: (\n        float | list[float] | None\n    ) = None,\n    cost: float | None = None,\n    learning_curve: (\n        list[float] | list[list[float]] | None\n    ) = None,\n    extra: dict[str, Any] = dict(),\n    exception: Exception | None = None,\n)\n</code></pre> <p>The parsed values out of the possibilities the user can return from the <code>evaluate_pipeline_function()</code>.</p> <p>See <code>UserResultDict</code> for the possibilities.</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResult.parse","title":"parse  <code>classmethod</code>","text":"<pre><code>parse(\n    user_result: EvaluatePipelineReturn,\n    *,\n    default_objective_to_minimize_value: (\n        float | list[float] | None\n    ),\n    default_cost_value: float | None = None,\n    default_learning_curve: (\n        Literal[\"objective_to_minimize\"]\n        | list[float]\n        | list[list[float]]\n        | None\n    ) = None\n) -&gt; UserResult\n</code></pre> <p>Parse the return type a user can provide from <code>evaluate_pipeline</code>.</p> Source code in <code>neps\\state\\pipeline_eval.py</code> <pre><code>@classmethod\ndef parse(  # noqa: C901, PLR0912, PLR0915\n    cls,\n    user_result: EvaluatePipelineReturn,\n    *,\n    default_objective_to_minimize_value: float | list[float] | None,\n    default_cost_value: float | None = None,\n    default_learning_curve: (\n        Literal[\"objective_to_minimize\"]\n        | list[float]  # Single obj curve\n        | list[list[float]]  # Multi obj curve\n        | None\n    ) = None,\n) -&gt; UserResult:\n    \"\"\"Parse the return type a user can provide from `evaluate_pipeline`.\"\"\"\n    objective_to_minimize: float | list[float] | None\n    cost: float | None\n    learning_curve: list[float] | list[list[float]] | None\n    extra_info: dict[str, Any]\n    exception: Exception | None\n\n    match user_result:\n        # Start easy, single objective result only\n        case int() | float() | np.number():\n            match default_learning_curve:\n                case \"objective_to_minimize\":  # Take the obj_to_minimize as the curve\n                    learning_curve = [float(user_result)]\n                case None | Sequence():  # Go with the default (list or None)\n                    learning_curve = default_learning_curve  # type: ignore\n                case _:\n                    raise ValueError(\n                        \"The default learning curve should be either None or\"\n                        f\" 'objective_to_minimize'. Got {default_learning_curve}\"\n                    )\n\n            return UserResult(\n                objective_to_minimize=user_result,\n                learning_curve=learning_curve,\n                cost=default_cost_value,\n                exception=None,\n                extra={},\n            )\n\n        # Multiobjective result\n        case Sequence():\n            if not all(isinstance(v, float | int | np.number) for v in user_result):\n                val_types = \", \".join(f\"{v}: {type(v)}\" for v in user_result)\n                raise ValueError(\n                    \"All values in the multiobjective result should be floats,\"\n                    f\" but got {val_types}\"\n                )\n            objective_to_minimize = [float(v) for v in user_result]\n\n            match default_learning_curve:\n                case \"objective_to_minimize\":\n                    learning_curve = [objective_to_minimize]\n                case None | Sequence():\n                    learning_curve = default_learning_curve  # type: ignore\n                case _:\n                    raise ValueError(\n                        \"The default learning curve should be either None or\"\n                        f\" 'objective_to_minimize'. Got {default_learning_curve}\"\n                    )\n\n            return UserResult(\n                objective_to_minimize=objective_to_minimize,\n                extra={},\n                cost=default_cost_value,\n                learning_curve=learning_curve,\n                exception=None,\n            )\n\n        # An Error\n        case Exception():\n            objective_to_minimize = default_objective_to_minimize_value\n            match default_learning_curve:\n                case \"objective_to_minimize\":\n                    learning_curve = (  # obj_to_minimize as the curve, if any\n                        [objective_to_minimize]  # type: ignore\n                        if objective_to_minimize is not None\n                        else None\n                    )\n                case None | Sequence():\n                    learning_curve = default_learning_curve  # type: ignore\n                case _:\n                    raise ValueError(\n                        \"The default learning curve should be either None or\"\n                        f\" 'objective_to_minimize'. Got {default_learning_curve}\"\n                    )\n\n            return UserResult(\n                objective_to_minimize=default_objective_to_minimize_value,\n                cost=default_cost_value,\n                learning_curve=learning_curve,\n                exception=user_result,\n                extra={},\n            )\n\n        # A UserResultDict, the most annoying to parse out as theoretically\n        # the could provide everything)\n        case Mapping():\n            _result = dict(user_result)\n\n            popped_exception = _result.pop(\"exception\", None)\n            if (\n                not isinstance(popped_exception, Exception)\n                and popped_exception is not None\n            ):\n                raise ValueError(\n                    \"The 'exception' should be an exception or None,\"\n                    f\" but got {popped_exception}\"\n                )\n            exception = popped_exception\n\n            popped_obj = _result.pop(\n                \"objective_to_minimize\", default_objective_to_minimize_value\n            )\n            match popped_obj:\n                case None:\n                    objective_to_minimize = popped_obj\n                case int() | float() | np.number():\n                    objective_to_minimize = float(popped_obj)\n                case Sequence():\n                    objective_to_minimize = [float(v) for v in popped_obj]\n                case _:\n                    raise ValueError(\n                        \"The 'objective_to_minimize' should be either a float,\"\n                        f\" a sequence of floats or None. Got {popped_obj}\"\n                    )\n\n            popped_cost = _result.pop(\"cost\", default_cost_value)\n            match popped_cost:\n                case None:\n                    cost = popped_cost\n                case int() | float() | np.number():\n                    cost = float(popped_cost)\n                case _:\n                    raise ValueError(\n                        f\"The 'cost' should be either a float or None.\"\n                        f\" Got {popped_cost}\"\n                    )\n\n            # Learning curve is annoying as we have a cross product of possible\n            # learning curve returns and what to do if there is a default.\n            popped_curve = _result.pop(\"learning_curve\", None)\n            match popped_curve:\n                case Sequence():\n                    # Easiest case, just use it and assume it's correctly shaped.\n                    # TODO: Could check the learning curve is 2d if multiobjective\n                    # objective # was provided.\n                    learning_curve = list(popped_curve)\n                case None:\n                    # If no learning curve, see what to do with the default\n                    match default_learning_curve:\n                        case None:\n                            learning_curve = None\n                        case \"objective_to_minimize\":\n                            learning_curve = (\n                                [objective_to_minimize]  # type: ignore\n                                if objective_to_minimize is not None\n                                else None\n                            )\n                        case Sequence():\n                            learning_curve = default_learning_curve  # type: ignore\n                        case _:\n                            raise ValueError(\n                                \"The default learning curve should be either None, \"\n                                \" 'objective_to_minimize' or sequence.\"\n                                f\" Got {default_learning_curve}\"\n                            )\n                case _:\n                    raise ValueError(\n                        \"The 'learning_curve' should be either a sequence of floats,\"\n                        f\" a sequence of sequences of floats or None.\"\n                        f\" Got {popped_curve}\"\n                    )\n\n            popped_extra_info = _result.pop(\"info_dict\", {})\n            if not isinstance(popped_extra_info, Mapping):\n                raise ValueError(\n                    \"The 'info_dict' should be a dictionary, but got\"\n                    f\" {popped_extra_info}\"\n                )\n            extra_info = dict(popped_extra_info)\n\n            # Legacy\n            if \"learning_curve\" in extra_info:\n                raise ValueError(\n                    \"Please provide 'learning_curve' in the top level of the\"\n                    \" dictionary, not in 'info_dict'.\"\n                )\n\n            return UserResult(\n                objective_to_minimize=objective_to_minimize,\n                cost=cost,\n                extra=extra_info,\n                learning_curve=learning_curve,\n                exception=exception,\n            )\n\n        case _:\n            raise ValueError(\n                \"The user result should be either a float, a sequence of floats,\"\n                f\" an exception or a dictionary. Got {user_result}\"\n            )\n</code></pre>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResultDict","title":"UserResultDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>The type of things you can include when returning a dictionary from <code>evaluate_pipeline</code>.</p> <p>All fields here are optional. If an exception is provided, we count the trial as having failed. Otherwise, we rely on at least <code>objective_to_minimize</code> to be provided.</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResultDict.cost","title":"cost  <code>instance-attribute</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>The cost of the evaluation, if any</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResultDict.exception","title":"exception  <code>instance-attribute</code>","text":"<pre><code>exception: Exception | None\n</code></pre> <p>The exception that occured.</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResultDict.info_dict","title":"info_dict  <code>instance-attribute</code>","text":"<pre><code>info_dict: Mapping[str, Any] | None\n</code></pre> <p>Extra information that will be stored with the trials result.</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResultDict.learning_curve","title":"learning_curve  <code>instance-attribute</code>","text":"<pre><code>learning_curve: (\n    Sequence[float] | Sequence[Sequence[float]] | None\n)\n</code></pre> <p>The learning curve for this configuration.</p> <p>If a <code>Sequence[float]</code> is provided, it is assumed to be a single objective learning curve.</p> <p>Otherwise a <code>Sequence[Sequence[float]]</code> is assumed to be a multiobjective learning curve.</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.UserResultDict.objective_to_minimize","title":"objective_to_minimize  <code>instance-attribute</code>","text":"<pre><code>objective_to_minimize: float | Sequence[float] | None\n</code></pre> <p>The objective value to minimize, single or multi-objective</p>"},{"location":"api/neps/state/pipeline_eval/#neps.state.pipeline_eval.evaluate_trial","title":"evaluate_trial","text":"<pre><code>evaluate_trial(\n    trial: Trial,\n    *,\n    evaluation_fn: Callable[..., Any],\n    default_report_values: DefaultReportValues\n) -&gt; tuple[Trial, Report]\n</code></pre> <p>Evaluates a trial from a user and parses the results into a <code>Report</code>.</p> Source code in <code>neps\\state\\pipeline_eval.py</code> <pre><code>def evaluate_trial(\n    trial: Trial,\n    *,\n    evaluation_fn: Callable[..., Any],\n    default_report_values: DefaultReportValues,\n) -&gt; tuple[Trial, Report]:\n    \"\"\"Evaluates a trial from a user and parses the results into a `Report`.\"\"\"\n    trial_location = Path(trial.metadata.location)\n    prev_trial_location = (\n        Path(trial.metadata.previous_trial_location)\n        if trial.metadata.previous_trial_location is not None\n        else None\n    )\n\n    params = {\n        \"pipeline_directory\": trial_location,\n        \"previous_pipeline_directory\": prev_trial_location,\n    }\n    sigkeys = inspect.signature(evaluation_fn).parameters.keys()\n    injectable_params = {key: val for key, val in params.items() if key in sigkeys}\n    report = _eval_trial(\n        trial=trial,\n        fn=evaluation_fn,\n        default_report_values=default_report_values,\n        **injectable_params,\n    )\n    return trial, report\n</code></pre>"},{"location":"api/neps/state/seed_snapshot/","title":"Seed snapshot","text":"<p>Snapshot of the global rng state.</p>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot","title":"SeedSnapshot  <code>dataclass</code>","text":"<pre><code>SeedSnapshot(\n    np_rng: NP_RNG_STATE,\n    py_rng: PY_RNG_STATE,\n    torch_rng: TORCH_RNG_STATE | None,\n    torch_cuda_rng: TORCH_CUDA_RNG_STATE | None,\n)\n</code></pre> <p>State of the global rng.</p> <p>Primarly enables storing of the rng state to disk using a binary format native to each library, allowing for potential version mistmatches between processes loading the state, as long as they can read the binary format.</p>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot.new_capture","title":"new_capture  <code>classmethod</code>","text":"<pre><code>new_capture() -&gt; SeedSnapshot\n</code></pre> <p>Current state of the global rng.</p> <p>Takes a snapshot, including cloning or copying any arrays, tensors, etc.</p> Source code in <code>neps\\state\\seed_snapshot.py</code> <pre><code>@classmethod\ndef new_capture(cls) -&gt; SeedSnapshot:\n    \"\"\"Current state of the global rng.\n\n    Takes a snapshot, including cloning or copying any arrays, tensors, etc.\n    \"\"\"\n    self = cls(None, None, None, None)  # type: ignore\n    self.recapture()\n    return self\n</code></pre>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot.recapture","title":"recapture","text":"<pre><code>recapture() -&gt; None\n</code></pre> <p>Reread the state of the global rng into this snapshot.</p> Source code in <code>neps\\state\\seed_snapshot.py</code> <pre><code>def recapture(self) -&gt; None:\n    \"\"\"Reread the state of the global rng into this snapshot.\"\"\"\n    # https://numpy.org/doc/stable/reference/random/generated/numpy.random.get_state.html\n\n    self.py_rng = random.getstate()\n\n    np_keys = np.random.get_state(legacy=True)\n    assert np_keys[0] == \"MT19937\"  # type: ignore\n    self.np_rng = (np_keys[0], np_keys[1].copy(), *np_keys[2:])  # type: ignore\n\n    with contextlib.suppress(Exception):\n        import torch\n\n        self.torch_rng = torch.random.get_rng_state().clone()\n        torch_cuda_keys: list[torch.Tensor] | None = None\n        if torch.cuda.is_available():\n            torch_cuda_keys = [c.clone() for c in torch.cuda.get_rng_state_all()]\n        self.torch_cuda_rng = torch_cuda_keys\n</code></pre>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot.set_as_global_seed_state","title":"set_as_global_seed_state","text":"<pre><code>set_as_global_seed_state() -&gt; None\n</code></pre> <p>Set the global rng to the given state.</p> Source code in <code>neps\\state\\seed_snapshot.py</code> <pre><code>def set_as_global_seed_state(self) -&gt; None:\n    \"\"\"Set the global rng to the given state.\"\"\"\n    np.random.set_state(self.np_rng)\n    random.setstate(self.py_rng)\n\n    if self.torch_rng is not None or self.torch_cuda_rng is not None:\n        import torch\n\n        if self.torch_rng is not None:\n            torch.random.set_rng_state(self.torch_rng)\n\n        if self.torch_cuda_rng is not None and torch.cuda.is_available():\n            torch.cuda.set_rng_state_all(self.torch_cuda_rng)\n</code></pre>"},{"location":"api/neps/state/settings/","title":"Settings","text":"<p>Settings for the worker and the global state of NePS.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues","title":"DefaultReportValues  <code>dataclass</code>","text":"<pre><code>DefaultReportValues(\n    objective_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    cost_if_not_provided: float | None = None,\n    learning_curve_on_error: list[float] | None = None,\n    learning_curve_if_not_provided: (\n        Literal[\"objective_to_minimize\"]\n        | list[float]\n        | None\n    ) = None,\n)\n</code></pre> <p>Values to use when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.cost_if_not_provided","title":"cost_if_not_provided  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost_if_not_provided: float | None = None\n</code></pre> <p>The value to use for the cost when the evaluation function does not provide one.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.cost_value_on_error","title":"cost_value_on_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost_value_on_error: float | None = None\n</code></pre> <p>The value to use for the cost when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.learning_curve_if_not_provided","title":"learning_curve_if_not_provided  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_curve_if_not_provided: (\n    Literal[\"objective_to_minimize\"] | list[float] | None\n) = None\n</code></pre> <p>The value to use for the learning curve when the evaluation function does not provide one.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.learning_curve_on_error","title":"learning_curve_on_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_curve_on_error: list[float] | None = None\n</code></pre> <p>The value to use for the learning curve when an error occurs.</p> <p>If <code>'objective_to_minimize'</code>, the learning curve will be set to the objective_to_minimize value but as a list with a single value.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.objective_value_on_error","title":"objective_value_on_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>objective_value_on_error: float | None = None\n</code></pre> <p>The value to use for the objective_to_minimize when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities","title":"OnErrorPossibilities","text":"<p>               Bases: <code>Enum</code></p> <p>Possible values for what to do when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.IGNORE","title":"IGNORE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IGNORE = 'ignore'\n</code></pre> <p>Ignore all errors and continue running.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.RAISE_ANY_ERROR","title":"RAISE_ANY_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RAISE_ANY_ERROR = 'raise_any_error'\n</code></pre> <p>Raise an error if there was an error from any worker, i.e. there is a trial in the NePSState that has an error.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.RAISE_WORKER_ERROR","title":"RAISE_WORKER_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RAISE_WORKER_ERROR = 'raise_worker_error'\n</code></pre> <p>Raise an error only if the error occurs in the worker.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.STOP_ANY_ERROR","title":"STOP_ANY_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STOP_ANY_ERROR = 'stop_any_error'\n</code></pre> <p>Stop the workers if any error occured from any worker, i.e. there is a trial in the NePSState that has an error.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.STOP_WORKER_ERROR","title":"STOP_WORKER_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STOP_WORKER_ERROR = 'stop_worker_error'\n</code></pre> <p>Stop the worker if an error occurs in the worker, without raising</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings","title":"WorkerSettings  <code>dataclass</code>","text":"<pre><code>WorkerSettings(\n    on_error: OnErrorPossibilities,\n    default_report_values: DefaultReportValues,\n    batch_size: int | None,\n    max_evaluations_total: int | None,\n    include_in_progress_evaluations_towards_maximum: bool,\n    max_cost_total: float | None,\n    max_evaluation_time_total_seconds: float | None,\n    max_evaluations_for_worker: int | None,\n    max_cost_for_worker: float | None,\n    max_evaluation_time_for_worker_seconds: float | None,\n    max_wallclock_time_for_worker_seconds: float | None,\n)\n</code></pre> <p>Settings for a running instance of NePS.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size: int | None\n</code></pre> <p>The number of configurations to sample in a single batch.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.default_report_values","title":"default_report_values  <code>instance-attribute</code>","text":"<pre><code>default_report_values: DefaultReportValues\n</code></pre> <p>Values to use when an error occurs or was not specified.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.include_in_progress_evaluations_towards_maximum","title":"include_in_progress_evaluations_towards_maximum  <code>instance-attribute</code>","text":"<pre><code>include_in_progress_evaluations_towards_maximum: bool\n</code></pre> <p>Whether to include currently evaluating configurations towards the stopping criterion <code>max_evaluations_total</code></p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_cost_for_worker","title":"max_cost_for_worker  <code>instance-attribute</code>","text":"<pre><code>max_cost_for_worker: float | None\n</code></pre> <p>The maximum cost incurred by a worker before finisihng.</p> <p>Once this cost total is reached, only this worker will stop evaluating new configurations.</p> <p>This cost is the sum of <code>'cost'</code> values that are returned by evaluation of the target function.</p> <p>If <code>None</code>, there is no limit and the worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_cost_total","title":"max_cost_total  <code>instance-attribute</code>","text":"<pre><code>max_cost_total: float | None\n</code></pre> <p>The maximum cost to run in total.</p> <p>Once this cost total is reached, all workers will stop evaluating new configurations.</p> <p>This cost is the sum of <code>'cost'</code> values that are returned by evaluation of the target function.</p> <p>If <code>None</code>, there is no limit and workers will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluation_time_for_worker_seconds","title":"max_evaluation_time_for_worker_seconds  <code>instance-attribute</code>","text":"<pre><code>max_evaluation_time_for_worker_seconds: float | None\n</code></pre> <p>The maximum time to allow this worker for evaluating configurations.</p> <p>Note</p> <p>This does not include time for sampling new configurations.</p> <p>If <code>None</code>, there is no limit and this worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluation_time_total_seconds","title":"max_evaluation_time_total_seconds  <code>instance-attribute</code>","text":"<pre><code>max_evaluation_time_total_seconds: float | None\n</code></pre> <p>The maximum wallclock time allowed for evaluation in total.</p> <p>Note</p> <p>This does not include time for sampling new configurations.</p> <p>Once this wallclock time is reached, all workers will stop once their current evaluation is finished.</p> <p>If <code>None</code>, there is no limit and workers will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluations_for_worker","title":"max_evaluations_for_worker  <code>instance-attribute</code>","text":"<pre><code>max_evaluations_for_worker: int | None\n</code></pre> <p>The maximum number of evaluations to run for the worker.</p> <p>This count is specific to each worker spawned by NePS. only the current worker will stop evaluating new configurations once this limit is reached.</p> <p>If <code>None</code>, there is no limit and this worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluations_total","title":"max_evaluations_total  <code>instance-attribute</code>","text":"<pre><code>max_evaluations_total: int | None\n</code></pre> <p>The maximum number of evaluations to run in total.</p> <p>Once this evaluation total is reached, all workers will stop evaluating new configurations.</p> <p>To control whether currently evaluating configurations are included in this total, see <code>include_in_progress_evaluations_towards_maximum</code>.</p> <p>If <code>None</code>, there is no limit and workers will continue to evaluate indefinitely.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_wallclock_time_for_worker_seconds","title":"max_wallclock_time_for_worker_seconds  <code>instance-attribute</code>","text":"<pre><code>max_wallclock_time_for_worker_seconds: float | None\n</code></pre> <p>The maximum wallclock time to run for this worker.</p> <p>Once this wallclock time is reached, only this worker will stop evaluating new configurations.</p> <p>Warning</p> <p>This will not stop the worker if it is currently evaluating a configuration.</p> <p>This is useful when the worker is deployed on some managed resource where there is a time limit.</p> <p>If <code>None</code>, there is no limit and this worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.on_error","title":"on_error  <code>instance-attribute</code>","text":"<pre><code>on_error: OnErrorPossibilities\n</code></pre> <p>What to do when an error occurs.</p> <ul> <li><code>'raise_worker_error'</code>: Raise an error only if the error occurs in the worker.</li> <li><code>'raise_any_error'</code>: Raise an error if any error occurs from any worker, i.e.     there is a trial in the NePSState that has an error.</li> <li><code>'ignore'</code>: Ignore all errors and continue running.</li> </ul>"},{"location":"api/neps/state/trial/","title":"Trial","text":"<p>A trial is a configuration and it's associated data.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.MetaData","title":"MetaData  <code>dataclass</code>","text":"<pre><code>MetaData(\n    id: str,\n    location: str,\n    state: State,\n    previous_trial_id: str | None,\n    previous_trial_location: str | None,\n    sampling_worker_id: str,\n    time_sampled: float,\n    evaluating_worker_id: str | None = None,\n    evaluation_duration: float | None = None,\n    time_submitted: float | None = None,\n    time_started: float | None = None,\n    time_end: float | None = None,\n)\n</code></pre> <p>Metadata for a trial.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.NotReportedYetError","title":"NotReportedYetError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when trying to access a report that has not been reported yet.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Report","title":"Report  <code>dataclass</code>","text":"<pre><code>Report(\n    objective_to_minimize: float | list[float] | None,\n    cost: float | None,\n    learning_curve: list[float] | list[list[float]] | None,\n    extra: Mapping[str, Any],\n    err: Exception | None,\n    tb: str | None,\n    reported_as: Literal[\"success\", \"failed\", \"crashed\"],\n    evaluation_duration: float | None,\n)\n</code></pre> <p>A failed report of the evaluation of a configuration.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.State","title":"State","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The state of a trial.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial","title":"Trial  <code>dataclass</code>","text":"<pre><code>Trial(\n    State: ClassVar = State,\n    Report: ClassVar = Report,\n    MetaData: ClassVar = MetaData,\n    NotReportedYetError: ClassVar = NotReportedYetError,\n    config: Mapping[str, Any],\n    metadata: MetaData,\n    report: Report | None,\n)\n</code></pre> <p>A trial is a configuration and it's associated data.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Return the id of the trial.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(\n    *,\n    trial_id: str,\n    config: Mapping[str, Any],\n    location: str,\n    previous_trial: str | None,\n    previous_trial_location: str | None,\n    time_sampled: float,\n    worker_id: int | str\n) -&gt; Self\n</code></pre> <p>Create a new trial object that was just sampled.</p> Source code in <code>neps\\state\\trial.py</code> <pre><code>@classmethod\ndef new(\n    cls,\n    *,\n    trial_id: str,\n    config: Mapping[str, Any],\n    location: str,\n    previous_trial: str | None,\n    previous_trial_location: str | None,\n    time_sampled: float,\n    worker_id: int | str,\n) -&gt; Self:\n    \"\"\"Create a new trial object that was just sampled.\"\"\"\n    worker_id = str(worker_id)\n    return cls(\n        config=config,\n        metadata=MetaData(\n            id=trial_id,\n            state=State.PENDING,\n            location=location,\n            time_sampled=time_sampled,\n            previous_trial_id=previous_trial,\n            previous_trial_location=previous_trial_location,\n            sampling_worker_id=worker_id,\n        ),\n        report=None,\n    )\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the trial to a pending state.</p> Source code in <code>neps\\state\\trial.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the trial to a pending state.\"\"\"\n    self.metadata = MetaData(\n        id=self.metadata.id,\n        state=State.PENDING,\n        location=self.metadata.location,\n        previous_trial_id=self.metadata.previous_trial_id,\n        previous_trial_location=self.metadata.previous_trial_location,\n        time_sampled=self.metadata.time_sampled,\n        sampling_worker_id=self.metadata.sampling_worker_id,\n    )\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_complete","title":"set_complete","text":"<pre><code>set_complete(\n    *,\n    report_as: Literal[\"success\", \"failed\", \"crashed\"],\n    time_end: float,\n    objective_to_minimize: float | list[float] | None,\n    cost: float | None,\n    learning_curve: list[float] | list[list[float]] | None,\n    err: Exception | None,\n    tb: str | None,\n    extra: dict[str, Any] | None,\n    evaluation_duration: float | None\n) -&gt; Report\n</code></pre> <p>Set the report for the trial.</p> Source code in <code>neps\\state\\trial.py</code> <pre><code>def set_complete(\n    self,\n    *,\n    report_as: Literal[\"success\", \"failed\", \"crashed\"],\n    time_end: float,\n    objective_to_minimize: float | list[float] | None,\n    cost: float | None,\n    learning_curve: list[float] | list[list[float]] | None,\n    err: Exception | None,\n    tb: str | None,\n    extra: dict[str, Any] | None,\n    evaluation_duration: float | None,\n) -&gt; Report:\n    \"\"\"Set the report for the trial.\"\"\"\n    if report_as == \"success\":\n        self.metadata.state = State.SUCCESS\n    elif report_as == \"failed\":\n        self.metadata.state = State.FAILED\n    elif report_as == \"crashed\":\n        self.metadata.state = State.CRASHED\n    else:\n        raise ValueError(f\"Invalid report_as: '{report_as}'\")\n\n    self.metadata.time_end = time_end\n    self.metadata.evaluation_duration = evaluation_duration\n\n    return Report(\n        reported_as=report_as,\n        evaluation_duration=evaluation_duration,\n        objective_to_minimize=objective_to_minimize,\n        cost=cost,\n        learning_curve=learning_curve,\n        extra=extra if extra is not None else {},\n        err=err,\n        tb=tb,\n    )\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_corrupted","title":"set_corrupted","text":"<pre><code>set_corrupted() -&gt; None\n</code></pre> <p>Set the trial as corrupted.</p> Source code in <code>neps\\state\\trial.py</code> <pre><code>def set_corrupted(self) -&gt; None:\n    \"\"\"Set the trial as corrupted.\"\"\"\n    self.metadata.state = State.CORRUPTED\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_evaluating","title":"set_evaluating","text":"<pre><code>set_evaluating(\n    *, time_started: float, worker_id: int | str\n) -&gt; None\n</code></pre> <p>Set the trial as in progress.</p> Source code in <code>neps\\state\\trial.py</code> <pre><code>def set_evaluating(self, *, time_started: float, worker_id: int | str) -&gt; None:\n    \"\"\"Set the trial as in progress.\"\"\"\n    self.metadata.time_started = time_started\n    self.metadata.evaluating_worker_id = str(worker_id)\n    self.metadata.state = State.EVALUATING\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_submitted","title":"set_submitted","text":"<pre><code>set_submitted(*, time_submitted: float) -&gt; None\n</code></pre> <p>Set the trial as submitted.</p> Source code in <code>neps\\state\\trial.py</code> <pre><code>def set_submitted(self, *, time_submitted: float) -&gt; None:\n    \"\"\"Set the trial as submitted.\"\"\"\n    self.metadata.time_submitted = time_submitted\n    self.metadata.state = State.SUBMITTED\n</code></pre>"},{"location":"api/neps/status/status/","title":"Status","text":"<p>Functions to get the status of a run and save the status to CSV files.</p>"},{"location":"api/neps/status/status/#neps.status.status.Summary","title":"Summary  <code>dataclass</code>","text":"<pre><code>Summary(\n    by_state: dict[State, list[Trial]],\n    best: tuple[Trial, float] | None,\n    is_multiobjective: bool,\n)\n</code></pre> <p>Summary of the current state of a neps run.</p>"},{"location":"api/neps/status/status/#neps.status.status.Summary.num_errors","title":"num_errors  <code>property</code>","text":"<pre><code>num_errors: int\n</code></pre> <p>Number of trials that have errored.</p>"},{"location":"api/neps/status/status/#neps.status.status.Summary.num_evaluated","title":"num_evaluated  <code>property</code>","text":"<pre><code>num_evaluated: int\n</code></pre> <p>Number of trials that have been evaluated.</p>"},{"location":"api/neps/status/status/#neps.status.status.Summary.num_pending","title":"num_pending  <code>property</code>","text":"<pre><code>num_pending: int\n</code></pre> <p>Number of trials that are pending.</p>"},{"location":"api/neps/status/status/#neps.status.status.Summary.completed","title":"completed","text":"<pre><code>completed() -&gt; list[Trial]\n</code></pre> <p>Return all trials which are in a completed state.</p> Source code in <code>neps\\status\\status.py</code> <pre><code>def completed(self) -&gt; list[Trial]:\n    \"\"\"Return all trials which are in a completed state.\"\"\"\n    return list(\n        itertools.chain(\n            self.by_state[State.SUCCESS],\n            self.by_state[State.FAILED],\n            self.by_state[State.CRASHED],\n        )\n    )\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.Summary.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Convert the summary into a dataframe.</p> Source code in <code>neps\\status\\status.py</code> <pre><code>def df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the summary into a dataframe.\"\"\"\n    trials = sorted(\n        itertools.chain(*self.by_state.values()),\n        key=lambda t: t.metadata.time_sampled,\n    )\n\n    # Config dataframe, config columns prefixed with `config.`\n    config_df = (\n        pd.DataFrame.from_records([trial.config for trial in trials])\n        .rename(columns=lambda name: f\"config.{name}\")\n        .convert_dtypes()\n    )\n\n    # Report dataframe\n    report_df = pd.DataFrame.from_records(\n        [asdict(t.report) if t.report is not None else {} for t in trials]\n    ).convert_dtypes()\n\n    extra_df = pd.DataFrame()\n    # We pop out the user extra column to flatten it\n    if \"extra\" in report_df.columns:\n        extra_column = report_df.pop(\"extra\")\n        extra_df = pd.json_normalize(extra_column).rename(  # type: ignore\n            columns=lambda name: f\"extra.{name}\"\n        )\n\n    # Metadata dataframe\n    metadata_df = pd.DataFrame.from_records(\n        [asdict(t.metadata) for t in trials]\n    ).convert_dtypes()\n\n    return (\n        pd.concat([config_df, extra_df, report_df, metadata_df], axis=\"columns\")\n        .set_index(\"id\")\n        .dropna(how=\"all\", axis=\"columns\")\n    )\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.Summary.formatted","title":"formatted","text":"<pre><code>formatted() -&gt; str\n</code></pre> <p>Return a formatted string of the summary.</p> Source code in <code>neps\\status\\status.py</code> <pre><code>def formatted(self) -&gt; str:\n    \"\"\"Return a formatted string of the summary.\"\"\"\n    state_summary = \"\\n\".join(\n        f\"    {state.name.lower()}: {len(trials)}\"\n        for state, trials in self.by_state.items()\n        if len(trials) &gt; 0\n    )\n\n    if self.best is None:\n        if self.is_multiobjective:\n            best_summary = \"Multiobjective summary not supported yet for best yet.\"\n        else:\n            best_summary = \"No best found yet.\"\n    else:\n        best_trial, best_objective_to_minimize = self.best\n        best_summary = (\n            f\"# Best Found (config {best_trial.metadata.id}):\"\n            \"\\n\"\n            f\"\\n    objective_to_minimize: {best_objective_to_minimize}\"\n            f\"\\n    config: {best_trial.config}\"\n            f\"\\n    path: {best_trial.metadata.location}\"\n        )\n        assert best_trial.report is not None\n        if best_trial.report.cost is not None:\n            best_summary += f\"\\n    cost: {best_trial.report.cost}\"\n        if len(best_trial.report.extra) &gt; 0:\n            best_summary += f\"\\n    extra: {best_trial.report.extra}\"\n\n    return f\"# Configs: {self.num_evaluated}\\n\\n{state_summary}\\n\\n{best_summary}\"\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.Summary.from_directory","title":"from_directory  <code>classmethod</code>","text":"<pre><code>from_directory(root_directory: str | Path) -&gt; Summary\n</code></pre> <p>Create a summary from a neps run directory.</p> Source code in <code>neps\\status\\status.py</code> <pre><code>@classmethod\ndef from_directory(cls, root_directory: str | Path) -&gt; Summary:\n    \"\"\"Create a summary from a neps run directory.\"\"\"\n    root_directory = Path(root_directory)\n\n    is_multiobjective: bool = False\n    best: tuple[Trial, float] | None = None\n    by_state: dict[State, list[Trial]] = {s: [] for s in State}\n\n    # NOTE: We don't lock the shared state since we are just reading and don't need to\n    # make decisions based on the state\n    try:\n        shared_state = get_workers_neps_state()\n    except RuntimeError:\n        shared_state = NePSState.create_or_load(root_directory, load_only=True)\n\n    trials = shared_state.lock_and_read_trials()\n\n    for _trial_id, trial in trials.items():\n        state = trial.metadata.state\n        by_state[state].append(trial)\n\n        if trial.report is not None:\n            objective_to_minimize = trial.report.objective_to_minimize\n            match objective_to_minimize:\n                case None:\n                    pass\n                case float() | int() | np.number() if not is_multiobjective:\n                    if best is None or objective_to_minimize &lt; best[1]:\n                        best = (trial, objective_to_minimize)\n                case Sequence():\n                    is_multiobjective = True\n                    best = None\n                case _:\n                    raise RuntimeError(\"Unexpected type for objective_to_minimize\")\n\n    return cls(by_state=by_state, best=best, is_multiobjective=is_multiobjective)\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.post_run_csv","title":"post_run_csv","text":"<pre><code>post_run_csv(\n    root_directory: str | Path,\n) -&gt; tuple[Path, Path]\n</code></pre> <p>Create CSV files summarizing the run data.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory of the NePS run.</p> <p> TYPE: <code>str | Path</code> </p> RETURNS DESCRIPTION <code>tuple[Path, Path]</code> <p>The paths to the configuration data CSV and the run data CSV.</p> Source code in <code>neps\\status\\status.py</code> <pre><code>def post_run_csv(root_directory: str | Path) -&gt; tuple[Path, Path]:\n    \"\"\"Create CSV files summarizing the run data.\n\n    Args:\n        root_directory: The root directory of the NePS run.\n\n    Returns:\n        The paths to the configuration data CSV and the run data CSV.\n    \"\"\"\n    full_df, short = status(root_directory, print_summary=False)\n    full_df_path, short_path, csv_locker = _initiate_summary_csv(root_directory)\n\n    with csv_locker.lock():\n        full_df.to_csv(full_df_path)\n        short.to_frame().to_csv(short_path)\n\n    return full_df_path, short_path\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.status","title":"status","text":"<pre><code>status(\n    root_directory: str | Path,\n    *,\n    print_summary: bool = False\n) -&gt; tuple[DataFrame, Series]\n</code></pre> <p>Print status information of a neps run and return results.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory given to neps.run.</p> <p> TYPE: <code>str | Path</code> </p> <code>print_summary</code> <p>If true, print a summary of the current run state</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[DataFrame, Series]</code> <p>Dataframe of full results and short summary series.</p> Source code in <code>neps\\status\\status.py</code> <pre><code>def status(\n    root_directory: str | Path,\n    *,\n    print_summary: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Print status information of a neps run and return results.\n\n    Args:\n        root_directory: The root directory given to neps.run.\n        print_summary: If true, print a summary of the current run state\n\n    Returns:\n        Dataframe of full results and short summary series.\n    \"\"\"\n    root_directory = Path(root_directory)\n    summary = Summary.from_directory(root_directory)\n\n    if print_summary:\n        print(summary.formatted())\n\n    df = summary.df()\n\n    if len(df) == 0:\n        return df, pd.Series()\n\n    short = (\n        df.groupby(\"state\")\n        .size()\n        .rename(lambda name: f\"num_{name.replace('State.', '').lower()}\")\n    )\n    short.name = \"value\"\n    short.index.name = \"summary\"\n    short.index = short.index.astype(str)\n    assert isinstance(short, pd.Series)\n\n    # Not implemented for hypervolume -_-\n    if summary.is_multiobjective:\n        return df, short\n\n    idx_min = df[\"objective_to_minimize\"].idxmin()\n    row = df.loc[idx_min]\n    assert isinstance(row, pd.Series)\n    short[\"best_objective_to_minimize\"] = row[\"objective_to_minimize\"]\n    short[\"best_config_id\"] = row.name\n\n    row = row.loc[row.index.str.startswith(\"config.\")]\n    row.index = row.index.str.replace(\"config.\", \"\")  # type: ignore\n    short = pd.concat([short, row])  # type: ignore\n    assert isinstance(short, pd.Series)\n    return df, short\n</code></pre>"},{"location":"api/neps/utils/common/","title":"Common","text":"<p>Common utility functions used across the library.</p>"},{"location":"api/neps/utils/common/#neps.utils.common.capture_function_arguments","title":"capture_function_arguments","text":"<pre><code>capture_function_arguments(\n    the_locals: dict, func: Callable\n) -&gt; dict\n</code></pre> <p>Capture the function arguments and their values from the locals dictionary.</p> PARAMETER DESCRIPTION <code>the_locals</code> <p>The locals dictionary of the function.</p> <p> TYPE: <code>dict</code> </p> <code>func</code> <p>The function to capture arguments from.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary of function arguments and their values.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def capture_function_arguments(the_locals: dict, func: Callable) -&gt; dict:\n    \"\"\"Capture the function arguments and their values from the locals dictionary.\n\n    Args:\n        the_locals: The locals dictionary of the function.\n        func: The function to capture arguments from.\n\n    Returns:\n        A dictionary of function arguments and their values.\n    \"\"\"\n    signature = inspect.signature(func)\n    return {\n        key: the_locals[key]\n        for key in signature.parameters\n        if key in the_locals and key != \"self\"\n    }\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.disable_warnings","title":"disable_warnings","text":"<pre><code>disable_warnings(\n    *warning_types: type[Warning],\n) -&gt; Iterator[None]\n</code></pre> <p>Disable certain warning categories for a specific block.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>@contextmanager\ndef disable_warnings(*warning_types: type[Warning]) -&gt; Iterator[None]:\n    \"\"\"Disable certain warning categories for a specific block.\"\"\"\n    with warnings.catch_warnings():\n        for warning_type in warning_types:\n            warnings.filterwarnings(\"ignore\", category=warning_type)\n        yield\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.dynamic_load_object","title":"dynamic_load_object","text":"<pre><code>dynamic_load_object(path: str, object_name: str) -&gt; object\n</code></pre> <p>Dynamically loads an object from a given module file path.</p> PARAMETER DESCRIPTION <code>path</code> <p>File system path or module path to the Python module.</p> <p> TYPE: <code>str</code> </p> <code>object_name</code> <p>Name of the object to import from the module.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>object</code> <p>The imported object from the module.</p> <p> TYPE: <code>object</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If the module or object cannot be found.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def dynamic_load_object(path: str, object_name: str) -&gt; object:\n    \"\"\"Dynamically loads an object from a given module file path.\n\n    Args:\n        path: File system path or module path to the Python module.\n        object_name: Name of the object to import from the module.\n\n    Returns:\n        object: The imported object from the module.\n\n    Raises:\n        ImportError: If the module or object cannot be found.\n    \"\"\"\n    # file system path\n    if os.sep in path:\n        _path = Path(path).with_suffix(\".py\")\n        if not _path.exists():\n            raise ImportError(\n                f\"Failed to import '{object_name}'. File '{path}' does not exist.\"\n            )\n        module_path = path.replace(os.sep, \".\").replace(\".py\", \"\")\n\n    # module path\n    else:\n        module_path = path\n\n    # Dynamically import the module.\n    spec = importlib.util.spec_from_file_location(module_path, path)\n\n    if spec is None or spec.loader is None:\n        raise ImportError(\n            f\"Failed to import '{object_name}'.\"\n            f\" Spec or loader is None for module '{module_path}'.\"\n        )\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_path] = module\n    spec.loader.exec_module(module)\n\n    # Retrieve the object.\n    imported_object = getattr(module, object_name, None)\n    if imported_object is None:\n        raise ImportError(\n            f\"Failed to import '{object_name}'.\"\n            f\"Object does not exist in module '{module_path}'.\"\n        )\n\n    return imported_object\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.extract_keyword_defaults","title":"extract_keyword_defaults","text":"<pre><code>extract_keyword_defaults(f: Callable) -&gt; dict[str, Any]\n</code></pre> <p>Extracts the keywords from a function, if any.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def extract_keyword_defaults(f: Callable) -&gt; dict[str, Any]:\n    \"\"\"Extracts the keywords from a function, if any.\"\"\"\n    if isinstance(f, partial):\n        return dict(f.keywords)\n\n    signature = inspect.signature(f)\n    return {\n        k: v.default\n        for k, v in signature.parameters.items()\n        if v.default is not inspect.Parameter.empty\n    }\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.gc_disabled","title":"gc_disabled","text":"<pre><code>gc_disabled() -&gt; Iterator[None]\n</code></pre> <p>Context manager to disable garbage collection for a block.</p> <p>We specifically put this around file I/O operations to minimize the time spend garbage collecting while having the file handle open.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>@contextmanager\ndef gc_disabled() -&gt; Iterator[None]:\n    \"\"\"Context manager to disable garbage collection for a block.\n\n    We specifically put this around file I/O operations to minimize the time\n    spend garbage collecting while having the file handle open.\n    \"\"\"\n    gc.disable()\n    try:\n        yield\n    finally:\n        gc.enable()\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.get_initial_directory","title":"get_initial_directory","text":"<pre><code>get_initial_directory(\n    pipeline_directory: Path | str | None = None,\n) -&gt; Path\n</code></pre> <p>Find the initial directory based on its existence and the presence of the \"previous_config.id\" file.</p> PARAMETER DESCRIPTION <code>pipeline_directory</code> <p>The current config directory.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>The initial directory.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def get_initial_directory(pipeline_directory: Path | str | None = None) -&gt; Path:\n    \"\"\"Find the initial directory based on its existence and the presence of\n    the \"previous_config.id\" file.\n\n    Args:\n        pipeline_directory: The current config directory.\n\n    Returns:\n        The initial directory.\n    \"\"\"\n    from neps.runtime import get_in_progress_trial, get_workers_neps_state\n\n    neps_state = get_workers_neps_state()\n    if pipeline_directory is not None:\n        # TODO: Hard coded assumption\n        config_id = Path(pipeline_directory).name.split(\"_\", maxsplit=1)[-1]\n        trial = neps_state.unsafe_retry_get_trial_by_id(config_id)\n    else:\n        trial = get_in_progress_trial()\n\n    if trial.metadata.id in _INTIAL_DIRECTORY_CACHE:\n        return _INTIAL_DIRECTORY_CACHE[trial.metadata.id]\n\n    # Recursively find the initial directory\n    while (prev_trial_id := trial.metadata.previous_trial_id) is not None:\n        trial = neps_state.unsafe_retry_get_trial_by_id(prev_trial_id)\n\n    initial_dir = trial.metadata.location\n\n    # TODO: Hard coded assumption that we are operating in a filebased neps\n    assert isinstance(initial_dir, str)\n    path = Path(initial_dir)\n\n    _INTIAL_DIRECTORY_CACHE[trial.metadata.id] = path\n    return path\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.get_value","title":"get_value","text":"<pre><code>get_value(obj: Any) -&gt; Any\n</code></pre> <p>Honestly, don't know why you would use this. Please try not to.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def get_value(obj: Any) -&gt; Any:\n    \"\"\"Honestly, don't know why you would use this. Please try not to.\"\"\"\n    if obj is None:\n        return None\n    if isinstance(obj, str | int | float | bool):\n        return obj\n    if isinstance(obj, dict):\n        return {key: get_value(value) for key, value in obj.items()}\n    if isinstance(obj, list):\n        return [get_value(item) for item in obj]\n\n    return obj.__name__\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.is_partial_class","title":"is_partial_class","text":"<pre><code>is_partial_class(obj: Any) -&gt; bool\n</code></pre> <p>Check if the object is a (partial) class, or an instance.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def is_partial_class(obj: Any) -&gt; bool:\n    \"\"\"Check if the object is a (partial) class, or an instance.\"\"\"\n    if isinstance(obj, partial):\n        obj = obj.func\n    return inspect.isclass(obj)\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    model: Module | None = None,\n    optimizer: Optimizer | None = None,\n) -&gt; dict | None\n</code></pre> <p>Load a checkpoint and return the model state_dict and checkpoint values.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Directory where the checkpoint is located.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_name</code> <p>The name of the checkpoint file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'checkpoint'</code> </p> <code>model</code> <p>The PyTorch model to load.</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to load.</p> <p> TYPE: <code>Optimizer | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict | None</code> <p>A dictionary containing the checkpoint values, or None if the checkpoint file does not exist hence no checkpointing was previously done.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def load_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    model: torch.nn.Module | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n) -&gt; dict | None:\n    \"\"\"Load a checkpoint and return the model state_dict and checkpoint values.\n\n    Args:\n        directory: Directory where the checkpoint is located.\n        checkpoint_name: The name of the checkpoint file.\n        model: The PyTorch model to load.\n        optimizer: The optimizer to load.\n\n    Returns:\n        A dictionary containing the checkpoint values, or None if the checkpoint file\n        does not exist hence no checkpointing was previously done.\n    \"\"\"\n    from neps.runtime import get_in_progress_trial\n\n    if directory is None:\n        trial = get_in_progress_trial()\n        directory = trial.metadata.previous_trial_location\n        if directory is None:\n            return None\n        assert isinstance(directory, str)\n\n    directory = Path(directory)\n    checkpoint_path = (directory / checkpoint_name).with_suffix(\".pth\")\n\n    if not checkpoint_path.exists():\n        return None\n\n    checkpoint = torch.load(checkpoint_path, weights_only=True)\n\n    if model is not None and \"model_state_dict\" in checkpoint:\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n    return checkpoint  # type: ignore\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.load_lightning_checkpoint","title":"load_lightning_checkpoint","text":"<pre><code>load_lightning_checkpoint(\n    checkpoint_dir: Path | str,\n    previous_pipeline_directory: Path | str | None = None,\n) -&gt; tuple[Path, dict] | tuple[None, None]\n</code></pre> <p>Load the latest checkpoint file from the specified directory.</p> <p>This function searches for possible checkpoint files in the <code>checkpoint_dir</code> and loads the latest one if found. It returns a tuple with the checkpoint path and the loaded checkpoint data.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The directory where checkpoint files are stored.</p> <p> TYPE: <code>Path | str</code> </p> <code>previous_pipeline_directory</code> <p>The previous pipeline directory.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Path, dict] | tuple[None, None]</code> <p>A tuple containing the checkpoint path (str) and the loaded checkpoint data (dict) or (None, None) if no checkpoint files are found in the directory.</p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def load_lightning_checkpoint(\n    checkpoint_dir: Path | str,\n    previous_pipeline_directory: Path | str | None = None,\n) -&gt; tuple[Path, dict] | tuple[None, None]:\n    \"\"\"Load the latest checkpoint file from the specified directory.\n\n    This function searches for possible checkpoint files in the `checkpoint_dir` and loads\n    the latest one if found. It returns a tuple with the checkpoint path and the loaded\n    checkpoint data.\n\n    Args:\n        checkpoint_dir: The directory where checkpoint files are stored.\n        previous_pipeline_directory: The previous pipeline directory.\n\n    Returns:\n        A tuple containing the checkpoint path (str) and the loaded checkpoint data (dict)\n        or (None, None) if no checkpoint files are found in the directory.\n    \"\"\"\n    from neps.runtime import get_in_progress_trial\n\n    if previous_pipeline_directory is None:\n        trial = get_in_progress_trial()\n        previous_pipeline_directory = trial.metadata.previous_trial_location\n        if previous_pipeline_directory is None:\n            return None, None\n\n    # Search for possible checkpoints to continue training\n    ckpt_files = list(Path(checkpoint_dir).glob(\"*.ckpt\"))\n\n    if len(ckpt_files) == 0:\n        raise FileNotFoundError(\n            \"No checkpoint files were located in the checkpoint directory\"\n        )\n\n    if len(ckpt_files) &gt; 1:\n        raise ValueError(\n            \"The number of checkpoint files is more than expected (1) \"\n            \"which makes if difficult to find the correct file.\"\n            \" Please save other checkpoint files in a different directory.\"\n        )\n\n    assert len(ckpt_files) == 1\n    checkpoint_path = ckpt_files[0]\n    checkpoint = torch.load(checkpoint_path, weights_only=True)\n    return checkpoint_path, checkpoint\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    values_to_save: dict | None = None,\n    model: Module | None = None,\n    optimizer: Optimizer | None = None,\n) -&gt; None\n</code></pre> <p>Save a checkpoint including model state_dict and optimizer state_dict to a file.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Directory where the checkpoint will be saved.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> <code>values_to_save</code> <p>Additional values to save in the checkpoint.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The PyTorch model to save.</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to save.</p> <p> TYPE: <code>Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_name</code> <p>The name of the checkpoint file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'checkpoint'</code> </p> Source code in <code>neps\\utils\\common.py</code> <pre><code>def save_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    values_to_save: dict | None = None,\n    model: torch.nn.Module | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n) -&gt; None:\n    \"\"\"Save a checkpoint including model state_dict and optimizer state_dict to a file.\n\n    Args:\n        directory: Directory where the checkpoint will be saved.\n        values_to_save: Additional values to save in the checkpoint.\n        model: The PyTorch model to save.\n        optimizer: The optimizer to save.\n        checkpoint_name: The name of the checkpoint file.\n    \"\"\"\n    from neps.runtime import get_in_progress_trial\n\n    if directory is None:\n        in_progress_trial = get_in_progress_trial()\n        directory = in_progress_trial.metadata.location\n\n    directory = Path(directory)\n    checkpoint_path = (directory / checkpoint_name).with_suffix(\".pth\")\n\n    saved_dict = {}\n\n    if model is not None:\n        saved_dict[\"model_state_dict\"] = model.state_dict()\n    if optimizer is not None:\n        saved_dict[\"optimizer_state_dict\"] = optimizer.state_dict()\n\n    if values_to_save is not None:\n        saved_dict.update(values_to_save)\n\n    torch.save(saved_dict, checkpoint_path)\n</code></pre>"},{"location":"api/neps/utils/files/","title":"Files","text":"<p>Utilities for file operations.</p>"},{"location":"api/neps/utils/files/#neps.utils.files.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(\n    file_path: Path | str, *args: Any, **kwargs: Any\n) -&gt; Iterator[IO]\n</code></pre> <p>Write to a file atomically.</p> <p>This means that the file will be flushed to disk and explicitly ask the operating systems to sync the contents to disk. This ensures that other processes that read from this file should see the contents immediately.</p> Source code in <code>neps\\utils\\files.py</code> <pre><code>@contextmanager\ndef atomic_write(file_path: Path | str, *args: Any, **kwargs: Any) -&gt; Iterator[IO]:\n    \"\"\"Write to a file atomically.\n\n    This means that the file will be flushed to disk and explicitly ask the operating\n    systems to sync the contents to disk. This ensures that other processes that read\n    from this file should see the contents immediately.\n    \"\"\"\n    with open(file_path, *args, **kwargs) as file_stream:  # noqa: PTH123\n        yield file_stream\n        file_stream.flush()\n        os.fsync(file_stream.fileno())\n        file_stream.close()\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.deserialize","title":"deserialize","text":"<pre><code>deserialize(\n    path: Path | str,\n    *,\n    file_format: Literal[\"json\", \"yaml\"] = \"yaml\"\n) -&gt; dict[str, Any]\n</code></pre> <p>Deserialize data from a yaml file.</p> Source code in <code>neps\\utils\\files.py</code> <pre><code>def deserialize(\n    path: Path | str,\n    *,\n    file_format: Literal[\"json\", \"yaml\"] = \"yaml\",\n) -&gt; dict[str, Any]:\n    \"\"\"Deserialize data from a yaml file.\"\"\"\n    with Path(path).open(\"r\") as file_stream:\n        if file_format == \"json\":\n            import json\n\n            data = json.load(file_stream)\n        elif file_format == \"yaml\":\n            data = yaml.load(file_stream, SafeLoader)\n        else:\n            raise ValueError(f\"Unknown format: {file_format}\")\n\n    if not isinstance(data, dict):\n        raise TypeError(\n            f\"Deserialized data at {path} is not a dictionary!\"\n            f\" Got {type(data)} instead.\\n{data}\"\n        )\n\n    return data\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.load_and_merge_yamls","title":"load_and_merge_yamls","text":"<pre><code>load_and_merge_yamls(\n    *paths: str | Path | IO[str],\n) -&gt; dict[str, Any]\n</code></pre> <p>Load and merge yaml files into a single dictionary.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If there are duplicate keys in the yaml files.</p> Source code in <code>neps\\utils\\files.py</code> <pre><code>def load_and_merge_yamls(*paths: str | Path | IO[str]) -&gt; dict[str, Any]:\n    \"\"\"Load and merge yaml files into a single dictionary.\n\n    Raises:\n        ValueError: If there are duplicate keys in the yaml files.\n    \"\"\"\n    config: dict[str, Any] = {}\n    for path in paths:\n        match path:\n            case str() | Path():\n                with Path(path).open(\"r\") as file:\n                    read_config = yaml.safe_load(file)\n\n            case _:\n                read_config = yaml.safe_load(path)\n\n        shared_keys = set(config) &amp; set(read_config)\n\n        if any(shared_keys):\n            raise ValueError(f\"Duplicate key(s) {shared_keys} in {paths}\")\n\n        config.update(read_config)\n\n    return config\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.serializable_format","title":"serializable_format","text":"<pre><code>serializable_format(data: Any) -&gt; Any\n</code></pre> <p>Format data to be serializable.</p> Source code in <code>neps\\utils\\files.py</code> <pre><code>def serializable_format(data: Any) -&gt; Any:  # noqa: PLR0911\n    \"\"\"Format data to be serializable.\"\"\"\n    if hasattr(data, \"serialize\"):\n        return serializable_format(data.serialize())\n\n    if dataclasses.is_dataclass(data) and not isinstance(data, type):\n        return serializable_format(dataclasses.asdict(data))  # type: ignore\n\n    if isinstance(data, Exception):\n        return str(data)\n\n    if isinstance(data, Enum):\n        return data.value\n\n    if isinstance(data, Mapping):\n        return {key: serializable_format(val) for key, val in data.items()}\n\n    if not isinstance(data, str) and isinstance(data, Iterable):\n        return [serializable_format(val) for val in data]\n\n    if type(data).__module__ in [\"numpy\", \"torch\"]:\n        data = data.tolist()  # type: ignore\n        if type(data).__module__ == \"numpy\":\n            data = data.item()\n\n        return serializable_format(data)\n\n    return data\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.serialize","title":"serialize","text":"<pre><code>serialize(\n    data: Any,\n    path: Path,\n    *,\n    check_serialized: bool = True,\n    file_format: Literal[\"json\", \"yaml\"] = \"yaml\",\n    sort_keys: bool = True\n) -&gt; None\n</code></pre> <p>Serialize data to a yaml file.</p> Source code in <code>neps\\utils\\files.py</code> <pre><code>def serialize(\n    data: Any,\n    path: Path,\n    *,\n    check_serialized: bool = True,\n    file_format: Literal[\"json\", \"yaml\"] = \"yaml\",\n    sort_keys: bool = True,\n) -&gt; None:\n    \"\"\"Serialize data to a yaml file.\"\"\"\n    if check_serialized:\n        data = serializable_format(data)\n\n    buf = io.StringIO()\n    if file_format == \"yaml\":\n        try:\n            yaml.dump(data, buf, YamlDumper, sort_keys=sort_keys)\n        except yaml.representer.RepresenterError as e:\n            raise TypeError(\n                \"Could not serialize to yaml! The object \"\n                f\"{e.args[1]} of type {type(e.args[1])} is not.\"\n            ) from e\n    elif file_format == \"json\":\n        import json\n\n        json.dump(data, buf, sort_keys=sort_keys)\n    else:\n        raise ValueError(f\"Unknown format: {file_format}\")\n\n    _str = buf.getvalue()\n    path.write_text(_str)\n</code></pre>"},{"location":"dev_docs/contributing/","title":"Introduction","text":""},{"location":"dev_docs/contributing/#getting-help","title":"Getting Help","text":"<p>Please use our github and raise an issue at: automl/neps</p>"},{"location":"dev_docs/contributing/#development-workflow","title":"Development Workflow","text":"<p>We use one main branch <code>master</code> and feature branches for development. We use pull requests to merge feature branches into <code>master</code>. Versions released to PyPI are tagged with a version number.</p> <p>Automatic checks are run on every pull request and on every commit to <code>master</code>.</p>"},{"location":"dev_docs/contributing/#installation","title":"Installation","text":"<p>There are three required steps and one optional:</p> <ol> <li>Install uv</li> <li>Install the neps package using uv</li> <li>Activate pre-commit for the repository</li> </ol> <p>For instructions see below.</p>"},{"location":"dev_docs/contributing/#1-install-uv","title":"1. Install uv","text":"<p>First, install uv, e.g., via</p> <pre><code># On macOS and Linux.\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code># On Windows.\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre>"},{"location":"dev_docs/contributing/#2-clone-the-neps-repository","title":"2. Clone the neps repository","text":"<pre><code>git clone https://github.com/automl/neps.git\ncd neps\n</code></pre>"},{"location":"dev_docs/contributing/#3-create-a-virtual-environment-and-install-the-neps-package","title":"3. Create a virtual environment and install the neps package","text":"<pre><code>uv venv --python 3.11\nsource .venv/bin/activate\n</code></pre> <p>Then, inside the main directory of neps run</p> <pre><code>uv pip install -e \".[dev]\"\n</code></pre> <p>This will installthe neps package but also additional dev dependencies.</p>"},{"location":"dev_docs/contributing/#4-activate-pre-commit-for-the-repository","title":"4. Activate pre-commit for the repository","text":"<p>With the python environment used to install the neps package run in the main directory of neps</p> <pre><code>pre-commit install\n</code></pre> <p>This install a set of hooks that will run basic linting and type checking before every comment. If you ever need to unsinstall the hooks, you can do so with <code>pre-commit uninstall</code>. These mostly consist of <code>ruff</code> for formatting and linting and <code>mypy</code> for type checking.</p> <p>We highly recommend you install at least <code>ruff</code> either on command line, or in the editor of your choice, e.g. VSCode, PyCharm.</p> <p>We have setup checks and tests at several points in the development flow:</p> <ul> <li>At every commit we automatically run a suite of pre-commit hooks that perform static code analysis, autoformating, and sanity checks. This is setup during our installation process.</li> <li>At every commit / push locally running a minimal suite of integration tests is encouraged. The tests correspond directly to examples in neps_examples and only check for crash-causing errors.</li> </ul>"},{"location":"dev_docs/contributing/#checks-and-tests","title":"Checks and tests","text":""},{"location":"dev_docs/contributing/#linting-ruff","title":"Linting (Ruff)","text":"<p>For linting we use <code>ruff</code> for checking code quality. You can install it locally and use it as so:</p> <pre><code>uv pip install ruff\nruff check --fix neps  # the --fix flag will try to fix issues it can automatically\n</code></pre> <p>This will also be run using <code>pre-commit</code> hooks.</p> <p>To ignore a rule for a specific line, you can add a comment with <code>ruff: disable</code> at the end of the line, e.g.</p> <pre><code>for x, y in zip(a, b):  # noqa: &lt;ERRCODE&gt;\n    pass\n</code></pre> <p>The configuration of <code>ruff</code> is in the <code>pyproject.toml</code> file and we refer you to the documentation if you require any changes to be made.</p> <p>There you can find the documentation for all of the rules employed.</p>"},{"location":"dev_docs/contributing/#type-checking-mypy","title":"Type Checking (Mypy)","text":"<p>For type checking we use <code>mypy</code>. You can install it locally and use it as so:</p> <pre><code>uv pip install mypy\nmypy neps\n</code></pre> <p>Types are helpful for making your code more understandable by your editor and tools, allowing them to warn you of potential issues, as well as allow for safer refactoring. Copilot also works better with types.</p> <p>To ignore some error you can use <code># type: ignore</code> at the end of the line, e.g.</p> <pre><code>code = \"foo\"  # type: ignore\n</code></pre> <p>A common place to ignore types is when dealing with numpy arrays, tensors and pandas, where the type checker can not be sure of the return type.</p> <pre><code>df.mean()  # Is this another dataframe, a series or a single number?\n</code></pre> <p>In the worse case, please just use <code>Any</code> and move on with your life, the type checker is meant to help you catch bugs, not hinder you. However it will take some experience to know whe it's trying to tell you something useful vs. something it just can not infer properly. A good rule of thumb is that you're only dealing with simple native types from python or types defined from NePS, there is probably a good reason for a mypy error.</p> <p>If you have issues regarding typing, please feel free to reach out for help <code>@eddiebergman</code>.</p>"},{"location":"dev_docs/contributing/#examples-and-integration-tests","title":"Examples and Integration Tests","text":"<p>We use some examples in neps_examples as integration tests, which we run from the main directory via</p> <pre><code>pytest\n</code></pre> <p>If tests fail for you on the master, please raise an issue on github, preferably with some information on the error, traceback and the environment in which you are running, i.e. python version, OS, etc.</p>"},{"location":"dev_docs/contributing/#disabling-and-skipping-checks-etc","title":"Disabling and Skipping Checks etc.","text":""},{"location":"dev_docs/contributing/#pre-commit-how-to-not-run-hooks","title":"Pre-commit: How to not run hooks?","text":"<p>To commit without running <code>pre-commit</code> use <code>git commit --no-verify -m &lt;COMMIT MESSAGE&gt;</code>.</p>"},{"location":"dev_docs/contributing/#mypy-how-to-ignore-warnings","title":"Mypy: How to ignore warnings?","text":"<p>There are two options:</p> <ul> <li>Disable the warning locally:</li> </ul> <pre><code>code = \"foo\"  # type: ignore\n</code></pre>"},{"location":"dev_docs/contributing/#managing-dependencies","title":"Managing Dependencies","text":"<p>To manage dependencies we use uv (replaces pip).</p>"},{"location":"dev_docs/contributing/#add-dependencies","title":"Add dependencies","text":"<p>To install a dependency use</p> <pre><code>uv add dependency\n</code></pre> <p>and commit the updated <code>pyproject.toml</code> to git.</p> <p>For more advanced dependency management see examples in <code>pyproject.toml</code> or have a look at the uv documentation.</p>"},{"location":"dev_docs/contributing/#install-dependencies-added-by-others","title":"Install dependencies added by others","text":"<p>When other contributors added dependencies to <code>pyproject.toml</code>, you can install them via</p> <pre><code>uv pip install -e \".[dev]\"\n</code></pre>"},{"location":"dev_docs/contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs, more specifically Material for MkDocs for documentation. To support documentation for multiple versions, we use the plugin mike.</p> <p>Source files for the documentation are under <code>/docs</code> and configuration at  mkdocs.yml.</p> <p>To build and view the documentation run</p> <pre><code>mike deploy 0.5.1 latest\nmike serve\n</code></pre> <p>and open the URL shown by the <code>mike serve</code> command.</p> <p>To publish the documentation run</p> <pre><code>mike deploy 0.5.1 latest -p\n</code></pre>"},{"location":"dev_docs/contributing/#releasing-a-new-version","title":"Releasing a New Version","text":"<p>There are four steps to releasing a new version of neps:</p> <ol> <li>Understand Semantic Versioning</li> <li>Update the Package Version</li> <li>Commit and Push With a Version Tag</li> <li>Update Documentation</li> <li>Publish on PyPI</li> </ol>"},{"location":"dev_docs/contributing/#0-understand-semantic-versioning","title":"0. Understand Semantic Versioning","text":"<p>We follow the semantic versioning scheme.</p>"},{"location":"dev_docs/contributing/#1-run-tests","title":"1. Run tests","text":"<pre><code>uv run pytest\n</code></pre>"},{"location":"dev_docs/contributing/#2-update-the-package-version-and-citationcff","title":"2. Update the Package Version and CITATION.cff","text":"<pre><code>bump-my-version bump &lt;major | minor | patch&gt;\n</code></pre> <p>This will automatically update the version in <code>pyproject.toml</code> and <code>CITATION.cff</code>, tag the commit and push it to the remote repository.</p>"},{"location":"dev_docs/contributing/#3-update-documentation","title":"3. Update Documentation","text":"<p>First check if the documentation has any issues via</p> <pre><code>mike deploy &lt;current version&gt; latest -u\nmike serve\n</code></pre> <p>and then looking at it.</p> <p>Afterwards, publish it via</p> <pre><code>mike deploy &lt;current version&gt; latest -up\n</code></pre>"},{"location":"dev_docs/contributing/#4-publish-on-pypi","title":"4. Publish on PyPI","text":"<p>To publish to PyPI:</p> <ol> <li>Get publishing rights, e.g., asking Danny or Neeratyoy.</li> <li>Be careful, once on PyPI we can not change things.</li> <li>Run</li> </ol> <pre><code>uv build\nuv publish\n</code></pre> <p>This will ask for your PyPI credentials.</p>"},{"location":"examples/","title":"Overview","text":"<ol> <li> <p>Basic usage examples demonstrate fundamental usage. Learn how to perform Hyperparameter Optimization (HPO) and analyze runs on a basic level.</p> </li> <li> <p>Convenience examples show tensorboard compatibility and its integration, SLURM-scripting and understand file management within the evaluate pipeline function used in NePS.</p> </li> <li> <p>Efficiency examples showcase how to enhance efficiency in NePS. Learn about expert priors, multi-fidelity, and parallelization to streamline your pipeline and optimize search processes.</p> </li> <li> <p>Experimental examples tailored for NePS contributors. These examples provide insights and practices for experimental scenarios.</p> </li> <li> <p>Real-world examples demonstrate how to apply NePS to real-world problems.</p> </li> </ol>"},{"location":"examples/basic_usage/analyse/","title":"Analyse","text":"<pre><code>\"\"\"How to generate a summary (neps.status) and visualizations (neps.plot) of a run.\n\nBefore running this example analysis, run the hyperparameters example with:\n\n    python -m neps_examples.basic_usage.hyperparameters\n\"\"\"\n\nimport neps\n\n# 1. At all times, NePS maintains several files in the root directory that are human\n# read-able and can be useful\n\n# 2. Printing a summary and reading in results.\n# Alternatively use `python -m neps.status results/hyperparameters_example`\nfull, summary = neps.status(\"results/hyperparameters_example\", print_summary=True)\nconfig_id = \"1\"\n\nprint(full.head())\nprint(\"\")\nprint(full.loc[config_id])\n</code></pre>"},{"location":"examples/basic_usage/hyperparameters/","title":"Hyperparameters","text":"<pre><code>import logging\nimport numpy as np\nimport neps\n\n# This example demonstrates how to use NePS to optimize hyperparameters\n# of a pipeline. The pipeline is a simple function that takes in\n# five hyperparameters and returns their sum.\n# Neps uses the default optimizer to minimize this objective function.\n\ndef evaluate_pipeline(float1, float2, categorical, integer1, integer2):\n    objective_to_minimize = -float(\n        np.sum([float1, float2, int(categorical), integer1, integer2])\n    )\n    return objective_to_minimize\n\n\npipeline_space = dict(\n    float1=neps.Float(lower=0, upper=1),\n    float2=neps.Float(lower=-10, upper=10),\n    categorical=neps.Categorical(choices=[0, 1]),\n    integer1=neps.Integer(lower=0, upper=1),\n    integer2=neps.Integer(lower=1, upper=1000, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hyperparameters_example\",\n    post_run_summary=True,\n    max_evaluations_total=30,\n)\n</code></pre>"},{"location":"examples/convenience/logging_additional_info/","title":"Logging additional info","text":"<pre><code>import logging\nimport time\nfrom warnings import warn\n\nimport numpy as np\n\nimport neps\n\n\ndef evaluate_pipeline(float1, float2, categorical, integer1, integer2):\n    start = time.time()\n    objective_to_minimize = -float(\n        np.sum([float1, float2, int(categorical), integer1, integer2])\n    )\n    end = time.time()\n    return {\n        \"objective_to_minimize\": objective_to_minimize,\n        \"info_dict\": {  # Optionally include additional information as an info_dict\n            \"train_time\": end - start,\n        },\n    }\n\n\npipeline_space = dict(\n    float1=neps.Float(lower=0, upper=1),\n    float2=neps.Float(lower=-10, upper=10),\n    categorical=neps.Categorical(choices=[0, 1]),\n    integer1=neps.Integer(lower=0, upper=1),\n    integer2=neps.Integer(lower=1, upper=1000, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/logging_additional_info\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/convenience/neps_tblogger_tutorial/","title":"Neps tblogger tutorial","text":"<pre><code>\"\"\"\nNePS tblogger With TensorBoard\n==============================\n\n1- Introduction\n---------------\nWelcome to the NePS tblogger with TensorBoard tutorial. This guide will walk you\nthrough the process of using the NePS tblogger class to monitor performance\ndata for different hyperparameter configurations during optimization.\n\nAssuming you have experience with NePS, this tutorial aims to showcase the power\nof visualization using tblogger. To go directly to that part, check lines 244-264\nor search for 'Start Tensorboard Logging'.\n\n2- Learning Objectives\n----------------------\nBy completing this tutorial, you will:\n\n- Understand the role of NePS tblogger in HPO and NAS.\n- Learn to define search spaces within NePS for different model configurations.\n- Build a comprehensive run pipeline to train and evaluate models.\n- Utilize TensorBoard to visualize and compare performance metrics of different\n  model configurations.\n\n3- Setup\n--------\nBefore we begin, ensure you have the necessary dependencies installed. To install\nthe 'NePS' package, use the following command:\n\n```bash\npip install neural-pipeline-search\n```\n\"\"\"\n\nimport logging\nimport random\nimport time\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.transforms import transforms\n\nimport neps\nfrom neps.plot.tensorboard_eval import tblogger\n\n\"\"\"\nSteps for a successful training pipeline:\n\n#1 Define the seeds for reproducibility.\n#2 Prepare the input data.\n#3 Design the model.\n#4 Design the pipeline search spaces.\n#5 Design the evaluate pipeline function.\n#6 Use neps.run the run the entire search using your specified optimizer.\n\nEach step will be covered in detail throughout the code\n\n\"\"\"\n\n#############################################################\n# Definig the seeds for reproducibility\n\n\ndef set_seed(seed=123):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\n#############################################################\n# Prepare the input data. For this tutorial we use the MNIST dataset.\n\n\ndef MNIST(\n    batch_size: int = 256,\n    n_train_size: float = 0.9,\n    data_reduction_factor: float = 0.5,\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    # Download MNIST training and test datasets if not already downloaded.\n    train_dataset = torchvision.datasets.MNIST(\n        root=\".data\", train=True, transform=transforms.ToTensor(), download=True\n    )\n    test_dataset = torchvision.datasets.MNIST(\n        root=\".data\", train=False, transform=transforms.ToTensor(), download=True\n    )\n\n    # Determine the size of the reduced training dataset for faster training\n    # and calculate the size of the training subset from the reduced dataset\n    reduced_dataset_train = int(data_reduction_factor * len(train_dataset))\n    train_size = int(n_train_size * reduced_dataset_train)\n\n    # Create a random sampler for the training and validation data\n    train_sampler = SubsetRandomSampler(range(train_size))\n    valid_sampler = SubsetRandomSampler(range(train_size, reduced_dataset_train))\n\n    # Create DataLoaders for training, validation, and test datasets.\n    train_dataloader = DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        sampler=train_sampler,\n    )\n    val_dataloader = DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        sampler=valid_sampler,\n    )\n    test_dataloader = DataLoader(\n        dataset=test_dataset, batch_size=batch_size, shuffle=False\n    )\n\n    return train_dataloader, val_dataloader, test_dataloader\n\n\n#############################################################\n# Design small MLP model to be able to represent the input data.\n\n\nclass MLP(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.linear1 = nn.Linear(in_features=784, out_features=196)\n        self.linear2 = nn.Linear(in_features=196, out_features=98)\n        self.linear3 = nn.Linear(in_features=98, out_features=10)\n\n    def forward(self, x: torch.Tensor):\n        # Flattening the grayscaled image from 1x28x28 (CxWxH) to 784.\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.linear1(x))\n        x = F.relu(self.linear2(x))\n        x = self.linear3(x)\n\n        return x\n\n\n#############################################################\n# Define the training step. Return the validation error and\n# misclassified images.\n\n\ndef objective_to_minimize_ev(model: nn.Module, data_loader: DataLoader) -&gt; float:\n    # Set the model in evaluation mode (no gradient computation).\n    model.eval()\n\n    correct = 0\n    total = 0\n\n    # Disable gradient computation for efficiency.\n    with torch.no_grad():\n        for x, y in data_loader:\n            output = model(x)\n\n            # Get the predicted class for each input.\n            _, predicted = torch.max(output.data, 1)\n\n            # Update the correct and total counts.\n            correct += (predicted == y).sum().item()\n            total += y.size(0)\n\n    # Calculate the accuracy and return the error rate.\n    accuracy = correct / total\n    error_rate = 1 - accuracy\n    return error_rate\n\n\ndef training(\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    criterion: nn.Module,\n    train_loader: DataLoader,\n    validation_loader: DataLoader,\n) -&gt; float:\n    \"\"\"\n    Function that trains the model for one epoch and evaluates the model\n    on the validation set.\n\n    Args:\n        model (nn.Module): Model to be trained.\n        optimizer (torch.optim.Optimizer): Optimizer used to train the weights.\n        criterion (nn.Module) : Loss function to use.\n        train_loader (DataLoader): DataLoader containing the training data.\n        validation_loader (DataLoader): DataLoader containing the validation data.\n\n    Returns:\n    float: The validation error (float)\n    \"\"\"\n    model.train()\n\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        output = model(x)\n        objective_to_minimize = criterion(output, y)\n        objective_to_minimize.backward()\n        optimizer.step()\n\n    # Calculate validation objective_to_minimize using the objective_to_minimize_ev function.\n    validation_objective_to_minimize = objective_to_minimize_ev(\n        model, validation_loader\n    )\n    return validation_objective_to_minimize\n\n\n#############################################################\n# Design the pipeline search spaces.\n\n\ndef pipeline_space() -&gt; dict:\n    pipeline = dict(\n        lr=neps.Float(lower=1e-5, upper=1e-1, log=True),\n        optim=neps.Categorical(choices=[\"Adam\", \"SGD\"]),\n        weight_decay=neps.Float(lower=1e-4, upper=1e-1, log=True),\n    )\n\n    return pipeline\n\n\n#############################################################\n# Implement the pipeline run search.\ndef evaluate_pipeline(lr, optim, weight_decay):\n    # Create the network model.\n    model = MLP()\n\n    if optim == \"Adam\":\n        optimizer = torch.optim.Adam(\n            model.parameters(), lr=lr, weight_decay=weight_decay\n        )\n    elif optim == \"SGD\":\n        optimizer = torch.optim.SGD(\n            model.parameters(), lr=lr, weight_decay=weight_decay\n        )\n    else:\n        raise ValueError(\n            \"Optimizer choices are defined differently in the pipeline_space\"\n        )\n\n    max_epochs = 2  # Epochs to train the model, can be parameterized as fidelity\n\n    # Load the MNIST dataset for training, validation, and testing.\n    train_loader, validation_loader, test_loader = MNIST(\n        batch_size=96, n_train_size=0.6, data_reduction_factor=0.75\n    )\n\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.75)\n    criterion = nn.CrossEntropyLoss()\n\n    # Substitute the Tensorboard SummaryWriter with ConfigWriter from NePS\n    # writer = SummaryWriter()\n    writer = tblogger.ConfigWriter(write_summary_incumbent=True)\n\n    for i in range(max_epochs):\n        objective_to_minimize = training(\n            optimizer=optimizer,\n            model=model,\n            criterion=criterion,\n            train_loader=train_loader,\n            validation_loader=validation_loader,\n        )\n\n        # Gathering the gradient mean in each layer\n        mean_gradient = []\n        for layer in model.children():\n            layer_gradients = [param.grad for param in layer.parameters()]\n            if layer_gradients:\n                mean_gradient.append(\n                    torch.mean(torch.cat([grad.view(-1) for grad in layer_gradients]))\n                )\n\n        ###################### Start Tensorboard Logging ######################\n        # 1. Loss curves of each configuration at each epoch.\n        # 2. Decay curve of the learning rate at each epoch.\n        # 3. First two layer gradients passed as scalar configs.\n\n        writer.add_scalar(tag=\"loss\", scalar_value=objective_to_minimize, global_step=i)\n        writer.add_scalar(\n            tag=\"lr_decay\", scalar_value=scheduler.get_last_lr()[0], global_step=i\n        )\n        writer.add_scalar(\n            tag=\"layer_gradient1\", scalar_value=mean_gradient[0], global_step=i\n        )\n        writer.add_scalar(\n            tag=\"layer_gradient2\", scalar_value=mean_gradient[1], global_step=i\n        )\n\n        scheduler.step()\n\n        print(f\"  Epoch {i + 1} / {max_epochs} Val Error: {objective_to_minimize} \")\n\n    # 4. Hparams comparison.\n    writer.add_hparams(\n        hparam_dict={\"lr\": lr, \"optim\": optim, \"wd\": weight_decay},\n        metric_dict={\"loss_val\": objective_to_minimize},\n    )\n    writer.close()\n\n    ###################### End Tensorboard Logging ######################\n\n    train_accuracy = objective_to_minimize_ev(model, train_loader)\n    test_accuracy = objective_to_minimize_ev(model, test_loader)\n\n    # Return a dictionary with relevant metrics and information.\n    return {\n        \"objective_to_minimize\": objective_to_minimize,\n        \"info_dict\": {\n            \"train_accuracy\": train_accuracy,\n            \"test_accuracy\": test_accuracy,\n            \"cost\": max_epochs,\n        },\n    }\n\n\n#############################################################\n# Running neps with BO as the optimizer.\n\nif __name__ == \"__main__\":\n    \"\"\"\n    When running this code without any arguments, it will by default\n    run bayesian optimization with 3 evaluations total.\n\n    ```bash\n    python neps_examples\\convenience\\neps_tblogger_tutorial.py\n    ```\n    \"\"\"\n    start_time = time.time()\n\n    set_seed(112)\n    logging.basicConfig(level=logging.INFO)\n\n    run_args = dict(\n        evaluate_pipeline=evaluate_pipeline,\n        pipeline_space=pipeline_space(),\n        root_directory=\"results/neps_tblogger_example\",\n        optimizer=\"random_search\",\n    )\n\n    neps.run(\n        **run_args,\n        max_evaluations_total=3,\n    )\n\n    \"\"\"\n    To check live plots during this search, please open a new terminal\n    and make sure to be at the same level directory of your project and\n    run the following command on the file created by neps root_directory.\n    Running both from root-directory, the command would be:\n\n    ```bash:\n    tensorboard --logdir results\\neps_tblogger_example\n    ```\n\n    To be able to check the visualization of tensorboard make sure to\n    follow the local link provided.\n\n    http://localhost:6006/\n    \"\"\"\n\n    end_time = time.time()  # Record the end time\n    execution_time = end_time - start_time\n    logging.info(f\"Execution time: {execution_time} seconds\\n\")\n</code></pre>"},{"location":"examples/convenience/running_on_slurm_scripts/","title":"Running on slurm scripts","text":"<pre><code>\"\"\"Example that shows HPO with NePS based on a slurm script.\"\"\"\n\nimport logging\nimport os\nimport time\nfrom pathlib import Path\n\nimport neps\n\n\ndef _ask_to_submit_slurm_script(pipeline_directory: Path, script: str):\n    script_path = pipeline_directory / \"submit.sh\"\n    logging.info(f\"Submitting the script {script_path} (see below): \\n\\n{script}\")\n\n    # You may want to remove the below check and not ask before submitting every time\n    if input(\"Ok to submit? [Y|n] -- \").lower() in {\"y\", \"\"}:\n        script_path.write_text(script)\n        os.system(f\"sbatch {script_path}\")\n    else:\n        raise ValueError(\"We generated a slurm script that should not be submitted.\")\n\n\ndef _get_validation_error(pipeline_directory: Path):\n    validation_error_file = pipeline_directory / \"validation_error_from_slurm_job.txt\"\n    if validation_error_file.exists():\n        return float(validation_error_file.read_text())\n    return None\n\n\ndef evaluate_pipeline_via_slurm(\n    pipeline_directory: Path, optimizer: str, learning_rate: float\n):\n    script = f\"\"\"#!/bin/bash\n#SBATCH --time 0-00:05\n#SBATCH --job-name test\n#SBATCH --partition cpu-cascadelake\n#SBATCH --error \"{pipeline_directory}/%N_%A_%x_%a.oe\"\n#SBATCH --output \"{pipeline_directory}/%N_%A_%x_%a.oe\"\n# Plugin your python script here\npython -c \"print('Learning rate {learning_rate} and optimizer {optimizer}')\"\n# At the end of training and validation create this file\necho -10 &gt; {pipeline_directory}/validation_error_from_slurm_job.txt\n\"\"\"\n\n    # Now we submit and wait until the job has created validation_error_from_slurm_job.txt\n    _ask_to_submit_slurm_script(pipeline_directory, script)\n    while validation_error := _get_validation_error(pipeline_directory) is None:\n        logging.info(\"Waiting until the job has finished.\")\n        time.sleep(60)  # Adjust to something reasonable\n    return validation_error\n\n\npipeline_space = dict(\n    optimizer=neps.Categorical(choices=[\"sgd\", \"adam\"]),\n    learning_rate=neps.Float(lower=10e-7, upper=10e-3, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline_via_slurm,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/slurm_script_example\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/convenience/working_directory_per_pipeline/","title":"Working directory per pipeline","text":"<pre><code>import logging\nfrom pathlib import Path\nfrom warnings import warn\n\nimport numpy as np\n\nimport neps\n\n\ndef evaluate_pipeline(pipeline_directory: Path, float1, categorical, integer1):\n    # When adding pipeline_directory to evaluate_pipeline, neps detects its presence and\n    # passes a directory unique for each pipeline configuration. You can then use this\n    # pipeline_directory to create / save files pertaining to a specific pipeline, e.g.:\n    pipeline_info = pipeline_directory / \"info_file.txt\"\n    pipeline_info.write_text(f\"{float1} - {categorical} - {integer1}\")\n\n    objective_to_minimize = -float(np.sum([float1, int(categorical), integer1]))\n    return objective_to_minimize\n\n\npipeline_space = dict(\n    float1=neps.Float(lower=0, upper=1),\n    categorical=neps.Categorical(choices=[0, 1]),\n    integer1=neps.Integer(lower=0, upper=1),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/working_directory_per_pipeline\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/efficiency/","title":"Parallelization","text":"<p>In order to run neps in parallel on multiple processes or multiple machines, simply call <code>neps.run</code> multiple times. All calls to <code>neps.run</code> need to use the same <code>root_directory</code> on the same filesystem to synchronize between the <code>neps.run</code>'s.</p> <p>For example, start the HPO example in two shells from the same directory as below.</p> <p>In shell 1:</p> <pre><code>python -m neps_examples.basic_usage.hyperparameters\n</code></pre> <p>In shell 2:</p> <pre><code>python -m neps_examples.basic_usage.hyperparameters\n</code></pre>"},{"location":"examples/efficiency/expert_priors_for_hyperparameters/","title":"Expert priors for hyperparameters","text":"<pre><code>import logging\nimport time\n\nimport neps\n\n\ndef evaluate_pipeline(some_float, some_integer, some_cat):\n    start = time.time()\n    if some_cat != \"a\":\n        y = some_float + some_integer\n    else:\n        y = -some_float - some_integer\n    end = time.time()\n    return {\n        \"objective_to_minimize\": y,\n        \"info_dict\": {\n            \"test_score\": y,\n            \"train_time\": end - start,\n        },\n    }\n\n\n# neps uses the default values and a confidence in this default value to construct a prior\n# that speeds up the search\npipeline_space = dict(\n    some_float=neps.Float(\n        lower=1,\n        upper=1000,\n        log=True,\n        prior=900,\n        prior_confidence=\"medium\",\n    ),\n    some_integer=neps.Integer(\n        lower=0,\n        upper=50,\n        prior=35,\n        prior_confidence=\"low\",\n    ),\n    some_cat=neps.Categorical(\n        choices=[\"a\", \"b\", \"c\"],\n        prior=\"a\",\n        prior_confidence=\"high\",\n    ),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/user_priors_example\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/efficiency/multi_fidelity/","title":"Multi fidelity","text":"<pre><code>import logging\n\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nimport neps\n\n\nclass TheModelClass(nn.Module):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef get_model_and_optimizer(learning_rate):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n    model = TheModelClass()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    return model, optimizer\n\n\n# Important: Include the \"pipeline_directory\" and \"previous_pipeline_directory\" arguments\n# in your evaluate_pipeline function. This grants access to NePS's folder system and is\n# critical for leveraging efficient multi-fidelity optimization strategies.\n# For more details, refer to the working_directory_per_pipeline example in convenience.\n\n\ndef evaluate_pipeline(\n    pipeline_directory: Path,  # The path associated with this configuration\n    previous_pipeline_directory: Path\n    | None,  # The path associated with any previous config\n    learning_rate: float,\n    epoch: int,\n) -&gt; dict:\n    model, optimizer = get_model_and_optimizer(learning_rate)\n    checkpoint_name = \"checkpoint.pth\"\n\n    if previous_pipeline_directory is not None:\n        # Read in state of the model after the previous fidelity rung\n        checkpoint = torch.load(previous_pipeline_directory / checkpoint_name)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        epochs_previously_spent = checkpoint[\"epoch\"]\n    else:\n        epochs_previously_spent = 0\n\n    # Train model here ...\n\n    # Save model to disk\n    torch.save(\n        {\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        },\n        pipeline_directory / checkpoint_name,\n    )\n\n    objective_to_minimize = np.log(learning_rate / epoch)  # Replace with actual error\n    epochs_spent_in_this_call = epoch - epochs_previously_spent  # Optional for stopping\n    return dict(\n        objective_to_minimize=objective_to_minimize, cost=epochs_spent_in_this_call\n    )\n\n\npipeline_space = dict(\n    learning_rate=neps.Float(lower=1e-4, upper=1e0, log=True),\n    epoch=neps.Integer(lower=1, upper=10, is_fidelity=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/multi_fidelity_example\",\n    # Optional: Do not start another evaluation after &lt;=50 epochs, corresponds to cost\n    # field above.\n    max_cost_total=50,\n)\n</code></pre>"},{"location":"examples/efficiency/multi_fidelity_and_expert_priors/","title":"Multi fidelity and expert priors","text":"<pre><code>import logging\n\nimport numpy as np\nimport neps\n\n# This example demonstrates NePS uses both fidelity and expert priors to\n# optimize hyperparameters of a pipeline.\n\ndef evaluate_pipeline(float1, float2, integer1, fidelity):\n    objective_to_minimize = -float(np.sum([float1, float2, integer1])) / fidelity\n    return objective_to_minimize\n\n\npipeline_space = dict(\n    float1=neps.Float(\n        lower=1,\n        upper=1000,\n        log=False,\n        prior=600,\n        prior_confidence=\"medium\",\n    ),\n    float2=neps.Float(\n        lower=-10,\n        upper=10,\n        prior=0,\n        prior_confidence=\"medium\",\n    ),\n    integer1=neps.Integer(\n        lower=0,\n        upper=50,\n        prior=35,\n        prior_confidence=\"low\",\n    ),\n    fidelity=neps.Integer(\n        lower=1,\n        upper=10,\n        is_fidelity=True,\n    ),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/multifidelity_priors\",\n    max_evaluations_total=25,  # For an alternate stopping method see multi_fidelity.py\n)\n</code></pre>"},{"location":"examples/efficiency/pytorch_lightning_ddp/","title":"Pytorch lightning ddp","text":"<pre><code>import logging\n\nimport lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nimport neps\n\nNUM_GPU = 8  # Number of GPUs to use for DDP\n\n\nclass ToyModel(nn.Module):\n    \"\"\" Taken from https://pytorch.org/tutorials/intermediate/ddp_tutorial.html \"\"\"\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\nclass LightningModel(L.LightningModule):\n    def __init__(self, lr):\n        super().__init__()\n        self.lr = lr\n        self.model = ToyModel()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.mse_loss(y_hat, y)\n        self.log(\"train_loss\", loss, prog_bar=True, sync_dist=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.mse_loss(y_hat, y)\n        self.log(\"val_loss\", loss, prog_bar=True, sync_dist=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.mse_loss(y_hat, y)\n        self.log(\"test_loss\", loss, prog_bar=True, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=self.lr)\n\ndef evaluate_pipeline(lr=0.1, epoch=20):\n    L.seed_everything(42)\n    # Model\n    model = LightningModel(lr=lr)\n\n    # Generate random tensors for data and labels\n    data = torch.rand((1000, 10))\n    labels = torch.rand((1000, 5))\n\n    dataset = list(zip(data, labels))\n\n    train_dataset, val_dataset, test_dataset = random_split(dataset, [600, 200, 200])\n\n    # Define simple data loaders using tensors and slicing\n    train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n    test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n\n    # Trainer with DDP Strategy\n    trainer = L.Trainer(gradient_clip_val=0.25,\n                        max_epochs=epoch,\n                        fast_dev_run=False,\n                        strategy='ddp',\n                        devices=NUM_GPU\n                        )\n    trainer.fit(model, train_dataloader, val_dataloader)\n    trainer.validate(model, test_dataloader)\n    return trainer.logged_metrics[\"val_loss\"].item()\n\n\npipeline_space = dict(\n    lr=neps.Float(\n        lower=0.001,\n        upper=0.1,\n        log=True,\n        prior=0.01\n        ),\n    epoch=neps.Integer(\n        lower=1,\n        upper=3,\n        is_fidelity=True\n        )\n    )\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    evaluate_pipeline=evaluate_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/pytorch_lightning_ddp\",\n    max_evaluations_total=5\n    )\n</code></pre>"},{"location":"examples/efficiency/pytorch_lightning_fsdp/","title":"Pytorch lightning fsdp","text":"<pre><code>\"\"\"Based on: https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html\n\nMind that this example does not run on Windows at the moment.\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\nfrom lightning.pytorch.demos import Transformer, WikiText2\n\n\nclass LanguageModel(L.LightningModule):\n    def __init__(self, vocab_size, lr):\n        super().__init__()\n        self.model = Transformer(  # 1B parameters\n            vocab_size=vocab_size,\n            nlayers=32,\n            nhid=4096,\n            ninp=1024,\n            nhead=64,\n        )\n        self.lr = lr\n\n    def training_step(self, batch):\n        input, target = batch\n        output = self.model(input, target)\n        loss = F.nll_loss(output, target.view(-1))\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\n\ndef evaluate_pipeline(lr=0.1, epoch=20):\n    L.seed_everything(42)\n\n    # Data\n    dataset = WikiText2()\n    train_dataloader = DataLoader(dataset)\n\n    # Model\n    model = LanguageModel(vocab_size=dataset.vocab_size, lr=lr)\n\n    # Trainer\n    trainer = L.Trainer(accelerator=\"cuda\", strategy=FSDPStrategy())\n    trainer.fit(model, train_dataloader, max_epochs=epoch)\n    return trainer.logged_metrics[\"train_loss\"].detach().item()\n\n\nif __name__ == \"__main__\":\n    import neps\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n\n    pipeline_space = dict(\n        lr=neps.Float(\n            lower=0.0001,\n            upper=0.1,\n            log=True,\n            prior=0.01\n            ),\n        epoch=neps.Integer(\n            lower=1,\n            upper=3,\n            is_fidelity=True\n            )\n        )\n\n    neps.run(\n        evaluate_pipeline=evaluate_pipeline,\n        pipeline_space=pipeline_space,\n        root_directory=\"results/pytorch_lightning_fsdp\",\n        max_evaluations_total=5\n        )\n</code></pre>"},{"location":"examples/efficiency/pytorch_native_ddp/","title":"Pytorch native ddp","text":"<pre><code>\"\"\" Some parts of this code are taken from https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n\nMind that this example does not run on Windows at the moment.\"\"\"\n\nimport os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nimport neps\nimport logging\n\nNUM_GPU = 8  # Number of GPUs to use for DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\nclass ToyModel(nn.Module):\n    \"\"\" Taken from https://pytorch.org/tutorials/intermediate/ddp_tutorial.html \"\"\"\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic(rank, world_size, loss_dict, learning_rate, epochs):\n    \"\"\" Taken from https://pytorch.org/tutorials/intermediate/ddp_tutorial.html (modified)\"\"\"\n    print(f\"Running basic DDP example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate)\n\n    total_loss = 0.0\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = ddp_model(torch.randn(20, 10))\n        labels = torch.randn(20, 5).to(rank)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        if rank == 0:\n            print(f\"Epoch {epoch} complete\")\n\n    loss_dict[rank] = total_loss\n\n    cleanup()\n    print(f\"Finished running basic DDP example on rank {rank}.\")\n\n\ndef evaluate_pipeline(learning_rate, epochs):\n    from torch.multiprocessing import Manager\n    world_size = NUM_GPU  # Number of GPUs\n\n    manager = Manager()\n    loss_dict = manager.dict()\n\n    mp.spawn(demo_basic,\n             args=(world_size, loss_dict, learning_rate, epochs),\n             nprocs=world_size,\n             join=True)\n\n    loss = sum(loss_dict.values()) // world_size\n    return {'loss': loss}\n\n\npipeline_space = dict(\n    learning_rate=neps.Float(lower=10e-7, upper=10e-3, log=True),\n    epochs=neps.Integer(lower=1, upper=3)\n)\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO)\n    neps.run(evaluate_pipeline=evaluate_pipeline,\n             pipeline_space=pipeline_space,\n             root_directory=\"results/pytorch_ddp\",\n             max_evaluations_total=25)\n</code></pre>"},{"location":"examples/efficiency/pytorch_native_fsdp/","title":"Pytorch native fsdp","text":"<pre><code>\"\"\"Based on: https://github.com/pytorch/examples/blob/master/mnist/main.py\n\nMind that this example does not run on Windows at the moment.\"\"\"\n\nimport math\nimport os\nimport functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\nfrom torch.optim.lr_scheduler import StepLR\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nfrom torch.distributed.fsdp.wrap import (\n    size_based_auto_wrap_policy,\n)\n\nNUM_GPU = 8 # Number of GPUs to use for FSDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    ddp_loss = torch.zeros(2).to(rank)\n    if sampler:\n        sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(rank), target.to(rank)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='sum')\n        loss.backward()\n        optimizer.step()\n        ddp_loss[0] += loss.item()\n        ddp_loss[1] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n    if rank == 0:\n        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n\ndef test(model, rank, world_size, test_loader):\n    model.eval()\n    correct = 0\n    ddp_loss = torch.zeros(3).to(rank)\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(rank), target.to(rank)\n            output = model(data)\n            ddp_loss[0] += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()\n            ddp_loss[2] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n\n    test_loss = math.inf\n    if rank == 0:\n        test_loss = ddp_loss[0] / ddp_loss[2]\n        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),\n            100. * ddp_loss[1] / ddp_loss[2]))\n    return test_loss\n\ndef fsdp_main(rank, world_size, test_loss_tensor, lr, epochs, save_model=False):\n    setup(rank, world_size)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    dataset1 = datasets.MNIST('./', train=True, download=True,\n                        transform=transform)\n    dataset2 = datasets.MNIST('./', train=False,\n                        transform=transform)\n\n    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)\n\n    train_kwargs = {'batch_size': 64, 'sampler': sampler1}\n    test_kwargs = {'batch_size': 1000, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n    my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=100\n    )\n    torch.cuda.set_device(rank)\n\n\n    init_start_event = torch.cuda.Event(enable_timing=True)\n    init_end_event = torch.cuda.Event(enable_timing=True)\n\n    model = Net().to(rank)\n\n    model = FSDP(model)\n\n    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n    init_start_event.record()\n\n    test_loss = math.inf\n    for epoch in range(1, epochs + 1):\n        train(model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        # calculate test loss for this epoch\n        test_loss = test(model, rank, world_size, test_loader)\n\n        scheduler.step()\n\n    if rank == 0:\n        test_loss_tensor[0] = test_loss\n\n    init_end_event.record()\n\n    if rank == 0:\n        init_end_event.synchronize()\n        print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec\")\n\n    if save_model:\n        # use a barrier to make sure training is done on all ranks\n        dist.barrier()\n        states = model.state_dict()\n        if rank == 0:\n            torch.save(states, \"mnist_cnn.pt\")\n    cleanup()\n\ndef evaluate_pipeline(lr=0.1, epoch=20):\n    torch.manual_seed(42)\n\n    test_loss_tensor = torch.zeros(1)\n    test_loss_tensor.share_memory_()\n\n    mp.spawn(fsdp_main,\n        args=(NUM_GPU, test_loss_tensor, lr, epoch),\n        nprocs=NUM_GPU,\n        join=True)\n\n    loss = test_loss_tensor.item()\n    return loss\n\n\nif __name__ == \"__main__\":\n    import neps\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n\n    pipeline_space = dict(\n        lr=neps.Float(\n            lower=0.0001,\n            upper=0.1,\n            log=True,\n            prior=0.01\n            ),\n        epoch=neps.Integer(\n            lower=1,\n            upper=3,\n            is_fidelity=True\n            )\n        )\n\n    neps.run(\n        evaluate_pipeline=evaluate_pipeline,\n        pipeline_space=pipeline_space,\n        root_directory=\"results/pytorch_fsdp\",\n        max_evaluations_total=20\n        )\n</code></pre>"},{"location":"examples/experimental/freeze_thaw/","title":"Freeze thaw","text":"<pre><code>import logging\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport neps\nfrom neps import tblogger\nfrom neps.plot.plot3D import Plotter3D\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, num_layers, num_neurons):\n        super().__init__()\n        layers = [nn.Flatten()]\n\n        for _ in range(num_layers):\n            layers.append(nn.Linear(input_size, num_neurons))\n            layers.append(nn.ReLU())\n            input_size = num_neurons  # Set input size for the next layer\n\n        layers.append(nn.Linear(num_neurons, 10))  # Output layer for 10 classes\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndef training_pipeline(\n    pipeline_directory,\n    previous_pipeline_directory,\n    num_layers,\n    num_neurons,\n    epochs,\n    learning_rate,\n    weight_decay,\n):\n    \"\"\"\n    Trains and validates a simple neural network on the MNIST dataset.\n\n    Args:\n        num_layers (int): Number of hidden layers in the network.\n        num_neurons (int): Number of neurons in each hidden layer.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate for the optimizer.\n        optimizer (str): Name of the optimizer to use ('adam' or 'sgd').\n\n    Returns:\n        float: The average objective_to_minimize over the validation set after training.\n\n    Raises:\n        KeyError: If the specified optimizer is not supported.\n    \"\"\"\n    # Transformations applied on each image\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                (0.1307,), (0.3081,)\n            ),  # Mean and Std Deviation for MNIST\n        ]\n    )\n\n    # Loading MNIST dataset\n    dataset = datasets.MNIST(\n        root=\"./.data\", train=True, download=True, transform=transform\n    )\n    train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=1000, shuffle=False)\n\n    model = SimpleNN(28 * 28, num_layers, num_neurons)\n    criterion = nn.CrossEntropyLoss()\n\n    # Select optimizer\n    optimizer = optim.AdamW(\n        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n    )\n\n    # Loading potential checkpoint\n    start_epoch = 1\n    if previous_pipeline_directory is not None:\n        if (Path(previous_pipeline_directory) / \"checkpoint.pt\").exists():\n            states = torch.load(\n                Path(previous_pipeline_directory) / \"checkpoint.pt\",\n                weights_only=False\n            )\n            model = states[\"model\"]\n            optimizer = states[\"optimizer\"]\n            start_epoch = states[\"epochs\"]\n\n    # Training loop\n    for epoch in range(start_epoch, epochs + 1):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            objective_to_minimize = criterion(output, target)\n            objective_to_minimize.backward()\n            optimizer.step()\n\n    # Validation loop\n    model.eval()\n    val_objective_to_minimize = 0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            output = model(data)\n            val_objective_to_minimize += criterion(output, target).item()\n\n            # Get the predicted class\n            _, predicted = torch.max(output.data, 1)\n\n            # Count correct predictions\n            val_total += target.size(0)\n            val_correct += (predicted == target).sum().item()\n\n    val_objective_to_minimize /= len(val_loader.dataset)\n    val_err = 1 - val_correct / val_total\n\n    # Saving checkpoint\n    states = {\n        \"model\": model,\n        \"optimizer\": optimizer,\n        \"epochs\": epochs,\n    }\n    torch.save(states, Path(pipeline_directory) / \"checkpoint.pt\")\n\n    # Logging\n    # tblogger.log(\n    #     objective_to_minimize=val_objective_to_minimize,\n    #     current_epoch=epochs,\n    #     # Set to `True` for a live incumbent trajectory.\n    #     write_summary_incumbent=True,\n    #     # Set to `True` for a live objective_to_minimize trajectory for each config.\n    #     writer_config_scalar=True,\n    #     # Set to `True` for live parallel coordinate, scatter plot matrix, and table view.\n    #     writer_config_hparam=True,\n    #     # Appending extra data\n    #     extra_data={\n    #         \"train_objective_to_minimize\": tblogger.scalar_logging(\n    #             objective_to_minimize.item()\n    #         ),\n    #         \"val_err\": tblogger.scalar_logging(val_err),\n    #     },\n    # )\n\n    return val_err\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n\n    pipeline_space = {\n        \"learning_rate\": neps.Float(1e-5, 1e-1, log=True),\n        \"num_layers\": neps.Integer(1, 5),\n        \"num_neurons\": neps.Integer(64, 128),\n        \"weight_decay\": neps.Float(1e-5, 0.1, log=True),\n        \"epochs\": neps.Integer(1, 10, is_fidelity=True),\n    }\n\n    neps.run(\n        pipeline_space=pipeline_space,\n        evaluate_pipeline=training_pipeline,\n        optimizer=\"ifbo\",\n        max_evaluations_total=50,\n        root_directory=\"./results/ifbo-mnist/\",\n        overwrite_working_directory=False,  # set to False for a multi-worker run\n    )\n\n    # NOTE: this is `experimental` and may not work as expected\n    ## plotting a 3D plot for learning curves explored by ifbo\n    plotter = Plotter3D(\n        run_path=\"./results/ifbo-mnist/\",  # same as `root_directory` above\n        fidelity_key=\"epochs\",  # same as `pipeline_space`\n    )\n    plotter.plot3D(filename=\"ifbo\")\n</code></pre>"},{"location":"examples/real_world/","title":"Real World Examples","text":"<ol> <li>Image Segmentation Pipeline Hyperparameter Optimization</li> </ol> <p>This example demonstrates how to perform hyperparameter optimization (HPO) for an image segmentation pipeline using NePS. The pipeline consists of a ResNet-50 model to segment images model trained on PASCAL Visual Object Classes (VOC) Dataset (host.robots.ox.ac.uk/pascal/VOC/).</p> <p>We compare the performance of the optimized hyperparameters with the default hyperparameters. using the validation loss achieved on the dataset after training the model with the respective hyperparameters.</p> <pre><code>python image_segmentation_pipeline_hpo.py\n</code></pre> <p>The search space has been set with the priors set to the hyperparameters found in this base example: lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning</p> <p>We run the HPO process for 188 trials and obtain new set of hyperpamereters that outperform the default hyperparameters.</p> Hyperparameter Prior Optimized Value learning_rate 0.02 0.006745150778442621 batch_size 4 5 momentum 0.5 0.5844767093658447 weight_decay 0.0001 0.00012664785026572645 <p></p> <p>The validation loss achieved on the dataset after training the model with the newly sampled hyperparameters is shown in the figure above.</p> <p>We compare the validation loss values when the model is trained with the default hyperparameters and the optimized hyperparameters:</p> <p>Validation Loss with Default Hyperparameters: 0.114094577729702</p> <p>Validation Loss with Optimized Hyperparameters: 0.0997161939740181</p> <p>The optimized hyperparameters outperform the default hyperparameters by 12.61%.</p>"},{"location":"examples/real_world/image_segmentation_hpo/","title":"Image segmentation hpo","text":"<pre><code># Example pipeline used from; https://lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning\n\nimport os\n\nimport torch\nfrom torchvision import transforms, datasets, models\nimport lightning as L\nfrom lightning.pytorch.strategies import DDPStrategy\nfrom torch.optim.lr_scheduler import PolynomialLR\n\n\nclass LitSegmentation(L.LightningModule):\n    def __init__(self, iters_per_epoch, lr, momentum, weight_decay):\n        super().__init__()\n        self.model = models.segmentation.fcn_resnet50(num_classes=21, aux_loss=True)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n        self.iters_per_epoch = iters_per_epoch\n        self.lr = lr\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n\n    def training_step(self, batch):\n        images, targets = batch\n        outputs = self.model(images)['out']\n        loss = self.loss_fn(outputs, targets.long().squeeze(1))\n        self.log(\"train_loss\", loss, sync_dist=True)\n        return loss\n\n    def validation_step(self, batch):\n        images, targets = batch\n        outputs = self.model(images)['out']\n        loss = self.loss_fn(outputs, targets.long().squeeze(1))\n        self.log(\"val_loss\", loss, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n        scheduler = PolynomialLR(\n            optimizer, total_iters=self.iters_per_epoch * self.trainer.max_epochs, power=0.9\n        )\n        return [optimizer], [scheduler]\n\n\n\nclass SegmentationData(L.LightningDataModule):\n    def __init__(self, batch_size=4):\n        super().__init__()\n        self.batch_size = batch_size\n\n    def prepare_data(self):\n        dataset_path = \".data/VOC/VOCtrainval_11-May-2012.tar\"\n        if not os.path.exists(dataset_path):\n            datasets.VOCSegmentation(root=\".data/VOC\", download=True)\n\n    def train_dataloader(self):\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((256, 256), antialias=True),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        target_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((256, 256), antialias=True)])\n        train_dataset = datasets.VOCSegmentation(root=\".data/VOC\", transform=transform, target_transform=target_transform)\n        return torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=16, persistent_workers=True)\n\n    def val_dataloader(self):\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((256, 256), antialias=True),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        target_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((256, 256), antialias=True)])\n        val_dataset = datasets.VOCSegmentation(root=\".data/VOC\", year='2012', image_set='val', transform=transform, target_transform=target_transform)\n        return torch.utils.data.DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=16, persistent_workers=True)\n\n\ndef evaluate_pipeline(**kwargs):\n    data = SegmentationData(kwargs.get(\"batch_size\", 4))\n    data.prepare_data()\n    iters_per_epoch = len(data.train_dataloader())\n    model = LitSegmentation(iters_per_epoch, kwargs.get(\"lr\", 0.02), kwargs.get(\"momentum\", 0.9), kwargs.get(\"weight_decay\", 1e-4))\n    trainer = L.Trainer(max_epochs=kwargs.get(\"epoch\", 30), strategy=DDPStrategy(find_unused_parameters=True), enable_checkpointing=False)\n    trainer.fit(model, data)\n    val_loss = trainer.logged_metrics[\"val_loss\"].detach().item()\n    return val_loss\n\n\nif __name__ == \"__main__\":\n    import neps\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n\n    # Search space for hyperparameters\n    pipeline_space = dict(\n        lr=neps.Float(\n            lower=0.0001,\n            upper=0.1,\n            log=True,\n            prior=0.02\n            ),\n        momentum=neps.Float(\n            lower=0.1,\n            upper=0.9,\n            prior=0.5\n            ),\n        weight_decay=neps.Float(\n            lower=1e-5,\n            upper=1e-3,\n            log=True,\n            prior=1e-4\n            ),\n        epoch=neps.Integer(\n            lower=10,\n            upper=30,\n            is_fidelity=True\n            ),\n        batch_size=neps.Integer(\n            lower=4,\n            upper=12,\n            prior=4\n        ),\n    )\n\n    neps.run(\n        evaluate_pipeline=evaluate_pipeline,\n        pipeline_space=pipeline_space,\n        root_directory=\"results/hpo_image_segmentation\",\n        max_evaluations_total=500\n    )\n</code></pre>"},{"location":"reference/analyse/","title":"Analysing Runs","text":"<p>NePS has some convenient utilities to help you to understand the results after you've run your runs. All of the results and state are stored and communicated on disk, which you can access using the <code>python -m neps.status ROOT_DIRECTORY</code> command or integrate live logging directly into your training loop and visualize the results using TensorBoard.</p> <p>To get a quick overview of the results, you can use the <code>python -m neps.plot ROOT_DIRECTORY</code> command.</p>"},{"location":"reference/analyse/#status","title":"Status","text":"<p>To show status information about a neural pipeline search run, use</p> <pre><code>python -m neps.status ROOT_DIRECTORY\n</code></pre> <p>If you need more status information than is printed per default (e.g., the best config over time), please have a look at</p> <pre><code>python -m neps.status --help\n</code></pre> <p>Using <code>watch</code></p> <p>To show the status repeatedly, on unix systems you can use</p> <pre><code>watch --interval 30 python -m neps.status ROOT_DIRECTORY\n</code></pre>"},{"location":"reference/analyse/#cli-commands","title":"CLI commands","text":"<p>To generate plots to the root directory, run</p> <pre><code>python -m neps.plot ROOT_DIRECTORY\n</code></pre> <p>Currently, this creates one plot that shows the best error value across the number of evaluations.</p>"},{"location":"reference/analyse/#whats-on-disk","title":"What's on disk?","text":"<p>In the root directory, NePS maintains several files at all times that are human readable and can be useful If you pass the <code>post_run_summary=</code> argument to <code>neps.run()</code>, NePS will also generate a summary CSV file for you.</p> <code>neps.run(..., post_run_summary=True)</code><code>neps.run(..., post_run_summary=False)</code> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 report.yaml\n\u251c\u2500\u2500 summary\n\u2502  \u251c\u2500\u2500 full.csv\n\u2502  \u2514\u2500\u2500 short.csv\n\u251c\u2500\u2500 optimizer_info.yaml\n\u2514\u2500\u2500 optimizer_state.pkl\n</code></pre> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 report.yaml\n\u251c\u2500\u2500 optimizer_info.yaml\n\u2514\u2500\u2500 optimizer_state.pkl\n</code></pre> <p>The <code>full.csv</code> contains all configuration details in CSV format. Details include configuration hyperparameters and any returned result and cost from the <code>evaluate_pipeline</code> function.</p> <p>The <code>run_status.csv</code> provides general run details, such as the number of failed and successful configurations, and the best configuration with its corresponding objective value.</p>"},{"location":"reference/analyse/#tensorboard-integration","title":"TensorBoard Integration","text":"<p>In NePS we replaced the traditional TensorBoard <code>SummaryWriter</code> with the <code>ConfigWriter</code> to streamline the logging process. This integration enhances the ability to visualize and diagnose hyperparameter optimization workflows, providing detailed insights into metrics and configurations during training.</p>"},{"location":"reference/analyse/#overview-of-configwriter","title":"Overview of ConfigWriter","text":"<p>The <code>ConfigWriter</code> serves as a versatile and efficient tool for logging various training metrics and hyperparameter configurations. It seamlessly integrates with the NePS, enabling better visualization and analysis of model performance during hyperparameter searches.</p> <p>To enable live logging of the incumbent trajectory, use the <code>write_summary_incumbent</code> argument when initializing <code>ConfigWriter</code>.</p> <p>If a user only wishes to log the incumbent and does not want a specific writer for each configuration (i.e., no other logging in the run pipeline), they should simply trigger the <code>neps.tblogger.WriteIncumbent()</code> function in their run pipeline</p>"},{"location":"reference/analyse/#example-usage","title":"Example Usage","text":"<p>Below is an example implementation of the <code>ConfigWriter</code> for logging metrics during the training process:</p> <pre><code>import neps\n# Substitute the TensorBoard SummaryWriter with ConfigWriter from NePS\nwriter = neps.tblogger.ConfigWriter(write_summary_incumbent=True)\n\nfor i in range(max_epochs):\n    objective_to_minimize = training(\n        optimizer=optimizer,\n        model=model,\n        criterion=criterion,\n        train_loader=train_loader,\n        validation_loader=validation_loader,\n    )\n\n    # Gathering the gradient mean in each layer\n    mean_gradient = []\n    for layer in model.children():\n        layer_gradients = [param.grad for param in layer.parameters()]\n        if layer_gradients:\n            mean_gradient.append(\n                torch.mean(torch.cat([grad.view(-1) for grad in layer_gradients]))\n            )\n\n    ###################### Start ConfigWriter Logging ######################\n    writer.add_scalar(tag=\"loss\", scalar_value=objective_to_minimize, global_step=i)\n    writer.add_scalar(\n        tag=\"lr_decay\", scalar_value=scheduler.get_last_lr()[0], global_step=i\n    )\n    writer.add_scalar(\n        tag=\"layer_gradient1\", scalar_value=mean_gradient[0], global_step=i\n    )\n    writer.add_scalar(\n        tag=\"layer_gradient2\", scalar_value=mean_gradient[1], global_step=i\n    )\n\n    scheduler.step()\n\n    print(f\"  Epoch {i + 1} / {max_epochs} Val Error: {objective_to_minimize} \")\n\n# Logging hyperparameters and metrics\nwriter.add_hparams(\n    hparam_dict={\"lr\": lr, \"optim\": optim, \"wd\": weight_decay},\n    metric_dict={\"loss_val\": objective_to_minimize},\n)\nwriter.close()\n</code></pre>"},{"location":"reference/analyse/#visualizing-results","title":"Visualizing Results","text":"<p>The following command will open a local host for TensorBoard visualizations, allowing you to view them either in real-time or after the run is complete.</p> <pre><code>tensorboard --logdir path/to/root_directory\n</code></pre> <p>This image shows visualizations related to scalar values logged during training. Scalars typically include metrics such as loss, incumbent trajectory, a summary of losses for all configurations, and any additional data provided via the <code>extra_data</code> argument in the <code>tblogger.log</code> function.</p> <p></p> <p>This image represents visualizations related to logged images during training. It could include snapshots of input data, model predictions, or any other image-related information. In our case, we use images to depict instances of incorrect predictions made by the model.</p> <p></p> <p>The following images showcase visualizations related to hyperparameter logging in TensorBoard. These plots include three different views, providing insights into the relationship between different hyperparameters and their impact on the model.</p> <p>In the table view, you can explore hyperparameter configurations across five different trials. The table displays various hyperparameter values alongside corresponding evaluation metrics.</p> <p></p> <p>The parallel coordinate plot offers a holistic perspective on hyperparameter configurations. By presenting multiple hyperparameters simultaneously, this view allows you to observe the interactions between variables, providing insights into their combined influence on the model.</p> <p></p> <p>The scatter plot matrix view provides an in-depth analysis of pairwise relationships between different hyperparameters. By visualizing correlations and patterns, this view aids in identifying key interactions that may influence the model's performance.</p> <p></p>"},{"location":"reference/evaluate_pipeline/","title":"The evaluate function","text":""},{"location":"reference/evaluate_pipeline/#introduction","title":"Introduction","text":"<p>The <code>evaluate_pipeline=</code> function is crucial for NePS. It encapsulates the objective function to be minimized, which could range from a regular equation to a full training and evaluation pipeline for a neural network.</p> <p>This function receives the configuration to be utilized from the parameters defined in the search space. Consequently, it executes the same set of instructions or equations based on the provided configuration to minimize the objective function.</p> <p>We will show some basic usages and some functionalites this function would require for successful implementation.</p>"},{"location":"reference/evaluate_pipeline/#types-of-returns","title":"Types of Returns","text":""},{"location":"reference/evaluate_pipeline/#1-single-value","title":"1. Single Value","text":"<p>Assuming the <code>pipeline_space=</code> was already created (have a look at pipeline space for more details). A <code>evaluate_pipeline=</code> function with an objective of minimizing the loss will resemble the following:</p> <pre><code>def evaluate_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n\n    return loss\n</code></pre>"},{"location":"reference/evaluate_pipeline/#2-dictionary","title":"2. Dictionary","text":"<p>In this section, we will outline the special variables that are expected to be returned when the <code>evaluate_pipeline=</code> function returns a dictionary.</p>"},{"location":"reference/evaluate_pipeline/#loss","title":"Loss","text":"<p>One crucial return variable is the <code>loss</code>. This metric serves as a fundamental indicator for the optimizer. One option is to return a dictionary with the <code>loss</code> as a key, along with other user-chosen metrics.</p> <p>Note</p> <p>Loss can be any value that is to be minimized by the objective function.</p> <pre><code>def evaluate_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n    reverse_loss = -loss\n\n    return {\n        \"objective_to_minimize\": loss,\n        \"info_dict\": {\n            \"reverse_loss\": reverse_loss\n            ...\n        }\n    }\n</code></pre>"},{"location":"reference/evaluate_pipeline/#cost","title":"Cost","text":"<p>Along with the return of the <code>loss</code>, the <code>evaluate_pipeline=</code> function would optionally need to return a <code>cost</code> in certain cases. Specifically when the <code>max_cost_total</code> parameter is being utilized in the <code>neps.run</code> function.</p> <p>Note</p> <p><code>max_cost_total</code> sums the cost from all returned configuration results and checks whether the maximum allowed cost has been reached (if so, the search will come to an end).</p> <pre><code>import neps\nimport logging\n\n\ndef evaluate_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n    cost = 2\n\n    return {\n        \"objective_to_minimize\": loss,\n        \"cost\": cost,\n    }\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    neps.run(\n        evaluate_pipeline=evaluate_pipeline,\n        pipeline_space=pipeline_space, # Assuming the pipeline space is defined\n        root_directory=\"results/bo\",\n        max_cost_total=10,\n        optimizer=\"bayesian_optimization\",\n    )\n</code></pre> <p>Each evaluation carries a cost of 2. Hence in this example, the Bayesian optimization search is set to perform 5 evaluations.</p>"},{"location":"reference/evaluate_pipeline/#arguments-for-convenience","title":"Arguments for Convenience","text":"<p>NePS also provides the <code>pipeline_directory</code> and the <code>previous_pipeline_directory</code> as arguments in the <code>evaluate_pipeline=</code> function for user convenience.</p> <p>Regard an example to be run with a multi-fidelity optimizer, some checkpointing would be advantageous such that one does not have to train the configuration from scratch when the configuration qualifies to higher fidelity brackets.</p> <pre><code>def evaluate_pipeline(\n    pipeline_directory,           # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the immediate lower fidelity config\n    **config,                     # The hyperparameters to be used in the pipeline\n):\n    # Assume the third element is our fidelity element\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    fidelity = config[\"fidelity\"]\n\n    # Load any saved checkpoints\n    checkpoint_name = \"checkpoint.pth\"\n    start_fidelity = 0\n\n    if previous_pipeline_directory is not None:\n        # Read in state of the model after the previous fidelity rung\n        checkpoint = torch.load(previous_pipeline_directory / checkpoint_name)\n        prev_fidelity = checkpoint[\"fidelity\"]\n    else:\n        prev_fidelity = 0\n\n    start_fidelity += prev_fidelity\n\n    loss = 0\n    for i in range(start_fidelity, fidelity):\n        loss += element_1 - element_2\n\n    torch.save(\n        {\n            \"fidelity\": fidelity,\n        },\n        pipeline_directory / checkpoint_name,\n    )\n\n    return loss\n</code></pre> <p>This could allow the proper navigation to the trained models and further train them on higher fidelities without repeating the entire training process.</p>"},{"location":"reference/neps_run/","title":"Configuring and Running Optimizations","text":"<p>The <code>neps.run()</code> function is the core interface for running Hyperparameter and/or architecture search using optimizers in NePS. You can find most of the features NePS provides through the API of this function.</p> <p>This document breaks down the core arguments that allow users to control the optimization process in NePS.</p>"},{"location":"reference/neps_run/#required-arguments","title":"Required Arguments","text":"<p>To operate, NePS requires at minimum the following two arguments <code>neps.run(evaluate_pipeline=..., pipeline_space=...)</code>:</p> <pre><code>import neps\n\ndef evaluate_pipeline(learning_rate: float, epochs: int) -&gt; float:\n    # Your code here\n\n    return loss\n\nneps.run(\n    evaluate_pipeline=evaluate_pipeline, # (1)!\n    pipeline_space={, # (2)!\n        \"learning_rate\": neps.Float(1e-3, 1e-1, log=True),\n        \"epochs\": neps.Integer(10, 100)\n    },\n    root_directory=\"path/to/result_dir\" # (3)!\n)\n</code></pre> <ol> <li>The objective function, targeted by NePS for minimization, by evaluation various configurations.     It requires these configurations as input and should return either a dictionary or a sole loss value as the output.</li> <li>This defines the search space for the configurations from which the optimizer samples.     It accepts either a dictionary with the configuration names as keys, a path to a YAML configuration file, or a <code>configSpace.ConfigurationSpace</code> object.     For comprehensive information and examples, please refer to the detailed guide available here</li> <li>The directory path where the information about the optimization and its progress gets stored.     This is also used to synchronize multiple calls to <code>neps.run()</code> for parallelization.</li> </ol> <p>See the following for more:</p> <ul> <li>What kind of pipeline space can you define?</li> <li>What goes in and what goes out of <code>evaluate_pipeline()</code>?</li> </ul>"},{"location":"reference/neps_run/#budget-how-long-to-run","title":"Budget, how long to run?","text":"<p>To define a budget, provide <code>max_evaluations_total=</code> to <code>neps.run()</code>, to specify the total number of evaluations to conduct before halting the optimization process, or <code>max_cost_total=</code> to specify a cost threshold for your own custom cost metric, such as time, energy, or monetary, as returned by each evaluation of the pipeline .</p> <pre><code>def evaluate_pipeline(learning_rate: float, epochs: int) -&gt; float:\n    start = time.time()\n\n    # Your code here\n    end = time.time()\n    duration = end - start\n    return {\"objective_function_to_minimize\": loss, \"cost\": duration}\n\nneps.run(\n    max_evaluations_total=10, # (1)!\n    max_cost_total=1000, # (2)!\n)\n</code></pre> <ol> <li>Specifies the total number of evaluations to conduct before halting the optimization process.</li> <li>Prevents the initiation of new evaluations once this cost threshold is surpassed.     This can be any kind of cost metric you like, such as time, energy, or monetary, as long as you can calculate it.     This requires adding a cost value to the output of the <code>evaluate_pipeline</code> function, for example, return <code>{'objective_to_minimize': loss, 'cost': cost}</code>.     For more details, please refer here</li> </ol>"},{"location":"reference/neps_run/#getting-some-feedback-logging","title":"Getting some feedback, logging","text":"<p>NePS will not print anything to the console. To view the progress of workers, you can enable logging through python's logging.basicConfig.</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nneps.run(...)\n</code></pre> <p>Please refer to Python's logging documentation for more information on how to customize the logging output.</p>"},{"location":"reference/neps_run/#continuing-runs","title":"Continuing Runs","text":"<p>To continue a run, all you need to do is provide the same <code>root_directory=</code> to <code>neps.run()</code> as before, with an increased <code>max_evaluations_total=</code> or <code>max_cost_total=</code>.</p> <pre><code>def run(learning_rate: float, epochs: int) -&gt; float:\n    start = time.time()\n\n    # Your code here\n    end = time.time()\n    duration = end - start\n    return {\"objective_to_minimize\": loss, \"cost\": duration}\n\nneps.run(\n    # Increase the total number of trials from 10 as set previously to 50\n    max_evaluations_total=50,\n)\n</code></pre> <p>If the run previously stopped due to reaching a budget and you specify the same budget, the worker will immediatly stop as it will remember the amount of budget it used previously.</p>"},{"location":"reference/neps_run/#overwriting-a-run","title":"Overwriting a Run","text":"<p>To overwrite a run, simply provide the same <code>root_directory=</code> to <code>neps.run()</code> as before, with the <code>overwrite_working_directory=True</code> argument.</p> <pre><code>neps.run(\n    ...,\n    root_directory=\"path/to/previous_result_dir\",\n    overwrite_working_directory=True,\n)\n</code></pre> <p>Warning</p> <p>This will delete the folder specified by <code>root_directory=</code> and all its contents.</p>"},{"location":"reference/neps_run/#getting-the-results","title":"Getting the results","text":"<p>The results of the optimization process are stored in the <code>root_directory=</code> provided to <code>neps.run()</code>. To obtain a summary of the optimization process, you can enable the <code>post_run_summary=True</code> argument in <code>neps.run()</code>, while will generate a summary csv after the run has finished.</p> Result Directorypython <p>The root directory after utilizing this argument will look like the following:</p> <pre><code>root_directory\n\u251c\u2500\u2500 configs\n\u2502   \u251c\u2500\u2500 config_1\n\u2502   \u2502   \u251c\u2500\u2500 config.yaml     # The configuration\n\u2502   \u2502   \u251c\u2500\u2500 report.yaml     # The results of this run, if any\n\u2502   \u2502   \u2514\u2500\u2500 metadata.json   # Metadata about this run, such as state and times\n\u2502   \u2514\u2500\u2500 config_2\n\u2502       \u251c\u2500\u2500 config.yaml\n\u2502       \u2514\u2500\u2500 metadata.json\n\u251c\u2500\u2500 summary                 # Only if post_run_summary=True\n\u2502  \u251c\u2500\u2500 full.csv\n\u2502  \u2514\u2500\u2500 short.csv\n\u251c\u2500\u2500 optimizer_info.yaml     # The optimizer's configuration\n\u2514\u2500\u2500 optimizer_state.pkl     # The optimizer's state, shared between workers\n</code></pre> <pre><code>neps.run(..., post_run_summary=True)\n</code></pre> <p>To capture the results of the optimization process, you can use tensorbaord logging with various utilities to integrate closer to NePS. For more information, please refer to the analyses page page.</p>"},{"location":"reference/neps_run/#parallelization","title":"Parallelization","text":"<p>NePS utilizes the file-system and locks as a means of communication for implementing parallelization and resuming runs. As a result, you can start multiple <code>neps.run()</code> from different processes however you like and they will synchronize, as long as they share the same <code>root_directory=</code>. Any new workers that come online will automatically pick up work and work together to until the budget is exhausted.</p> Worker scriptShell <pre><code># worker.py\nneps.run(\n    evaluate_pipeline=...,\n    pipeline_space=...,\n    root_directory=\"some/path\",\n    max_evaluations_total=100,\n    max_evaluations_per_run=10, # (1)!\n    continue_until_max_evaluation_completed=True, # (2)!\n    overwrite_working_directory=False, #!!!\n)\n</code></pre> <ol> <li>Limits the number of evaluations for this specific call of <code>neps.run()</code>.</li> <li>Evaluations in-progress count towards max_evaluations_total, halting new ones when this limit is reached.     Setting this to <code>True</code> enables continuous sampling of new evaluations until the total of completed ones meets max_evaluations_total, optimizing resource use in time-sensitive scenarios.</li> </ol> <p>Warning</p> <p>Ensure <code>overwrite_working_directory=False</code> to prevent newly spawned workers from deleting the shared directory!</p> <pre><code># Start 3 workers\npython worker.py &amp;\npython worker.py &amp;\npython worker.py &amp;\n</code></pre>"},{"location":"reference/neps_run/#handling-errors","title":"Handling Errors","text":"<p>Things go wrong during optimization runs and it's important to consider what to do in these cases. By default, NePS will halt the optimization process when an error but you can choose to <code>ignore_errors=</code>, providing a <code>loss_value_on_error=</code> and <code>cost_value_on_error=</code> to control what values should be reported to the optimization process.</p> <pre><code>def run(learning_rate: float, epochs: int) -&gt; float:\n    if whoops_my_gpu_died():\n        raise RuntimeError(\"Oh no! GPU died!\")\n\n    ...\n    return loss\n\nneps.run(\n    loss_value_on_error=100, # (1)!\n    cost_value_on_error=10, # (2)!\n    ignore_errors=True, # (3)!\n)\n</code></pre> <ol> <li>If an error occurs, the loss value for that configuration will be set to 100.</li> <li>If an error occurs, the cost value for that configuration will be set to 100.</li> <li>Continue the optimization process even if an error occurs, otherwise throwing an exception and halting the process.</li> </ol> <p>Note</p> <p>Any runs that error will still count towards the total <code>max_evaluations_total</code> or <code>max_evaluations_per_run</code>.</p>"},{"location":"reference/neps_run/#re-running-failed-configurations","title":"Re-running Failed Configurations","text":"<p>Sometimes things go wrong but not due to the configuration itself. Sometimes you'd also like to change the state so that you re-evaluate that configuration.</p> <p>If you need to go in there and change anything, the entire optimization state is editable on disk. You can follow these steps to modify the state of things.</p> <pre><code>root_directory\n\u251c\u2500\u2500 configs\n\u2502   \u251c\u2500\u2500 .trial_cache.pkl    # A cache of all trial information for optimizers\n\u2502   \u251c\u2500\u2500 config_1\n\u2502   \u2502   \u251c\u2500\u2500 config.yaml     # The configuration\n\u2502   \u2502   \u251c\u2500\u2500 report.yaml     # The results of this run, if any\n\u2502   \u2502   \u251c\u2500\u2500 metadata.json   # Metadata about this run, such as state and times\n\u2502   \u2514\u2500\u2500 config_2\n\u2502       \u251c\u2500\u2500 config.yaml\n\u2502       \u2514\u2500\u2500 metadata.json\n\u251c\u2500\u2500 optimizer_info.yaml\n\u2514\u2500\u2500 optimizer_state.pkl     # The optimizer's state, shared between workers\n</code></pre> <ol> <li>The first thing you should do is make sure no workers are running.</li> <li>Next, delete <code>optimizer_state.pkl</code> and <code>configs/.trial_cache.pkl</code>. This is cached information to share between the    workers.</li> <li> <p>Lastly, you can go in and modify any of the following files:</p> <ul> <li><code>config.yaml</code> - The configuration to be run. This was sampled from your search space.</li> <li><code>report.yaml</code> - The results of the run. This is where you can change what was reported back.</li> <li><code>metadata.json</code> - Metadata about the run. Here you can change the <code>\"state\"</code> key to one     of <code>State</code> to re-run the configuration, usually you'd want to set it     to <code>\"pending\"</code> such that the next worker will pick it up and re-run it.</li> <li>Once you've made your changes, you can start the workers again and they will pick up the new state re-creating the caches as necessary.</li> </ul> </li> </ol>"},{"location":"reference/neps_run/#selecting-an-optimizer","title":"Selecting an Optimizer","text":"<p>By default NePS intelligently selects the most appropriate optimizer based on your defined configurations in <code>pipeline_space=</code>, one of the arguments to <code>neps.run()</code>.</p> <p>The characteristics of your search space, as represented in the <code>pipeline_space=</code>, play a crucial role in determining which optimizer NePS will choose. This automatic selection process ensures that the optimizer aligns with the specific requirements and nuances of your search space, thereby optimizing the effectiveness of the hyperparameter and/or architecture optimization.</p> <p>You can also manually select a specific or custom optimizer that better matches your specific needs. For more information about the available optimizers and how to customize your own, refer here.</p>"},{"location":"reference/neps_spaces/","title":"NePS Spaces: Joint Architecture and Hyperparameter Search","text":"<p>NePS Spaces provides a powerful framework for defining and optimizing complex search spaces, enabling both and joint architecture and hyperparameter search (JAHS).</p>"},{"location":"reference/neps_spaces/#constructing-neps-spaces","title":"Constructing NePS Spaces","text":"<p>NePS spaces include all the necessary components to define a Hyperparameter Optimization (HPO) search space like:</p> <ul> <li><code>Integer</code>: Discrete integer values</li> <li><code>Float</code>: Continuous float values</li> <li><code>Categorical</code>: Discrete categorical values</li> <li><code>Fidelity</code>: Special type for float or integer, multi-fidelity parameters (e.g., epochs, dataset size)</li> </ul> <p>Additionally, NePS spaces can describe complex (hierarchical) architectures using:</p> <ul> <li><code>Operation</code>: Define operations (e.g., convolution, pooling, activation) with arguments</li> <li><code>Resampled</code>: Resample other parameters</li> </ul>"},{"location":"reference/neps_spaces/#hpo-search-spaces","title":"HPO Search Spaces","text":"<p>A NePS space is defined as a subclass of <code>Pipeline</code>:</p> <pre><code>class pipeline_space(Pipeline):\n</code></pre> <p>Here we define the hyperparameters that make up the space, like so:</p> <pre><code>    float_param = Float(min_value=0.1, max_value=1.0)\n    int_param = Integer(min_value=1, max_value=10)\n    cat_param = Categorical(choices=[\"A\", \"B\", \"C\"])\n</code></pre> <p>Using your knowledge, providing a Prior</p> <p>You can provide your knowledge about where a good value for this parameter lies by indicating a <code>prior=</code>. You can also specify a <code>prior_confidence=</code> to indicate how strongly you want NePS to focus on these, one of either <code>\"low\"</code>, <code>\"medium\"</code>, or <code>\"high\"</code>:</p> <pre><code>    # Categorical parameters can also choose between other parameters\n    # Here the float parameter (index 0) is used as a prior\n    float_or_int = Categorical(choices=(float_param, int_param), prior=0, prior_confidence=\"high\")\n</code></pre>"},{"location":"reference/neps_spaces/#hierarchies-and-architectures","title":"Hierarchies and Architectures","text":"<p>Resampling and operations allow you to define complex architectures akin to Context-Free Grammars (CFGs).</p> <p>With <code>Resampled</code> you can reuse parameters in for other parameters, even themselves recursively:</p> <pre><code>    # The new parameter will have the same range but will be resampled\n    # independently, so it can take different values than its source\n    resampled_float = Resampled(source=float_param)\n\n    # If you only use a parameter to resample from it later, prefix it with an underscore\n    # This way, your evaluation function will not receive it as an argument\n    _float = Float(min_value=1, max_value=3)\n    resampled_float_2 = Resampled(source=_float)\n</code></pre> <p>Note</p> <p>This is a note admonition.</p> <p>Abstract</p> <p>This is an abstract/summary admonition.</p> <p>Info</p> <p>This is an info admonition.</p> <p>Tip</p> <p>This is a tip admonition.</p> <p>Success</p> <p>This is a success admonition.</p> <p>Question</p> <p>This is a question admonition.</p> <p>Warning</p> <p>This is a warning admonition.</p> <p>Failure</p> <p>This is a failure admonition.</p> <p>Danger</p> <p>This is a danger admonition.</p> <p>Bug</p> <p>This is a bug admonition.</p> <p>Example</p> <p>This is an example admonition.</p> <p>Quote</p> <p>This is a quote admonition.</p> Self- and future references <p>When referencing itself or a not yet defined parameter use a string of that parameters name:</p> <pre><code>self_reference = Categorical(choices=(Resampled(\"self_reference\"), Resampled(\"next_param\")))\nnext_param = Float(min_value=0, max_value=5)\n</code></pre> <p>Operations can be Callables, (e.g. pytorch objects) whose arguments can themselves be parameters:</p> <pre><code>from torch.nn import Sequential, Conv2d, ReLU\n\nclass NN_Space(Pipeline):\n\n    # Define an operation for a ReLU activation\n    _relu = Operation(operator=ReLU)\n\n    # Define a convolution operation with an optimizable kernel size parameter\n    _convolution = Operation(\n        operator=Conv2d,\n        kwargs={\"kernel_size\": Integer(min_value=1, max_value=10)}\n        # You could also define _kernel_size separately and use Resampled\n    )\n\n    _model_args = Categorical(\n        choices=(\n            # The Sequential will either get a convolution followed by a ReLU\n            (Resampled(_convolution), _relu,),\n            # Or two (different, hence the resampling) convolutions\n            (Resampled(_convolution), Resampled(_convolution)),\n            # Or just a ReLU activation\n            (_relu,),\n        )\n    )\n\n    # Define a sequential operation, using the previously defined _model_args\n    # This model will be the only parameter passed to the evaluation function\n    model = Operation(\n        operator=Sequential,\n        args=_model_args\n    )\n</code></pre> Tuples as choice <p>When using a tuple as one of the choices in a <code>Categorical</code>, all choices must be tuples, as in the example above with <code>python (_relu,)</code>.</p>"},{"location":"reference/neps_spaces/#using-neps-spaces","title":"Using NePS Spaces","text":"<p>To use a NePS space, pass it as the <code>pipeline_space</code> argument to the <code>neps.run()</code> function:</p> <pre><code>import neps\nneps.run(\n    ...,\n    pipeline_space=NN_Space()\n)\n</code></pre> <p>NePS Space-compatible optimizers</p> <p>Currently, NePS Spaces is compatible with these optimizers, which can be imported from neps.optimizers.neps_algorithms:</p> <ul> <li><code>Random Search</code>, which can sample the space uniformly at random</li> <li><code>Complex Random Search</code>, which can sample the space uniformly at random, using priors and mutating previously sampled configurations</li> <li><code>PriorBand</code>, which uses multi-fidelity and the prior knowledge encoded in the NePS space</li> </ul>"},{"location":"reference/neps_spaces/#inspecting-configurations","title":"Inspecting Configurations","text":"<p>NePS saves the configurations as paths, where each sampling decision is recorded. As they are hard to read, so you can load the configuration from the <code>results</code> directory using the <code>NepsCompatConverter</code> class, which converts the configuration such that it can be used with the NePS Spaces API:</p> <pre><code>from neps.space.neps_spaces import neps_space\nimport yaml\n\nwith open(\"Path/to/config.yaml\", \"r\") as f:\n    conf_dict = yaml.safe_load(f)\nresolution_context = NepsCompatConverter.from_neps_config(conf_dict)\n\n# Use the resolution context to sample the configuration using a\n# Sampler that follows the instructions in the configuration\nresolved_pipeline, resolution_context = neps_space.resolve(pipeline=NN_Space(),\n    # Predefined samplings are the decisions made at each sampling step\n    domain_sampler=neps_space.OnlyPredefinedValuesSampler(predefined_samplings=config.predefined_samplings),\n    # Environment values are the fidelities and any arguments of the evaluation function not part of the search space\n    environment_values=config.environment_values)\n\n# The resolved_pipeline now contains all the parameters and their values, e.g. the Callable model\nmodel_callable = neps_space.convert_operation_to_callable(operation=resolved_pipeline.model)\n</code></pre>"},{"location":"reference/optimizers/","title":"Optimizer Configuration","text":""},{"location":"reference/optimizers/#1-what-optimizer-works-best-for-my-problem","title":"1 What optimizer works best for my problem?","text":"<p>The best optimizers utilizes all information available in the search space to guide the optimization process. Besides a fully black-box search, there are two sources of information an optimizer can draw from: using small scale proxies (Multi-Fidelity) and intuition (Priors).</p>"},{"location":"reference/optimizers/#11-multi-fidelity-mf","title":"1.1 Multi-Fidelity (MF)","text":"<p>Multi-Fidelity uses small scale version of the problem, which run cheaper and faster. This could mean training models for a shorter time, using only a subset of the training data, or a smaller model entirely. From these low fidelity runs, MF-algorithms can infer which configurations are likely to perform well on the full problem.</p> <p>It is defined using the <code>is_fidelity</code> parameter in the <code>pipeline_space</code> definition.</p> <pre><code>pipeline_space = {\n    \"epoch\": neps.Integer(lower=1, upper=100, is_fidelity=True),\n    # epoch will be available as fidelity to the optimizer\n}\n</code></pre> <p>For a more detailed explanation of Multi-Fidelity and a list of NePS-optimizers using MF please refer here.</p>"},{"location":"reference/optimizers/#12-priors","title":"1.2 Priors","text":"<p>Optimization with Priors is used, when there already exists an intuition for what region or specific value of a hyperparameter could work well. By providing this intuition as Prior (knowledge) to the optimizer, it can prioritize these most promising regions of the search space, potentially saving a lot of compute.</p> <p>It is defined using the <code>prior</code> parameter in the <code>pipeline_space</code> definition.</p> <pre><code>pipeline_space = {\n    \"alpha\": neps.Float(lower=0.1, upper=1.0, prior=0.4, prior_confidence=\"high\"),\n    # alpha will have a prior pointing towards 0.4 with high confidence\n}\n</code></pre> <p>For a more detailed explanation of Priors and a list of NePS-optimizers using Priors please refer here.</p>"},{"location":"reference/optimizers/#2-neps-optimizer-selection","title":"2 NePS Optimizer Selection","text":""},{"location":"reference/optimizers/#21-automatic-optimizer-selection","title":"2.1 Automatic Optimizer Selection","text":"<p>NePS provides a multitude of optimizers from the literature, the algorithms section goes into detail on each of them. This chapter focusses on how to select them when using NePS.</p> <p>\u2705 = supported/necessary, \u274c = not supported, \u2714\ufe0f* = optional, click for details, \u2716\ufe0f* ignorable, click for details</p> Algorithm Multi-Fidelity Priors Model-based <code>Grid Search</code> \ufe0f\ufe0f\u2716\ufe0f* \u274c \u274c <code>Random Search</code> \ufe0f\ufe0f\u2716\ufe0f* \u2714\ufe0f* \u274c <code>Bayesian Optimization</code> \ufe0f\ufe0f\u2716\ufe0f* \u274c \u2705 <code>Successive Halving</code> \u2705 \u2714\ufe0f* \u274c <code>ASHA</code> \u2705 \u2714\ufe0f* \u274c <code>Hyperband</code> \u2705 \u2714\ufe0f* \u274c <code>Asynch HB</code> \u2705 \u2714\ufe0f* \u274c <code>IfBO</code> \u2705 \u2714\ufe0f* \u2705 <code>PiBO</code> \ufe0f\ufe0f\u2716\ufe0f* \u2705 \u2705 <code>PriorBand</code> \u2705 \u2705 \u2705 <p>If you prefer not to specify a particular optimizer for your AutoML task, you can simply pass <code>\"auto\"</code> or <code>None</code> for the neps optimizer. This provides a hassle-free way to get started quickly, as NePS will automatically choose the best optimizer based on the characteristics of your search space:</p> <ul> <li>If it has fidelity: <code>hyperband</code></li> <li>If it has both fidelity and a prior: <code>priorband</code></li> <li>If it has a prior: <code>pibo</code></li> <li>If it has neither: <code>bayesian_optimization</code></li> </ul> <p>For example, running the following format, without specifying a optimizer will choose an optimizer depending on the <code>pipeline_space</code> passed.</p> <pre><code>neps.run(\n    evaluate_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    # no optimizer specified\n)\n</code></pre>"},{"location":"reference/optimizers/#22-choosing-one-of-neps-optimizers","title":"2.2 Choosing one of NePS Optimizers","text":"<p>We have also prepared some optimizers with specific hyperparameters that we believe can generalize well to most AutoML tasks and use cases. The available optimizers are imported via the <code>neps.algorithms</code> module. You can use either the optimizer name or the optimizer class itself as the optimizer argument.</p> <pre><code>neps.run(\n    evaluate_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    # optimizer specified, along with an argument\n    optimizer=neps.algorithms.bayesian_optimization, # or as string: \"bayesian_optimization\"\n)\n</code></pre> <p>For a list of available optimizers, please refer here.</p>"},{"location":"reference/optimizers/#23-hyperparameter-overrides","title":"2.3 Hyperparameter Overrides","text":"<p>For users who want more control over the optimizer's hyperparameters, you can input a dictionary with your parameter choices together with the optimizer name.</p> <pre><code>neps.run(\n    evaluate_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    optimizer=(\"bayesian_optimization\", {\"initial_design_size\": 5})\n)\n</code></pre>"},{"location":"reference/optimizers/#3-custom-optimizers","title":"3 Custom Optimizers","text":"<p>To design entirely new optimizers, you can define them as class with a <code>__call__</code> method outside of NePS and pass them to the <code>neps.run()</code> function:</p> <pre><code>@dataclass\nclass MyOptimizer:\n    space: SearchSpace\n    sampler: Sampler\n    encoder: ConfigEncoder\n\n    def __call__(\n        self,\n        trials: Mapping[str, Trial],\n        budget_info: BudgetInfo | None,\n        n: int | None,\n    ) -&gt; SampledConfig | list[SampledConfig]:\n        # Your custom sampling logic here\n        ...\n</code></pre> <p>The class is then passed to the <code>neps.run()</code> function just like the built-in optimizers and can be configured the same way, using a dictionary:</p> <pre><code>neps.run(\n    evaluate_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    optimizer=MyOptimizer,\n)\n</code></pre> <p>For more details on how to define a custom optimizer see the Optimizer Interface.</p>"},{"location":"reference/pipeline_space/","title":"Initializing the Pipeline Space","text":"<p>In NePS, we need to define a <code>pipeline_space</code>. This space can be structured through various approaches, including a Python dictionary, or ConfigSpace. Each of these methods allows you to specify a set of parameter types, ranging from Float and Categorical to specialized architecture parameters. Whether you choose a dictionary, or ConfigSpace, your selected method serves as a container or framework within which these parameters are defined and organized. This section not only guides you through the process of setting up your <code>pipeline_space</code> using these methods but also provides detailed instructions and examples on how to effectively incorporate various parameter types, ensuring that NePS can utilize them in the optimization process.</p>"},{"location":"reference/pipeline_space/#parameters","title":"Parameters","text":"<p>NePS currently features 4 primary hyperparameter types:</p> <ul> <li><code>Categorical</code></li> <li><code>Float</code></li> <li><code>Integer</code></li> <li><code>Constant</code></li> </ul> <p>Using these types, you can define the parameters that NePS will optimize during the search process. The most basic way to pass these parameters is through a Python dictionary, where each key-value pair represents a parameter name and its respective type. For example, the following Python dictionary defines a <code>pipeline_space</code> with four parameters for optimizing a deep learning model:</p> <pre><code>pipeline_space = {\n    \"learning_rate\": neps.Float(0.00001, 0.1, log=True),\n    \"num_epochs\": neps.Integer(3, 30, is_fidelity=True),\n    \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"], # Categorical\n    \"dropout_rate\": 0.5, # Constant\n}\n\nneps.run(.., pipeline_space=pipeline_space)\n</code></pre> Quick Parameter Reference <code>Categorical</code><code>Float</code><code>Integer</code><code>Constant</code> <p>A list of unordered choices for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters that can take on a discrete set of unordered values. For example, the <code>optimizer</code> hyperparameter in a neural network search space can be a <code>Categorical</code> with choices like <code>[\"adam\", \"sgd\", \"rmsprop\"]</code>.</p> <pre><code>import neps\n\noptimizer_choice = neps.Categorical(\n    [\"adam\", \"sgd\", \"rmsprop\"],\n    prior=\"adam\"\n)\n</code></pre> <p>A float value for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters with continuous float values, optionally specifying if it exists on a log scale.</p> <p>For example, <code>l2_norm</code> could be a value in <code>(0.1)</code>, while the <code>learning_rate</code> hyperparameter in a neural network search space can be a <code>Float</code> with a range of <code>(0.0001, 0.1)</code> but on a log scale.</p> <pre><code>import neps\n\nl2_norm = neps.Float(0, 1)\nlearning_rate = neps.Float(1e-4, 1e-1, log=True)\n</code></pre> <p>An integer value for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters with continuous integer values, optionally specifying f it exists on a log scale.</p> <p>For example, <code>batch_size</code> could be a value in <code>(32, 128)</code>, while the <code>num_layers</code> hyperparameter in a neural network search space can be a <code>Integer</code> with a range of <code>(1, 1000)</code> but on a log scale.</p> <pre><code>import neps\n\nbatch_size = neps.Integer(32, 128)\nnum_layers = neps.Integer(1, 1000, log=True)\n</code></pre> <p>A constant value for a parameter.</p> <p>This kind of parameter is used to represent hyperparameters with values that should not change during optimization.</p> <p>For example, the <code>batch_size</code> hyperparameter in a neural network search space can be a <code>Constant</code> with a value of <code>32</code>.</p> <pre><code>import neps\n\nbatch_size = neps.Constant(32)\n</code></pre>"},{"location":"reference/pipeline_space/#neps.space.Categorical.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: float | int | str = field(init=False)\n</code></pre> <p>The center value of the categorical hyperparameter.</p> <p>As there is no natural center for a categorical parameter, this is the first value in the choices list.</p>"},{"location":"reference/pipeline_space/#neps.space.Categorical.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: list[float | int | str]\n</code></pre> <p>The list of choices for the categorical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Categorical.prior","title":"prior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior: float | int | str | None = None\n</code></pre> <p>The default value for the categorical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Categorical.prior_confidence","title":"prior_confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior_confidence: Literal['low', 'medium', 'high'] = 'low'\n</code></pre> <p>Confidence score for the prior value when considering prior based optimization.</p>"},{"location":"reference/pipeline_space/#neps.space.Float.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: float = field(init=False)\n</code></pre> <p>The center value of the numerical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Float.is_fidelity","title":"is_fidelity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_fidelity: bool = False\n</code></pre> <p>Whether the hyperparameter is fidelity.</p>"},{"location":"reference/pipeline_space/#neps.space.Float.log","title":"log  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log: bool = False\n</code></pre> <p>Whether the hyperparameter is in log space.</p>"},{"location":"reference/pipeline_space/#neps.space.Float.lower","title":"lower  <code>instance-attribute</code>","text":"<pre><code>lower: float\n</code></pre> <p>The lower bound of the numerical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Float.prior","title":"prior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior: float | None = None\n</code></pre> <p>Prior value for the hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Float.prior_confidence","title":"prior_confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior_confidence: Literal['low', 'medium', 'high'] = 'low'\n</code></pre> <p>Confidence score for the prior value when considering prior based optimization.</p>"},{"location":"reference/pipeline_space/#neps.space.Float.upper","title":"upper  <code>instance-attribute</code>","text":"<pre><code>upper: float\n</code></pre> <p>The upper bound of the numerical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Integer.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: int = field(init=False)\n</code></pre> <p>The center value of the numerical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Integer.is_fidelity","title":"is_fidelity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_fidelity: bool = False\n</code></pre> <p>Whether the hyperparameter is fidelity.</p>"},{"location":"reference/pipeline_space/#neps.space.Integer.log","title":"log  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log: bool = False\n</code></pre> <p>Whether the hyperparameter is in log space.</p>"},{"location":"reference/pipeline_space/#neps.space.Integer.lower","title":"lower  <code>instance-attribute</code>","text":"<pre><code>lower: int\n</code></pre> <p>The lower bound of the numerical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Integer.prior","title":"prior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior: int | None = None\n</code></pre> <p>Prior value for the hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Integer.prior_confidence","title":"prior_confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prior_confidence: Literal['low', 'medium', 'high'] = 'low'\n</code></pre> <p>Confidence score for the prior value when considering prior based optimization.</p>"},{"location":"reference/pipeline_space/#neps.space.Integer.upper","title":"upper  <code>instance-attribute</code>","text":"<pre><code>upper: int\n</code></pre> <p>The upper bound of the numerical hyperparameter.</p>"},{"location":"reference/pipeline_space/#neps.space.Constant.center","title":"center  <code>property</code>","text":"<pre><code>center: Any\n</code></pre> <p>The center of the hyperparameter.</p> <p>Warning</p> <p>There is no real center of a constant value, hence we take this to be the value itself.</p>"},{"location":"reference/pipeline_space/#using-your-knowledge-providing-a-prior","title":"Using your knowledge, providing a Prior","text":"<p>When optimizing, you can provide your own knowledge using the parameter <code>prior=</code>. By indicating a <code>prior=</code> we take this to be your user prior, your knowledge about where a good value for this parameter lies.</p> <p>You can also specify a <code>prior_confidence=</code> to indicate how strongly you want NePS, to focus on these, one of either <code>\"low\"</code>, <code>\"medium\"</code>, or <code>\"high\"</code>.</p> <pre><code>import neps\n\nneps.run(\n    ...,\n    pipeline_space={\n        \"learning_rate\": neps.Float(1e-4, 1e-1, log=True, prior=1e-2, prior_confidence=\"medium\"),\n        \"num_epochs\": neps.Integer(3, 30, is_fidelity=True),\n        \"optimizer\": neps.Categorical([\"adam\", \"sgd\", \"rmsprop\"], prior=\"adam\", prior_confidence=\"low\"),\n        \"dropout_rate\": neps.Constant(0.5),\n    }\n)\n</code></pre> <p>Interaction with <code>is_fidelity</code></p> <p>If you specify <code>is_fidelity=True</code> and <code>prior=</code> for one parameter, this will raise an error.</p> <p>Currently the two major algorithms that exploit this in NePS are <code>PriorBand</code> (prior-based <code>HyperBand</code>) and <code>PiBO</code>, a version of Bayesian Optimization which uses Priors. For more information on priors and algorithms using them, please refer to the prior documentation.</p>"},{"location":"reference/pipeline_space/#using-configspace","title":"Using ConfigSpace","text":"<p>For users familiar with the <code>ConfigSpace</code> library, can also define the <code>pipeline_space</code> through <code>ConfigurationSpace()</code></p> <pre><code>from configspace import ConfigurationSpace, Float\n\nconfigspace = ConfigurationSpace(\n    {\n        \"learning_rate\": Float(\"learning_rate\", bounds=(1e-4, 1e-1), log=True)\n        \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"],\n        \"dropout_rate\": 0.5,\n    }\n)\n</code></pre> <p>Warning</p> <p>Parameters you wish to use as a fidelity are not support through ConfigSpace at this time.</p> <p>For additional information on ConfigSpace and its features, please visit the following link.</p>"},{"location":"reference/seeding/","title":"Seeding","text":"<p>Seeding is only rudimentarily supported in NePS, as we provide a function to capture the global rng state of <code>Python</code>, <code>numpy</code> and <code>torch</code>. It is not yet possible to seed only NePS internally.</p> <p>See the Seeding API for the details on how to capture and use this global rng state.</p>"},{"location":"reference/search_algorithms/bayesian_optimization/","title":"Bayesian Optimization","text":""},{"location":"reference/search_algorithms/bayesian_optimization/#what-is-bayesian-optimization","title":"What is Bayesian Optimization?","text":"<p><code>Bayesian Optimization</code>/<code>BO</code> is a fundamental optimization technique for finding (local) optima of expensive-to-evaluate functions. The main idea of <code>BO</code> is an interplay of a model (the <code>surrogate function</code>) of the objective function, built from the data collected during the optimization process, and an <code>acquisition function</code> that guides the search for the next evaluation point.</p>"},{"location":"reference/search_algorithms/bayesian_optimization/#the-surrogate-function","title":"The surrogate function","text":"<p>For each dimension of the search space, the surrogate function models the objective function as a <code>Gaussian Process</code> (GP). A GP consists of a mean function and a covariance function, which are both learned from the data. The mean function represents the expected value of the objective function, while the covariance function models the uncertainty of the predictions. The following image shows a GP with its mean function and the 95% confidence interval:</p> The dashed line represents the (hidden) objective function, while the solid line is the surrogate mean function. The shaded area around the mean function is its confidence interval. Note that the confidence interval collapses where observations have been made and gets large in regions where no data is available yet. (Image Source: Medium.com, Jan 27, 2025)"},{"location":"reference/search_algorithms/bayesian_optimization/#the-acquisition-function","title":"The acquisition function","text":"<p>The acquisition function is the guiding force in <code>BO</code>. From the information contained in the surrogate function, the acquisition function suggests the next evaluation point. It balances the trade-off between exploration (sampling points where the surrogate function is uncertain) and exploitation (sampling points where the surrogate function is promising).</p> The image shows the surrogate function from before, now with the acquisition function plotted below. The maximum of the acquisition function is the point that will usually be evaluated next. (Image Source: Medium.com, Jan 27, 2025) <p>There are numerous acquisition functions, with the most popular being <code>Expected Improvement</code> (EI):</p> <ul> <li>EI is defined as the expected improvement over the current best observation:</li> </ul> \\[EI(\\boldsymbol{x}) = \\mathbb{E}[\\max(0, f(\\boldsymbol{x}) - f(\\boldsymbol{x}^+))]\\] <p>And <code>Probability of Improvement</code> (PI):</p> <ul> <li>PI is defined as the probability that the surrogate function is better than the current best observation:</li> </ul> \\[PI(\\boldsymbol{x}) = P(f(\\boldsymbol{x}) &gt; f(\\boldsymbol{x}^+))\\] <p>where \\(f(\\boldsymbol{x})\\) is the surrogate function and \\(f(\\boldsymbol{x}^+)\\) is the best observation so far.</p> <p>To read more about <code>BO</code>, please refer to this <code>Bayesian Optimization</code> tutorial or this article on Towards Data Science.</p> <p>Practical Tips</p> <ul> <li><code>BO</code> can handle expensive-to-evaluate, noisy, high-dimensional and black-box objectives and can be used in the optimization of hyperparameters, neural architectures, and the entire pipeline.</li> <li>It is highly costumizable with many choices for the surrogate and acquisition functions, but even the basic settings work well in many cases.</li> </ul> <p>Info</p> <p>Therefore, <code>BO</code> is chosen as the default optimizer in NePS when there is no Prior or Multi-Fidelity information available.</p>"},{"location":"reference/search_algorithms/landing_page_algo/","title":"Algorithms","text":"<p>Algorithms are the search strategies determining what configurations to evaluate next. In NePS, we provide a variety of pre-implemented algorithms and offer the possibility to implement custom algorithms. This chapter gives an overview of the different algorithms available in NePS and practical tips for their usage.</p> <p>We distinguish between algorithms that use different types of information and strategies to guide the search process:</p> <p>\u2705 = supported/necessary, \u274c = not supported, \u2714\ufe0f* = optional, click for details, \u2716\ufe0f* ignorable, click for details</p> Algorithm Multi-Fidelity Priors Model-based <code>Grid Search</code> \ufe0f\ufe0f\u2716\ufe0f* \u274c \u274c <code>Random Search</code> \ufe0f\ufe0f\u2716\ufe0f* \u2714\ufe0f* \u274c <code>Bayesian Optimization</code> \ufe0f\ufe0f\u2716\ufe0f* \u274c \u2705 <code>Successive Halving</code> \u2705 \u2714\ufe0f* \u274c <code>ASHA</code> \u2705 \u2714\ufe0f* \u274c <code>Hyperband</code> \u2705 \u2714\ufe0f* \u274c <code>Asynch HB</code> \u2705 \u2714\ufe0f* \u274c <code>IfBO</code> \u2705 \u2714\ufe0f* \u2705 <code>PiBO</code> \ufe0f\ufe0f\u2716\ufe0f* \u2705 \u2705 <code>PriorBand</code> \u2705 \u2705 \u2705"},{"location":"reference/search_algorithms/landing_page_algo/#what-is-multi-fidelity-optimization","title":"What is Multi-Fidelity Optimization?","text":"<p>Multi-Fidelity (MF) optimization leverages the idea of running an AutoML problem on a small scale, which is cheaper and faster, and then using this information to train full-scale models. The low-fidelity runs could be on a smaller dataset, a smaller model, or for shorter training times. MF-algorithms then infer which configurations are likely to perform well on the full problem, before investing larger compute amounts.</p> <p>Advantages of Multi-Fidelity</p> <ul> <li>Parallelization: MF-algorithms can use the information from many parallel low-fidelity runs to guide the search in the few high-fidelity runs.</li> <li>Exploration: By using low-fidelity runs, the optimizer can explore more of the search space.</li> </ul> <p>Disadvantages of Multi-Fidelity</p> <ul> <li>Variance: The performance of a configuration on a low-fidelity run might not correlate well with its performance on a high-fidelity run. This can result in misguided decisions.</li> </ul> <p>We present a collection of MF-algorithms here and algorithms that combine MF with priors here.</p>"},{"location":"reference/search_algorithms/landing_page_algo/#what-are-priors","title":"What are Priors?","text":"<p>Priors are used when there exists some information about the search space, that can be used to guide the optimization process. This information could come from expert domain knowledge or previous experiments. A Prior is provided in the form of a distribution over one dimension of the search space, with a <code>mean</code> (the suspected optimum) and a <code>confidence level</code>, or <code>variance</code>. We discuss how Priors can be included in your NePS-search space here.</p> <p>Advantages of using Priors</p> <ul> <li>Less compute: By providing a Prior, the optimizer can focus on the most promising regions of the search space, potentially saving a lot of compute.</li> <li>More exploitation: By focusing on these regions, the optimizer might find a better final solution.</li> </ul> <p>Disadvantages of using Priors</p> <ul> <li>Less exploration: By focusing on these regions, the optimizer might miss out on other regions that could potentially be better.</li> <li>Bad priors: If the Prior is not a good representation of the search space, the optimizer might deliver suboptimal results, compared to a search without Priors. The optimizers we provide in NePS are specifically designed to handle bad priors, but they still slow down the search process.</li> </ul> <p>We present a collection of algorithms that use Priors here and algorithms that combine priors with Multi-Fidelity here.</p>"},{"location":"reference/search_algorithms/multifidelity/","title":"Multi-Fidelity Optimizers","text":"<p>This section concerns optimizers that utilize Multi-Fidelity information to guide the search process. Multi-Fidelity is explained in detail here.</p>"},{"location":"reference/search_algorithms/multifidelity/#1-successive-halfing","title":"1 <code>Successive Halfing</code>","text":"<p><code>Successive Halfing</code>/<code>SH</code> (see paper) is a simple but effective Multi-Fidelity algorithm.</p> <p>It starts with a large number of random configurations and evaluates them on a low-fidelity. The best-performing \\(1/\\eta\\) configurations are then promoted to the next fidelity, where they are evaluated again. This process is repeated until only a few configurations remain, evaluated on the highest fidelity. The process allows for broad exploration in the beginning and focus on the most promising configurations towards the end.</p> <p>See the algorithm's implementation details in the api.</p> Practical Tips <ul> <li>For the same total compute, <code>SH</code> outperforms uninformed search algorithms like random search or grid search.</li> <li>It highly depends on the correlation between lower and higher fidelities. If the correlation is low, <code>SH</code> underperforms.</li> <li><code>SH</code> has two parameters: \\(\\eta\\) and \\(n\\), where \\(\\eta\\) is the promotion factor and \\(n\\) is the number of configurations at the lowest fidelity. This results in a total of \\(\\frac{n*r}{\\eta^r}\\) steps (from one fidelity level to the next), where \\(r\\) is the number of fidelity levels. For more details, see the api.</li> </ul>"},{"location":"reference/search_algorithms/multifidelity/#asynchronous-successive-halving","title":"Asynchronous Successive Halving","text":"<p><code>Asynchronous Successive Halving</code>/<code>ASHA</code> (see paper) is an asynchronous version of <code>SH</code> that maximizes parallel evaluations.</p> <p>Instead of waiting for all \\(n\\) configurations to finish on one fidelity, <code>ASHA</code> promotes the best configuration to the next fidelity as soon as there are enough evaluations to make a decision (\\(1/\\eta*n\\geq 1\\)). This allows for quicker promotions and earlier high fidelity-results. When there are no promotable configurations, <code>ASHA</code> spawns new configurations at the lowest fidelity, so it always utilizes the available compute and increases exploration compared to <code>SH</code>.</p>"},{"location":"reference/search_algorithms/multifidelity/#prior-extended-successive-halving","title":"Prior-extended Successive Halving","text":"<p>Although not inherently a Prior-optimizer, <code>SH</code> (and <code>ASHA</code>) can make use of Priors. Instead of sampling configurations uniformly, the optimizer can directly sample from the Prior, which results in a more focused search - highly beneficial if the Prior is reliable. Alternatively, the <code>SH</code> can bias the promotion of configurations towards the Prior, keeping worse-performing, but recommended configurations longer in the optimization process.</p> <p>See the algorithm's implementation details in the api.</p>"},{"location":"reference/search_algorithms/multifidelity/#2-hyperband","title":"2 <code>HyperBand</code>","text":"<p><code>HyperBand</code>/<code>HB</code> (see paper) is an extension of <code>Successive Halfing</code> that employs multiple <code>Successive Halfing</code>-rounds in parallel.</p> <p>Each of these runs has a different resource budget and different number of configurations. This makes <code>HyperBand</code> more flexible and parallelizable than <code>SH</code>.</p> <p>See the algorithm's implementation details in the api.</p> Practical Tips <ul> <li><code>HyperBand</code> is a good choice when you have a limited budget and want to parallelize your search.</li> <li>It is more efficient than <code>SH</code> when the correlation between lower and higher fidelities is low.</li> <li><code>Hyperband</code> has two parameters: \\(\\eta\\) (typically 3 or 4) and \\(R\\), where \\(\\eta\\) is the promotion factor and \\(R\\) is the maximum budget any single configuration will be trained on. A larger \\(R\\) will result in better, but slower results, while a larger \\(\\eta\\) will result in faster, but more noisy, potentially worse results. HB then spawns \\(\\lfloor \\log_\\eta(R)\\rfloor\\) <code>Successive Halfing</code>-rounds. For more details, see the api.</li> </ul> <p>Info</p> <p><code>HyperBand</code> is chosen as the default optimizer in NePS when there is no Prior, only Multi-Fidelity information available.</p>"},{"location":"reference/search_algorithms/multifidelity/#3-in-context-freeze-thaw-bayesian-optimization","title":"3 <code>In-Context Freeze-Thaw Bayesian Optimization</code>","text":"<p><code>In-Context Freeze-Thaw Bayesian Optimization</code>/<code>IfBO</code> (see paper) expands on the idea of Freeze-Thaw Bayesian Optimization (<code>FT-BO</code>) by using a <code>Prior-data fitted network</code> (PFN) as a surrogate for the <code>FT-BO</code>.</p> <p>Standard <code>FT-BO</code> models the performance of a configuration with a Gaussian Process, assuming exponential loss decay. It uses this joint GP to fantasize results and decides for the most informative configurations. The <code>Entropy Search</code>-acquisition function (see paper) quantifies this information gain: $$ a(\\boldsymbol{x}) = \\int\\left(H\\left(P^y_{\\min}\\right)\\right) - \\left(H\\left(P_{\\min}\\right)\\right)P(y| { \\lbrace (\\boldsymbol{x}_n,y_n) \\rbrace }^N)dy $$ where \\(H\\) is the entropy, \\(P_{\\min}\\) is the distribution of the minimum value, \\(P^y_{\\min}\\) is the same distribution, but given a new observation \\(y\\) and \\(P(y| \\cdot)\\) is the probability of this result \\(y\\), from a configuration \\(\\boldsymbol{x}\\) (given the \\(N\\) observations \\({\\lbrace (\\boldsymbol{x}_n,y_n) \\rbrace}^N\\) so far). So the acquisition function maximizes the information gain about the location of the minimum from evaluating any configuration \\(\\boldsymbol{x}\\), by maximizing the entropy-reduction.</p> The image shows the fantasizing of exponential loss decay in<code>FT-BO</code> . (Image Source: FT-BO-paper, Jan 27, 2025) <p><code>IfBO</code> employs the same concept, but instead of a GP, it uses a PFN to model the performance of configurations. PFNs (see paper) are transformer networks, fitted to many (synthetic) runs. They can model the performance of configurations across all fidelities and are used in <code>IfBO</code> to fantasize the outcomes. The deciding advantage is that PFNs can model complex relationships between configurations and fidelities, not just exponential decay. On top of that, PFNs utilize in-context learning to quickly adapt from their general prior to the current optimization process, resulting in a better overall performance compared to GPs.</p> <p>Lastly, <code>IfBO</code> adapts the <code>FT-BO</code> idea of freezing (pausing training on) configurations that are not informative anymore and thawing (resuming training on) them when they become interesting again. It therefore chooses automatically between starting new configurations or thawing old ones.</p> The image shows the Freeze-Thaw-mechanism, with the colors indicating, at what iteration a configuration has been evaluated at this fidelity. Note for example some yellow configurations being reused much later, ending in red. (Image Source: FT-BO-paper, Jan 27, 2025) <p>See the algorithm's implementation details in the api.</p> Practical Tips <ul> <li><code>IfBO</code> is a good choice when the problem allows for low-fidelity configurations to be continued to retrieve high-fidelity results, utilizing neps's checkpointing feature.</li> </ul> <p>For optimizers using both Priors and Multi-Fidelity, please refer here.</p>"},{"location":"reference/search_algorithms/multifidelity_prior/","title":"Multi-Fidelity and Prior Optimizers","text":"<p>This section concerns optimizers that use both Multi-Fidelity and Priors. They combine the advantages and disadvantages of both methods to exploit all available information. For a detailed explanation of Multi-Fidelity and Priors, please refer here.</p>"},{"location":"reference/search_algorithms/multifidelity_prior/#optimizers-using-multi-fidelity-and-priors","title":"Optimizers using Multi-Fidelity and Priors","text":""},{"location":"reference/search_algorithms/multifidelity_prior/#1-priorband","title":"1 <code>PriorBand</code>","text":"<p><code>PriorBand</code> (see paper) is an extension of <code>HyperBand</code> that utilizes expert Priors to choose the next configuration.</p> <p><code>PriorBand</code>'s sampling module \\(\\mathcal{E}_\\pi\\) balances the influence of the Prior, the incumbent configurations and randomness to select configurations.</p> The <code>PriorBand</code> sampling module balances the influence of the Prior, the \\(1/\\eta\\) incumbent configurations and randomness to select configurations. (Image Source: PriorBand-paper, Jan 27, 2025) <p>The Prior sampling \\(p_\\pi\\) is most meaningful at full fidelity and when not much data is available yet, while the incumbent sampling \\(p_{\\hat{\\lambda}}\\), coming from actual data, is most significant but sparse, and random sampling \\(p_{\\mathcal{U}}\\) is needed for exploration, especially at lower fidelities. This results in these inital sampling probabilities when there is no incumbent yet:</p> \\[ p_{\\mathcal{U}}=1/(1+\\eta^r) \\] \\[ p_\\pi=1-p_{\\mathcal{U}} \\] \\[ p_{\\hat{\\lambda}}=0 \\] <p>where \\(\\eta\\) is the promotion-hyperparameter from <code>HyperBand</code> and \\(r\\) is the current fidelity level (rung), showing the decay of the random sampling probability with increasing fidelity.</p> <p>When there is an incumbent, the probabilities are adjusted to:</p> \\[ p_{\\mathcal{U}}=1/(1+\\eta^r) \\] \\[ p_\\pi=p_\\pi\\cdot\\mathcal{S}_{\\hat{\\lambda}}/(\\mathcal{S}_\\pi+\\mathcal{S}_{\\hat{\\lambda}}) \\] \\[ p_{\\hat{\\lambda}}=p_{\\hat{\\lambda}}\\cdot\\mathcal{S}_{\\pi}/(\\mathcal{S}_\\pi+\\mathcal{S}_{\\hat{\\lambda}}) \\] <p>where \\(\\mathcal{S}_\\pi\\) and \\(\\mathcal{S}_{\\hat{\\lambda}}\\) are the summed probabilities of the top \\(1/\\eta\\) configurations under Prior and incumbent sampling, respectively. This way, the balance is shifted towards the distribution that would have yielded the best configurations so far. Crucially, this compensates for potentially bad Priors, as the incumbent sampling will take over when it has proven to be better.</p> <p>See the algorithm's implementation details in the api.</p> Practical Tips <ul> <li><code>PriorBand</code> is a good choice when you have a Prior but are wary of its quality and you can utilize Multi-Fidelity.</li> </ul> <p>Info</p> <p><code>PriorBand</code> is chosen as the default optimizer in NePS when there is both Prior and Multi-Fidelity information available.</p>"},{"location":"reference/search_algorithms/multifidelity_prior/#model-based-priorband","title":"Model-based <code>PriorBand</code>","text":"<p><code>PriorBand</code> can also be extended with a model, where after \\(n\\) evaluations, a <code>BO</code> model is trained to advise the sampling module.</p>"},{"location":"reference/search_algorithms/prior/","title":"Prior Optimizers","text":"<p>This section concerns optimizers that utilize priors to guide the search process. Priors are explained in detail here.</p>"},{"location":"reference/search_algorithms/prior/#1-pibo","title":"1 <code>PiBO</code>","text":"<p><code>PiBO</code> (see paper) is an extension of <code>Bayesian Optimization</code> (<code>BO</code>) that uses a specific <code>acquisition function</code> that incorporates Priors, by including a <code>Prior-factor</code> that decays over time. This way, the optimizer first relies on the Prior knowledge, before shifting focus to the data acquired during the optimization process. The altered acquisition function takes this form:</p> \\[\\boldsymbol{x}_n\\in \\underset{\\boldsymbol{x}\\in\\mathcal{X}}{\\operatorname{argmax}}\\alpha(\\boldsymbol{x},\\mathcal{D}_n)\\pi(\\boldsymbol{x})^{\\beta/n}\\] <p>where after \\(n\\) evaluations, the Prior-function \\(\\pi(\\boldsymbol{x})\\) is decayed by the factor \\(\\beta/n\\) and multiplied with the acquisition function \\(\\alpha(\\boldsymbol{x},\\mathcal{D}_n)\\). In our <code>PiBO</code> implementation, we use <code>Expected Improvement</code> as the acquisition function.</p> <p>The following illustration from the <code>PiBO</code>-paper shows the influence of a well-chosen and a bad, decaying Prior on the optimization process:</p> Left: A well-located Prior influences the acquisition function leading to quicker convergence and even more exploration. Right: An off-center Prior slows down, but does not prevent convergence. (Image Source: PiBO-paper, Jan 27, 2025) <p>In both cases, the optimization process uses the additional information provided by the Prior to arrive at the solution, however, the bad Prior (right) results in a slower convergence to the optimum.</p> <p>See the algorithm's implementation details in the api.</p> Practical Tips <p>TODO</p> <p>Info</p> <p><code>PiBO</code> is chosen as the default optimizer in NePS when there is only Prior, but no Multi-Fidelity information available.</p> <p>For optimizers using both Priors and Multi-Fidelity, please refer here.</p>"}]}