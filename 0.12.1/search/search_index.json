{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neural Pipeline Search (NePS)","text":"<p>Welcome to NePS, a powerful and flexible Python library for hyperparameter optimization (HPO) and neural architecture search (NAS) with its primary goal: make HPO and NAS usable for deep learners in practice.</p> <p>NePS houses recently published and also well-established algorithms that can all be run massively parallel on distributed setups, with tools to analyze runs, restart runs, etc., all tailored to the needs of deep learning experts.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>In addition to the features offered by traditional HPO and NAS libraries, NePS, e.g., stands out with:</p> <ol> <li>Hyperparameter Optimization (HPO) with Prior Knowledge and Cheap Proxies:     NePS excels in efficiently tuning hyperparameters using algorithms that enable users to make use of their prior knowledge within the search space. This is leveraged by the insights presented in:<ul> <li>PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning</li> <li>\u03c0BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization</li> </ul> </li> <li>Neural Architecture Search (NAS) with General Search Spaces:     NePS is equipped to handle context-free grammar search spaces, providing advanced capabilities for designing and optimizing architectures. this is leveraged by the insights presented in:<ul> <li>Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars</li> </ul> </li> <li>Easy Parallelization and Tailored to DL:     NePS simplifies the process of parallelizing optimization tasks both on individual computers and in distributed     computing environments. As NePS is made for deep learners, all technical choices are made with DL in mind and common     DL tools such as Tensorboard are embraced.</li> </ol> <p>Tip</p> <p>Check out:</p> <ul> <li>Reference documentation for a quick overview.</li> <li>API for a more detailed reference.</li> <li>Examples for copy-pastable code to get started.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the latest release from PyPI run</p> <pre><code>pip install neural-pipeline-search\n</code></pre> <p>To get the latest version from Github run</p> <pre><code>pip install git+https://github.com/automl/neps.git\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Using <code>neps</code> always follows the same pattern:</p> <ol> <li>Define a <code>run_pipeline</code> function capable of evaluating different architectural and/or hyperparameter configurations    for your problem.</li> <li>Define a search space named <code>pipeline_space</code> of those Parameters e.g. via a dictionary</li> <li>Call <code>neps.run</code> to optimize <code>run_pipeline</code> over <code>pipeline_space</code></li> </ol> <p>In code, the usage pattern can look like this:</p> <pre><code>import neps\nimport logging\n\n\n# 1. Define a function that accepts hyperparameters and computes the validation error\ndef run_pipeline(\n    hyperparameter_a: float, hyperparameter_b: int, architecture_parameter: str\n) -&gt; dict:\n    # Create your model\n    model = MyModel(architecture_parameter)\n\n    # Train and evaluate the model with your training pipeline\n    validation_error = train_and_eval(\n        model, hyperparameter_a, hyperparameter_b\n    )\n    return validation_error\n\n\n# 2. Define a search space of parameters; use the same parameter names as in run_pipeline\npipeline_space = dict(\n    hyperparameter_a=neps.FloatParameter(\n        lower=0.001, upper=0.1, log=True  # The search space is sampled in log space\n    ),\n    hyperparameter_b=neps.IntegerParameter(lower=1, upper=42),\n    architecture_parameter=neps.CategoricalParameter([\"option_a\", \"option_b\"]),\n)\n\n\n# 3. Run the NePS optimization\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"path/to/save/results\",  # Replace with the actual path.\n    max_evaluations_total=100,\n)\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p>Discover how NePS works through these examples:</p> <ul> <li> <p>Hyperparameter Optimization: Learn the essentials of hyperparameter optimization with NePS.</p> </li> <li> <p>Multi-Fidelity Optimization: Understand how to leverage multi-fidelity optimization for efficient model tuning.</p> </li> <li> <p>Utilizing Expert Priors for Hyperparameters: Learn how to incorporate expert priors for more efficient hyperparameter selection.</p> </li> <li> <p>Architecture Search: Dive into (hierarchical) architecture search in NePS.</p> </li> <li> <p>Additional NePS Examples: Explore more examples, including various use cases and advanced configurations in NePS.</p> </li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Please see the documentation for contributors.</p>"},{"location":"#citations","title":"Citations","text":"<p>For pointers on citing the NePS package and papers refer to our documentation on citations.</p>"},{"location":"citations/","title":"Citations","text":""},{"location":"citations/#citation-of-the-software","title":"Citation of The Software","text":"<p>For citing NePS, please refer to the following:</p>"},{"location":"citations/#apa-style","title":"APA Style","text":"<pre><code>Stoll, D., Mallik, N., Schrodi, S., Janowski, M., Garibov, S., Abou Chakra, T., Rogalla, D., Bergman, E., Hvarfner, C., Binxin, R., Kober, N., Vallaeys, T., &amp; Hutter, F. (2023). Neural Pipeline Search (NePS) (Version 0.11.0) [Computer software]. https://github.com/automl/neps\n</code></pre>"},{"location":"citations/#bibtex-style","title":"BibTex Style","text":"<pre><code>@software{Stoll_Neural_Pipeline_Search_2023,\nauthor = {Stoll, Danny and Mallik, Neeratyoy and Schrodi, Simon and Janowski, Maciej and Garibov, Samir and Abou Chakra, Tarek and Rogalla, Daniel and Bergman, Eddie and Hvarfner, Carl and Binxin, Ru and Kober, Nils and Vallaeys, Th\u00e9ophane and Hutter, Frank},\nmonth = oct,\ntitle = {{Neural Pipeline Search (NePS)}},\nurl = {https://github.com/automl/neps},\nversion = {0.11.0},\nyear = {2023}\n}\n</code></pre>"},{"location":"citations/#citation-of-papers","title":"Citation of Papers","text":""},{"location":"citations/#priorband","title":"PriorBand","text":"<p>If you have used PriorBand as the optimizer, please use the bibtex below:</p> <pre><code>@inproceedings{mallik2023priorband,\ntitle = {PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning},\nauthor = {Neeratyoy Mallik and Eddie Bergman and Carl Hvarfner and Danny Stoll and Maciej Janowski and Marius Lindauer and Luigi Nardi and Frank Hutter},\nyear = {2023},\nbooktitle = {Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)},\nkeywords = {}\n}\n</code></pre>"},{"location":"citations/#hierarchichal-nas-with-context-free-grammars","title":"Hierarchichal NAS with Context-free Grammars","text":"<p>If you have used the context-free grammar search space and the graph kernels implemented in NePS for the paper Hierarchical NAS, please use the bibtex below:</p> <pre><code>@inproceedings{schrodi2023hierarchical,\ntitle = {Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars},\nauthor = {Simon Schrodi and Danny Stoll and Binxin Ru and Rhea Sanjay Sukthanker and Thomas Brox and Frank Hutter},\nyear = {2023},\nbooktitle = {Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)},\nkeywords = {}\n}\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Getting started with NePS involves a straightforward yet powerful process, centering around its three main components. This approach ensures flexibility and efficiency in evaluating different architecture and hyperparameter configurations for your problem.</p> <p>NePS requires Python 3.8 or higher. You can install it via pip or from source.</p> <pre><code>pip install neural-pipeline-search\n</code></pre>"},{"location":"getting_started/#the-3-main-components","title":"The 3 Main Components","text":"<ol> <li> <p>Execute with <code>neps.run()</code>: Optimize your <code>run_pipeline=</code> over the <code>pipeline_space=</code> using this function. For a thorough overview of the arguments and their explanations, check out the detailed documentation.</p> </li> <li> <p>Define a <code>run_pipeline=</code> Function: This function is essential for evaluating different configurations. You'll implement the specific logic for your problem within this function. For detailed instructions on initializing and effectively using <code>run_pipeline=</code>, refer to the guide.</p> </li> <li> <p>Establish a <code>pipeline_space=</code>: Your search space for defining parameters. You can structure this in various formats, including dictionaries, YAML, or ConfigSpace. The guide offers insights into defining and configuring your search space.</p> </li> </ol> <p>By following these steps and utilizing the extensive resources provided in the guides, you can tailor NePS to meet your specific requirements, ensuring a streamlined and effective optimization process.</p>"},{"location":"getting_started/#basic-usage","title":"Basic Usage","text":"<p>In code, the usage pattern can look like this:</p> <pre><code>import neps\nimport logging\n\ndef run_pipeline( # (1)!\n    hyperparameter_a: float,\n    hyperparameter_b: int,\n    architecture_parameter: str,\n) -&gt; dict:\n    # insert here your own model\n    model = MyModel(architecture_parameter)\n\n    # insert here your training/evaluation pipeline\n    validation_error, training_error = train_and_eval(\n        model, hyperparameter_a, hyperparameter_b\n    )\n\n    return {\n        \"loss\": validation_error, #! (2)\n        \"info_dict\": {\n            \"training_error\": training_error\n            # + Other metrics\n        },\n    }\n\n\npipeline_space = {  # (3)!\n    \"hyperparameter_b\":neps.IntegerParameter(1, 42, is_fidelity=True), #! (4)\n    \"hyperparameter_a\":neps.FloatParameter(1e-3, 1e-1, log=True) #! (5)\n    \"architecture_parameter\": neps.CategoricalParameter([\"option_a\", \"option_b\", \"option_c\"]),\n}\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    neps.run(\n        run_pipeline=run_pipeline,\n        pipeline_space=pipeline_space,\n        root_directory=\"path/to/save/results\",  # Replace with the actual path.\n        max_evaluations_total=100,\n        searcher=\"hyperband\"  # Optional specifies the search strategy,\n        # otherwise NePs decides based on your data.\n    )\n</code></pre> <ol> <li>Define a function that accepts hyperparameters and computes the validation error.</li> <li>Return a dictionary with the objective to minimize and any additional information.</li> <li>Define a search space of the parameters of interest; ensure that the names are consistent with those defined in the run_pipeline function.</li> <li>Use <code>is_fidelity=True</code> for a multi-fidelity approach.</li> <li>Use <code>log=True</code> for a log-spaced hyperparameter.</li> </ol> <p>Tip</p> <p>Please visit the full reference for a more comprehensive walkthrough of defining budgets, optimizers, YAML configuration, parallelism, and more.</p>"},{"location":"getting_started/#examples","title":"Examples","text":"<p>Discover the features of NePS through these practical examples:</p> <ul> <li> <p>Hyperparameter Optimization (HPO): Learn the essentials of hyperparameter optimization with NePS.</p> </li> <li> <p>Architecture Search with Primitives: Dive into architecture search using primitives in NePS.</p> </li> <li> <p>Multi-Fidelity Optimization: Understand how to leverage multi-fidelity optimization for efficient model tuning.</p> </li> <li> <p>Utilizing Expert Priors for Hyperparameters: Learn how to incorporate expert priors for more efficient hyperparameter selection.</p> </li> <li> <p>Additional NePS Examples: Explore more examples, including various use cases and advanced configurations in NePS.</p> </li> </ul>"},{"location":"api/","title":"API","text":"<p>Use the tree to navigate the API documentation.</p>"},{"location":"api/neps/api/","title":"Api","text":""},{"location":"api/neps/api/#neps.api","title":"neps.api","text":"<p>API for the neps package.</p>"},{"location":"api/neps/api/#neps.api.run","title":"run","text":"<pre><code>run(\n    run_pipeline: Callable | None = None,\n    root_directory: str | Path | None = None,\n    pipeline_space: (\n        dict[str, Parameter | ConfigurationSpace]\n        | str\n        | Path\n        | ConfigurationSpace\n        | None\n    ) = None,\n    run_args: str | Path | None = None,\n    overwrite_working_directory: bool = False,\n    post_run_summary: bool = True,\n    development_stage_id=None,\n    task_id=None,\n    max_evaluations_total: int | None = None,\n    max_evaluations_per_run: int | None = None,\n    continue_until_max_evaluation_completed: bool = False,\n    max_cost_total: int | float | None = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    pre_load_hooks: Iterable | None = None,\n    searcher: (\n        Literal[\n            \"default\",\n            \"bayesian_optimization\",\n            \"random_search\",\n            \"hyperband\",\n            \"priorband\",\n            \"mobster\",\n            \"asha\",\n            \"regularized_evolution\",\n        ]\n        | BaseOptimizer\n        | Path\n    ) = \"default\",\n    **searcher_kwargs\n) -&gt; None\n</code></pre> <p>Run a neural pipeline search.</p> To parallelize <p>To run a neural pipeline search with multiple processes or machines, simply call run(.) multiple times (optionally on different machines). Make sure that root_directory points to the same folder on the same filesystem, otherwise, the multiple calls to run(.) will be independent.</p> PARAMETER DESCRIPTION <code>run_pipeline</code> <p>The objective function to minimize.</p> <p> TYPE: <code>Callable | None</code> DEFAULT: <code>None</code> </p> <code>pipeline_space</code> <p>The search space to minimize over.</p> <p> TYPE: <code>dict[str, Parameter | ConfigurationSpace] | str | Path | ConfigurationSpace | None</code> DEFAULT: <code>None</code> </p> <code>root_directory</code> <p>The directory to save progress to. This is also used to synchronize multiple calls to run(.) for parallelization.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>run_args</code> <p>An option for providing the optimization settings e.g. max_evaluation_total in a YAML file.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>overwrite_working_directory</code> <p>If true, delete the working directory at the start of the run. This is, e.g., useful when debugging a run_pipeline function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>post_run_summary</code> <p>If True, creates a csv file after each worker is done, holding summary information about the configs and results.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>development_stage_id</code> <p>ID for the current development stage. Only needed if you work with multiple development stages.</p> <p> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID for the current task. Only needed if you work with multiple tasks.</p> <p> DEFAULT: <code>None</code> </p> <code>max_evaluations_total</code> <p>Number of evaluations after which to terminate.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>max_evaluations_per_run</code> <p>Number of evaluations the specific call to run(.) should maximally do.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>continue_until_max_evaluation_completed</code> <p>If true, only stop after max_evaluations_total have been completed. This is only relevant in the parallel setting.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>max_cost_total</code> <p>No new evaluations will start when this cost is exceeded. Requires returning a cost in the run_pipeline function, e.g., <code>return dict(loss=loss, cost=cost)</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>None</code> </p> <code>ignore_errors</code> <p>Ignore hyperparameter settings that threw an error and do not raise an error. Error configs still count towards max_evaluations_total.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>pre_load_hooks</code> <p>List of functions that will be called before load_results().</p> <p> TYPE: <code>Iterable | None</code> DEFAULT: <code>None</code> </p> <code>searcher</code> <p>Which optimizer to use. Can be a string identifier, an instance of BaseOptimizer, or a Path to a custom optimizer.</p> <p> TYPE: <code>Literal['default', 'bayesian_optimization', 'random_search', 'hyperband', 'priorband', 'mobster', 'asha', 'regularized_evolution'] | BaseOptimizer | Path</code> DEFAULT: <code>'default'</code> </p> <code>**searcher_kwargs</code> <p>Will be passed to the searcher. This is usually only needed by neps develolpers.</p> <p> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If deprecated argument working_directory is used.</p> <code>ValueError</code> <p>If root_directory is None.</p> Example <p>import neps</p> <p>def run_pipeline(some_parameter: float):    validation_error = -some_parameter    return validation_error</p> <p>pipeline_space = dict(some_parameter=neps.FloatParameter(lower=0, upper=1))</p> <p>logging.basicConfig(level=logging.INFO) neps.run(    run_pipeline=run_pipeline,    pipeline_space=pipeline_space,    root_directory=\"usage_example\",    max_evaluations_total=5, )</p> Source code in <code>neps/api.py</code> <pre><code>def run(\n    run_pipeline: Callable | None = None,\n    root_directory: str | Path | None = None,\n    pipeline_space: (\n        dict[str, Parameter | CS.ConfigurationSpace]\n        | str\n        | Path\n        | CS.ConfigurationSpace\n        | None\n    ) = None,\n    run_args: str | Path | None = None,\n    overwrite_working_directory: bool = False,\n    post_run_summary: bool = True,\n    development_stage_id=None,\n    task_id=None,\n    max_evaluations_total: int | None = None,\n    max_evaluations_per_run: int | None = None,\n    continue_until_max_evaluation_completed: bool = False,\n    max_cost_total: int | float | None = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    pre_load_hooks: Iterable | None = None,\n    searcher: (\n        Literal[\n            \"default\",\n            \"bayesian_optimization\",\n            \"random_search\",\n            \"hyperband\",\n            \"priorband\",\n            \"mobster\",\n            \"asha\",\n            \"regularized_evolution\",\n        ]\n        | BaseOptimizer | Path\n    ) = \"default\",\n    **searcher_kwargs,\n) -&gt; None:\n    \"\"\"Run a neural pipeline search.\n\n    To parallelize:\n        To run a neural pipeline search with multiple processes or machines,\n        simply call run(.) multiple times (optionally on different machines). Make sure\n        that root_directory points to the same folder on the same filesystem, otherwise,\n        the multiple calls to run(.) will be independent.\n\n    Args:\n        run_pipeline: The objective function to minimize.\n        pipeline_space: The search space to minimize over.\n        root_directory: The directory to save progress to. This is also used to\n            synchronize multiple calls to run(.) for parallelization.\n        run_args: An option for providing the optimization settings e.g.\n            max_evaluation_total in a YAML file.\n        overwrite_working_directory: If true, delete the working directory at the start of\n            the run. This is, e.g., useful when debugging a run_pipeline function.\n        post_run_summary: If True, creates a csv file after each worker is done,\n            holding summary information about the configs and results.\n        development_stage_id: ID for the current development stage. Only needed if\n            you work with multiple development stages.\n        task_id: ID for the current task. Only needed if you work with multiple\n            tasks.\n        max_evaluations_total: Number of evaluations after which to terminate.\n        max_evaluations_per_run: Number of evaluations the specific call to run(.) should\n            maximally do.\n        continue_until_max_evaluation_completed: If true, only stop after\n            max_evaluations_total have been completed. This is only relevant in the\n            parallel setting.\n        max_cost_total: No new evaluations will start when this cost is exceeded. Requires\n            returning a cost in the run_pipeline function, e.g.,\n            `return dict(loss=loss, cost=cost)`.\n        ignore_errors: Ignore hyperparameter settings that threw an error and do not raise\n            an error. Error configs still count towards max_evaluations_total.\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error and will use given loss value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error and will use given cost value instead. default: None\n        pre_load_hooks: List of functions that will be called before load_results().\n        searcher: Which optimizer to use. Can be a string identifier, an\n            instance of BaseOptimizer, or a Path to a custom optimizer.\n        **searcher_kwargs: Will be passed to the searcher. This is usually only needed by\n            neps develolpers.\n\n    Raises:\n        ValueError: If deprecated argument working_directory is used.\n        ValueError: If root_directory is None.\n\n\n    Example:\n        &gt;&gt;&gt; import neps\n\n        &gt;&gt;&gt; def run_pipeline(some_parameter: float):\n        &gt;&gt;&gt;    validation_error = -some_parameter\n        &gt;&gt;&gt;    return validation_error\n\n        &gt;&gt;&gt; pipeline_space = dict(some_parameter=neps.FloatParameter(lower=0, upper=1))\n\n        &gt;&gt;&gt; logging.basicConfig(level=logging.INFO)\n        &gt;&gt;&gt; neps.run(\n        &gt;&gt;&gt;    run_pipeline=run_pipeline,\n        &gt;&gt;&gt;    pipeline_space=pipeline_space,\n        &gt;&gt;&gt;    root_directory=\"usage_example\",\n        &gt;&gt;&gt;    max_evaluations_total=5,\n        &gt;&gt;&gt; )\n    \"\"\"\n    if \"working_directory\" in searcher_kwargs:\n        raise ValueError(\n            \"The argument 'working_directory' is deprecated, please use 'root_directory' \"\n            \"instead\"\n        )\n\n    if \"budget\" in searcher_kwargs:\n        warnings.warn(\n            \"The argument: 'budget' is deprecated. In the neps.run call, please, use \"\n            \"'max_cost_total' instead. In future versions using `budget` will fail.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        max_cost_total = searcher_kwargs[\"budget\"]\n        del searcher_kwargs[\"budget\"]\n    logger = logging.getLogger(\"neps\")\n\n    if run_args:\n        optim_settings = get_run_args_from_yaml(run_args)\n        check_double_reference(run, locals(), optim_settings)\n        run_pipeline = optim_settings.get(\"run_pipeline\", run_pipeline)\n        root_directory = optim_settings.get(\"root_directory\", root_directory)\n        pipeline_space = optim_settings.get(\"pipeline_space\", pipeline_space)\n        overwrite_working_directory = optim_settings.get(\n            \"overwrite_working_directory\", overwrite_working_directory\n        )\n        post_run_summary = optim_settings.get(\"post_run_summary\", post_run_summary)\n        development_stage_id = optim_settings.get(\"development_stage_id\",\n                                                  development_stage_id)\n        task_id = optim_settings.get(\"task_id\", task_id)\n        max_evaluations_total = optim_settings.get(\"max_evaluations_total\",\n                                                   max_evaluations_total)\n        max_evaluations_per_run = optim_settings.get(\"max_evaluations_per_run\",\n                                                     max_evaluations_per_run)\n        continue_until_max_evaluation_completed = optim_settings.get(\n            \"continue_until_max_evaluation_completed\",\n            continue_until_max_evaluation_completed)\n        max_cost_total = optim_settings.get(\"max_cost_total\", max_cost_total)\n        ignore_errors = optim_settings.get(\"ignore_errors\", ignore_errors)\n        loss_value_on_error = optim_settings.get(\"loss_value_on_error\",\n                                                 loss_value_on_error)\n        cost_value_on_error = optim_settings.get(\"cost_value_on_error\",\n                                                 cost_value_on_error)\n        pre_load_hooks = optim_settings.get(\"pre_load_hooks\", pre_load_hooks)\n        searcher = optim_settings.get(\"searcher\", searcher)\n        # considers arguments of a provided SubClass of BaseOptimizer\n        searcher_class_arguments = optim_settings.get(\"custom_class_searcher_kwargs\", {})\n\n    # check if necessary arguments are provided.\n    check_essential_arguments(\n        run_pipeline,\n        root_directory,\n        pipeline_space,\n        max_cost_total,\n        max_evaluations_total,\n        searcher,\n        run_args,\n    )\n\n    if pre_load_hooks is None:\n        pre_load_hooks = []\n\n    logger.info(f\"Starting neps.run using root directory {root_directory}\")\n\n    # Used to create the yaml holding information about the searcher.\n    # Also important for testing and debugging the api.\n    searcher_info = {\n        \"searcher_name\": \"\",\n        \"searcher_alg\": \"\",\n        \"searcher_selection\": \"\",\n        \"neps_decision_tree\": True,\n        \"searcher_args\": {},\n    }\n\n    # special case if you load your own optimizer via run_args\n    if inspect.isclass(searcher):\n        if issubclass(searcher, BaseOptimizer):\n            search_space = SearchSpace(**pipeline_space)\n            # aligns with the behavior of the internal neps searcher which also overwrites\n            # its arguments by using searcher_kwargs\n            merge_kwargs = {**searcher_class_arguments, **searcher_kwargs}\n            searcher_info[\"searcher_args\"] = merge_kwargs\n            searcher = searcher(search_space, **merge_kwargs)\n        else:\n            # Raise an error if searcher is not a subclass of BaseOptimizer\n            raise TypeError(\n                \"The provided searcher must be a class that inherits from BaseOptimizer.\"\n            )\n\n    if isinstance(searcher, BaseOptimizer):\n        searcher_instance = searcher\n        searcher_info[\"searcher_name\"] = \"baseoptimizer\"\n        searcher_info[\"searcher_alg\"] = searcher.whoami()\n        searcher_info[\"searcher_selection\"] = \"user-instantiation\"\n        searcher_info[\"neps_decision_tree\"] = False\n    else:\n        (\n            searcher_instance,\n            searcher_info,\n        ) = _run_args(\n            searcher_info=searcher_info,\n            pipeline_space=pipeline_space,\n            max_cost_total=max_cost_total,\n            ignore_errors=ignore_errors,\n            loss_value_on_error=loss_value_on_error,\n            cost_value_on_error=cost_value_on_error,\n            logger=logger,\n            searcher=searcher,\n            **searcher_kwargs,\n        )\n\n    # Check to verify if the target directory contains history of another optimizer state\n    # This check is performed only when the `searcher` is built during the run\n    if not isinstance(searcher, (BaseOptimizer, str, dict, Path)):\n        raise ValueError(\n            f\"Unrecognized `searcher` of type {type(searcher)}. Not str or BaseOptimizer.\"\n        )\n    elif isinstance(searcher, BaseOptimizer):\n        # This check is not strict when a user-defined neps.optimizer is provided\n        logger.warning(\n            \"An instantiated optimizer is provided. The safety checks of NePS will be \"\n            \"skipped. Accurate continuation of runs can no longer be guaranteed!\"\n        )\n\n    if task_id is not None:\n        root_directory = Path(root_directory) / f\"task_{task_id}\"\n    if development_stage_id is not None:\n        root_directory = Path(root_directory) / f\"dev_{development_stage_id}\"\n\n    launch_runtime(\n        evaluation_fn=run_pipeline,\n        sampler=searcher_instance,\n        optimizer_info=searcher_info,\n        optimization_dir=root_directory,\n        max_evaluations_total=max_evaluations_total,\n        max_evaluations_per_run=max_evaluations_per_run,\n        continue_until_max_evaluation_completed=continue_until_max_evaluation_completed,\n        logger=logger,\n        loss_value_on_error=loss_value_on_error,\n        ignore_errors=ignore_errors,\n        overwrite_optimization_dir=overwrite_working_directory,\n        pre_load_hooks=pre_load_hooks,\n    )\n\n    if post_run_summary:\n        assert root_directory is not None\n        post_run_csv(root_directory)\n</code></pre>"},{"location":"api/neps/runtime/","title":"Runtime","text":""},{"location":"api/neps/runtime/#neps.runtime","title":"neps.runtime","text":"<p>Module for the runtime of a single instance of NePS running.</p> <p>An important advantage of NePS with a running instance per worker and no multiprocessing is that we can reliably use globals to store information such as the currently running configuration, without interfering with other workers which have launched.</p> <p>This allows us to have a global <code>Trial</code> object which can be accessed using <code>import neps.runtime; neps.get_in_progress_trial()</code>.</p> <p>This module primarily handles the worker loop where important concepts are: * State: The state of optimization is all of the configurations, their results and  the current state of the optimizer. * Shared State: Whenever a worker wishes to read or write any state, they will lock the shared state, declaring themselves as operating on it. At this point, no other worker can access the shared state. * Optimizer Hydration: This is the process through which an optimizer instance is hydrated with the Shared State so it can make a decision, i.e. for sampling. Equally we serialize the optimizer when writing it back to Shared State * Trial Lock: When evaluating a configuration, a worker must lock it to declared itself as evaluating it. This communicates to other workers that this configuration is in progress.</p>"},{"location":"api/neps/runtime/#neps.runtime--loop","title":"Loop","text":"<p>We mark lines with <code>+</code> as the worker having locked the Shared State and <code>~</code> as the worker having locked the Trial. The trial lock <code>~</code> is allowed to fail, in which case all steps with a <code>~</code> are skipped and the loop continues.</p> <ol> <li> <ul> <li>Check exit conditions</li> </ul> </li> <li> <ul> <li>Hydrate the optimizer</li> </ul> </li> <li> <ul> <li>Sample a new Trial</li> </ul> </li> <li>Unlock the Shared State</li> <li>~ Obtain a Trial Lock</li> <li>~ Set the global trial for this work to the current trial</li> <li>~ Evaluate the trial</li> <li>~+ Lock the shared state</li> <li>~+ Write the results of the config to disk</li> <li>~+ Update the optimizer if required (used budget for evaluating trial)</li> <li>~ Unlock the shared state</li> <li>Unlock Trial Lock</li> </ol>"},{"location":"api/neps/runtime/#neps.runtime.ErrorReport","title":"ErrorReport  <code>dataclass</code>","text":"<pre><code>ErrorReport(\n    err: Exception,\n    tb: str | None,\n    loss: float | None,\n    cost: float | None,\n    account_for_cost: bool,\n    results: Mapping[str, Any],\n)\n</code></pre> <p>A failed report of the evaluation of a configuration.</p>"},{"location":"api/neps/runtime/#neps.runtime.SharedState","title":"SharedState  <code>dataclass</code>","text":"<pre><code>SharedState(\n    base_dir: Path,\n    create_dirs: bool = False,\n    trials: dict[ConfigID, tuple[Trial, State]] = dict(),\n)\n</code></pre> <p>The shared state of the optimization process that workers communicate through.</p> ATTRIBUTE DESCRIPTION <code>base_dir</code> <p>The base directory from which the optimization is running.</p> <p> TYPE: <code>Path</code> </p> <code>create_dirs</code> <p>Whether to create the directories if they do not exist.</p> <p> TYPE: <code>bool</code> </p> <code>lock</code> <p>The lock to signify that a worker is operating on the shared state.</p> <p> TYPE: <code>Locker</code> </p> <code>optimizer_state_file</code> <p>The path to the optimizers state.</p> <p> TYPE: <code>Locker</code> </p> <code>optimizer_info_file</code> <p>The path to the file containing information about the optimizer's setup.</p> <p> TYPE: <code>Locker</code> </p> <code>seed_state_dir</code> <p>Directory where the seed state is stored.</p> <p> TYPE: <code>Locker</code> </p> <code>results_dir</code> <p>Directory where results for configurations are stored.</p> <p> TYPE: <code>Locker</code> </p>"},{"location":"api/neps/runtime/#neps.runtime.SharedState.trials","title":"trials  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trials: dict[ConfigID, tuple[Trial, State]] = field(\n    default_factory=dict\n)\n</code></pre> <p>Mapping from a configid to the trial and it's last known state, including if it's been evaluated.</p>"},{"location":"api/neps/runtime/#neps.runtime.SharedState.check_optimizer_info_on_disk_matches","title":"check_optimizer_info_on_disk_matches","text":"<pre><code>check_optimizer_info_on_disk_matches(\n    optimizer_info: dict[str, Any],\n    *,\n    excluded_keys: Iterable[str] = (\"searcher_name\")\n) -&gt; None\n</code></pre> <p>Sanity check that the provided info matches the one on disk (if any).</p> PARAMETER DESCRIPTION <code>optimizer_info</code> <p>The optimizer info to check.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>excluded_keys</code> <p>Any keys to exclude during the comparison.</p> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>('searcher_name')</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If there is optimizer info on disk and it does not match the</p> Source code in <code>neps/runtime.py</code> <pre><code>def check_optimizer_info_on_disk_matches(\n    self,\n    optimizer_info: dict[str, Any],\n    *,\n    excluded_keys: Iterable[str] = (\"searcher_name\",),\n) -&gt; None:\n    \"\"\"Sanity check that the provided info matches the one on disk (if any).\n\n    Args:\n        optimizer_info: The optimizer info to check.\n        excluded_keys: Any keys to exclude during the comparison.\n\n    Raises:\n        ValueError: If there is optimizer info on disk and it does not match the\n        provided info.\n    \"\"\"\n    optimizer_info_copy = optimizer_info.copy()\n    loaded_info = deserialize(self.paths.optimizer_info_file)\n\n    for key in excluded_keys:\n        optimizer_info_copy.pop(key, None)\n        loaded_info.pop(key, None)\n\n    if optimizer_info_copy != loaded_info:\n        raise ValueError(\n            f\"The sampler_info in the file {self.paths.optimizer_info_file} is not\"\n            f\" valid. Expected: {optimizer_info_copy}, Found: {loaded_info}\",\n        )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.SharedState.sync","title":"sync","text":"<pre><code>sync(*, lock: bool = True) -&gt; Iterator[None]\n</code></pre> <p>Sync up with what's on disk.</p> Source code in <code>neps/runtime.py</code> <pre><code>@contextmanager\ndef sync(self, *, lock: bool = True) -&gt; Iterator[None]:\n    \"\"\"Sync up with what's on disk.\"\"\"\n    if lock:\n        _poll, _timeout = get_shared_state_poll_and_timeout()\n        with self.lock(poll=_poll, timeout=_timeout):\n            self.update_from_disk()\n            yield\n    else:\n        yield\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.SharedState.trials_by_state","title":"trials_by_state","text":"<pre><code>trials_by_state() -&gt; dict[State, list[Trial]]\n</code></pre> <p>Get the trials grouped by their state.</p> Source code in <code>neps/runtime.py</code> <pre><code>def trials_by_state(self) -&gt; dict[Trial.State, list[Trial]]:\n    \"\"\"Get the trials grouped by their state.\"\"\"\n    _dict: dict[Trial.State, list[Trial]] = {state: [] for state in Trial.State}\n    for trial, state in self.trials.values():\n        _dict[state].append(trial)\n    return _dict\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.SharedState.update_from_disk","title":"update_from_disk","text":"<pre><code>update_from_disk() -&gt; None\n</code></pre> <p>Update the shared state from disk.</p> Source code in <code>neps/runtime.py</code> <pre><code>def update_from_disk(self) -&gt; None:\n    \"\"\"Update the shared state from disk.\"\"\"\n    trial_dirs = (p for p in self.paths.results_dir.iterdir() if p.is_dir())\n    _disks = [Trial.Disk.from_dir(p) for p in trial_dirs]\n    _disk_lookup = {disk.config_id: disk for disk in _disks}\n\n    # NOTE: We sort all trials such that we process previous trials first, i.e.\n    # if trial_3 has trial_2 as previous, we process trial_2 first, which\n    # requires trial_1 to have been processed first.\n    def _depth(trial: Trial.Disk) -&gt; int:\n        depth = 0\n        previous = trial.previous_config_id\n        while previous is not None:\n            depth += 1\n            previous_trial = _disk_lookup.get(previous)\n            if previous_trial is None:\n                raise RuntimeError(\n                    \"Previous trial not found on disk when processing a trial.\"\n                    \" This should not happen as if a tria has a previous trial,\"\n                    \" then it should be present and evaluated on disk.\",\n                )\n            previous = previous_trial.previous_config_id\n\n        return depth\n\n    # This allows is to traverse linearly and used cached values of previous\n    # trial data loading, as done below.\n    _disks.sort(key=_depth)\n\n    for disk in _disks:\n        config_id = disk.config_id\n        state = disk.state()\n\n        if state is Trial.State.CORRUPTED:\n            logger.warning(f\"Trial {config_id} was corrupted somehow!\")\n\n        previous: Trial | None = None\n        if disk.previous_config_id is not None:\n            previous, _ = self.trials.get(disk.previous_config_id, (None, None))\n            if previous is None:\n                raise RuntimeError(\n                    \"Previous trial not found in memory when processing a trial.\"\n                    \" This should not happen as if a trial has a previous trial,\"\n                    \" then it should be present and evaluated in memory.\",\n                )\n\n        cached_trial = self.trials.get(config_id, None)\n\n        # If not currently cached or it was and had a state change\n        if cached_trial is None or cached_trial[1] != state:\n            trial = Trial.from_disk(disk, previous=previous)\n            self.trials[config_id] = (trial, state)\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.SharedState.use_sampler","title":"use_sampler","text":"<pre><code>use_sampler(\n    sampler: BaseOptimizer, *, serialize_seed: bool = True\n) -&gt; Iterator[BaseOptimizer]\n</code></pre> <p>Use the sampler with the shared state.</p> Source code in <code>neps/runtime.py</code> <pre><code>@contextmanager\ndef use_sampler(\n    self,\n    sampler: BaseOptimizer,\n    *,\n    serialize_seed: bool = True,\n) -&gt; Iterator[BaseOptimizer]:\n    \"\"\"Use the sampler with the shared state.\"\"\"\n    if serialize_seed:\n        with SeedState.use(self.paths.seed_state_dir), sampler.using_state(\n            self.paths.optimizer_state_file\n        ):\n            yield sampler\n    else:\n        with sampler.using_state(self.paths.optimizer_state_file):\n            yield sampler\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.StatePaths","title":"StatePaths  <code>dataclass</code>","text":"<pre><code>StatePaths(root: Path, create_dirs: bool = False)\n</code></pre> <p>The paths used for the state of the optimization process.</p> <p>Most important method is <code>config_dir</code>, which gives the directory to use for a configuration.</p> ATTRIBUTE DESCRIPTION <code>root</code> <p>The root directory of the optimization process.</p> <p> TYPE: <code>Path</code> </p> <code>create_dirs</code> <p>Whether to create the directories if they do not exist.</p> <p> TYPE: <code>bool</code> </p> <code>optimizer_state_file</code> <p>The path to the optimizer state file.</p> <p> TYPE: <code>Path</code> </p> <code>optimizer_info_file</code> <p>The path to the optimizer info file.</p> <p> TYPE: <code>Path</code> </p> <code>seed_state_dir</code> <p>The directory where the seed state is stored.</p> <p> TYPE: <code>Path</code> </p> <code>results_dir</code> <p>The directory where results are stored.</p> <p> TYPE: <code>Path</code> </p>"},{"location":"api/neps/runtime/#neps.runtime.StatePaths.config_dir","title":"config_dir","text":"<pre><code>config_dir(config_id: ConfigID) -&gt; Path\n</code></pre> <p>Get the directory for a configuration.</p> Source code in <code>neps/runtime.py</code> <pre><code>def config_dir(self, config_id: ConfigID) -&gt; Path:\n    \"\"\"Get the directory for a configuration.\"\"\"\n    return self.results_dir / f\"config_{config_id}\"\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.SuccessReport","title":"SuccessReport  <code>dataclass</code>","text":"<pre><code>SuccessReport(\n    loss: float,\n    cost: float | None,\n    account_for_cost: bool,\n    results: Mapping[str, Any],\n)\n</code></pre> <p>A successful report of the evaluation of a configuration.</p>"},{"location":"api/neps/runtime/#neps.runtime.Trial","title":"Trial  <code>dataclass</code>","text":"<pre><code>Trial(\n    id: ConfigID,\n    config: Mapping[str, Any],\n    pipeline_dir: Path,\n    previous: Trial | None,\n    report: Report | None,\n    time_sampled: float,\n    metadata: dict[str, Any],\n)\n</code></pre> <p>A trial is a configuration and it's associated data.</p> <p>The object is considered mutable and the global trial currently being evaluated can be access using <code>get_in_progress_trial()</code>.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for the configuration</p> <p> TYPE: <code>ConfigID</code> </p> <code>config</code> <p>The configuration to evaluate</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>pipeline_dir</code> <p>Directory where the configuration is evaluated</p> <p> TYPE: <code>Path</code> </p> <code>previous</code> <p>The previous trial before this trial.</p> <p> TYPE: <code>Trial | None</code> </p> <code>time_sampled</code> <p>The time the configuration was sampled</p> <p> TYPE: <code>float</code> </p> <code>metadata</code> <p>Additional metadata about the configuration</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/neps/runtime/#neps.runtime.Trial.config_file","title":"config_file  <code>property</code>","text":"<pre><code>config_file: Path\n</code></pre> <p>The path to the configuration file.</p>"},{"location":"api/neps/runtime/#neps.runtime.Trial.metadata_file","title":"metadata_file  <code>property</code>","text":"<pre><code>metadata_file: Path\n</code></pre> <p>The path to the metadata file.</p>"},{"location":"api/neps/runtime/#neps.runtime.Trial.previous_config_id_file","title":"previous_config_id_file  <code>property</code>","text":"<pre><code>previous_config_id_file: Path\n</code></pre> <p>The path to the previous configuration id file.</p>"},{"location":"api/neps/runtime/#neps.runtime.Trial.Disk","title":"Disk  <code>dataclass</code>","text":"<pre><code>Disk(pipeline_dir: Path)\n</code></pre> <p>The disk information of a trial.</p> ATTRIBUTE DESCRIPTION <code>pipeline_dir</code> <p>The directory where the trial is stored</p> <p> TYPE: <code>Path</code> </p> <code>id</code> <p>The unique identifier of the trial</p> <p> TYPE: <code>Path</code> </p> <code>config_file</code> <p>The path to the configuration file</p> <p> TYPE: <code>Path</code> </p> <code>result_file</code> <p>The path to the result file</p> <p> TYPE: <code>Path</code> </p> <code>metadata_file</code> <p>The path to the metadata file</p> <p> TYPE: <code>Path</code> </p> <code>optimization_dir</code> <p>The directory from which optimization is running</p> <p> TYPE: <code>Path</code> </p> <code>previous_config_id_file</code> <p>The path to the previous config id file</p> <p> TYPE: <code>Path</code> </p> <code>previous_pipeline_dir</code> <p>The directory of the previous configuration</p> <p> TYPE: <code>Path | None</code> </p> <code>lock</code> <p>The lock for the trial. Obtaining this lock indicates the worker is evaluating this trial.</p> <p> TYPE: <code>Locker</code> </p>"},{"location":"api/neps/runtime/#neps.runtime.Trial.Disk.from_dir","title":"from_dir  <code>classmethod</code>","text":"<pre><code>from_dir(pipeline_dir: Path) -&gt; Disk\n</code></pre> <p>Create a <code>Trial.Disk</code> object from a directory.</p> Source code in <code>neps/runtime.py</code> <pre><code>@classmethod\ndef from_dir(cls, pipeline_dir: Path) -&gt; Trial.Disk:\n    \"\"\"Create a `Trial.Disk` object from a directory.\"\"\"\n    return cls(pipeline_dir=pipeline_dir)\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.Trial.Disk.raw_config","title":"raw_config","text":"<pre><code>raw_config() -&gt; dict[str, Any]\n</code></pre> <p>Deserialize the configuration from disk.</p> Source code in <code>neps/runtime.py</code> <pre><code>def raw_config(self) -&gt; dict[str, Any]:\n    \"\"\"Deserialize the configuration from disk.\"\"\"\n    return deserialize(self.config_file)\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.Trial.Disk.state","title":"state","text":"<pre><code>state() -&gt; State\n</code></pre> <p>The state of the trial.</p> Source code in <code>neps/runtime.py</code> <pre><code>def state(self) -&gt; Trial.State:  # noqa: PLR0911\n    \"\"\"The state of the trial.\"\"\"\n    result_file_exists = not empty_file(self.result_file)\n    error_file_exists = not empty_file(self.error_file)\n    config_file_exists = not empty_file(self.config_file)\n\n    # NOTE: We don't handle the case where it's locked and there is a result\n    # or error file existing, namely as this might introduce a race condition,\n    # where the result/error is being written to while the lock still exists.\n\n    if error_file_exists:\n        # Should not have a results file if there is an error file\n        if result_file_exists:\n            return Trial.State.CORRUPTED\n\n        # Should have a config file if there is an error file\n        if not config_file_exists:\n            return Trial.State.CORRUPTED\n\n        return Trial.State.ERROR\n\n    if result_file_exists:\n        # Should have a config file if there is a results file\n        if not config_file_exists:\n            return Trial.State.CORRUPTED\n\n        return Trial.State.SUCCESS\n\n    if self.lock.is_locked():\n        # Should have a config to evaluate if it's locked\n        if not config_file_exists:\n            return Trial.State.CORRUPTED\n\n        return Trial.State.IN_PROGRESS\n\n    return Trial.State.PENDING\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.Trial.State","title":"State","text":"<p>               Bases: <code>Enum</code></p> <p>The state of a trial.</p>"},{"location":"api/neps/runtime/#neps.runtime.Trial.create_error_report","title":"create_error_report","text":"<pre><code>create_error_report(\n    err: Exception, tb: str | None = None\n) -&gt; ErrorReport\n</code></pre> <p>Create a <code>Report</code> object with an error.</p> Source code in <code>neps/runtime.py</code> <pre><code>def create_error_report(self, err: Exception, tb: str | None = None) -&gt; ErrorReport:\n    \"\"\"Create a [`Report`][neps.runtime.Report] object with an error.\"\"\"\n    # TODO(eddiebergman): For now we assume the loss and cost for an error is None\n    # and that we don't account for cost and there are no possible results.\n    return ErrorReport(\n        loss=None,\n        cost=None,\n        account_for_cost=False,\n        results={},\n        err=err,\n        tb=tb,\n    )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.Trial.create_success_report","title":"create_success_report","text":"<pre><code>create_success_report(\n    result: float | Mapping[str, Any]\n) -&gt; SuccessReport\n</code></pre> <p>Check if the trial has succeeded.</p> Source code in <code>neps/runtime.py</code> <pre><code>def create_success_report(self, result: float | Mapping[str, Any]) -&gt; SuccessReport:\n    \"\"\"Check if the trial has succeeded.\"\"\"\n    _result: dict[str, Any] = {}\n    if isinstance(result, Mapping):\n        if \"loss\" not in result:\n            raise KeyError(\"The 'loss' should be provided in the evaluation result\")\n\n        _result = dict(result)\n        loss = _result[\"loss\"]\n    else:\n        loss = result\n\n    try:\n        _result[\"loss\"] = float(loss)\n    except (TypeError, ValueError) as e:\n        raise ValueError(\n            \"The evaluation result should be a dictionnary or a float but got\"\n            f\" a `{type(loss)}` with value of {loss}\",\n        ) from e\n\n    # TODO(eddiebergman): For now we have no access to the cost for crash\n    # so we just set it to None.\n    _cost: float | None = _result.get(\"cost\", None)\n    if _cost is not None:\n        try:\n            _result[\"cost\"] = float(_cost)\n        except (TypeError, ValueError) as e:\n            raise ValueError(\n                \"The evaluation result should be a dictionnary or a float but got\"\n                f\" a `{type(_cost)}` with value of {_cost}\",\n            ) from e\n\n    # TODO(eddiebergman): Should probably be a global user setting for this.\n    _account_for_cost = _result.get(\"account_for_cost\", True)\n\n    return SuccessReport(\n        loss=_result[\"loss\"],\n        cost=_cost,\n        account_for_cost=_account_for_cost,\n        results=_result,\n    )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.Trial.from_dir","title":"from_dir  <code>classmethod</code>","text":"<pre><code>from_dir(\n    pipeline_dir: Path, *, previous: Trial | None = None\n) -&gt; Self\n</code></pre> <p>Create a <code>Trial</code> object from a directory.</p> PARAMETER DESCRIPTION <code>pipeline_dir</code> <p>The directory where the trial is stored</p> <p> TYPE: <code>Path</code> </p> <code>previous</code> <p>The previous trial before this trial. You can use this to prevent loading the previous trial from disk, if it exists, i.e. a caching shortcut.</p> <p> TYPE: <code>Trial | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The trial object.</p> Source code in <code>neps/runtime.py</code> <pre><code>@classmethod\ndef from_dir(cls, pipeline_dir: Path, *, previous: Trial | None = None) -&gt; Self:\n    \"\"\"Create a `Trial` object from a directory.\n\n    Args:\n        pipeline_dir: The directory where the trial is stored\n        previous: The previous trial before this trial.\n            You can use this to prevent loading the previous trial from disk,\n            if it exists, i.e. a caching shortcut.\n\n    Returns:\n        The trial object.\n    \"\"\"\n    return cls.from_disk(\n        Trial.Disk.from_dir(pipeline_dir),\n        previous=previous,\n    )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.Trial.from_disk","title":"from_disk  <code>classmethod</code>","text":"<pre><code>from_disk(\n    disk: Disk, *, previous: Trial | None = None\n) -&gt; Self\n</code></pre> <p>Create a <code>Trial</code> object from a disk.</p> PARAMETER DESCRIPTION <code>disk</code> <p>The disk information of the trial.</p> <p> TYPE: <code>Disk</code> </p> <code>previous</code> <p>The previous trial before this trial. You can use this to prevent loading the previous trial from disk, if it exists, i.e. a caching shortcut.</p> <p> TYPE: <code>Trial | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The trial object.</p> Source code in <code>neps/runtime.py</code> <pre><code>@classmethod\ndef from_disk(cls, disk: Trial.Disk, *, previous: Trial | None = None) -&gt; Self:\n    \"\"\"Create a `Trial` object from a disk.\n\n    Args:\n        disk: The disk information of the trial.\n        previous: The previous trial before this trial.\n            You can use this to prevent loading the previous trial from disk,\n            if it exists, i.e. a caching shortcut.\n\n    Returns:\n        The trial object.\n    \"\"\"\n    try:\n        config = deserialize(disk.config_file)\n    except Exception as e:\n        logger.error(\n            f\"Error loading config from {disk.config_file}: {e}\",\n            exc_info=True,\n        )\n        config = {}\n\n    try:\n        metadata = deserialize(disk.metadata_file)\n        time_sampled = metadata[\"time_sampled\"]\n    except Exception as e:\n        logger.error(\n            f\"Error loading metadata from {disk.metadata_file}: {e}\",\n            exc_info=True,\n        )\n        metadata = {}\n        time_sampled = float(\"nan\")\n\n    try:\n        result: dict[str, Any] | tuple[Exception, str | None] | None\n        report: Report | None\n        if not empty_file(disk.result_file):\n            result = deserialize(disk.result_file)\n\n            assert isinstance(result, dict)\n            report = SuccessReport(\n                loss=result[\"loss\"],\n                cost=result.get(\"cost\", None),\n                account_for_cost=result.get(\"account_for_cost\", True),\n                results=result,\n            )\n        elif not empty_file(disk.error_file):\n            error_tb = deserialize(disk.error_file)\n            result = deserialize(disk.result_file)\n            report = ErrorReport(\n                # NOTE: Not sure we can easily get the original exception type,\n                # once serialized\n                err=Exception(error_tb[\"err\"]),\n                tb=error_tb.get(\"tb\"),\n                loss=result.get(\"loss\", None),\n                cost=result.get(\"cost\", None),\n                account_for_cost=result.get(\"account_for_cost\", True),\n                results=result,\n            )\n        else:\n            report = None\n    except Exception as e:\n        logger.error(\n            f\"Error loading result from {disk.result_file}: {e}\",\n            exc_info=True,\n        )\n        report = None\n\n    try:\n        if previous is None and disk.previous_pipeline_dir is not None:\n            previous = Trial.from_dir(disk.previous_pipeline_dir)\n    except Exception as e:\n        logger.error(\n            f\"Error loading previous from {disk.previous_pipeline_dir}: {e}\",\n            exc_info=True,\n        )\n        previous = None\n\n    return cls(\n        id=disk.config_id,\n        config=config,\n        pipeline_dir=disk.pipeline_dir,\n        report=report,\n        previous=previous,\n        time_sampled=time_sampled,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.Trial.to_config_result","title":"to_config_result","text":"<pre><code>to_config_result(\n    config_to_search_space: Callable[\n        [RawConfig], SearchSpace\n    ]\n) -&gt; ConfigResult\n</code></pre> <p>Convert the report to a <code>ConfigResult</code> object.</p> Source code in <code>neps/runtime.py</code> <pre><code>def to_config_result(\n    self,\n    config_to_search_space: Callable[[RawConfig], SearchSpace],\n) -&gt; ConfigResult:\n    \"\"\"Convert the report to a `ConfigResult` object.\"\"\"\n    result: ERROR | Mapping[str, Any] = (\n        \"error\"\n        if self.report is None or isinstance(self.report, ErrorReport)\n        else self.report.results\n    )\n    return ConfigResult(\n        self.id,\n        config=config_to_search_space(self.config),\n        result=result,\n        metadata=self.metadata,\n    )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.get_in_progress_trial","title":"get_in_progress_trial","text":"<pre><code>get_in_progress_trial() -&gt; Trial | None\n</code></pre> <p>Get the currently running trial in this process.</p> Source code in <code>neps/runtime.py</code> <pre><code>def get_in_progress_trial() -&gt; Trial | None:\n    \"\"\"Get the currently running trial in this process.\"\"\"\n    return _CURRENTLY_RUNNING_TRIAL_IN_PROCESS\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.get_shared_state_poll_and_timeout","title":"get_shared_state_poll_and_timeout","text":"<pre><code>get_shared_state_poll_and_timeout() -&gt; (\n    tuple[float, float | None]\n)\n</code></pre> <p>Get the poll and timeout for the shared state.</p> Source code in <code>neps/runtime.py</code> <pre><code>def get_shared_state_poll_and_timeout() -&gt; tuple[float, float | None]:\n    \"\"\"Get the poll and timeout for the shared state.\"\"\"\n    poll = float(os.environ.get(ENVIRON_STATE_POLL_KEY, DEFAULT_STATE_POLL))\n    timeout = os.environ.get(ENVIRON_STATE_TIMEOUT_KEY, DEFAULT_STATE_TIMEOUT)\n    timeout = float(timeout) if timeout is not None else None\n    return poll, timeout\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.launch_runtime","title":"launch_runtime","text":"<pre><code>launch_runtime(\n    *,\n    evaluation_fn: Callable[..., float | Mapping[str, Any]],\n    sampler: BaseOptimizer,\n    optimizer_info: dict,\n    optimization_dir: Path | str,\n    max_evaluations_total: int | None = None,\n    max_evaluations_per_run: int | None = None,\n    continue_until_max_evaluation_completed: bool = False,\n    logger: Logger | None = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    overwrite_optimization_dir: bool = False,\n    pre_load_hooks: (\n        Iterable[Callable[[BaseOptimizer], BaseOptimizer]]\n        | None\n    ) = None\n) -&gt; None\n</code></pre> <p>Launch the runtime of a single instance of NePS.</p> <p>Please refer to the module docstring for a detailed explanation of the runtime. Runs until some exit condition is met.</p> PARAMETER DESCRIPTION <code>evaluation_fn</code> <p>The evaluation function to use.</p> <p> TYPE: <code>Callable[..., float | Mapping[str, Any]]</code> </p> <code>sampler</code> <p>The optimizer to use for sampling configurations.</p> <p> TYPE: <code>BaseOptimizer</code> </p> <code>optimizer_info</code> <p>Information about the optimizer.</p> <p> TYPE: <code>dict</code> </p> <code>optimization_dir</code> <p>The directory where the optimization is running.</p> <p> TYPE: <code>Path | str</code> </p> <code>max_evaluations_total</code> <p>The maximum number of evaluations to run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>max_evaluations_per_run</code> <p>The maximum number of evaluations to run in a single run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>continue_until_max_evaluation_completed</code> <p>Whether to continue until the maximum evaluations are completed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>logger</code> <p>The logger to use.</p> <p> TYPE: <code>Logger | None</code> DEFAULT: <code>None</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>ignore_errors</code> <p>Ignore hyperparameter settings that threw an error and do not raise an error. Error configs still count towards max_evaluations_total.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>overwrite_optimization_dir</code> <p>Whether to overwrite the optimization directory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pre_load_hooks</code> <p>Hooks to run before loading the results.</p> <p> TYPE: <code>Iterable[Callable[[BaseOptimizer], BaseOptimizer]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/runtime.py</code> <pre><code>def launch_runtime(  # noqa: PLR0913, C901, PLR0915\n    *,\n    evaluation_fn: Callable[..., float | Mapping[str, Any]],\n    sampler: BaseOptimizer,\n    optimizer_info: dict,\n    optimization_dir: Path | str,\n    max_evaluations_total: int | None = None,\n    max_evaluations_per_run: int | None = None,\n    continue_until_max_evaluation_completed: bool = False,\n    logger: logging.Logger | None = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    overwrite_optimization_dir: bool = False,\n    pre_load_hooks: Iterable[Callable[[BaseOptimizer], BaseOptimizer]] | None = None,\n) -&gt; None:\n    \"\"\"Launch the runtime of a single instance of NePS.\n\n    Please refer to the module docstring for a detailed explanation of the runtime.\n    Runs until some exit condition is met.\n\n    Args:\n        evaluation_fn: The evaluation function to use.\n        sampler: The optimizer to use for sampling configurations.\n        optimizer_info: Information about the optimizer.\n        optimization_dir: The directory where the optimization is running.\n        max_evaluations_total: The maximum number of evaluations to run.\n        max_evaluations_per_run: The maximum number of evaluations to run in a single run.\n        continue_until_max_evaluation_completed: Whether to continue until the maximum\n            evaluations are completed.\n        logger: The logger to use.\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error and will use given loss value instead. default: None\n        ignore_errors: Ignore hyperparameter settings that threw an error and do not raise\n            an error. Error configs still count towards max_evaluations_total.\n        overwrite_optimization_dir: Whether to overwrite the optimization directory.\n        pre_load_hooks: Hooks to run before loading the results.\n    \"\"\"\n    # NOTE(eddiebergman): This was deprecated a while ago and called at\n    # evaluate, now we just crash immediatly instead. Should probably\n    # promote this check closer to the user, i.e. `neps.run()`\n    evaluation_fn_params = inspect.signature(evaluation_fn).parameters\n    if \"previous_working_directory\" in evaluation_fn_params:\n        raise RuntimeError(\n            \"the argument: 'previous_working_directory' was deprecated. \"\n            f\"In the function: '{evaluation_fn.__name__}', please,  \"\n            \"use 'previous_pipeline_directory' instead. \",\n        )\n    if \"working_directory\" in evaluation_fn_params:\n        raise RuntimeError(\n            \"the argument: 'working_directory' was deprecated. \"\n            f\"In the function: '{evaluation_fn.__name__}', please,  \"\n            \"use 'pipeline_directory' instead. \",\n        )\n\n    if logger is None:\n        logger = logging.getLogger(\"neps\")\n\n    optimization_dir = Path(optimization_dir)\n\n    # TODO(eddiebergman): Not sure how overwriting works with multiple workers....\n    if overwrite_optimization_dir and optimization_dir.exists():\n        logger.warning(\"Overwriting working_directory\")\n        shutil.rmtree(optimization_dir)\n\n    shared_state = SharedState(optimization_dir, create_dirs=True)\n\n    _poll, _timeout = get_shared_state_poll_and_timeout()\n    with shared_state.sync(lock=True):\n        if not shared_state.paths.optimizer_info_file.exists():\n            serialize(\n                optimizer_info,\n                shared_state.paths.optimizer_info_file,\n                sort_keys=False,\n            )\n        else:\n            shared_state.check_optimizer_info_on_disk_matches(optimizer_info)\n\n    _max_evals_this_run = (\n        max_evaluations_per_run if max_evaluations_per_run is not None else np.inf\n    )\n\n    evaluations_in_this_run = 0\n    while True:\n        if evaluations_in_this_run &gt;= _max_evals_this_run:\n            logger.info(\"Maximum evaluations per run is reached, shutting down\")\n            break\n\n        with shared_state.sync(lock=True):\n            trials_by_state = shared_state.trials_by_state()\n            if not _worker_should_continue(\n                max_evaluations_total,\n                n_inprogress=len(trials_by_state[Trial.State.IN_PROGRESS]),\n                n_evaluated=(\n                    len(trials_by_state[Trial.State.SUCCESS])\n                    + len(trials_by_state[Trial.State.ERROR])\n                ),\n                continue_until_max_evaluation_completed=continue_until_max_evaluation_completed,\n            ):\n                logger.info(\"Maximum total evaluations is reached, shutting down\")\n                break\n\n            # While we have the decision lock, we will now sample\n            # with the optimizer in this process\n            with shared_state.use_sampler(sampler) as sampler:\n                if sampler.is_out_of_budget():\n                    logger.info(\"Maximum budget reached, shutting down\")\n                    break\n\n                if pre_load_hooks is not None:\n                    for hook in pre_load_hooks:\n                        sampler = hook(sampler)  # noqa: PLW2901\n\n                logger.debug(\"Sampling a new configuration\")\n\n                evaluated = (\n                    trials_by_state[Trial.State.SUCCESS]\n                    + trials_by_state[Trial.State.ERROR]\n                )\n                pending = (\n                    trials_by_state[Trial.State.PENDING]\n                    + trials_by_state[Trial.State.IN_PROGRESS]\n                )\n                trial = _sample_trial_from_optimizer(\n                    sampler,\n                    shared_state.paths.config_dir,\n                    evaluated_trials={trial.id: trial for trial in evaluated},\n                    pending_trials={trial.id: trial for trial in pending},\n                )\n                serialize(trial.config, trial.config_file)\n                serialize(trial.metadata, trial.metadata_file)\n                if trial.previous is not None:\n                    trial.previous_config_id_file.write_text(trial.previous.id)\n\n            logger.debug(f\"Sampled config {trial.id}\")\n\n        # Obtain the lock on this trial and evaluate it,\n        # otherwise continue back to waiting to sampling\n        with trial._lock.try_lock() as acquired:\n            if not acquired:\n                continue\n\n            # Inform the global state that this trial is being evaluated\n            _set_in_progress_trial(trial)\n\n            # TODO(eddiebergman): Right now if a trial crashes, it's cost is not accounted\n            # for, this should probably removed from BaseOptimizer as it does not need\n            # to know this and the runtime can fill this in for it.\n            try:\n                user_result = _evaluate_config(trial, evaluation_fn, logger)\n            except Exception as e:  # noqa: BLE001\n                # TODO(eddiebergman): Right now this never accounts for cost!\n                # NOTE: It's important to lock the shared state such that any\n                # sampling done is with taking this result into account\n                # accidentally reads this config as un-evaluated\n                with shared_state.lock(poll=_poll, timeout=_timeout):\n                    # TODO(eddiebergman): We should add an option to just crash here\n                    # if something goes wrong and raise up this error to the top.\n                    logger.error(\n                        f\"Error during evaluation of '{trial.id}': {trial.config}.\"\n                    )\n                    logger.exception(e)\n                    tb = traceback.format_exc()\n\n                    trial.report = trial.create_error_report(e, tb=tb)\n                    trial.metadata[\"time_end\"] = time.time()\n\n                    shared_state.trials[trial.id] = (trial, Trial.State.ERROR)\n\n                    serialize({\"err\": str(e), \"tb\": tb}, trial.disk.error_file)\n                    serialize(trial.metadata, trial.disk.metadata_file)\n            else:\n                trial.report = trial.create_success_report(user_result)\n                trial.metadata[\"time_end\"] = time.time()\n                if sampler.budget is not None and trial.report.cost is None:\n                    raise ValueError(\n                        \"The evaluation function result should contain a 'cost'\"\n                        f\"field when used with a budget. Got {trial.report.results}\",\n                    )\n\n                with shared_state.lock(poll=_poll, timeout=_timeout):\n                    shared_state.trials[trial.id] = (trial, Trial.State.SUCCESS)\n\n                    eval_cost = trial.report.cost\n                    account_for_cost = False\n                    if eval_cost is not None:\n                        account_for_cost = trial.report.account_for_cost\n                        budget_metadata = {\n                            \"max\": sampler.budget,\n                            \"used\": sampler.used_budget,\n                            \"eval_cost\": eval_cost,\n                            \"account_for_cost\": account_for_cost,\n                        }\n                        trial.metadata.update(budget_metadata)\n\n                    serialize(trial.metadata, trial.disk.metadata_file)\n                    serialize(trial.report.results, trial.disk.result_file)\n                    if account_for_cost:\n                        assert eval_cost is not None\n                        with shared_state.use_sampler(sampler, serialize_seed=False):\n                            sampler.used_budget += eval_cost\n\n            _result: ERROR | dict[str, Any]\n            report = trial.report\n            if isinstance(report, ErrorReport):\n                _result = \"error\"\n            elif isinstance(report, SuccessReport):\n                _result = dict(report.results)\n            else:\n                _type = type(report)\n                raise TypeError(f\"Unknown result type '{_type}' for report: {report}\")\n\n            _post_evaluation_hook(\n                trial,\n                _result,\n                logger,\n                loss_value_on_error,\n                ignore_errors,\n            )\n\n            evaluations_in_this_run += 1\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/","title":"Base optimizer","text":""},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer","title":"neps.optimizers.base_optimizer","text":""},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer","title":"BaseOptimizer","text":"<pre><code>BaseOptimizer(\n    pipeline_space: SearchSpace,\n    patience: int = 50,\n    logger: Logger | None = None,\n    budget: int | float | None = None,\n    loss_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    learning_curve_on_error: (\n        float | list[float] | None\n    ) = None,\n    ignore_errors=False,\n)\n</code></pre> <p>Base sampler class. Implements all the low-level work.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    patience: int = 50,\n    logger: logging.Logger | None = None,\n    budget: int | float | None = None,\n    loss_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    learning_curve_on_error: float | list[float] | None = None,\n    ignore_errors=False,\n) -&gt; None:\n    if patience &lt; 1:\n        raise ValueError(\"Patience should be at least 1\")\n\n    self.used_budget: float = 0.0\n    self.budget = budget\n    self.pipeline_space = pipeline_space\n    self.patience = patience\n    self.logger = logger or logging.getLogger(\"neps\")\n    self.loss_value_on_error = loss_value_on_error\n    self.cost_value_on_error = cost_value_on_error\n    self.learning_curve_on_error = learning_curve_on_error\n    self.ignore_errors = ignore_errors\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.get_config_and_ids","title":"get_config_and_ids  <code>abstractmethod</code>","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>Sample a new configuration</p> RETURNS DESCRIPTION <code>config</code> <p>serializable object representing the configuration config_id: unique identifier for the configuration previous_config_id: if provided, id of a previous on which this     configuration is based</p> <p> TYPE: <code>tuple[RawConfig, str, str | None]</code> </p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>@abstractmethod\ndef get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"Sample a new configuration\n\n    Returns:\n        config: serializable object representing the configuration\n        config_id: unique identifier for the configuration\n        previous_config_id: if provided, id of a previous on which this\n            configuration is based\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/info/","title":"Info","text":""},{"location":"api/neps/optimizers/info/#neps.optimizers.info","title":"neps.optimizers.info","text":""},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs","title":"SearcherConfigs","text":"<p>This class provides methods to access default configuration details for NePS optimizers.</p>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_available_algorithms","title":"get_available_algorithms  <code>staticmethod</code>","text":"<pre><code>get_available_algorithms() -&gt; list[str]\n</code></pre> <p>List all available algorithms used by NePS searchers.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of algorithm names.</p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_available_algorithms() -&gt; list[str]:\n    \"\"\"\n    List all available algorithms used by NePS searchers.\n\n    Returns:\n        list[str]: A list of algorithm names.\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n    prev_algorithms = set()\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".yaml\"):\n            file_path = os.path.join(folder_path, filename)\n            with open(file_path) as file:\n                searcher_config = yaml.safe_load(file)\n                algorithm = searcher_config.get(\"strategy\")\n                if algorithm:\n                    prev_algorithms.add(algorithm)\n\n    return list(prev_algorithms)\n</code></pre>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_searcher_from_algorithm","title":"get_searcher_from_algorithm  <code>staticmethod</code>","text":"<pre><code>get_searcher_from_algorithm(algorithm: str) -&gt; list[str]\n</code></pre> <p>Get all NePS searchers that use a specific searching algorithm.</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>The name of the algorithm needed for the search.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of searcher names using the specified algorithm.</p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_searcher_from_algorithm(algorithm: str) -&gt; list[str]:\n    \"\"\"\n    Get all NePS searchers that use a specific searching algorithm.\n\n    Args:\n        algorithm (str): The name of the algorithm needed for the search.\n\n    Returns:\n        list[str]: A list of searcher names using the specified algorithm.\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n    searchers = []\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".yaml\"):\n            file_path = os.path.join(folder_path, filename)\n            with open(file_path) as file:\n                searcher_config = yaml.safe_load(file)\n                if searcher_config.get(\"strategy\") == algorithm:\n                    searchers.append(os.path.splitext(filename)[0])\n\n    return searchers\n</code></pre>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_searcher_kwargs","title":"get_searcher_kwargs  <code>staticmethod</code>","text":"<pre><code>get_searcher_kwargs(searcher: str) -&gt; str\n</code></pre> <p>Get the kwargs and algorithm setup for a specific searcher.</p> PARAMETER DESCRIPTION <code>searcher</code> <p>The name of the searcher to check the details of.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The raw content of the searcher's configuration</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_searcher_kwargs(searcher: str) -&gt; str:\n    \"\"\"\n    Get the kwargs and algorithm setup for a specific searcher.\n\n    Args:\n        searcher (str): The name of the searcher to check the details of.\n\n    Returns:\n        str: The raw content of the searcher's configuration\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".yaml\") and filename.startswith(searcher):\n            file_path = os.path.join(folder_path, filename)\n            with open(file_path) as file:\n                searcher_config = file.read()\n\n    return searcher_config\n</code></pre>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_searchers","title":"get_searchers  <code>staticmethod</code>","text":"<pre><code>get_searchers() -&gt; list[str]\n</code></pre> <p>List all the searcher names that can be used in neps run.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of searcher names.</p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_searchers() -&gt; list[str]:\n    \"\"\"\n    List all the searcher names that can be used in neps run.\n\n    Returns:\n        list[str]: A list of searcher names.\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n    searchers = []\n\n    for file_name in os.listdir(folder_path):\n        if file_name.endswith(\".yaml\"):\n            searcher_name = os.path.splitext(file_name)[0]\n            searchers.append(searcher_name)\n\n    return searchers\n</code></pre>"},{"location":"api/neps/optimizers/utils/","title":"Utils","text":""},{"location":"api/neps/optimizers/utils/#neps.optimizers.utils","title":"neps.optimizers.utils","text":""},{"location":"api/neps/optimizers/utils/#neps.optimizers.utils.map_real_hyperparameters_from_tabular_ids","title":"map_real_hyperparameters_from_tabular_ids","text":"<pre><code>map_real_hyperparameters_from_tabular_ids(\n    x: Series, pipeline_space: SearchSpace\n) -&gt; Series\n</code></pre> <p>Maps the tabular IDs to the actual HPs from the pipeline space.</p> PARAMETER DESCRIPTION <code>x</code> <p>A pandas series with the tabular IDs. TODO: Mention expected format of the series.</p> <p> TYPE: <code>Series</code> </p> <code>pipeline_space</code> <p>The pipeline space.</p> <p> TYPE: <code>SearchSpace</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pd.Series: A pandas series with the actual HPs.     TODO: Mention expected format of the series.</p> Source code in <code>neps/optimizers/utils.py</code> <pre><code>def map_real_hyperparameters_from_tabular_ids(\n    x: pd.Series, pipeline_space: SearchSpace\n) -&gt; pd.Series:\n    \"\"\" Maps the tabular IDs to the actual HPs from the pipeline space.\n\n    Args:\n        x (pd.Series): A pandas series with the tabular IDs.\n            TODO: Mention expected format of the series.\n        pipeline_space (SearchSpace): The pipeline space.\n\n    Returns: \n        pd.Series: A pandas series with the actual HPs.\n            TODO: Mention expected format of the series.\n    \"\"\"\n    if len(x) == 0:\n        return x\n    # extract fid name\n    _x = x.iloc[0].hp_values()\n    _x.pop(\"id\")\n    fid_name = list(_x.keys())[0]\n    for i in x.index.values:\n        # extracting actual HPs from the tabular space\n        _config = pipeline_space.custom_grid_table.loc[x.loc[i][\"id\"].value].to_dict()\n        # updating fidelities as per the candidate set passed\n        _config.update({fid_name: x.loc[i][fid_name].value})\n        # placeholder config from the raw tabular space\n        config = pipeline_space.raw_tabular_space.sample(\n            patience=100, \n            user_priors=True, \n            ignore_fidelity=True  # True allows fidelity to appear in the sample\n        )\n        # copying values from table to placeholder config of type SearchSpace\n        config.load_from(_config)\n        # replacing the ID in the candidate set with the actual HPs of the config\n        x.loc[i] = config\n    return x\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/","title":"Cost cooling","text":""},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/#neps.optimizers.bayesian_optimization.cost_cooling","title":"neps.optimizers.bayesian_optimization.cost_cooling","text":""},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/#neps.optimizers.bayesian_optimization.cost_cooling.CostCooling","title":"CostCooling","text":"<pre><code>CostCooling(\n    pipeline_space: SearchSpace,\n    initial_design_size: int = 10,\n    surrogate_model: str | Any = \"gp\",\n    cost_model: str | Any = \"gp\",\n    surrogate_model_args: dict = None,\n    cost_model_args: dict = None,\n    optimal_assignment: bool = False,\n    domain_se_kernel: str = None,\n    graph_kernels: list = None,\n    hp_kernels: list = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"mutation\",\n    random_interleave_prob: float = 0.0,\n    patience: int = 100,\n    budget: None | int | float = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    logger=None,\n)\n</code></pre> <p>               Bases: <code>BayesianOptimization</code></p> <p>Implements a basic cost-cooling as described in \"Cost-aware Bayesian Optimization\" (arxiv.org/abs/2003.10870) by Lee et al.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>initial_design_size</code> <p>Number of 'x' samples that need to be evaluated before selecting a sample using a strategy instead of randomly.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>surrogate_model</code> <p>Surrogate model</p> <p> TYPE: <code>str | Any</code> DEFAULT: <code>'gp'</code> </p> <code>cost_model</code> <p>Cost model</p> <p> TYPE: <code>str | Any</code> DEFAULT: <code>'gp'</code> </p> <code>surrogate_model_args</code> <p>Arguments that will be given to the surrogate model (the Gaussian processes model).</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>cost_model_args</code> <p>Arguments that will be given to the cost model (the Gaussian processes model).</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>optimal_assignment</code> <p>whether the optimal assignment kernel should be used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>domain_se_kernel</code> <p>Stationary kernel name</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>graph_kernels</code> <p>Kernels for NAS</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>hp_kernels</code> <p>Kernels for HPO</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>acquisition</code> <p>Acquisition strategy</p> <p> TYPE: <code>str | BaseAcquisition</code> DEFAULT: <code>'EI'</code> </p> <code>log_prior_weighted</code> <p>if to use log for prior</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>acquisition_sampler</code> <p>Acquisition function fetching strategy</p> <p> TYPE: <code>str | AcquisitionSampler</code> DEFAULT: <code>'mutation'</code> </p> <code>random_interleave_prob</code> <p>Frequency at which random configurations are sampled instead of configurations from the acquisition strategy.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>patience</code> <p>How many times we try something that fails before giving up.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>None | int | float</code> DEFAULT: <code>None</code> </p> <code>ignore_errors</code> <p>Ignore hyperparameter settings that threw an error and do not raise an error. Error configs still count towards max_evaluations_total.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>logger object, or None to use the neps logger</p> <p> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if patience &lt; 1</p> <code>ValueError</code> <p>if initial_design_size &lt; 1</p> <code>ValueError</code> <p>if random_interleave_prob is not between 0.0 and 1.0</p> <code>ValueError</code> <p>if no kernel is provided</p> Source code in <code>neps/optimizers/bayesian_optimization/cost_cooling.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    initial_design_size: int = 10,\n    surrogate_model: str | Any = \"gp\",\n    cost_model: str | Any = \"gp\",\n    surrogate_model_args: dict = None,\n    cost_model_args: dict = None,\n    optimal_assignment: bool = False,\n    domain_se_kernel: str = None,\n    graph_kernels: list = None,\n    hp_kernels: list = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str | AcquisitionSampler = \"mutation\",\n    random_interleave_prob: float = 0.0,\n    patience: int = 100,\n    budget: None | int | float = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    logger=None,\n):\n    \"\"\"Initialise the BO loop.\n\n    Args:\n        pipeline_space: Space in which to search\n        initial_design_size: Number of 'x' samples that need to be evaluated before\n            selecting a sample using a strategy instead of randomly.\n        surrogate_model: Surrogate model\n        cost_model: Cost model\n        surrogate_model_args: Arguments that will be given to the surrogate model\n            (the Gaussian processes model).\n        cost_model_args: Arguments that will be given to the cost model\n            (the Gaussian processes model).\n        optimal_assignment: whether the optimal assignment kernel should be used.\n        domain_se_kernel: Stationary kernel name\n        graph_kernels: Kernels for NAS\n        hp_kernels: Kernels for HPO\n        acquisition: Acquisition strategy\n        log_prior_weighted: if to use log for prior\n        acquisition_sampler: Acquisition function fetching strategy\n        random_interleave_prob: Frequency at which random configurations are sampled\n            instead of configurations from the acquisition strategy.\n        patience: How many times we try something that fails before giving up.\n        budget: Maximum budget\n        ignore_errors: Ignore hyperparameter settings that threw an error and do not\n            raise an error. Error configs still count towards max_evaluations_total.\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        logger: logger object, or None to use the neps logger\n\n    Raises:\n        ValueError: if patience &lt; 1\n        ValueError: if initial_design_size &lt; 1\n        ValueError: if random_interleave_prob is not between 0.0 and 1.0\n        ValueError: if no kernel is provided\n    \"\"\"\n    super().__init__(\n        pipeline_space=pipeline_space,\n        patience=patience,\n        logger=logger,\n        budget=budget,\n        ignore_errors=ignore_errors,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n    )\n\n    if initial_design_size &lt; 1:\n        raise ValueError(\n            \"BayesianOptimization needs initial_design_size to be at least 1\"\n        )\n    if not 0 &lt;= random_interleave_prob &lt;= 1:\n        raise ValueError(\"random_interleave_prob should be between 0.0 and 1.0\")\n\n    self._initial_design_size = initial_design_size\n    self._random_interleave_prob = random_interleave_prob\n    self._num_train_x: int = 0\n    self._pending_evaluations: list = []\n    self._model_update_failed: bool = False\n\n    if ignore_errors:\n        self.logger.warning(\n            \"ignore_errors was set, but this optimizer does not support it\"\n        )\n\n    surrogate_model_args = surrogate_model_args or {}\n    cost_model_args = cost_model_args or {}\n    graph_kernels, hp_kernels = get_kernels(\n        self.pipeline_space,\n        domain_se_kernel,\n        graph_kernels,\n        hp_kernels,\n        optimal_assignment,\n    )\n    if \"graph_kernels\" not in surrogate_model_args:\n        surrogate_model_args[\"graph_kernels\"] = graph_kernels\n    if \"hp_kernels\" not in surrogate_model_args:\n        surrogate_model_args[\"hp_kernels\"] = hp_kernels\n\n    if (\n        not surrogate_model_args[\"graph_kernels\"]\n        and not surrogate_model_args[\"hp_kernels\"]\n    ):\n        raise ValueError(\"No kernels are provided!\")\n\n    if \"vectorial_features\" not in surrogate_model_args:\n        surrogate_model_args[\n            \"vectorial_features\"\n        ] = self.pipeline_space.get_vectorial_dim()\n\n    self.surrogate_model = instance_from_map(\n        SurrogateModelMapping,\n        surrogate_model,\n        name=\"surrogate model\",\n        kwargs=surrogate_model_args,\n    )\n\n    if \"graph_kernels\" not in cost_model_args:\n        cost_model_args[\"graph_kernels\"] = graph_kernels\n    if \"hp_kernels\" not in cost_model_args:\n        cost_model_args[\"hp_kernels\"] = hp_kernels\n\n    if not cost_model_args[\"graph_kernels\"] and not cost_model_args[\"hp_kernels\"]:\n        raise ValueError(\"No kernels are provided!\")\n\n    if \"vectorial_features\" not in cost_model_args:\n        cost_model_args[\n            \"vectorial_features\"\n        ] = self.pipeline_space.get_vectorial_dim()\n\n    self.cost_model = instance_from_map(\n        SurrogateModelMapping,\n        cost_model,\n        name=\"cost model\",  # does changing this string work?\n        kwargs=cost_model_args,\n    )\n\n    orig_acquisition = instance_from_map(\n        AcquisitionMapping,\n        acquisition,\n        name=\"acquisition function\",\n    )\n\n    self.acquisition = CostCooler(orig_acquisition)\n\n    if self.pipeline_space.has_prior:\n        self.acquisition = DecayingPriorWeightedAcquisition(\n            self.acquisition, log=log_prior_weighted\n        )\n\n    self.acquisition_sampler = instance_from_map(\n        AcquisitionSamplerMapping,\n        acquisition_sampler,\n        name=\"acquisition sampler function\",\n        kwargs={\"patience\": self.patience, \"pipeline_space\": self.pipeline_space},\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/#neps.optimizers.bayesian_optimization.cost_cooling.CostCooling.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/#neps.optimizers.bayesian_optimization.cost_cooling.CostCooling.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/#neps.optimizers.bayesian_optimization.cost_cooling.CostCooling.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/#neps.optimizers.bayesian_optimization.cost_cooling.CostCooling.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Decides if optimization is still under the warmstart phase/model-based search.</p> Source code in <code>neps/optimizers/bayesian_optimization/optimizer.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Decides if optimization is still under the warmstart phase/model-based search.\"\"\"\n    if self._num_train_x &gt;= self._initial_design_size:\n        return False\n    return True\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/cost_cooling/#neps.optimizers.bayesian_optimization.cost_cooling.CostCooling.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/","title":"Mf tpe","text":""},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe","title":"neps.optimizers.bayesian_optimization.mf_tpe","text":""},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe.MultiFidelityPriorWeightedTreeParzenEstimator","title":"MultiFidelityPriorWeightedTreeParzenEstimator","text":"<pre><code>MultiFidelityPriorWeightedTreeParzenEstimator(\n    pipeline_space: SearchSpace,\n    use_priors: bool = True,\n    prior_num_evals: float = 2.5,\n    good_fraction: float = 0.3334,\n    random_interleave_prob: float = 0.0,\n    initial_design_size: int = 0,\n    prior_as_samples: bool = True,\n    pending_as_bad: bool = True,\n    fidelity_weighting: Literal[\n        \"linear\", \"spearman\"\n    ] = \"spearman\",\n    surrogate_model: str = \"kde\",\n    good_model_bw_factor: int = 1.5,\n    joint_kde_modelling: bool = False,\n    threshold_improvement: bool = True,\n    promote_from_acq: bool = True,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"mutation\",\n    prior_draws: int = 1000,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    surrogate_model_args: dict = None,\n    soft_promotion: bool = True,\n    patience: int = 50,\n    logger=None,\n    budget: None | int | float = None,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>prior_num_evals</code> <p>[description]. Defaults to 2.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.5</code> </p> <code>good_fraction</code> <p>[description]. Defaults to 0.333.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3334</code> </p> <code>random_interleave_prob</code> <p>Frequency at which random configurations are sampled instead of configurations from the acquisition strategy.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>initial_design_size</code> <p>Number of 'x' samples that are to be evaluated before selecting a sample using a strategy instead of randomly. If there is a user prior, we can rely on the model from the very first iteration.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>prior_as_samples</code> <p>Whether to sample from the KDE and incorporate that way, or</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>pending_as_bad</code> <p>Whether to treat pending observations as bad, assigning them to</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>prior_draws</code> <p>The number of samples drawn from the prior if there is one. This</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>patience</code> <p>How many times we try something that fails before giving up.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>None | int | float</code> DEFAULT: <code>None</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>logger object, or None to use the neps logger</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/mf_tpe.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    use_priors: bool = True,\n    prior_num_evals: float = 2.5,\n    good_fraction: float = 0.3334,\n    random_interleave_prob: float = 0.0,\n    initial_design_size: int = 0,\n    prior_as_samples: bool = True,\n    pending_as_bad: bool = True,\n    fidelity_weighting: Literal[\"linear\", \"spearman\"] = \"spearman\",\n    surrogate_model: str = \"kde\",\n    good_model_bw_factor: int = 1.5,\n    joint_kde_modelling: bool = False,\n    threshold_improvement: bool = True,\n    promote_from_acq: bool = True,\n    acquisition_sampler: str | AcquisitionSampler = \"mutation\",\n    prior_draws: int = 1000,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    surrogate_model_args: dict = None,\n    soft_promotion: bool = True,\n    patience: int = 50,\n    logger=None,\n    budget: None | int | float = None,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n):\n    \"\"\"[summary]\n\n    Args:\n        pipeline_space: Space in which to search\n        prior_num_evals (float, optional): [description]. Defaults to 2.5.\n        good_fraction (float, optional): [description]. Defaults to 0.333.\n        random_interleave_prob: Frequency at which random configurations are sampled\n            instead of configurations from the acquisition strategy.\n        initial_design_size: Number of 'x' samples that are to be evaluated before\n            selecting a sample using a strategy instead of randomly. If there is a\n            user prior, we can rely on the model from the very first iteration.\n        prior_as_samples: Whether to sample from the KDE and incorporate that way, or\n        just have the distribution be an linear combination of the KDE and the prior.\n        Should be True if the prior happens to be unnormalized.\n        pending_as_bad: Whether to treat pending observations as bad, assigning them to\n        the bad KDE to encourage diversity among samples queried in parallel\n        prior_draws: The number of samples drawn from the prior if there is one. This\n        # does not affect the strength of the prior, just how accurately it\n        # is reconstructed by the KDE.\n        patience: How many times we try something that fails before giving up.\n        budget: Maximum budget\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        logger: logger object, or None to use the neps logger\n    \"\"\"\n    super().__init__(\n        pipeline_space=pipeline_space,\n        patience=patience,\n        logger=logger,\n        budget=budget,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n    )\n    self.pipeline_space = pipeline_space\n    self.good_fraction = good_fraction\n    if self.pipeline_space.has_fidelity:\n        self.min_fidelity = pipeline_space.fidelity.lower\n        self.max_fidelity = pipeline_space.fidelity.upper\n        self.rung_map, self.inverse_rung_map = self._get_rung_maps()\n        self.min_rung = 0\n        self.max_rung = len(self.rung_map) - 1\n\n    else:\n        self.min_rung = 0\n        self.max_rung = 0\n        self.min_fidelity = 1\n        self.max_fidelity = 1\n        self.rung_map, self.inverse_rung_map = self._get_rung_maps()\n\n    if initial_design_size == 0:\n        self._initial_design_size = len(self.pipeline_space) * np.round(\n            1 / self.good_fraction\n        ).astype(int)\n    else:\n        self._initial_design_size = initial_design_size\n    self.promote_from_acq = promote_from_acq\n\n    self.num_rungs = len(self.rung_map)\n    self.use_priors = use_priors\n    self.prior_num_evals = prior_num_evals\n    self._random_interleave_prob = random_interleave_prob\n    self._pending_as_bad = pending_as_bad\n    self.prior_draws = prior_draws\n    self._has_promotable_configs = False\n    self.soft_promotion = soft_promotion\n    self.joint_kde_modelling = joint_kde_modelling\n    # if we use priors, we don't add conigurations as good until is is within the top fraction\n    # This heuristic has not been tried further, but makes sense in the context when we have priors\n    self.round_up = not use_priors\n    self.fidelity_weighting = fidelity_weighting\n    self.threshold_improvement = threshold_improvement\n    # TODO have this read in as part of load_results - it cannot be saved as an attribute when\n    # running parallel instances of the algorithm (since the old configs are shared, not instance-specific)\n    self.old_configs_per_fid = [[] for i in range(self.num_rungs)]\n    # We assume that the information conveyed per fidelity (and the cost) is linear in the\n    # fidelity levels if nothing else is specified\n    if surrogate_model != \"kde\":\n        raise NotImplementedError(\n            \"Only supports KDEs for now. Could (maybe?) support binary classification in the future.\"\n        )\n    self.acquisition_sampler = instance_from_map(\n        AcquisitionSamplerMapping,\n        acquisition_sampler,\n        name=\"acquisition sampler function\",\n        kwargs={\"patience\": self.patience, \"pipeline_space\": self.pipeline_space},\n    )\n    self.prior_confidence = prior_confidence\n    self._enhance_priors()\n    surrogate_model_args = surrogate_model_args or {}\n\n    param_types, num_options, logged_params, is_fidelity = self._get_types()\n    surrogate_model_args[\"param_types\"] = param_types\n    surrogate_model_args[\"num_options\"] = num_options\n    surrogate_model_args[\"is_fidelity\"] = is_fidelity\n    surrogate_model_args[\"logged_params\"] = logged_params\n    good_model_args = deepcopy(surrogate_model_args)\n    good_model_args[\"bandwidth_factor\"] = good_model_bw_factor\n    if self.pipeline_space.has_prior and use_priors:\n        if prior_as_samples:\n            self.prior_samples = [\n                self.pipeline_space.sample(\n                    patience=self.patience, user_priors=True, ignore_fidelity=False\n                )\n                for idx in range(self.prior_draws)\n            ]\n        else:\n            pass\n            # TODO work out affine combination\n    else:\n        self.prior_samples = []\n\n    self.surrogate_models = {\n        \"good\": instance_from_map(\n            SurrogateModelMapping,\n            surrogate_model,\n            name=\"surrogate model\",\n            kwargs=good_model_args,\n        ),\n        \"bad\": instance_from_map(\n            SurrogateModelMapping,\n            surrogate_model,\n            name=\"surrogate model\",\n            kwargs=surrogate_model_args,\n        ),\n        \"all\": instance_from_map(\n            SurrogateModelMapping,\n            surrogate_model,\n            name=\"surrogate model\",\n            kwargs=surrogate_model_args,\n        ),\n    }\n    self.acquisition = self\n    self.acquisition_sampler = instance_from_map(\n        AcquisitionSamplerMapping,\n        acquisition_sampler,\n        name=\"acquisition sampler function\",\n        kwargs={\"patience\": self.patience, \"pipeline_space\": self.pipeline_space},\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe.MultiFidelityPriorWeightedTreeParzenEstimator.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Iterable,\n    asscalar: bool = False,\n    only_lowest_fidelity=True,\n    only_good=False,\n) -&gt; ndarray | Tensor | float\n</code></pre> <p>Return the negative expected improvement at the query point</p> Source code in <code>neps/optimizers/bayesian_optimization/mf_tpe.py</code> <pre><code>def __call__(\n    self,\n    x: Iterable,\n    asscalar: bool = False,\n    only_lowest_fidelity=True,\n    only_good=False,\n) -&gt; np.ndarray | torch.Tensor | float:\n    \"\"\"\n    Return the negative expected improvement at the query point\n    \"\"\"\n    # this is to only make the lowest fidelity viable\n    # TODO have this as a setting in the acq_sampler instead\n    if only_lowest_fidelity:\n        is_lowest_fidelity = (\n            np.array([x_.fidelity.value for x_ in x]) == self.rung_map[self.min_rung]\n        )\n        return np.log(self.surrogate_models[\"good\"].pdf(x)) - np.log(\n            self.surrogate_models[\"bad\"].pdf(x)\n        )\n    else:\n        return np.log(self.surrogate_models[\"good\"].pdf(x)) - np.log(\n            self.surrogate_models[\"bad\"].pdf(x)\n        )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe.MultiFidelityPriorWeightedTreeParzenEstimator.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe.MultiFidelityPriorWeightedTreeParzenEstimator.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe.MultiFidelityPriorWeightedTreeParzenEstimator.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe.MultiFidelityPriorWeightedTreeParzenEstimator.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Decides if optimization is still under the warmstart phase/model-based search.</p> Source code in <code>neps/optimizers/bayesian_optimization/mf_tpe.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Decides if optimization is still under the warmstart phase/model-based search.\"\"\"\n    if self._num_train_x &gt;= self._initial_design_size:\n        return False\n    return True\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/mf_tpe/#neps.optimizers.bayesian_optimization.mf_tpe.MultiFidelityPriorWeightedTreeParzenEstimator.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer","title":"neps.optimizers.bayesian_optimization.optimizer","text":""},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization","title":"BayesianOptimization","text":"<pre><code>BayesianOptimization(\n    pipeline_space: SearchSpace,\n    initial_design_size: int = 10,\n    surrogate_model: str | Any = \"gp\",\n    surrogate_model_args: dict = None,\n    optimal_assignment: bool = False,\n    domain_se_kernel: str = None,\n    graph_kernels: list = None,\n    hp_kernels: list = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"mutation\",\n    random_interleave_prob: float = 0.0,\n    patience: int = 100,\n    budget: None | int | float = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    logger=None,\n    disable_priors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = None,\n    sample_default_first: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Implements the basic BO loop.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>initial_design_size</code> <p>Number of 'x' samples that need to be evaluated before selecting a sample using a strategy instead of randomly.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>surrogate_model</code> <p>Surrogate model</p> <p> TYPE: <code>str | Any</code> DEFAULT: <code>'gp'</code> </p> <code>surrogate_model_args</code> <p>Arguments that will be given to the surrogate model (the Gaussian processes model).</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>optimal_assignment</code> <p>whether the optimal assignment kernel should be used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>domain_se_kernel</code> <p>Stationary kernel name</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>graph_kernels</code> <p>Kernels for NAS</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>hp_kernels</code> <p>Kernels for HPO</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>acquisition</code> <p>Acquisition strategy</p> <p> TYPE: <code>str | BaseAcquisition</code> DEFAULT: <code>'EI'</code> </p> <code>log_prior_weighted</code> <p>if to use log for prior</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>acquisition_sampler</code> <p>Acquisition function fetching strategy</p> <p> TYPE: <code>str | AcquisitionSampler</code> DEFAULT: <code>'mutation'</code> </p> <code>random_interleave_prob</code> <p>Frequency at which random configurations are sampled instead of configurations from the acquisition strategy.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>patience</code> <p>How many times we try something that fails before giving up.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>None | int | float</code> DEFAULT: <code>None</code> </p> <code>ignore_errors</code> <p>Ignore hyperparameter settings that threw an error and do not raise an error. Error configs still count towards max_evaluations_total.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>logger object, or None to use the neps logger</p> <p> DEFAULT: <code>None</code> </p> <code>disable_priors</code> <p>allows to choose between BO and piBO regardless the search space definition</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sample_default_first</code> <p>if True and a default prior exists, the first sampel is the default configuration</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if patience &lt; 1</p> <code>ValueError</code> <p>if initial_design_size &lt; 1</p> <code>ValueError</code> <p>if random_interleave_prob is not between 0.0 and 1.0</p> <code>ValueError</code> <p>if no kernel is provided</p> Source code in <code>neps/optimizers/bayesian_optimization/optimizer.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    initial_design_size: int = 10,\n    surrogate_model: str | Any = \"gp\",\n    surrogate_model_args: dict = None,\n    optimal_assignment: bool = False,\n    domain_se_kernel: str = None,\n    graph_kernels: list = None,\n    hp_kernels: list = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str | AcquisitionSampler = \"mutation\",\n    random_interleave_prob: float = 0.0,\n    patience: int = 100,\n    budget: None | int | float = None,\n    ignore_errors: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    logger=None,\n    disable_priors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = None,\n    sample_default_first: bool = False,\n):\n    \"\"\"Initialise the BO loop.\n\n    Args:\n        pipeline_space: Space in which to search\n        initial_design_size: Number of 'x' samples that need to be evaluated before\n            selecting a sample using a strategy instead of randomly.\n        surrogate_model: Surrogate model\n        surrogate_model_args: Arguments that will be given to the surrogate model\n            (the Gaussian processes model).\n        optimal_assignment: whether the optimal assignment kernel should be used.\n        domain_se_kernel: Stationary kernel name\n        graph_kernels: Kernels for NAS\n        hp_kernels: Kernels for HPO\n        acquisition: Acquisition strategy\n        log_prior_weighted: if to use log for prior\n        acquisition_sampler: Acquisition function fetching strategy\n        random_interleave_prob: Frequency at which random configurations are sampled\n            instead of configurations from the acquisition strategy.\n        patience: How many times we try something that fails before giving up.\n        budget: Maximum budget\n        ignore_errors: Ignore hyperparameter settings that threw an error and do not\n            raise an error. Error configs still count towards max_evaluations_total.\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        logger: logger object, or None to use the neps logger\n        disable_priors: allows to choose between BO and piBO regardless the search\n            space definition\n        sample_default_first: if True and a default prior exists, the first sampel is\n            the default configuration\n\n    Raises:\n        ValueError: if patience &lt; 1\n        ValueError: if initial_design_size &lt; 1\n        ValueError: if random_interleave_prob is not between 0.0 and 1.0\n        ValueError: if no kernel is provided\n    \"\"\"\n    if disable_priors:\n        pipeline_space.has_prior = False\n        self.prior_confidence = None\n    else:\n        self.prior_confidence = prior_confidence\n\n    super().__init__(\n        pipeline_space=pipeline_space,\n        patience=patience,\n        logger=logger,\n        budget=budget,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n    )\n\n    if initial_design_size &lt; 1:\n        raise ValueError(\n            \"BayesianOptimization needs initial_design_size to be at least 1\"\n        )\n    if not 0 &lt;= random_interleave_prob &lt;= 1:\n        raise ValueError(\"random_interleave_prob should be between 0.0 and 1.0\")\n\n    self._initial_design_size = initial_design_size\n    self._random_interleave_prob = random_interleave_prob\n    self._num_train_x: int = 0\n    self._num_error_evaluations: int = 0\n    self._pending_evaluations: list = []\n    self._model_update_failed: bool = False\n    self.sample_default_first = sample_default_first\n\n    surrogate_model_args = surrogate_model_args or {}\n    graph_kernels, hp_kernels = get_kernels(\n        self.pipeline_space,\n        domain_se_kernel,\n        graph_kernels,\n        hp_kernels,\n        optimal_assignment,\n    )\n    if \"graph_kernels\" not in surrogate_model_args:\n        surrogate_model_args[\"graph_kernels\"] = graph_kernels\n    if \"hp_kernels\" not in surrogate_model_args:\n        surrogate_model_args[\"hp_kernels\"] = hp_kernels\n\n    if (\n        not surrogate_model_args[\"graph_kernels\"]\n        and not surrogate_model_args[\"hp_kernels\"]\n    ):\n        raise ValueError(\"No kernels are provided!\")\n\n    if \"vectorial_features\" not in surrogate_model_args:\n        surrogate_model_args[\"vectorial_features\"] = (\n            self.pipeline_space.get_vectorial_dim()\n        )\n\n    self.surrogate_model = instance_from_map(\n        SurrogateModelMapping,\n        surrogate_model,\n        name=\"surrogate model\",\n        kwargs=surrogate_model_args,\n    )\n\n    self.acquisition = instance_from_map(\n        AcquisitionMapping,\n        acquisition,\n        name=\"acquisition function\",\n    )\n    if self.pipeline_space.has_prior:\n        self.acquisition = DecayingPriorWeightedAcquisition(\n            self.acquisition, log=log_prior_weighted\n        )\n\n    self.acquisition_sampler = instance_from_map(\n        AcquisitionSamplerMapping,\n        acquisition_sampler,\n        name=\"acquisition sampler function\",\n        kwargs={\"patience\": self.patience, \"pipeline_space\": self.pipeline_space},\n    )\n    self._enhance_priors()\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Decides if optimization is still under the warmstart phase/model-based search.</p> Source code in <code>neps/optimizers/bayesian_optimization/optimizer.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Decides if optimization is still under the warmstart phase/model-based search.\"\"\"\n    if self._num_train_x &gt;= self._initial_design_size:\n        return False\n    return True\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/","title":"Base acquisition","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition","title":"neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition.BaseAcquisition","title":"BaseAcquisition","text":"<pre><code>BaseAcquisition()\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition.py</code> <pre><code>def __init__(self):\n    self.surrogate_model = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition.BaseAcquisition.eval","title":"eval  <code>abstractmethod</code>","text":"<pre><code>eval(x, asscalar: bool = False)\n</code></pre> <p>Evaluate the acquisition function at point x2.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition.py</code> <pre><code>@abstractmethod\ndef eval(self, x, asscalar: bool = False):\n    \"\"\"Evaluate the acquisition function at point x2.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/cost_cooling/","title":"Cost cooling","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/cost_cooling/#neps.optimizers.bayesian_optimization.acquisition_functions.cost_cooling","title":"neps.optimizers.bayesian_optimization.acquisition_functions.cost_cooling","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/","title":"Ei","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/#neps.optimizers.bayesian_optimization.acquisition_functions.ei","title":"neps.optimizers.bayesian_optimization.acquisition_functions.ei","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/#neps.optimizers.bayesian_optimization.acquisition_functions.ei.ComprehensiveExpectedImprovement","title":"ComprehensiveExpectedImprovement","text":"<pre><code>ComprehensiveExpectedImprovement(\n    augmented_ei: bool = False,\n    xi: float = 0.0,\n    in_fill: str = \"best\",\n    log_ei: bool = False,\n    optimize_on_max_fidelity: bool = True,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisition</code></p> <ol> <li> <p>The input x2 is a networkx graph instead of a vectorial input</p> </li> <li> <p>The search space (a collection of x1_graphs) is discrete, so there is no    gradient-based optimisation. Instead, we compute the EI at all candidate points    and empirically select the best position during optimisation</p> </li> </ol> PARAMETER DESCRIPTION <code>augmented_ei</code> <p>Using the Augmented EI heuristic modification to the standard expected improvement algorithm according to Huang (2006).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>xi</code> <p>manual exploration-exploitation trade-off parameter.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>in_fill</code> <p>the criterion to be used for in-fill for the determination of mu_star 'best' means the empirical best observation so far (but could be susceptible to noise), 'posterior' means the best posterior GP mean encountered so far, and is recommended for optimization of more noisy functions. Defaults to \"best\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'best'</code> </p> <code>log_ei</code> <p>log-EI if true otherwise usual EI.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/ei.py</code> <pre><code>def __init__(\n    self,\n    augmented_ei: bool = False,\n    xi: float = 0.0,\n    in_fill: str = \"best\",\n    log_ei: bool = False,\n    optimize_on_max_fidelity: bool = True,\n):\n    \"\"\"This is the graph BO version of the expected improvement\n    key differences are:\n\n    1. The input x2 is a networkx graph instead of a vectorial input\n\n    2. The search space (a collection of x1_graphs) is discrete, so there is no\n       gradient-based optimisation. Instead, we compute the EI at all candidate points\n       and empirically select the best position during optimisation\n\n    Args:\n        augmented_ei: Using the Augmented EI heuristic modification to the standard\n            expected improvement algorithm according to Huang (2006).\n        xi: manual exploration-exploitation trade-off parameter.\n        in_fill: the criterion to be used for in-fill for the determination of mu_star\n            'best' means the empirical best observation so far (but could be\n            susceptible to noise), 'posterior' means the best *posterior GP mean*\n            encountered so far, and is recommended for optimization of more noisy\n            functions. Defaults to \"best\".\n        log_ei: log-EI if true otherwise usual EI.\n    \"\"\"\n    super().__init__()\n\n    if in_fill not in [\"best\", \"posterior\"]:\n        raise ValueError(f\"Invalid value for in_fill ({in_fill})\")\n    self.augmented_ei = augmented_ei\n    self.xi = xi\n    self.in_fill = in_fill\n    self.log_ei = log_ei\n    self.incumbent = None\n    self.optimize_on_max_fidelity = optimize_on_max_fidelity\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/#neps.optimizers.bayesian_optimization.acquisition_functions.ei.ComprehensiveExpectedImprovement.eval","title":"eval","text":"<pre><code>eval(\n    x: Sequence[SearchSpace], asscalar: bool = False\n) -&gt; Union[ndarray, Tensor, float]\n</code></pre> <p>Return the negative expected improvement at the query point x2</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/ei.py</code> <pre><code>def eval(\n    self, x: Sequence[SearchSpace], asscalar: bool = False,\n) -&gt; Union[np.ndarray, torch.Tensor, float]:\n    \"\"\"\n    Return the negative expected improvement at the query point x2\n    \"\"\"\n    assert self.incumbent is not None, \"EI function not fitted on model\"\n\n    if x[0].has_fidelity and self.optimize_on_max_fidelity:\n        _x = [e.clone() for e in x]\n        for e in _x:\n            e.set_to_max_fidelity()\n    else:\n        _x = x\n\n    try:\n        mu, cov = self.surrogate_model.predict(_x)\n    except ValueError as e:\n        raise e\n        # return -1.0  # in case of error. return ei of -1\n    std = torch.sqrt(torch.diag(cov))\n    mu_star = self.incumbent\n    gauss = Normal(torch.zeros(1, device=mu.device), torch.ones(1, device=mu.device))\n    # u = (mu - mu_star - self.xi) / std\n    # ei = std * updf + (mu - mu_star - self.xi) * ucdf\n    if self.log_ei:\n        # we expect that f_min is in log-space\n        f_min = mu_star - self.xi\n        v = (f_min - mu) / std\n        ei = torch.exp(f_min) * gauss.cdf(v) - torch.exp(\n            0.5 * torch.diag(cov) + mu\n        ) * gauss.cdf(v - std)\n    else:\n        u = (mu_star - mu - self.xi) / std\n        ucdf = gauss.cdf(u)\n        updf = torch.exp(gauss.log_prob(u))\n        ei = std * updf + (mu_star - mu - self.xi) * ucdf\n    if self.augmented_ei:\n        sigma_n = self.surrogate_model.likelihood\n        ei *= 1.0 - torch.sqrt(torch.tensor(sigma_n, device=mu.device)) / torch.sqrt(\n            sigma_n + torch.diag(cov)\n        )\n    if isinstance(_x, list) and asscalar:\n        return ei.detach().numpy()\n    if asscalar:\n        ei = ei.detach().numpy().item()\n    return ei\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei/","title":"Mf ei","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei/#neps.optimizers.bayesian_optimization.acquisition_functions.mf_ei","title":"neps.optimizers.bayesian_optimization.acquisition_functions.mf_ei","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei/#neps.optimizers.bayesian_optimization.acquisition_functions.mf_ei.MFEI","title":"MFEI","text":"<pre><code>MFEI(\n    pipeline_space: SearchSpace,\n    surrogate_model_name: str = None,\n    augmented_ei: bool = False,\n    xi: float = 0.0,\n    in_fill: str = \"best\",\n    log_ei: bool = False,\n)\n</code></pre> <p>               Bases: <code>ComprehensiveExpectedImprovement</code></p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    surrogate_model_name: str = None,\n    augmented_ei: bool = False,\n    xi: float = 0.0,\n    in_fill: str = \"best\",\n    log_ei: bool = False,\n):\n    super().__init__(augmented_ei, xi, in_fill, log_ei)\n    self.pipeline_space = pipeline_space\n    self.surrogate_model_name = surrogate_model_name\n    self.surrogate_model = None\n    self.observations = None\n    self.b_step = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei/#neps.optimizers.bayesian_optimization.acquisition_functions.mf_ei.MFEI.eval_gp_ei","title":"eval_gp_ei","text":"<pre><code>eval_gp_ei(\n    x: Iterable, inc_list: Iterable\n) -&gt; Union[ndarray, Tensor, float]\n</code></pre> <p>Vanilla-EI modified to preprocess samples and accept list of incumbents.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei.py</code> <pre><code>def eval_gp_ei(\n    self, x: Iterable, inc_list: Iterable\n) -&gt; Union[np.ndarray, torch.Tensor, float]:\n    \"\"\"Vanilla-EI modified to preprocess samples and accept list of incumbents.\"\"\"\n    # x, inc_list = self.preprocess(x)  # IMPORTANT change from vanilla-EI\n    _x = x.copy()\n    try:\n        mu, cov = self.surrogate_model.predict(_x)\n    except ValueError as e:\n        raise e\n        # return -1.0  # in case of error. return ei of -1\n    std = torch.sqrt(torch.diag(cov))\n\n    mu_star = inc_list.to(mu.device)  # IMPORTANT change from vanilla-EI\n\n    gauss = Normal(torch.zeros(1, device=mu.device), torch.ones(1, device=mu.device))\n    # u = (mu - mu_star - self.xi) / std\n    # ei = std * updf + (mu - mu_star - self.xi) * ucdf\n    if self.log_ei:\n        # we expect that f_min is in log-space\n        f_min = mu_star - self.xi\n        v = (f_min - mu) / std\n        ei = torch.exp(f_min) * gauss.cdf(v) - torch.exp(\n            0.5 * torch.diag(cov) + mu\n        ) * gauss.cdf(v - std)\n    else:\n        u = (mu_star - mu - self.xi) / std\n        ucdf = gauss.cdf(u)\n        updf = torch.exp(gauss.log_prob(u))\n        ei = std * updf + (mu_star - mu - self.xi) * ucdf\n    if self.augmented_ei:\n        sigma_n = self.surrogate_model.likelihood\n        ei *= 1.0 - torch.sqrt(torch.tensor(sigma_n, device=mu.device)) / torch.sqrt(\n            sigma_n + torch.diag(cov)\n        )\n    return ei\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei/#neps.optimizers.bayesian_optimization.acquisition_functions.mf_ei.MFEI.eval_pfn_ei","title":"eval_pfn_ei","text":"<pre><code>eval_pfn_ei(\n    x: Iterable, inc_list: Iterable\n) -&gt; Union[ndarray, Tensor, float]\n</code></pre> <p>PFN-EI modified to preprocess samples and accept list of incumbents.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei.py</code> <pre><code>def eval_pfn_ei(\n    self, x: Iterable, inc_list: Iterable\n) -&gt; Union[np.ndarray, torch.Tensor, float]:\n    \"\"\"PFN-EI modified to preprocess samples and accept list of incumbents.\"\"\"\n    # x, inc_list = self.preprocess(x)  # IMPORTANT change from vanilla-EI\n    # _x = x.copy()\n    ei = self.surrogate_model.get_ei(x.to(self.surrogate_model.device), inc_list)\n    if len(ei.shape) == 2:\n        ei = ei.flatten()\n    return ei\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei/#neps.optimizers.bayesian_optimization.acquisition_functions.mf_ei.MFEI.preprocess","title":"preprocess","text":"<pre><code>preprocess(x: Series) -&gt; Tuple[Iterable, Iterable]\n</code></pre> <p>Prepares the configurations for appropriate EI calculation.</p> <p>Takes a set of points and computes the budget and incumbent for each point, as required by the multi-fidelity Expected Improvement acquisition function.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei.py</code> <pre><code>def preprocess(self, x: pd.Series) -&gt; Tuple[Iterable, Iterable]:\n    \"\"\"Prepares the configurations for appropriate EI calculation.\n\n    Takes a set of points and computes the budget and incumbent for each point, as\n    required by the multi-fidelity Expected Improvement acquisition function.\n    \"\"\"\n    budget_list = []\n\n    if self.pipeline_space.has_tabular:\n        # preprocess tabular space differently\n        # expected input: IDs pertaining to the tabular data\n        # expected output: IDs pertaining to current observations and set of HPs\n        x = map_real_hyperparameters_from_tabular_ids(x, self.pipeline_space)\n    indices_to_drop = []\n    for i, config in x.items():\n        target_fidelity = config.fidelity.lower\n        if i &lt;= max(self.observations.seen_config_ids):\n            # IMPORTANT to set the fidelity at which EI will be calculated only for\n            # the partial configs that have been observed already\n            target_fidelity = config.fidelity.value + self.b_step\n\n            if np.less_equal(target_fidelity, config.fidelity.upper):\n                # only consider the configs with fidelity lower than the max fidelity\n                config.fidelity.set_value(target_fidelity)\n                budget_list.append(self.get_budget_level(config))\n            else:\n                # if the target_fidelity higher than the max drop the configuration\n                indices_to_drop.append(i)\n        else:\n            config.fidelity.set_value(target_fidelity)\n            budget_list.append(self.get_budget_level(config))\n\n    # Drop unused configs\n    x.drop(labels=indices_to_drop, inplace=True)\n\n    performances = self.observations.get_best_performance_for_each_budget()\n    inc_list = []\n    for budget_level in budget_list:\n        if budget_level in performances.index:\n            inc = performances[budget_level]\n        else:\n            inc = self.observations.get_best_seen_performance()\n        inc_list.append(inc)\n\n    return x, torch.Tensor(inc_list)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei/#neps.optimizers.bayesian_optimization.acquisition_functions.mf_ei.MFEI.preprocess_pfn","title":"preprocess_pfn","text":"<pre><code>preprocess_pfn(\n    x: Iterable,\n) -&gt; Tuple[Iterable, Iterable, Iterable]\n</code></pre> <p>Prepares the configurations for appropriate EI calculation.</p> <p>Takes a set of points and computes the budget and incumbent for each point, as required by the multi-fidelity Expected Improvement acquisition function.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/mf_ei.py</code> <pre><code>def preprocess_pfn(self, x: Iterable) -&gt; Tuple[Iterable, Iterable, Iterable]:\n    \"\"\"Prepares the configurations for appropriate EI calculation.\n\n    Takes a set of points and computes the budget and incumbent for each point, as\n    required by the multi-fidelity Expected Improvement acquisition function.\n    \"\"\"\n    _x, inc_list = self.preprocess(x.copy())\n    _x_tok = self.observations.tokenize(_x, as_tensor=True)\n    len_partial = len(self.observations.seen_config_ids)\n    z_min = x[0].fidelity.lower\n    # converting fidelity to the discrete budget level\n    # STRICT ASSUMPTION: fidelity is the first dimension\n    _x_tok[:len_partial, 0] = (\n        _x_tok[:len_partial, 0] + self.b_step - z_min\n    ) / self.b_step\n    return _x_tok, _x, inc_list\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/prior_weighted/","title":"Prior weighted","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/prior_weighted/#neps.optimizers.bayesian_optimization.acquisition_functions.prior_weighted","title":"neps.optimizers.bayesian_optimization.acquisition_functions.prior_weighted","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ucb/","title":"Ucb","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ucb/#neps.optimizers.bayesian_optimization.acquisition_functions.ucb","title":"neps.optimizers.bayesian_optimization.acquisition_functions.ucb","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ucb/#neps.optimizers.bayesian_optimization.acquisition_functions.ucb.UpperConfidenceBound","title":"UpperConfidenceBound","text":"<pre><code>UpperConfidenceBound(\n    beta: float = 1.0, maximize: bool = False\n)\n</code></pre> <p>               Bases: <code>BaseAcquisition</code></p> PARAMETER DESCRIPTION <code>beta</code> <p>Controls the balance between exploration and exploitation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>maximize</code> <p>If True, maximize the given model, else minimize. DEFAULT=False, assumes minimzation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/ucb.py</code> <pre><code>def __init__(self, beta: float=1.0, maximize: bool=False):\n    \"\"\"Upper Confidence Bound (UCB) acquisition function.\n\n    Args:\n        beta: Controls the balance between exploration and exploitation.\n        maximize: If True, maximize the given model, else minimize.\n            DEFAULT=False, assumes minimzation.\n    \"\"\"\n    super().__init__()\n    self.beta = beta  # can be updated as part of the state for dynamism or a schedule\n    self.maximize = maximize\n\n    # to be initialized as part of the state\n    self.surrogate_model = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/base_acq_sampler/","title":"Base acq sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/base_acq_sampler/#neps.optimizers.bayesian_optimization.acquisition_samplers.base_acq_sampler","title":"neps.optimizers.bayesian_optimization.acquisition_samplers.base_acq_sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/evolution_sampler/","title":"Evolution sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/evolution_sampler/#neps.optimizers.bayesian_optimization.acquisition_samplers.evolution_sampler","title":"neps.optimizers.bayesian_optimization.acquisition_samplers.evolution_sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/freeze_thaw_sampler/","title":"Freeze thaw sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/freeze_thaw_sampler/#neps.optimizers.bayesian_optimization.acquisition_samplers.freeze_thaw_sampler","title":"neps.optimizers.bayesian_optimization.acquisition_samplers.freeze_thaw_sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/freeze_thaw_sampler/#neps.optimizers.bayesian_optimization.acquisition_samplers.freeze_thaw_sampler.FreezeThawSampler","title":"FreezeThawSampler","text":"<pre><code>FreezeThawSampler(**kwargs)\n</code></pre> <p>               Bases: <code>AcquisitionSampler</code></p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_samplers/freeze_thaw_sampler.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n    self.observations = None\n    self.b_step = None\n    self.n = None\n    self.pipeline_space = None\n    # args to manage tabular spaces/grid\n    self.is_tabular = False\n    self.sample_full_table = None\n    self.set_sample_full_tabular(True)  # sets flag that samples full table\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/freeze_thaw_sampler/#neps.optimizers.bayesian_optimization.acquisition_samplers.freeze_thaw_sampler.FreezeThawSampler.sample","title":"sample","text":"<pre><code>sample(\n    acquisition_function=None,\n    n: int = None,\n    set_new_sample_fidelity: int | float = None,\n) -&gt; list()\n</code></pre> <p>Samples a new set and returns the total set of observed + new configs.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_samplers/freeze_thaw_sampler.py</code> <pre><code>def sample(\n    self,\n    acquisition_function=None,\n    n: int = None,\n    set_new_sample_fidelity: int | float = None,\n) -&gt; list():\n    \"\"\"Samples a new set and returns the total set of observed + new configs.\"\"\"\n    partial_configs = self.observations.get_partial_configs_at_max_seen()\n    new_configs = self._sample_new(\n        index_from=self.observations.next_config_id(), n=n, ignore_fidelity=False\n    )\n\n    def __sample_single_new_tabular(index: int):\n        \"\"\"\n        A function to use in a list comprehension to slightly speed up\n        the sampling process when self.SAMPLE_TO_DRAW is large\n        \"\"\"\n        config = self.pipeline_space.sample(\n            patience=self.patience, user_priors=False, ignore_fidelity=False\n        )\n        config[\"id\"].set_value(_new_configs[index])\n        config.fidelity.set_value(set_new_sample_fidelity)\n        return config\n\n    if self.is_tabular:\n        _n = n if n is not None else self.SAMPLES_TO_DRAW\n        _partial_ids = {conf[\"id\"].value for conf in partial_configs}\n        _all_ids = set(self.pipeline_space.custom_grid_table.index.values)\n\n        # accounting for unseen configs only, samples remaining table if flag is set\n        max_n = len(_all_ids) + 1 if self.sample_full_table else _n\n        _n = min(max_n, len(_all_ids - _partial_ids))\n\n        _new_configs = np.random.choice(\n            list(_all_ids - _partial_ids), size=_n, replace=False\n        )\n        new_configs = [__sample_single_new_tabular(i) for i in range(_n)]\n        new_configs = pd.Series(\n            new_configs,\n            index=np.arange(\n                len(partial_configs), len(partial_configs) + len(new_configs)\n            ),\n        )\n\n    elif set_new_sample_fidelity is not None:\n        for config in new_configs:\n            config.fidelity.set_value(set_new_sample_fidelity)\n\n    # Deep copy configs for fidelity updates\n    partial_configs_list = []\n    index_list = []\n    for idx, config in partial_configs.items():\n        _config = config.clone()\n        partial_configs_list.append(_config)\n        index_list.append(idx)\n\n    # We build a new series of partial configs to avoid\n    # incrementing fidelities multiple times due to pass-by-reference\n    partial_configs = pd.Series(partial_configs_list, index=index_list)\n\n    configs = pd.concat([partial_configs, new_configs])\n\n    return configs\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/mutation_sampler/","title":"Mutation sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/mutation_sampler/#neps.optimizers.bayesian_optimization.acquisition_samplers.mutation_sampler","title":"neps.optimizers.bayesian_optimization.acquisition_samplers.mutation_sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/random_sampler/","title":"Random sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_samplers/random_sampler/#neps.optimizers.bayesian_optimization.acquisition_samplers.random_sampler","title":"neps.optimizers.bayesian_optimization.acquisition_samplers.random_sampler","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels/","title":"Combine kernels","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels/#neps.optimizers.bayesian_optimization.kernels.combine_kernels","title":"neps.optimizers.bayesian_optimization.kernels.combine_kernels","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels/#neps.optimizers.bayesian_optimization.kernels.combine_kernels.SumKernel","title":"SumKernel","text":"<pre><code>SumKernel(*kernels, **kwargs)\n</code></pre> <p>               Bases: <code>CombineKernel</code></p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/combine_kernels.py</code> <pre><code>def __init__(self, *kernels, **kwargs):\n    super().__init__(\"sum\", *kernels, **kwargs)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels/#neps.optimizers.bayesian_optimization.kernels.combine_kernels.SumKernel.forward_t","title":"forward_t","text":"<pre><code>forward_t(\n    weights: Tensor,\n    gr2: list,\n    x2=None,\n    gr1: list = None,\n    x1=None,\n    feature_lengthscale=None,\n)\n</code></pre> <p>Compute the kernel gradient w.r.t the feature vector Parameters</p> <p>feature_lengthscale x2 x1 gr1 weights gr2</p> <p>Returns ------- grads: k list of 2-tuple. (K, x2) where K is the weighted Gram matrix of that matrix, x2 is the leaf variable on which Jacobian-vector product to be computed.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/combine_kernels.py</code> <pre><code>def forward_t(\n    self,\n    weights: torch.Tensor,\n    gr2: list,\n    x2=None,\n    gr1: list = None,\n    x1=None,\n    feature_lengthscale=None,\n):\n    \"\"\"\n    Compute the kernel gradient w.r.t the feature vector\n    Parameters\n    ----------\n    feature_lengthscale\n    x2\n    x1\n    gr1\n    weights\n    gr2\n\n    Returns ------- grads: k list of 2-tuple. (K, x2) where K is the weighted Gram\n    matrix of that matrix, x2 is the leaf variable on which Jacobian-vector product\n    to be computed.\n\n    \"\"\"\n    grads = []\n    for i, k in enumerate(self.kernels):\n        if isinstance(k, GraphKernels):\n            handle = k.forward_t(gr2, gr1=gr1)\n            grads.append((weights[i] * handle[0], handle[1], handle[2]))\n        elif isinstance(k, Stationary):\n            key = _select_dimensions(k)\n            handle = k.forward_t(x2=x2[key], x1=x1[key], l=feature_lengthscale[i])\n            grads.append((weights[i] * handle[0], handle[1], handle[2]))\n        else:\n            logging.warning(\n                \"Gradient not implemented for kernel type\" + str(k.__name__)\n            )\n            grads.append((None, None))\n    assert len(grads) == len(self.kernels)\n    return grads\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels_hierarchy/","title":"Combine kernels hierarchy","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels_hierarchy/#neps.optimizers.bayesian_optimization.kernels.combine_kernels_hierarchy","title":"neps.optimizers.bayesian_optimization.kernels.combine_kernels_hierarchy","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels_hierarchy/#neps.optimizers.bayesian_optimization.kernels.combine_kernels_hierarchy.SumKernel","title":"SumKernel","text":"<pre><code>SumKernel(*kernels, **kwargs)\n</code></pre> <p>               Bases: <code>CombineKernel</code></p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/combine_kernels_hierarchy.py</code> <pre><code>def __init__(self, *kernels, **kwargs):\n    super().__init__(\"sum\", *kernels, **kwargs)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels_hierarchy/#neps.optimizers.bayesian_optimization.kernels.combine_kernels_hierarchy.SumKernel.forward_t","title":"forward_t","text":"<pre><code>forward_t(\n    weights: Tensor,\n    gr2: list,\n    x2=None,\n    gr1: list = None,\n    x1=None,\n    feature_lengthscale=None,\n)\n</code></pre> <p>Compute the kernel gradient w.r.t the feature vector Parameters</p> <p>feature_lengthscale x2 x1 gr1 weights gr2</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/combine_kernels_hierarchy/#neps.optimizers.bayesian_optimization.kernels.combine_kernels_hierarchy.SumKernel.forward_t--returns","title":"Returns","text":"<p>grads: k list of 2-tuple. (K, x2) where K is the weighted Gram matrix of that matrix, x2 is the leaf variable on which Jacobian-vector product to be computed.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/combine_kernels_hierarchy.py</code> <pre><code>def forward_t(\n    self,\n    weights: torch.Tensor,\n    gr2: list,\n    x2=None,\n    gr1: list = None,\n    x1=None,\n    feature_lengthscale=None,\n):\n    \"\"\"\n    Compute the kernel gradient w.r.t the feature vector\n    Parameters\n    ----------\n    feature_lengthscale\n    x2\n    x1\n    gr1\n    weights\n    gr2\n\n    Returns\n    -------\n    grads: k list of 2-tuple.\n    (K, x2) where K is the weighted Gram matrix of that matrix, x2 is the leaf variable on which Jacobian-vector\n    product to be computed.\n\n    \"\"\"\n    weights = transform_weights(weights.clone())\n    grads = []\n    for i, k in enumerate(self.kernels):\n        if isinstance(k, GraphKernels):\n            handle = k.forward_t(gr2, gr1=gr1)\n            grads.append((weights[i] * handle[0], handle[1], handle[2]))\n        elif isinstance(k, Stationary):\n            handle = k.forward_t(x2=x2, x1=x1, l=feature_lengthscale)\n            grads.append((weights[i] * handle[0], handle[1], handle[2]))\n        else:\n            logging.warning(\n                \"Gradient not implemented for kernel type\" + str(k.__name__)\n            )\n            grads.append((None, None))\n    assert len(grads) == len(self.kernels)\n    return grads\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/","title":"Encoding","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding","title":"neps.optimizers.bayesian_optimization.kernels.encoding","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.NASBOTDistance","title":"NASBOTDistance","text":"<pre><code>NASBOTDistance(\n    node_name=\"op_name\",\n    include_op_list=None,\n    exclude_op_list=None,\n    lengthscale=3.0,\n    normalize=True,\n    max_size=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>GraphKernels</code></p> <p>NASBOT OATMANN distance according to BANANAS paper</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/encoding.py</code> <pre><code>def __init__(\n    self,\n    node_name=\"op_name\",\n    include_op_list=None,\n    exclude_op_list=None,\n    lengthscale=3.0,\n    normalize=True,\n    max_size=None,\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    self.node_name = node_name\n    self.include_op_list = include_op_list if include_op_list is not None else OPS\n    self.exclude_op_list = exclude_op_list if exclude_op_list is not None else []\n    self.normalize = normalize\n    self.lengthscale = lengthscale\n    self.max_size = max_size\n    self._gram = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.NASBOTDistance.forward_t","title":"forward_t","text":"<pre><code>forward_t(gr2, gr1: list = None)\n</code></pre> <p>Compute the derivative of the kernel function k(phi, phi) with respect to phi (the training point)</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/graph_kernel.py</code> <pre><code>def forward_t(self, gr2, gr1: list = None):\n    \"\"\"\n    Compute the derivative of the kernel function k(phi, phi*) with respect to phi* (the training point)\n    \"\"\"\n    raise NotImplementedError(\n        \"The kernel gradient is not implemented for the graph kernel called!\"\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.PathDistance","title":"PathDistance","text":"<pre><code>PathDistance(\n    node_name=\"op_name\",\n    include_op_list=None,\n    exclude_op_list=None,\n    lengthscale=3.0,\n    normalize=True,\n    max_size=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>NASBOTDistance</code></p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/encoding.py</code> <pre><code>def __init__(\n    self,\n    node_name=\"op_name\",\n    include_op_list=None,\n    exclude_op_list=None,\n    lengthscale=3.0,\n    normalize=True,\n    max_size=None,\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    self.node_name = node_name\n    self.include_op_list = include_op_list if include_op_list is not None else OPS\n    self.exclude_op_list = exclude_op_list if exclude_op_list is not None else []\n    self.normalize = normalize\n    self.lengthscale = lengthscale\n    self.max_size = max_size\n    self._gram = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.PathDistance.encode_paths","title":"encode_paths","text":"<pre><code>encode_paths(g: Graph)\n</code></pre> <p>output one-hot encoding of paths</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/encoding.py</code> <pre><code>def encode_paths(self, g: nx.Graph):\n    \"\"\"output one-hot encoding of paths\"\"\"\n    if \"~\" in g.name:\n        LONGEST_PATH_LENGTH = 3\n        num_paths = sum(len(OPS_201) ** i for i in range(1, LONGEST_PATH_LENGTH + 1))\n        path_indices = self.get_path_indices_201(g)\n    elif \"101\" in g.name:\n        num_paths = sum(len(OPS_EX) ** i for i in range(OP_SPOTS + 1))\n        path_indices = self.get_path_indices(g)\n    else:\n        num_paths = sum(len(self.op_list) ** i for i in range(self.max_size - 1))\n        path_indices = self.get_paths(g)\n    path_encoding = np.zeros(num_paths)\n    for index in path_indices:\n        path_encoding[index] = 1\n    return path_encoding\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.PathDistance.forward_t","title":"forward_t","text":"<pre><code>forward_t(gr2, gr1: list = None)\n</code></pre> <p>Compute the derivative of the kernel function k(phi, phi) with respect to phi (the training point)</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/graph_kernel.py</code> <pre><code>def forward_t(self, gr2, gr1: list = None):\n    \"\"\"\n    Compute the derivative of the kernel function k(phi, phi*) with respect to phi* (the training point)\n    \"\"\"\n    raise NotImplementedError(\n        \"The kernel gradient is not implemented for the graph kernel called!\"\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.PathDistance.get_path_indices","title":"get_path_indices","text":"<pre><code>get_path_indices(g: Graph)\n</code></pre> <p>compute the index of each path There are 3^0 + ... + 3^5 paths total. (Paths can be length 0 to 5, and for each path, for each node, there are three choices for the operation.)</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/encoding.py</code> <pre><code>def get_path_indices(self, g: nx.Graph):\n    \"\"\"\n    compute the index of each path\n    There are 3^0 + ... + 3^5 paths total.\n    (Paths can be length 0 to 5, and for each path, for each node, there\n    are three choices for the operation.)\n    \"\"\"\n    paths = self.get_paths(g)\n    mapping = {CONV3X3: 0, CONV1X1: 1, MAXPOOL3X3: 2}\n    path_indices = []\n\n    for path in paths:\n        index = 0\n        for i in range(NUM_VERTICES - 1):\n            if i == len(path):\n                path_indices.append(index)\n                break\n            else:\n                index += len(OPS_EX) ** i * (mapping[path[i]] + 1)\n\n    return tuple(path_indices)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.PathDistance.get_path_indices_201","title":"get_path_indices_201","text":"<pre><code>get_path_indices_201(g: Graph)\n</code></pre> <p>compute the index of each path</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/encoding.py</code> <pre><code>def get_path_indices_201(self, g: nx.Graph):\n    \"\"\"\n    compute the index of each path\n    \"\"\"\n    paths = self.get_paths_201(g)\n    path_indices = []\n    NUM_OPS = len(OPS_201)\n    for i, path in enumerate(paths):\n        if i == 0:\n            index = 0\n        elif i in [1, 2]:\n            index = NUM_OPS\n        else:\n            index = NUM_OPS + NUM_OPS**2\n        for j, op in enumerate(path):\n            index += OPS_201.index(op) * NUM_OPS**j\n        path_indices.append(index)\n\n    return tuple(path_indices)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.PathDistance.get_paths","title":"get_paths","text":"<pre><code>get_paths(g: Graph)\n</code></pre> <p>return all paths from input to output</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/encoding.py</code> <pre><code>def get_paths(self, g: nx.Graph):\n    \"\"\"\n    return all paths from input to output\n    \"\"\"\n    paths: list = []\n    matrix = nx.to_numpy_array(g)\n    ops: list = []\n    for _, attr in g.nodes(data=True):\n        ops.append(attr[self.node_name])\n    for j in range(0, NUM_VERTICES):\n        if matrix[0][j]:\n            paths.append([[]])\n        else:\n            paths.append([])\n\n    # create paths sequentially\n    for i in range(1, NUM_VERTICES - 1):\n        for j in range(1, NUM_VERTICES):\n            if matrix[i][j]:\n                for path in paths[i]:\n                    paths[j].append([*path, ops[i]])\n    return paths[-1]\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/encoding/#neps.optimizers.bayesian_optimization.kernels.encoding.PathDistance.get_paths_201","title":"get_paths_201  <code>staticmethod</code>","text":"<pre><code>get_paths_201(g: Graph)\n</code></pre> <p>return all paths from input to output</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/encoding.py</code> <pre><code>@staticmethod\ndef get_paths_201(g: nx.Graph):\n    \"\"\"\n    return all paths from input to output\n    \"\"\"\n    path_blueprints = [[3], [0, 4], [1, 5], [0, 2, 5]]\n    ops = get_op_list(g.name)\n    paths = []\n    for blueprint in path_blueprints:\n        paths.append([ops[node] for node in blueprint])\n\n    return paths\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/get_kernels/","title":"Get kernels","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/get_kernels/#neps.optimizers.bayesian_optimization.kernels.get_kernels","title":"neps.optimizers.bayesian_optimization.kernels.get_kernels","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/graph_kernel/","title":"Graph kernel","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/graph_kernel/#neps.optimizers.bayesian_optimization.kernels.graph_kernel","title":"neps.optimizers.bayesian_optimization.kernels.graph_kernel","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/graph_kernel/#neps.optimizers.bayesian_optimization.kernels.graph_kernel.GraphKernels","title":"GraphKernels","text":"<pre><code>GraphKernels(**kwargs)\n</code></pre> Source code in <code>neps/optimizers/bayesian_optimization/kernels/graph_kernel.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n    self.n_hyperparameters = 0\n    self.rbf_lengthscale = False\n    self.kern = None\n    self.__name__ = \"GraphKernelBase\"\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/graph_kernel/#neps.optimizers.bayesian_optimization.kernels.graph_kernel.GraphKernels.forward_t","title":"forward_t","text":"<pre><code>forward_t(gr2, gr1: list = None)\n</code></pre> <p>Compute the derivative of the kernel function k(phi, phi) with respect to phi (the training point)</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/graph_kernel.py</code> <pre><code>def forward_t(self, gr2, gr1: list = None):\n    \"\"\"\n    Compute the derivative of the kernel function k(phi, phi*) with respect to phi* (the training point)\n    \"\"\"\n    raise NotImplementedError(\n        \"The kernel gradient is not implemented for the graph kernel called!\"\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/utils/","title":"Utils","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/utils/#neps.optimizers.bayesian_optimization.kernels.utils","title":"neps.optimizers.bayesian_optimization.kernels.utils","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/utils/#neps.optimizers.bayesian_optimization.kernels.utils.extract_configs","title":"extract_configs","text":"<pre><code>extract_configs(\n    configs: list[SearchSpace],\n) -&gt; Tuple[list, list]\n</code></pre> <p>Extracts graph &amp; HPs from configs objects</p> PARAMETER DESCRIPTION <code>configs</code> <p>Object holding graph and/or HPs</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Tuple[list, list]</code> <p>Tuple[list, list]: list of graphs, list of HPs</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/utils.py</code> <pre><code>def extract_configs(configs: list[SearchSpace]) -&gt; Tuple[list, list]:\n    \"\"\"Extracts graph &amp; HPs from configs objects\n\n    Args:\n        configs (list): Object holding graph and/or HPs\n\n    Returns:\n        Tuple[list, list]: list of graphs, list of HPs\n    \"\"\"\n    config_hps = [conf.get_normalized_hp_categories() for conf in configs]\n    graphs = [hps[\"graphs\"] for hps in config_hps]\n    # Don't call np.array on structured objects\n    # https://github.com/numpy/numpy/issues/24546#issuecomment-1693913119\n    # _nested_graphs = np.array(graphs, dtype=object)\n    # if _nested_graphs.ndim == 3\n    #   graphs = _nested_graphs[:, :, 0].reshape(-1).tolist()\n    # Long hand way of doing the above\n    if (len(graphs) &gt; 0 and isinstance(graphs[0], list)\n        and len(graphs[0]) &gt; 0 and isinstance(graphs[0][0], list)):\n        res = [_list for list_of_list in graphs for _list in list_of_list]\n        graphs = res\n    return graphs, config_hps\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/utils/#neps.optimizers.bayesian_optimization.kernels.utils.extract_configs_hierarchy","title":"extract_configs_hierarchy","text":"<pre><code>extract_configs_hierarchy(\n    configs: list,\n    d_graph_features: int,\n    hierarchy_consider=None,\n) -&gt; Tuple[list, list]\n</code></pre> <p>Extracts graph &amp; graph features from configs objects Args:     configs (list): Object holding graph and/or graph features     d_graph_features (int): Number of global graph features used; if d_graph_features=0, indicate not using global graph features     hierarchy_consider (list or None): Specify graphs at which earlier hierarchical levels to be considered Returns:     Tuple[list, list]: list of graphs, list of HPs</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/utils.py</code> <pre><code>def extract_configs_hierarchy(\n    configs: list, d_graph_features: int, hierarchy_consider=None\n) -&gt; Tuple[list, list]:\n    \"\"\"Extracts graph &amp; graph features from configs objects\n    Args:\n        configs (list): Object holding graph and/or graph features\n        d_graph_features (int): Number of global graph features used; if d_graph_features=0, indicate not using global graph features\n        hierarchy_consider (list or None): Specify graphs at which earlier hierarchical levels to be considered\n    Returns:\n        Tuple[list, list]: list of graphs, list of HPs\n    \"\"\"\n    N = len(configs)\n\n    config_hps = [conf.get_normalized_hp_categories() for conf in configs]\n    combined_graphs = [hps[\"graphs\"] for hps in config_hps]\n    if N &gt; 0 and hierarchy_consider is not None and combined_graphs[0]:\n        # graphs = list(\n        #     map(\n        #         list,\n        #         zip(\n        #             *[\n        #                 [g[0][0]]\n        #                 + [g[0][1][hierarchy_id] for hierarchy_id in hierarchy_consider]\n        #                 for g in combined_graphs\n        #             ]\n        #         ),\n        #     )\n        # )\n        graphs = list(\n            map(\n                list,\n                zip(\n                    *[\n                        [g[0][0]]\n                        + [\n                            g[0][1][hierarchy_id]\n                            if hierarchy_id in g[0][1]\n                            else g[0][1][max(g[0][1].keys())]\n                            for hierarchy_id in hierarchy_consider\n                        ]\n                        for g in combined_graphs\n                    ]\n                ),\n            )\n        )\n        ### full graph, 0th hierarchy (high-level, smallest), 1st hierarchy, 2nd hierarchy, 3rd hierarchy, ...\n        ### graph gets bigger of hierarchies\n        ### list shape: (1+4) x N\n\n        # modify the node attribute labels on earlier hierarchy graphs e.g.\n        # note the node feature for graph in earlier hierarchical level should be more coarse\n        # e.g. {'op_name': '(Cell diamond (OPS id) (OPS avg_pool) (OPS id) (OPS avg_pool))'} -&gt; {'op_name': 'Cell diamond '}\n        for hg_list in graphs[1:]:\n            for G in hg_list:\n                original_node_labels = nx.get_node_attributes(G, \"op_name\")\n                new_node_labels = {\n                    k: v.split(\"(\")[1]\n                    for k, v in original_node_labels.items()\n                    if \"(\" in v and \")\" in v\n                }\n                nx.set_node_attributes(G, new_node_labels, name=\"op_name\")\n    else:\n        # graphs = [g[0][0] for g in combined_graphs]\n        graphs = combined_graphs\n\n    if N &gt; 0 and d_graph_features &gt; 0:\n        # graph_features = [c['metafeature'] for c in configs]\n        # these feature values are normalised between 0 and 1\n        # the two graph features used are 'avg_path_length', 'density'\n        graph_features = [\n            [\n                graph_metrics(g[0][0], metric=\"avg_path_length\"),\n                graph_metrics(g[0][0], metric=\"density\"),\n            ]\n            for g in combined_graphs\n        ]\n        graph_features_array = np.vstack(graph_features)  # shape n_archs x 2 (nx(2+d_hp))\n    else:\n        # if not using global graph features of the final architectures, set them to None\n        graph_features_array = [None] * N\n\n    return graphs, graph_features_array\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/utils/#neps.optimizers.bayesian_optimization.kernels.utils.transform_to_undirected","title":"transform_to_undirected","text":"<pre><code>transform_to_undirected(gr: list)\n</code></pre> <p>Transform a list of directed graphs by undirected graphs.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/utils.py</code> <pre><code>def transform_to_undirected(gr: list):\n    \"\"\"Transform a list of directed graphs by undirected graphs.\"\"\"\n    undirected_gr = []\n    for g in gr:\n        if not isinstance(g, nx.Graph):\n            continue\n        if isinstance(g, nx.DiGraph):\n            undirected_gr.append(g.to_undirected())\n        else:\n            undirected_gr.append(g)\n    return undirected_gr\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/vectorial_kernels/","title":"Vectorial kernels","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/vectorial_kernels/#neps.optimizers.bayesian_optimization.kernels.vectorial_kernels","title":"neps.optimizers.bayesian_optimization.kernels.vectorial_kernels","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/vectorial_kernels/#neps.optimizers.bayesian_optimization.kernels.vectorial_kernels.LayeredRBFKernel","title":"LayeredRBFKernel","text":"<pre><code>LayeredRBFKernel(\n    lengthscale: Union[float, Tuple[float, ...]] = 1.0,\n    lengthscale_bounds: Tuple[float, float] = (\n        np.exp(-6.754111155189306),\n        np.exp(0.0858637988771976),\n    ),\n    outputscale=1.0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RBFKernel</code></p> <p>Same as the conventional RBF kernel, but adapted in a way as a midway between spherical RBF and ARD RBF. In this case, one weight is assigned to each Weisfiler-Lehman iteration only (e.g. one weight for h=0, another for h=1 and etc.)</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/vectorial_kernels.py</code> <pre><code>def __init__(\n    self,\n    lengthscale: Union[float, Tuple[float, ...]] = 1.0,\n    lengthscale_bounds: Tuple[float, float] = (\n        np.exp(-6.754111155189306),\n        np.exp(0.0858637988771976),\n    ),\n    outputscale=1.0,\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    self.lengthscale = lengthscale\n    self.lengthscale_bounds = lengthscale_bounds\n    self.outputscale = outputscale\n\n    self._gram = None\n    self._train = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/vectorial_kernels/#neps.optimizers.bayesian_optimization.kernels.vectorial_kernels.Stationary","title":"Stationary","text":"<pre><code>Stationary(\n    lengthscale: Union[float, Tuple[float, ...]] = 1.0,\n    lengthscale_bounds: Tuple[float, float] = (\n        np.exp(-6.754111155189306),\n        np.exp(0.0858637988771976),\n    ),\n    outputscale=1.0,\n    **kwargs\n)\n</code></pre> <p>Here we follow the structure of GPy to build a sub class of stationary kernel. All the classes (i.e. the class of stationary kernel_operators) derived from this class use the scaled distance to compute the Gram matrix.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/vectorial_kernels.py</code> <pre><code>def __init__(\n    self,\n    lengthscale: Union[float, Tuple[float, ...]] = 1.0,\n    lengthscale_bounds: Tuple[float, float] = (\n        np.exp(-6.754111155189306),\n        np.exp(0.0858637988771976),\n    ),\n    outputscale=1.0,\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    self.lengthscale = lengthscale\n    self.lengthscale_bounds = lengthscale_bounds\n    self.outputscale = outputscale\n\n    self._gram = None\n    self._train = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/","title":"Weisfilerlehman","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman","title":"neps.optimizers.bayesian_optimization.kernels.weisfilerlehman","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman","title":"WeisfilerLehman","text":"<pre><code>WeisfilerLehman(\n    h: int = 0,\n    base_type: str = \"subtree\",\n    se_kernel: Stationary = None,\n    layer_weights=None,\n    node_weights=None,\n    oa: bool = False,\n    node_label: str = \"op_name\",\n    edge_label: tuple = \"op_name\",\n    n_jobs: int = None,\n    return_tensor: bool = True,\n    requires_grad: bool = False,\n    undirected: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>GraphKernels</code></p> <p>Weisfiler Lehman kernel using grakel functions</p> <p>h: int: The number of Weisfeiler-Lehman iterations base_type: str: defines the base kernel of WL iteration. Possible types are 'subtree' (default), 'sp': shortest path and 'edge' (The latter two are untested) se_kernel: Stationary. defines a stationary vector kernel to be used for successive embedding (i.e. the kernel     function on which the vector embedding inner products are computed). if None, use the default linear kernel node_weights oa: whether the optimal assignment variant of the Weisfiler-Lehman kernel should be used node_label: the node_label defining the key node attribute. edge_label: the edge label defining the key edge attribute. only relevant when base_type == 'edge' n_jobs: Parallisation to be used. *current version does not support parallel computing' return_tensor: whether return a torch tensor. If False, a numpy array will be returned. kwargs</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/weisfilerlehman.py</code> <pre><code>def __init__(\n    self,\n    h: int = 0,\n    base_type: str = \"subtree\",\n    se_kernel: Stationary = None,\n    layer_weights=None,\n    node_weights=None,\n    oa: bool = False,\n    node_label: str = \"op_name\",\n    edge_label: tuple = \"op_name\",\n    n_jobs: int = None,\n    return_tensor: bool = True,\n    requires_grad: bool = False,\n    undirected: bool = False,\n    **kwargs,\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    h: int: The number of Weisfeiler-Lehman iterations\n    base_type: str: defines the base kernel of WL iteration. Possible types are 'subtree' (default), 'sp': shortest path\n    and 'edge' (The latter two are untested)\n    se_kernel: Stationary. defines a stationary vector kernel to be used for successive embedding (i.e. the kernel\n        function on which the vector embedding inner products are computed). if None, use the default linear kernel\n    node_weights\n    oa: whether the optimal assignment variant of the Weisfiler-Lehman kernel should be used\n    node_label: the node_label defining the key node attribute.\n    edge_label: the edge label defining the key edge attribute. only relevant when base_type == 'edge'\n    n_jobs: Parallisation to be used. *current version does not support parallel computing'\n    return_tensor: whether return a torch tensor. If False, a numpy array will be returned.\n    kwargs\n    \"\"\"\n    super().__init__(**kwargs)\n    if se_kernel is not None and oa:\n        raise ValueError(\n            \"Only one or none of se (successive embedding) and oa (optimal assignment) may be true!\"\n        )\n    self.h = h\n    self.oa = oa\n    self.node_label = node_label\n    self.edge_label = edge_label\n    self.layer_weights = layer_weights\n    self.se = se_kernel\n    self.requires_grad = requires_grad\n    self.undirected = undirected\n\n    if base_type not in [\"subtree\", \"sp\", \"edge\"]:\n        raise ValueError(f\"Invalid value for base_type ({base_type})\")\n    if base_type == \"subtree\":\n        base_kernel = VertexHistogram, {\n            \"sparse\": False,\n            \"requires_ordered_features\": requires_grad,\n        }\n        if oa:\n            base_kernel = VertexHistogram, {\n                \"oa\": True,\n                \"sparse\": False,\n                \"requires_ordered_features\": requires_grad,\n            }\n        elif se_kernel is not None:\n            base_kernel = VertexHistogram, {\n                \"se_kernel\": se_kernel,\n                \"sparse\": False,\n                \"requires_ordered_features\": requires_grad,\n            }\n    elif base_type == \"edge\":\n        base_kernel = EdgeHistogram, {\"sparse\": False}\n        if oa:\n            base_kernel = EdgeHistogram, {\n                \"oa\": True,\n                \"sparse\": False,\n                \"requires_ordered_features\": requires_grad,\n            }\n        elif se_kernel is not None:\n            base_kernel = EdgeHistogram, {\n                \"se_kernel\": se_kernel,\n                \"sparse\": False,\n                \"requires_ordered_features\": requires_grad,\n            }\n\n    elif base_type == \"sp\":\n        base_kernel = ShortestPathAttr, {}\n    else:\n        raise NotImplementedError(\n            \"The selected WL base kernel type\"\n            + str(base_type)\n            + \" is not implemented.\"\n        )\n    self.base_type = base_type\n    self.kern = _WL(\n        n_jobs,\n        h=h,\n        base_graph_kernel=base_kernel,\n        normalize=True,\n        layer_weights=self.layer_weights,\n        node_weights=node_weights,\n    )\n    self.return_tensor = return_tensor\n    self._gram = None\n    self._train, self._train_transformed = None, None\n    self.__name__ = \"WeisfeilerLehman\"\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.change_se_params","title":"change_se_params","text":"<pre><code>change_se_params(params: dict)\n</code></pre> <p>Change the kernel parameter of the successive embedding kernel.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/weisfilerlehman.py</code> <pre><code>def change_se_params(self, params: dict):\n    \"\"\"Change the kernel parameter of the successive embedding kernel.\"\"\"\n    if self.se is None:\n        logging.warning(\"SE kernel is None. change_se_params action voided.\")\n        return\n    for k, v in params.items():\n        try:\n            setattr(self.se, k, v)\n        except AttributeError:\n            logging.warning(\n                str(k) + \" is not a valid attribute name of the SE kernel.\"\n            )\n            continue\n    self.kern.change_se_kernel(self.se)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.feature_map","title":"feature_map","text":"<pre><code>feature_map(flatten=True)\n</code></pre> <p>Get the feature map in term of encoding (position in the feature index): the feature string. Parameters</p> <p>flatten: whether flatten the dict (originally, the result is layered in term of h (the number of WL iterations).</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.feature_map--returns","title":"Returns","text":"Source code in <code>neps/optimizers/bayesian_optimization/kernels/weisfilerlehman.py</code> <pre><code>def feature_map(self, flatten=True):\n    \"\"\"\n    Get the feature map in term of encoding (position in the feature index): the feature string.\n    Parameters\n    ----------\n    flatten: whether flatten the dict (originally, the result is layered in term of h (the number of WL iterations).\n\n    Returns\n    -------\n\n    \"\"\"\n    if not self.requires_grad:\n        logging.warning(\n            \"Requires_grad flag is off -- in this case, there is risk that the element order in the \"\n            \"feature map DOES NOT correspond to the order in the feature matrix. To suppress this warning,\"\n            \"when initialising the WL kernel, do WeisfilerLehman(requires_grad=True)\"\n        )\n    if self._gram is None:\n        return None\n    if not flatten:\n        return self.kern._label_node_attr\n    else:\n        res = {}\n        for _, map_ in self.kern._label_node_attr.items():\n            for k, v in map_.items():\n                res.update({k: v})\n        return res\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.feature_value","title":"feature_value","text":"<pre><code>feature_value(X_s)\n</code></pre> <p>Given a list of architectures X_s, compute their WL embedding of size N_s x D, where N_s is the length of the list and D is the number of training set features.</p> RETURNS DESCRIPTION <code>embedding</code> <p>torch.Tensor of shape N_s x D, described above names: list of shape D, which has 1-to-1 correspondence to each element of the embedding matrix above</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/weisfilerlehman.py</code> <pre><code>def feature_value(self, X_s):\n    \"\"\"Given a list of architectures X_s, compute their WL embedding of size N_s x D, where N_s is the length\n    of the list and D is the number of training set features.\n\n    Returns:\n        embedding: torch.Tensor of shape N_s x D, described above\n        names: list of shape D, which has 1-to-1 correspondence to each element of the embedding matrix above\n    \"\"\"\n    if not self.requires_grad:\n        logging.warning(\n            \"Requires_grad flag is off -- in this case, there is risk that the element order in the \"\n            \"feature map DOES NOT correspond to the order in the feature matrix. To suppress this warning,\"\n            \"when initialising the WL kernel, do WeisfilerLehman(requires_grad=True)\"\n        )\n    feat_map = self.feature_map(flatten=False)\n    len_feat_map = [len(f) for f in feat_map.values()]\n    X_s = graph_from_networkx(\n        X_s,\n        self.node_label,\n    )\n    embedding = self.kern.transform(X_s, return_embedding_only=True)\n    for j, em in enumerate(embedding):\n        # Remove some of the spurious features that pop up sometimes\n        embedding[j] = em[:, : len_feat_map[j]]\n\n    # Generate the final embedding\n    embedding = torch.tensor(np.concatenate(embedding, axis=1))\n    return embedding, list(self.feature_map(flatten=True).values())\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.forward_t","title":"forward_t","text":"<pre><code>forward_t(gr2, gr1=None)\n</code></pre> <p>Forward pass, but in tensor format.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.forward_t--parameters","title":"Parameters","text":"<p>gr1: single networkx graph</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.forward_t--returns","title":"Returns","text":"<p>K: the kernel matrix x2 or y: the leaf variable(s) with requires_grad enabled. This allows future Jacobian-vector product to be efficiently computed.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/weisfilerlehman.py</code> <pre><code>def forward_t(self, gr2, gr1=None):\n    \"\"\"\n    Forward pass, but in tensor format.\n\n    Parameters\n    ----------\n    gr1: single networkx graph\n\n    Returns\n    -------\n    K: the kernel matrix\n    x2 or y: the leaf variable(s) with requires_grad enabled.\n    This allows future Jacobian-vector product to be efficiently computed.\n    \"\"\"\n    if self.undirected:\n        gr2 = transform_to_undirected(gr2)\n\n    # Convert into GraKel compatible graph format\n    if self.base_type == \"edge\":\n        gr2 = graph_from_networkx(gr2, self.node_label, self.edge_label)\n    else:\n        gr2 = graph_from_networkx(gr2, self.node_label)\n\n    if gr1 is None:\n        gr1 = self._train_transformed\n    else:\n        if self.undirected:\n            gr1 = transform_to_undirected(gr1)\n        if self.base_type == \"edge\":\n            gr1 = graph_from_networkx(gr1, self.node_label, self.edge_label)\n        else:\n            gr1 = graph_from_networkx(gr1, self.node_label)\n\n    x_ = torch.tensor(\n        np.concatenate(self.kern.transform(gr1, return_embedding_only=True), axis=1)\n    )\n    y_ = torch.tensor(\n        np.concatenate(self.kern.transform(gr2, return_embedding_only=True), axis=1)\n    )\n\n    # Note that the vector length of the WL procedure is indeterminate, and thus dim(Y) != dim(X) in general.\n    # However, since the newly observed features in the test data is always concatenated at the end of the feature\n    # matrix, these features will not matter for the inference, and as such we can safely truncate the feature\n    # matrix for the test data so that only those appearing in both the training and testing datasets are included.\n\n    x_.requires_grad_()\n    y_ = y_[:, : x_.shape[1]].requires_grad_()\n    K = calculate_kernel_matrix_as_tensor(x_, y_, oa=self.oa, se_kernel=self.se)\n    return K, y_, x_\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/weisfilerlehman/#neps.optimizers.bayesian_optimization.kernels.weisfilerlehman.WeisfilerLehman.transform","title":"transform","text":"<pre><code>transform(gr: list)\n</code></pre> <p>transpose: by default, the grakel produces output in shape of len(y) * len(x2). Use transpose to reshape that to a more conventional shape..</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/weisfilerlehman.py</code> <pre><code>def transform(\n    self,\n    gr: list,\n):\n    \"\"\"transpose: by default, the grakel produces output in shape of len(y) * len(x2). Use transpose to\n    reshape that to a more conventional shape..\"\"\"\n    if self.undirected:\n        gr = transform_to_undirected(gr)\n    if self.base_type == \"edge\":\n        if not all([g.graph_type == \"edge_attr\" for g in gr]):\n            raise ValueError(\n                \"One or more graphs passed are not edge-attributed graphs. You need all graphs to be\"\n                \"in edge format to use 'edge' type Weisfiler-Lehman kernel.\"\n            )\n        gr_ = graph_from_networkx(gr, self.node_label, self.edge_label)\n    else:\n        gr_ = graph_from_networkx(\n            gr,\n            self.node_label,\n        )\n\n    K = self.kern.transform(gr_)\n    if self.return_tensor and not isinstance(K, torch.Tensor):\n        K = torch.tensor(K)\n    return K\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/","title":"Edge histogram","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram","title":"neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram","text":"<p>The Edge Histogram kernel as defined in :cite:<code>sugiyama2015halting</code>.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram","title":"EdgeHistogram","text":"<pre><code>EdgeHistogram(\n    n_jobs=None,\n    normalize=False,\n    sparse=\"auto\",\n    oa=False,\n    mahalanobis_precision=None,\n    se_kernel: Stationary = None,\n    requires_ordered_features: bool = False,\n    as_tensor: bool = True,\n)\n</code></pre> <p>               Bases: <code>VertexHistogram</code></p> <p>Edge Histogram kernel as found in :cite:<code>sugiyama2015halting</code>.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram--parameters","title":"Parameters","text":"<p>sparse : bool, or 'auto', default='auto'     Defines if the data will be stored in a sparse format.     Sparse format is slower, but less memory consuming and in some cases the only solution.     If 'auto', uses a sparse matrix when the number of zeros is more than the half of the matrix size.     In all cases if the dense matrix doesn't fit system memory, I sparse approach will be tried.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram--attributes","title":"Attributes","text":"<p>None.</p> bool <p>Whether the ordering of the features in the feature matrix matters. If True, the features will be parsed in the same order as the WL node label.</p> <p>Note that if called directly (not from Weisfiler Lehman kernel), turning this option on could break the code, as the label in general is non-int.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def __init__(\n    self,\n    n_jobs=None,\n    normalize=False,\n    sparse=\"auto\",\n    oa=False,\n    mahalanobis_precision=None,\n    se_kernel: Stationary = None,\n    requires_ordered_features: bool = False,\n    as_tensor: bool = True,\n):\n    \"\"\"Initialise a vertex histogram kernel.\n\n    require_ordered_features: bool\n        Whether the ordering of the features in the feature matrix matters.\n        If True, the features will be parsed in the same order as the WL\n        node label.\n\n        Note that if called directly (not from Weisfiler Lehman kernel), turning\n        this option on could break the code, as the label in general is non-int.\n\n    \"\"\"\n    super().__init__(n_jobs=n_jobs, normalize=normalize)\n    self.as_tensor = as_tensor\n    if self.as_tensor:\n        self.sparse = False\n    else:\n        self.sparse = sparse\n    self.oa = oa\n    self.se_kernel = se_kernel\n    self._initialized.update({\"sparse\": True})\n    self.mahalanobis_precision = mahalanobis_precision\n    self.require_ordered_features = requires_ordered_features\n\n    self._X_diag = None\n    self.X_tensor = None\n    self.Y_tensor = None\n\n    self._labels = None\n    self.sparse_ = None\n    self._method_calling = None\n    self._Y = None\n    self._is_transformed = None\n    self.X = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.diagonal","title":"diagonal","text":"<pre><code>diagonal(use_tensor=False)\n</code></pre> <p>Calculate the kernel matrix diagonal of the fitted data.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.diagonal--parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.diagonal--returns","title":"Returns","text":"<p>X_diag : np.array     The diagonal of the kernel matrix, of the fitted. This consists     of each element calculated with itself.</p> bool: <p>The flag to use whether return tensor instead of numpy array. All other operations are the same</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def diagonal(self, use_tensor=False):\n    \"\"\"Calculate the kernel matrix diagonal of the fitted data.\n\n    Parameters\n    ----------\n    None.\n\n    Returns\n    -------\n    X_diag : np.array\n        The diagonal of the kernel matrix, of the fitted. This consists\n        of each element calculated with itself.\n\n    use_tensor: bool:\n        The flag to use whether return tensor instead of numpy array. All other operations are the same\n\n    \"\"\"\n    # Check is fit had been called\n    check_is_fitted(self, [\"X\", \"sparse_\"])\n    try:\n        check_is_fitted(self, [\"_X_diag\"])\n    except NotFittedError:\n        # Calculate diagonal of X\n        if use_tensor:\n            self._X_diag = torch.einsum(\"ij,ij-&gt;i\", [self.X_tensor, self.X_tensor])\n        else:\n            if self.sparse_:\n                self._X_diag = squeeze(array(self.X.multiply(self.X).sum(axis=1)))\n            else:\n                self._X_diag = einsum(\"ij,ij-&gt;i\", self.X, self.X)\n    try:\n        check_is_fitted(self, [\"_Y\"])\n        if use_tensor:\n            Y_diag = torch.einsum(\"ij, ij-&gt;i\", [self.Y_tensor, self.Y_tensor])\n            return self._X_diag, Y_diag\n        else:\n            if self.sparse_:\n                Y_diag = squeeze(array(self._Y.multiply(self._Y).sum(axis=1)))\n            else:\n                Y_diag = einsum(\"ij,ij-&gt;i\", self._Y, self._Y)\n            return self._X_diag, Y_diag\n    except NotFittedError:\n        return self._X_diag\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.fit","title":"fit","text":"<pre><code>fit(X, y=None, **kwargs)\n</code></pre> <p>Fit a dataset, for a transformer.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.fit--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). The train samples.</p> None <p>There is no need of a target in a transformer, yet the pipeline API requires this parameter.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.fit--returns","title":"Returns","text":"<p>self : object Returns self.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def fit(self, X, y=None, **kwargs):\n    \"\"\"Fit a dataset, for a transformer.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). The train samples.\n\n    y : None\n        There is no need of a target in a transformer, yet the pipeline API\n        requires this parameter.\n\n    Returns\n    -------\n    self : object\n    Returns self.\n\n    \"\"\"\n    self._is_transformed = False\n    self._method_calling = 1\n\n    # Parameter initialization\n    self.initialize()\n\n    # Input validation and parsing\n    if X is None:\n        raise ValueError(\"`fit` input cannot be None\")\n    else:\n        self.X = self.parse_input(X, **kwargs)\n\n    # Return the transformer\n    return self\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(X, **kwargs)\n</code></pre> <p>Fit and transform, on the same dataset.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.fit_transform--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). If None the kernel matrix is calculated upon fit data.     The test samples.</p> None <p>There is no need of a target in a transformer, yet the pipeline API requires this parameter.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.fit_transform--returns","title":"Returns","text":"<p>K : numpy array, shape = [n_targets, n_input_graphs]     corresponding to the kernel matrix, a calculation between     all pairs of graphs between target an features</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def fit_transform(self, X, **kwargs):\n    \"\"\"Fit and transform, on the same dataset.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). If None the kernel matrix is calculated upon fit data.\n        The test samples.\n\n    y : None\n        There is no need of a target in a transformer, yet the pipeline API\n        requires this parameter.\n\n    Returns\n    -------\n    K : numpy array, shape = [n_targets, n_input_graphs]\n        corresponding to the kernel matrix, a calculation between\n        all pairs of graphs between target an features\n\n    \"\"\"\n    self._method_calling = 2\n    self.fit(X, **kwargs)\n\n    # Transform - calculate kernel matrix\n    km = self._calculate_kernel_matrix()\n\n    self._X_diag = np.diagonal(km)\n    if self.normalize:\n        km = km / np.sqrt(np.outer(self._X_diag, self._X_diag))\n    if self.as_tensor:\n        km = torch.tensor(km)\n    return km\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize all transformer arguments, needing initialization.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def initialize(self):\n    \"\"\"Initialize all transformer arguments, needing initialization.\"\"\"\n    if not self._initialized[\"n_jobs\"]:\n        if self.n_jobs is not None:\n            warn(\"no implemented parallelization for VertexHistogram\")\n        self._initialized[\"n_jobs\"] = True\n    if not self._initialized[\"sparse\"]:\n        if self.sparse not in [\"auto\", False, True]:\n            TypeError(\"sparse could be False, True or auto\")\n        self._initialized[\"sparse\"] = True\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.parse_input","title":"parse_input","text":"<pre><code>parse_input(X: Iterable, **kwargs)\n</code></pre> <p>Parse and check the given input for EH kernel.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.parse_input--parameters","title":"Parameters","text":"<p>X : iterable     For the input to pass the test, we must have:     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format).</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.parse_input--returns","title":"Returns","text":"<p>out : np.array, shape=(len(X), n_labels)     A np array for frequency (cols) histograms for all Graphs (rows).</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram.py</code> <pre><code>def parse_input(self, X: Iterable, **kwargs):\n    \"\"\"Parse and check the given input for EH kernel.\n\n    Parameters\n    ----------\n    X : iterable\n        For the input to pass the test, we must have:\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format).\n\n    Returns\n    -------\n    out : np.array, shape=(len(X), n_labels)\n        A np array for frequency (cols) histograms for all Graphs (rows).\n\n    \"\"\"\n    if not isinstance(X, Iterable):\n        raise TypeError(\"input must be an iterable\\n\")\n    else:\n        rows, cols, data = list(), list(), list()\n        if self._method_calling in [1, 2]:\n            labels = dict()\n            self._labels = labels\n        elif self._method_calling == 3:\n            labels = dict(self._labels)\n        ni = 0\n        for i, x in enumerate(iter(X)):\n            is_iter = isinstance(x, Iterable)\n            if is_iter:\n                x = list(x)\n            if is_iter and len(x) in [0, 3]:\n                if len(x) == 0:\n                    warn(\"Ignoring empty element on index: \" + str(i))\n                    continue\n                else:\n                    # Our element is an iterable of at least 2 elements\n                    L = x[2]\n            elif isinstance(x, Graph):\n                # get labels in any existing format\n                L = x.get_labels(purpose=\"any\", label_type=\"edge\")\n            else:\n                raise TypeError(\n                    \"each element of X must be either a \"\n                    + \"graph object or a list with at least \"\n                    + \"a graph like object and node labels \"\n                    + \"dict \\n\"\n                )\n\n            if L is None:\n                raise ValueError(\"Invalid graph entry at location \" + str(i) + \"!\")\n            # construct the data input for the numpy array\n            for label, frequency in Counter(L.values()).items():\n                # for the row that corresponds to that graph\n                rows.append(ni)\n\n                # and to the value that this label is indexed\n                col_idx = labels.get(label, None)\n                if col_idx is None:\n                    # if not indexed, add the new index (the next)\n                    col_idx = len(labels)\n                    labels[label] = col_idx\n\n                # designate the certain column information\n                cols.append(col_idx)\n\n                # as well as the frequency value to data\n                data.append(frequency)\n            ni += 1\n\n        # Initialise the feature matrix\n        if self._method_calling in [1, 2]:\n            if self.sparse == \"auto\":\n                self.sparse_ = (\n                    len(cols) / float(ni * len(labels)) &lt;= 0.5\n                )\n            else:\n                self.sparse_ = bool(self.sparse)\n\n        if self.sparse_:\n            features = csr_matrix(\n                (data, (rows, cols)), shape=(ni, len(labels)), copy=False\n            )\n        else:\n            # Initialise the feature matrix\n            try:\n                features = zeros(shape=(ni, len(labels)))\n                features[rows, cols] = data\n            except MemoryError:\n                warn(\"memory-error: switching to sparse\")\n                self.sparse_, features = True, csr_matrix(\n                    (data, (rows, cols)), shape=(ni, len(labels)), copy=False\n                )\n\n        if ni == 0:\n            raise ValueError(\"parsed input is empty\")\n        return features\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.transform","title":"transform","text":"<pre><code>transform(X, return_embedding_only=False, **kwargs)\n</code></pre> <p>Calculate the kernel matrix, between given and fitted dataset.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.transform--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). If None the kernel matrix is calculated upon fit data.     The test samples.</p> bool <p>Whether returns the vector embedding of the kernel only (without actually computing the kernel function). This is used when computing the derivative of the kernel w.r.t. the test points/</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/edge_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.edge_histogram.EdgeHistogram.transform--returns","title":"Returns","text":"<p>K : numpy array, shape = [n_targets, n_input_graphs]     corresponding to the kernel matrix, a calculation between     all pairs of graphs between target an features</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def transform(self, X, return_embedding_only=False, **kwargs):\n    \"\"\"Calculate the kernel matrix, between given and fitted dataset.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). If None the kernel matrix is calculated upon fit data.\n        The test samples.\n\n    return_embedding_only: bool\n        Whether returns the vector embedding of the kernel only (without actually\n        computing the kernel function). This is used when computing the derivative\n        of the kernel w.r.t. the test points/\n\n    Returns\n    -------\n    K : numpy array, shape = [n_targets, n_input_graphs]\n        corresponding to the kernel matrix, a calculation between\n        all pairs of graphs between target an features\n\n    \"\"\"\n    self._method_calling = 3\n    # Check is fit had been called\n    check_is_fitted(self, [\"X\"])\n\n    # Input validation and parsing\n    if X is None:\n        raise ValueError(\"`transform` input cannot be None\")\n    else:\n        Y = self.parse_input(X, **kwargs)\n    if return_embedding_only:\n        return Y\n\n    self._Y = Y\n    self._is_transformed = True\n\n    # Transform - calculate kernel matrix\n    km = self._calculate_kernel_matrix(Y)\n    # Self transform must appear before the diagonal call on normilization\n    if self.normalize:\n        X_diag, Y_diag = self.diagonal()\n        km /= np.sqrt(np.outer(Y_diag, X_diag))\n    if self.as_tensor:\n        km = torch.tensor(km)\n    return km\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/utils/","title":"Utils","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/utils/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.utils","title":"neps.optimizers.bayesian_optimization.kernels.grakel_replace.utils","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/utils/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.utils.calculate_kernel_matrix_as_tensor","title":"calculate_kernel_matrix_as_tensor","text":"<pre><code>calculate_kernel_matrix_as_tensor(\n    X, Y=None, oa=False, se_kernel=None, normalize=True\n) -&gt; Tensor\n</code></pre> <p>Same as calculate kernel matrix, but in pytorch framework and uses autodiff to compute the gradient of the kernel function with respect to the feature vector.</p> <p>This function is taken out of the class to facilitate derivative computation.</p> <p>One difference is that to prevent the un-differentiable point at the min operation if optimal assignment kernel is used, we replace the hard-min with a soft-min differentiable approximation that uses the x-norm approximation.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/utils/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.utils.calculate_kernel_matrix_as_tensor--parameters","title":"Parameters","text":"<p>X, Y: the feature vectors (X: train, Y: test). When Y is not supplied, the kernel matrix is computed with     respect to itself.</p> <p>oa: bool: whether the optimal assignment kernel should be used.</p> Defines any successive embedding kernel to be applied over the inner produce of X and Y. If none, <p>a simple</p> <p>normalize: bool: Whether to normalize the GP covariance matrix to the range of [0, 1]. Default is True.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/utils/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.utils.calculate_kernel_matrix_as_tensor--returns","title":"Returns","text":"<p>K: pytorch tensor, shape = [n_targets, n_inputs] dK_dY: pytorch tensor, of the same shape of K. The derivative of the value of the kernel function with respect to each of the X. If Y is None, the derivative is instead taken at the training point (i.e. X).</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/utils.py</code> <pre><code>def calculate_kernel_matrix_as_tensor(\n    X, Y=None, oa=False, se_kernel=None, normalize=True\n) -&gt; torch.Tensor:\n    \"\"\"\n    Same as calculate kernel matrix, but in pytorch framework and uses autodiff to compute the gradient of\n    the kernel function with respect to the feature vector.\n\n    This function is taken out of the class to facilitate derivative computation.\n\n    One difference is that to prevent the un-differentiable point at the min operation if optimal assignment\n    kernel is used, we replace the hard-min with a soft-min differentiable approximation that uses the x-norm\n    approximation.\n\n    Parameters\n    ----------\n    X, Y: the feature vectors (X: train, Y: test). When Y is not supplied, the kernel matrix is computed with\n        respect to itself.\n\n    oa: bool: whether the optimal assignment kernel should be used.\n\n    se_kernel: Defines any successive embedding kernel to be applied over the inner produce of X and Y. If none,\n        a simple\n\n    normalize: bool: Whether to normalize the GP covariance matrix to the range of [0, 1]. Default is True.\n\n    Returns\n    -------\n    K: pytorch tensor, shape = [n_targets, n_inputs]\n    dK_dY: pytorch tensor, of the same shape of K. The derivative of the value of the kernel function with\n    respect to each of the X. If Y is None, the derivative is instead taken at the *training point* (i.e. X).\n    \"\"\"\n\n    if Y is None:\n        if se_kernel is not None:\n            K = se_kernel.forward(X, X)\n        else:\n            K = X @ X.t()\n        if normalize:\n            K_diag = torch.sqrt(torch.diag(K))\n            K_diag_outer = torch.ger(K_diag, K_diag)\n            return K / K_diag_outer\n    else:\n        assert Y.shape[1] == X.shape[1], (\n            \"got Y shape \" + str(Y.shape[1]) + \" but X shape \" + str(X.shape[1])\n        )\n        if se_kernel is not None:\n            K = se_kernel.forward(X, Y)\n        else:\n            K = Y @ X.t()\n        if normalize:\n            Kxx = calculate_kernel_matrix_as_tensor(\n                X, X, oa=oa, se_kernel=se_kernel, normalize=False\n            )\n            Kyy = calculate_kernel_matrix_as_tensor(\n                Y, Y, oa=oa, se_kernel=se_kernel, normalize=False\n            )\n            K_diag_outer = torch.ger(\n                torch.sqrt(torch.diag(Kyy)), torch.sqrt(torch.diag(Kxx))\n            )\n            return K / K_diag_outer\n    return K\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/","title":"Vertex histogram","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram","title":"neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram","text":"<p>The vertex kernel as defined in :cite:<code>sugiyama2015halting</code>.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram","title":"VertexHistogram","text":"<pre><code>VertexHistogram(\n    n_jobs=None,\n    normalize=False,\n    sparse=\"auto\",\n    oa=False,\n    mahalanobis_precision=None,\n    se_kernel: Stationary = None,\n    requires_ordered_features: bool = False,\n    as_tensor: bool = True,\n)\n</code></pre> <p>               Bases: <code>Kernel</code></p> <p>Vertex Histogram kernel as found in :cite:<code>sugiyama2015halting</code>.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram--parameters","title":"Parameters","text":"<p>sparse : bool, or 'auto', default='auto'     Defines if the data will be stored in a sparse format.     Sparse format is slower, but less memory consuming and in some cases the only solution.     If 'auto', uses a sparse matrix when the number of zeros is more than the half of the matrix size.     In all cases if the dense matrix doesn't fit system memory, I sparse approach will be tried.</p> bool: default=True <p>Defines whether optimal assignment variant of the kernel should be used.</p> default=None <p>The standard vectorial kernel to be used for successive embedding (i.e. after the transformation from graph to the vector embedding, whether to use an additional kernel to compute the vector similarity.</p> dict, default=None <p>Any parameters to be passed to the se_kernel</p> np.array: <p>If supplied, the Malahanobis distance with the precision matrix as supplied will be computed in the dot product, instead of the vanilla dot product.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram--attributes","title":"Attributes","text":"<p>None.</p> bool <p>Whether the ordering of the features in the feature matrix matters. If True, the features will be parsed in the same order as the WL node label.</p> <p>Note that if called directly (not from Weisfiler Lehman kernel), turning this option on could break the code, as the label in general is non-int.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def __init__(\n    self,\n    n_jobs=None,\n    normalize=False,\n    sparse=\"auto\",\n    oa=False,\n    mahalanobis_precision=None,\n    se_kernel: Stationary = None,\n    requires_ordered_features: bool = False,\n    as_tensor: bool = True,\n):\n    \"\"\"Initialise a vertex histogram kernel.\n\n    require_ordered_features: bool\n        Whether the ordering of the features in the feature matrix matters.\n        If True, the features will be parsed in the same order as the WL\n        node label.\n\n        Note that if called directly (not from Weisfiler Lehman kernel), turning\n        this option on could break the code, as the label in general is non-int.\n\n    \"\"\"\n    super().__init__(n_jobs=n_jobs, normalize=normalize)\n    self.as_tensor = as_tensor\n    if self.as_tensor:\n        self.sparse = False\n    else:\n        self.sparse = sparse\n    self.oa = oa\n    self.se_kernel = se_kernel\n    self._initialized.update({\"sparse\": True})\n    self.mahalanobis_precision = mahalanobis_precision\n    self.require_ordered_features = requires_ordered_features\n\n    self._X_diag = None\n    self.X_tensor = None\n    self.Y_tensor = None\n\n    self._labels = None\n    self.sparse_ = None\n    self._method_calling = None\n    self._Y = None\n    self._is_transformed = None\n    self.X = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.diagonal","title":"diagonal","text":"<pre><code>diagonal(use_tensor=False)\n</code></pre> <p>Calculate the kernel matrix diagonal of the fitted data.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.diagonal--parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.diagonal--returns","title":"Returns","text":"<p>X_diag : np.array     The diagonal of the kernel matrix, of the fitted. This consists     of each element calculated with itself.</p> bool: <p>The flag to use whether return tensor instead of numpy array. All other operations are the same</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def diagonal(self, use_tensor=False):\n    \"\"\"Calculate the kernel matrix diagonal of the fitted data.\n\n    Parameters\n    ----------\n    None.\n\n    Returns\n    -------\n    X_diag : np.array\n        The diagonal of the kernel matrix, of the fitted. This consists\n        of each element calculated with itself.\n\n    use_tensor: bool:\n        The flag to use whether return tensor instead of numpy array. All other operations are the same\n\n    \"\"\"\n    # Check is fit had been called\n    check_is_fitted(self, [\"X\", \"sparse_\"])\n    try:\n        check_is_fitted(self, [\"_X_diag\"])\n    except NotFittedError:\n        # Calculate diagonal of X\n        if use_tensor:\n            self._X_diag = torch.einsum(\"ij,ij-&gt;i\", [self.X_tensor, self.X_tensor])\n        else:\n            if self.sparse_:\n                self._X_diag = squeeze(array(self.X.multiply(self.X).sum(axis=1)))\n            else:\n                self._X_diag = einsum(\"ij,ij-&gt;i\", self.X, self.X)\n    try:\n        check_is_fitted(self, [\"_Y\"])\n        if use_tensor:\n            Y_diag = torch.einsum(\"ij, ij-&gt;i\", [self.Y_tensor, self.Y_tensor])\n            return self._X_diag, Y_diag\n        else:\n            if self.sparse_:\n                Y_diag = squeeze(array(self._Y.multiply(self._Y).sum(axis=1)))\n            else:\n                Y_diag = einsum(\"ij,ij-&gt;i\", self._Y, self._Y)\n            return self._X_diag, Y_diag\n    except NotFittedError:\n        return self._X_diag\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.fit","title":"fit","text":"<pre><code>fit(X, y=None, **kwargs)\n</code></pre> <p>Fit a dataset, for a transformer.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.fit--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). The train samples.</p> None <p>There is no need of a target in a transformer, yet the pipeline API requires this parameter.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.fit--returns","title":"Returns","text":"<p>self : object Returns self.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def fit(self, X, y=None, **kwargs):\n    \"\"\"Fit a dataset, for a transformer.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). The train samples.\n\n    y : None\n        There is no need of a target in a transformer, yet the pipeline API\n        requires this parameter.\n\n    Returns\n    -------\n    self : object\n    Returns self.\n\n    \"\"\"\n    self._is_transformed = False\n    self._method_calling = 1\n\n    # Parameter initialization\n    self.initialize()\n\n    # Input validation and parsing\n    if X is None:\n        raise ValueError(\"`fit` input cannot be None\")\n    else:\n        self.X = self.parse_input(X, **kwargs)\n\n    # Return the transformer\n    return self\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(X, **kwargs)\n</code></pre> <p>Fit and transform, on the same dataset.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.fit_transform--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). If None the kernel matrix is calculated upon fit data.     The test samples.</p> None <p>There is no need of a target in a transformer, yet the pipeline API requires this parameter.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.fit_transform--returns","title":"Returns","text":"<p>K : numpy array, shape = [n_targets, n_input_graphs]     corresponding to the kernel matrix, a calculation between     all pairs of graphs between target an features</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def fit_transform(self, X, **kwargs):\n    \"\"\"Fit and transform, on the same dataset.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). If None the kernel matrix is calculated upon fit data.\n        The test samples.\n\n    y : None\n        There is no need of a target in a transformer, yet the pipeline API\n        requires this parameter.\n\n    Returns\n    -------\n    K : numpy array, shape = [n_targets, n_input_graphs]\n        corresponding to the kernel matrix, a calculation between\n        all pairs of graphs between target an features\n\n    \"\"\"\n    self._method_calling = 2\n    self.fit(X, **kwargs)\n\n    # Transform - calculate kernel matrix\n    km = self._calculate_kernel_matrix()\n\n    self._X_diag = np.diagonal(km)\n    if self.normalize:\n        km = km / np.sqrt(np.outer(self._X_diag, self._X_diag))\n    if self.as_tensor:\n        km = torch.tensor(km)\n    return km\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize all transformer arguments, needing initialization.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def initialize(self):\n    \"\"\"Initialize all transformer arguments, needing initialization.\"\"\"\n    if not self._initialized[\"n_jobs\"]:\n        if self.n_jobs is not None:\n            warn(\"no implemented parallelization for VertexHistogram\")\n        self._initialized[\"n_jobs\"] = True\n    if not self._initialized[\"sparse\"]:\n        if self.sparse not in [\"auto\", False, True]:\n            TypeError(\"sparse could be False, True or auto\")\n        self._initialized[\"sparse\"] = True\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.parse_input","title":"parse_input","text":"<pre><code>parse_input(X, label_start_idx=0, label_end_idx=None)\n</code></pre> <p>Parse and check the given input for VH kernel.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.parse_input--parameters","title":"Parameters","text":"<p>X : iterable     For the input to pass the test, we must have:     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format).</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.parse_input--returns","title":"Returns","text":"<p>out : np.array, shape=(len(X), n_labels)     A np.array for frequency (cols) histograms for all Graphs (rows).</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def parse_input(self, X, label_start_idx=0, label_end_idx=None):\n    \"\"\"Parse and check the given input for VH kernel.\n\n    Parameters\n    ----------\n    X : iterable\n        For the input to pass the test, we must have:\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format).\n\n\n\n    Returns\n    -------\n    out : np.array, shape=(len(X), n_labels)\n        A np.array for frequency (cols) histograms for all Graphs (rows).\n\n    \"\"\"\n    if self.require_ordered_features:\n        if label_start_idx is None or label_end_idx is None:\n            raise ValueError(\n                \"When requires_ordered_features flag is True, you must supply the start and end\"\n                \"indices of the feature matrix to have consistent feature dimensions!\"\n            )\n        assert (\n            label_end_idx &gt; label_start_idx\n        ), \"End index must be larger than the start index!\"\n\n    if not isinstance(X, Iterable):\n        raise TypeError(\"input must be an iterable\\n\")\n    else:\n        rows, cols, data = list(), list(), list()\n        if self._method_calling in [0, 1, 2]:\n            labels = dict()\n            self._labels = labels\n        elif self._method_calling == 3:\n            labels = dict(self._labels)\n        ni = 0\n        for i, x in enumerate(iter(X)):\n            is_iter = isinstance(x, Iterable)\n            if is_iter:\n                x = list(x)\n            if is_iter and len(x) in [0, 2, 3]:\n                if len(x) == 0:\n                    warn(\"Ignoring empty element on index: \" + str(i))\n                    continue\n                else:\n                    # Our element is an iterable of at least 2 elements\n                    L = x[1]\n            elif isinstance(x, Graph):\n                # get labels in any existing format\n                L = x.get_labels(purpose=\"any\")\n            else:\n                raise TypeError(\n                    \"each element of X must be either a \"\n                    \"graph object or a list with at least \"\n                    \"a graph like object and node labels \"\n                    \"dict \\n\"\n                )\n\n            # construct the data input for the numpy array\n            for label, frequency in Counter(L.values()).items():\n                # for the row that corresponds to that graph\n                rows.append(ni)\n\n                # and to the value that this label is indexed\n                if self.require_ordered_features:\n                    try:\n                        col_idx = int(label) - label_start_idx  # Offset\n                    except ValueError:\n                        logging.error(\n                            \"Failed to convert label to a valid integer. Check whether all labels are\"\n                            \"numeric, and whether you called this kernel directly instead of from the\"\n                            \"Weisfiler-Lehman kernel. Falling back to the default unordered feature\"\n                            \"matrix.\"\n                        )\n                        self.require_ordered_features = False\n                if not self.require_ordered_features:\n                    col_idx = labels.get(label, None)\n                    if col_idx is None:\n                        # if not indexed, add the new index (the next)\n                        col_idx = len(labels)\n                        labels[label] = col_idx\n\n                # designate the certain column information\n                cols.append(col_idx)\n\n                # as well as the frequency value to data\n                data.append(frequency)\n            ni += 1\n\n        if self.require_ordered_features:\n            label_length = max(label_end_idx - label_start_idx, max(cols)) + 1\n        else:\n            label_length = len(labels)\n\n        if self._method_calling in [0, 1, 2]:\n            if self.sparse == \"auto\":\n                self.sparse_ = len(cols) / float(ni * label_length) &lt;= 0.5\n            else:\n                self.sparse_ = bool(self.sparse)\n\n        if self.sparse_:\n            features = csr_matrix(\n                (data, (rows, cols)), shape=(ni, label_length), copy=False\n            )\n        else:\n            # Initialise the feature matrix\n            try:\n                features = zeros(shape=(ni, label_length))\n                features[rows, cols] = data\n\n            except MemoryError:\n                warn(\"memory-error: switching to sparse\")\n                self.sparse_, features = True, csr_matrix(\n                    (data, (rows, cols)), shape=(ni, label_length), copy=False\n                )\n\n        if ni == 0:\n            raise ValueError(\"parsed input is empty\")\n        return features\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.transform","title":"transform","text":"<pre><code>transform(X, return_embedding_only=False, **kwargs)\n</code></pre> <p>Calculate the kernel matrix, between given and fitted dataset.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.transform--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). If None the kernel matrix is calculated upon fit data.     The test samples.</p> bool <p>Whether returns the vector embedding of the kernel only (without actually computing the kernel function). This is used when computing the derivative of the kernel w.r.t. the test points/</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.vertex_histogram.VertexHistogram.transform--returns","title":"Returns","text":"<p>K : numpy array, shape = [n_targets, n_input_graphs]     corresponding to the kernel matrix, a calculation between     all pairs of graphs between target an features</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/vertex_histogram.py</code> <pre><code>def transform(self, X, return_embedding_only=False, **kwargs):\n    \"\"\"Calculate the kernel matrix, between given and fitted dataset.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). If None the kernel matrix is calculated upon fit data.\n        The test samples.\n\n    return_embedding_only: bool\n        Whether returns the vector embedding of the kernel only (without actually\n        computing the kernel function). This is used when computing the derivative\n        of the kernel w.r.t. the test points/\n\n    Returns\n    -------\n    K : numpy array, shape = [n_targets, n_input_graphs]\n        corresponding to the kernel matrix, a calculation between\n        all pairs of graphs between target an features\n\n    \"\"\"\n    self._method_calling = 3\n    # Check is fit had been called\n    check_is_fitted(self, [\"X\"])\n\n    # Input validation and parsing\n    if X is None:\n        raise ValueError(\"`transform` input cannot be None\")\n    else:\n        Y = self.parse_input(X, **kwargs)\n    if return_embedding_only:\n        return Y\n\n    self._Y = Y\n    self._is_transformed = True\n\n    # Transform - calculate kernel matrix\n    km = self._calculate_kernel_matrix(Y)\n    # Self transform must appear before the diagonal call on normilization\n    if self.normalize:\n        X_diag, Y_diag = self.diagonal()\n        km /= np.sqrt(np.outer(Y_diag, X_diag))\n    if self.as_tensor:\n        km = torch.tensor(km)\n    return km\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/","title":"Weisfeiler lehman","text":""},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman","title":"neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman","text":"<p>The weisfeiler lehman kernel :cite:<code>shervashidze2011weisfeiler</code>.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman","title":"WeisfeilerLehman","text":"<pre><code>WeisfeilerLehman(\n    n_jobs=None,\n    normalize: bool = False,\n    h: int = 5,\n    base_graph_kernel=VertexHistogram,\n    node_weights=None,\n    layer_weights=None,\n    as_tensor: bool = True,\n)\n</code></pre> <p>               Bases: <code>Kernel</code></p> <p>Compute the Weisfeiler Lehman Kernel.</p> <p>See :cite:<code>shervashidze2011weisfeiler</code>.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman--parameters","title":"Parameters","text":"<p>h : int, default=5     The number of iterations.</p> <code>grakel.kernel_operators.Kernel</code> or tuple, default=None <p>If tuple it must consist of a valid kernel object and a dictionary of parameters. General parameters concerning normalization, concurrency, .. will be ignored, and the ones of given on <code>__init__</code> will be passed in case it is needed. Default <code>base_graph_kernel</code> is <code>VertexHistogram</code>.</p> iterable <p>If not None, the nodes will be assigned different weights according to this vector. Must be a dictionary with the following format: {'node_name1': weight1, 'node_name2': weight2 ... } Must be of the same length as the number of different node attributes</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman--attributes","title":"Attributes","text":"<p>X : dict  Holds a dictionary of fitted subkernel modules for all levels.</p> number <p>Holds the number of inputs.</p> int <p>Holds the number, of iterations.</p> function <p>A void function that initializes a base kernel object.</p> dict <p>An inverse dictionary, used for relabeling on each iteration.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def __init__(\n    self,\n    n_jobs=None,\n    normalize: bool = False,\n    h: int = 5,\n    base_graph_kernel=VertexHistogram,\n    node_weights=None,\n    layer_weights=None,\n    as_tensor: bool = True,\n):\n    \"\"\"Initialise a `weisfeiler_lehman` kernel.\"\"\"\n    super().__init__(n_jobs=n_jobs, normalize=normalize)\n\n    self.h = h\n    self.base_graph_kernel = base_graph_kernel\n    self._initialized.update(\n        {\"h\": False, \"base_graph_kernel\": False, \"layer_weights\": False}\n    )\n    self._base_graph_kernel = None\n    self.weights = None\n    self.node_weights = node_weights\n    self.as_tensor = as_tensor\n    self.layer_weights = layer_weights  # The weights of each layer. If None, each WL iteration has same weight\n    self.feature_dims = [\n        0,\n    ]  # Record the dimensions of the vectors of each WL iteration\n    self._params = None\n    self._h = None\n    self._nx = None\n    self._inv_labels = None\n    self._inv_label_node_attr = None\n    self._label_node_attr = None\n    self._feature_weight = None\n    self._method_calling = None\n    self._is_transformed = None\n    self.X = None\n    self._X_diag = None\n\n    self.X_fit = dict()\n    self.K_precomputed = dict()\n    self.base_graph_kernel_precomputed = dict()\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.dK_dX","title":"dK_dX","text":"<pre><code>dK_dX(X_test: None)\n</code></pre> <p>Do additional forward and backward pass, compute the kernel derivative wrt the testing location. If no test locations are provided, the derivatives are evaluated at the training points Returns</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def dK_dX(self, X_test: None):\n    \"\"\"\n    Do additional forward and backward pass, compute the kernel derivative wrt the testing location.\n    If no test locations are provided, the derivatives are evaluated at the training points\n    Returns\n    -------\n\n    \"\"\"\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.diagonal","title":"diagonal","text":"<pre><code>diagonal()\n</code></pre> <p>Calculate the kernel matrix diagonal for fitted data.</p> <p>A funtion called on transform on a seperate dataset to apply normalization on the exterior.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.diagonal--parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.diagonal--returns","title":"Returns","text":"<p>X_diag : np.array     The diagonal of the kernel matrix, of the fitted data.     This consists of kernel calculation for each element with itself.</p> np.array <p>The diagonal of the kernel matrix, of the transformed data. This consists of kernel calculation for each element with itself.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def diagonal(self):\n    \"\"\"Calculate the kernel matrix diagonal for fitted data.\n\n    A funtion called on transform on a seperate dataset to apply\n    normalization on the exterior.\n\n    Parameters\n    ----------\n    None.\n\n    Returns\n    -------\n    X_diag : np.array\n        The diagonal of the kernel matrix, of the fitted data.\n        This consists of kernel calculation for each element with itself.\n\n    Y_diag : np.array\n        The diagonal of the kernel matrix, of the transformed data.\n        This consists of kernel calculation for each element with itself.\n\n    \"\"\"\n    # Check if fit had been called\n    check_is_fitted(self, [\"X\"])\n    try:\n        check_is_fitted(self, [\"_X_diag\"])\n        if self._is_transformed:\n            Y_diag = self.X[0].diagonal()[1]\n            for i in range(1, self._h):\n                Y_diag += self.X[i].diagonal()[1]\n    except NotFittedError:\n        # Calculate diagonal of X\n        if self._is_transformed:\n            X_diag, Y_diag = self.X[0].diagonal()\n            # X_diag is considered a mutable and should not affect the kernel matrix itself.\n            X_diag.flags.writeable = True\n            for i in range(1, self._h):\n                x, y = self.X[i].diagonal()\n                X_diag += x\n                Y_diag += y\n                self._X_diag = X_diag\n\n            # case sub kernel is only fitted\n            X_diag = self.X[0].diagonal()\n            # X_diag is considered a mutable and should not affect the kernel matrix itself.\n            X_diag.flags.writeable = True\n            for i in range(1, self._n_iter):\n                x = self.X[i].diagonal()\n                X_diag += x\n            self._X_diag = X_diag\n\n    if self.as_tensor:\n        self._X_diag = torch.tensor(self._X_diag)\n        if Y_diag is not None:\n            Y_diag = torch.tensor(Y_diag)\n    if self._is_transformed:\n        return self._X_diag, Y_diag\n    else:\n        return self._X_diag\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(X: Iterable, y=None, gp_fit: bool = True)\n</code></pre> <p>Fit and transform, on the same dataset.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.fit_transform--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). If None the kernel matrix is calculated upon fit data.     The test samples.</p> Object, default=None <p>Ignored argument, added for the pipeline.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.fit_transform--returns","title":"Returns","text":"<p>K : numpy array, shape = [n_targets, n_input_graphs]     corresponding to the kernel matrix, a calculation between     all pairs of graphs between target an features</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def fit_transform(\n    self, X: Iterable, y=None, gp_fit: bool = True\n):\n    \"\"\"Fit and transform, on the same dataset.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). If None the kernel matrix is calculated upon fit data.\n        The test samples.\n\n    y : Object, default=None\n        Ignored argument, added for the pipeline.\n\n    Returns\n    -------\n    K : numpy array, shape = [n_targets, n_input_graphs]\n        corresponding to the kernel matrix, a calculation between\n        all pairs of graphs between target an features\n\n    \"\"\"\n    self._method_calling = 2\n    self._is_transformed = False\n    self.initialize()\n    self.feature_dims = [\n        0,\n    ]  # Flush the feature dimensions\n    if X is None:\n        raise ValueError(\"transform input cannot be None\")\n    else:\n        km, self.X = self.parse_input(X, gp_fit=gp_fit)\n\n    return km\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize all transformer arguments, needing initialization.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def initialize(self):\n    \"\"\"Initialize all transformer arguments, needing initialization.\"\"\"\n    super().initialize()\n    if not self._initialized[\"base_graph_kernel\"]:\n        base_graph_kernel = self.base_graph_kernel\n        if base_graph_kernel is None:\n            base_graph_kernel, params = VertexHistogram, dict()\n        # TODO: make sure we're always passing like this\n        elif type(base_graph_kernel) is type and issubclass(\n            base_graph_kernel, Kernel\n        ):\n            params = dict()\n        else:\n            try:\n                base_graph_kernel, params = base_graph_kernel\n            except Exception as _error:\n                raise TypeError(\n                    \"Base kernel was not formulated in \"\n                    \"the correct way. \"\n                    \"Check documentation.\"\n                ) from _error\n\n            if not (\n                type(base_graph_kernel) is type\n                and issubclass(base_graph_kernel, Kernel)\n            ):\n                raise TypeError(\n                    \"The first argument must be a valid \"\n                    \"grakel.kernel.kernel Object\"\n                )\n            if not isinstance(params, dict):\n                raise ValueError(\n                    \"If the second argument of base \"\n                    \"kernel exists, it must be a diction\"\n                    \"ary between parameters names and \"\n                    \"values\"\n                )\n            params.pop(\"normalize\", None)\n\n        params[\"normalize\"] = False\n        params[\"n_jobs\"] = None\n        self._base_graph_kernel = base_graph_kernel\n        self._params = params\n        self._initialized[\"base_graph_kernel\"] = True\n\n    if not self._initialized[\"h\"]:\n        if not isinstance(self.h, int) or self.h &lt; 0:\n            raise TypeError(\n                \"'h' must be a non-negative integer. Got h:\" + str(self.h)\n            )\n        self._h = self.h + 1\n        self._initialized[\"h\"] = True\n\n        if self.layer_weights is None or self.layer_weights.shape[0] != self._h:\n            self.layer_weights = np.ones((self._h,))\n        if self.as_tensor and not isinstance(self.layer_weights, torch.Tensor):\n            self.layer_weights = torch.tensor(self.layer_weights)\n\n        self._initialized[\"h\"] = True\n        self._initialized[\"layer_weights\"] = True\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.parse_input","title":"parse_input","text":"<pre><code>parse_input(\n    X: Iterable,\n    return_embedding_only: bool = False,\n    gp_fit: bool = True,\n)\n</code></pre> <p>Parse input for weisfeiler lehman.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.parse_input--parameters","title":"Parameters","text":"<p>X : iterable     For the input to pass the test, we must have:     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that correspond to the given     graph format). A valid input also consists of graph type objects.</p> bool <p>Whether to return the embedding of the graphs only, instead of computing the kernel all the way to the end.</p> bool <p>If False use precomputed vals for first N values, else compute them and save them</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.parse_input--returns","title":"Returns","text":"<p>base_graph_kernel : object Returns base_graph_kernel.</p> <p>if requires_grad is enabled and we call fit_transform or transform, an additional torch tensor K_grad is returned as well.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def parse_input(\n    self, X: Iterable, return_embedding_only: bool = False, gp_fit: bool = True\n):\n    \"\"\"Parse input for weisfeiler lehman.\n\n    Parameters\n    ----------\n    X : iterable\n        For the input to pass the test, we must have:\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that correspond to the given\n        graph format). A valid input also consists of graph type objects.\n\n    return_embedding_only: bool\n        Whether to return the embedding of the graphs only, instead of computing the kernel all\n        the way to the end.\n\n    gp_fit: bool\n        If False use precomputed vals for first N values, else compute them and save them\n\n    Returns\n    -------\n    base_graph_kernel : object\n    Returns base_graph_kernel.\n\n    if requires_grad is enabled and we call fit_transform or transform, an additional torch tensor\n    K_grad is returned as well.\n\n    \"\"\"\n    if self._method_calling not in [1, 2]:\n        raise ValueError(\n            \"method call must be called either from fit \" + \"or fit-transform\"\n        )\n    elif hasattr(self, \"_X_diag\"):\n        # Clean _X_diag value\n        delattr(self, \"_X_diag\")\n\n    # skip kernel computation if we have already computed the corresponding kernel\n    if self._h in self.K_precomputed.keys() and self.X_fit[self._h] == X:\n        K = self.K_precomputed[self._h]\n        base_graph_kernel = self.base_graph_kernel_precomputed[self._h]\n    else:\n        # Input validation and parsing\n        if not isinstance(X, collections.abc.Iterable):\n            raise TypeError(\"input must be an iterable\\n\")\n        else:\n            nx = 0\n            Gs_ed, L, distinct_values, extras = dict(), dict(), set(), dict()\n            for idx, x in enumerate(iter(X)):\n                is_iter = isinstance(x, collections.abc.Iterable)\n                if is_iter:\n                    x = list(x)\n                if is_iter and (len(x) == 0 or len(x) &gt;= 2):\n                    if len(x) == 0:\n                        warnings.warn(\"Ignoring empty element on index: \" + str(idx))\n                        continue\n                    else:\n                        if len(x) &gt; 2:\n                            extra = tuple()\n                            if len(x) &gt; 3:\n                                extra = tuple(x[3:])\n                            x = Graph(\n                                x[0], x[1], x[2], graph_format=self._graph_format\n                            )\n                            extra = (\n                                x.get_labels(\n                                    purpose=self._graph_format,\n                                    label_type=\"edge\",\n                                    return_none=True,\n                                ),\n                            ) + extra\n                        else:\n                            x = Graph(x[0], x[1], {}, graph_format=self._graph_format)\n                            extra = tuple()\n\n                elif isinstance(x, Graph):\n                    x.desired_format(self._graph_format)\n                    el = x.get_labels(\n                        purpose=self._graph_format,\n                        label_type=\"edge\",\n                        return_none=True,\n                    )\n                    if el is None:\n                        extra = tuple()\n                    else:\n                        extra = (el,)\n\n                else:\n                    raise TypeError(\n                        \"each element of X must be either a \"\n                        + \"graph object or a list with at least \"\n                        + \"a graph like object and node labels \"\n                        + \"dict \\n\"\n                    )\n                Gs_ed[nx] = x.get_edge_dictionary()\n                L[nx] = x.get_labels(purpose=\"dictionary\")\n                extras[nx] = extra\n                distinct_values |= set(L[nx].values())\n                nx += 1\n            if nx == 0:\n                raise ValueError(\"parsed input is empty\")\n\n        # Save the number of \"fitted\" graphs.\n        self._nx = nx\n        WL_labels_inverse = OrderedDict()\n\n        # assign a number to each label\n        label_count = 0\n        for dv in sorted(list(distinct_values)):\n            WL_labels_inverse[dv] = label_count\n            label_count += 1\n\n        # Initalize an inverse dictionary of labels for all iterations\n        self._inv_labels = (\n            OrderedDict()\n        )  # Inverse dictionary of labels, in term of the *previous layer*\n        self._inv_labels[0] = deepcopy(WL_labels_inverse)\n        self.feature_dims.append(\n            len(WL_labels_inverse)\n        )  # Update the zeroth iteration feature dim\n\n        self._inv_label_node_attr = (\n            OrderedDict()\n        )  # Inverse dictionary of labels, in term of the *node attribute*\n        self._label_node_attr = (\n            OrderedDict()\n        )  # Same as above, but with key and value inverted\n        self._label_node_attr[0], self._inv_label_node_attr[0] = self.translate_label(\n            WL_labels_inverse, 0\n        )\n\n        if self.node_weights is not None:\n            self._feature_weight = OrderedDict()\n            # Ensure the order is the same\n            self._feature_weight[0] = self._compute_feature_weight(\n                self.node_weights, 0, WL_labels_inverse\n            )[1]\n        else:\n            self._feature_weight = None\n\n        def generate_graphs(label_count: int, WL_labels_inverse):\n            new_graphs = list()\n            for j in range(self._nx):\n                new_labels = dict()\n                for k in L[j].keys():\n                    new_labels[k] = WL_labels_inverse[L[j][k]]\n                L[j] = new_labels\n                # add new labels\n                new_graphs.append((Gs_ed[j], new_labels) + extras[j])\n            yield new_graphs\n\n            for i in range(1, self._h):\n                label_set, WL_labels_inverse, L_temp = set(), dict(), dict()\n                for j in range(nx):\n                    # Find unique labels and sort\n                    # them for both graphs\n                    # Keep for each node the temporary\n                    L_temp[j] = dict()\n                    for v in Gs_ed[j].keys():\n                        credential = (\n                            str(L[j][v])\n                            + \",\"\n                            + str(sorted(L[j][n] for n in Gs_ed[j][v].keys()))\n                        )\n                        L_temp[j][v] = credential\n                        label_set.add(credential)\n\n                label_list = sorted(list(label_set))\n                for dv in label_list:\n                    WL_labels_inverse[dv] = label_count\n                    label_count += 1\n\n                # Recalculate labels\n                new_graphs = list()\n                for j in range(nx):\n                    new_labels = dict()\n                    for k in L_temp[j].keys():\n                        new_labels[k] = WL_labels_inverse[L_temp[j][k]]\n                    L[j] = new_labels\n                    # relabel\n                    new_graphs.append((Gs_ed[j], new_labels) + extras[j])\n                self._inv_labels[i] = WL_labels_inverse\n                # Compute the translated inverse node label\n                (\n                    self._label_node_attr[i],\n                    self._inv_label_node_attr[i],\n                ) = self.translate_label(\n                    WL_labels_inverse, i, self._label_node_attr[i - 1]\n                )\n                self.feature_dims.append(\n                    self.feature_dims[-1] + len(self._label_node_attr[i])\n                )\n                # Compute the feature weight of the current layer\n                if self.node_weights is not None:\n                    self._feature_weight[i] = self._compute_feature_weight(\n                        self.node_weights, i, self._inv_label_node_attr[i]\n                    )[1]\n                # assert len(self._feature_weight[i] == len(WL_labels_inverse))\n                yield new_graphs\n\n        # Initialise the base graph kernel.\n        base_graph_kernel = {}\n\n        K = []\n        for i, g in enumerate(generate_graphs(label_count, WL_labels_inverse)):\n            param = self._params\n            if self._feature_weight is not None:\n                param.update({\"mahalanobis_precision\": self._feature_weight[i]})\n            base_graph_kernel.update({i: self._base_graph_kernel(**param)})\n            if return_embedding_only:\n                K.append(\n                    base_graph_kernel[i].parse_input(\n                        g,\n                        label_start_idx=self.feature_dims[i],\n                        label_end_idx=self.feature_dims[i + 1],\n                    )\n                )\n            else:\n                if self._method_calling == 1:\n                    base_graph_kernel[i].fit(\n                        g,\n                        label_start_idx=self.feature_dims[i],\n                        label_end_idx=self.feature_dims[i + 1],\n                    )\n                else:\n                    K.append(\n                        self.layer_weights[i]\n                        * base_graph_kernel[i].fit_transform(\n                            g,\n                            label_start_idx=self.feature_dims[i],\n                            label_end_idx=self.feature_dims[i + 1],\n                        )\n                    )\n\n        if gp_fit:\n            self.X_fit[self._h] = X\n            self.K_precomputed[self._h] = K\n            self.base_graph_kernel_precomputed[self._h] = base_graph_kernel\n\n    if return_embedding_only:\n        return K\n    elif self._method_calling == 1:\n        return base_graph_kernel\n    elif self._method_calling == 2:\n        if self.as_tensor:\n            K = torch.stack(K, dim=0).sum(dim=0)\n            return K, base_graph_kernel\n        return np.sum(K, axis=0), base_graph_kernel\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.transform","title":"transform","text":"<pre><code>transform(X: Iterable, return_embedding_only: bool = True)\n</code></pre> <p>Calculate the kernel matrix, between given and fitted dataset.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.transform--parameters","title":"Parameters","text":"<p>X : iterable     Each element must be an iterable with at most three features and at     least one. The first that is obligatory is a valid graph structure     (adjacency matrix or edge_dictionary) while the second is     node_labels and the third edge_labels (that fitting the given graph     format). If None the kernel matrix is calculated upon fit data.     The test samples.</p> bool <p>Whether to return the embedding of the graphs only, instead of computing the kernel all the way to the end.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.transform--returns","title":"Returns","text":"<p>K : numpy array, shape = [n_targets, n_input_graphs]     corresponding to the kernel matrix, a calculation between     all pairs of graphs between target an features</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def transform(self, X: Iterable, return_embedding_only: bool = True):\n    \"\"\"Calculate the kernel matrix, between given and fitted dataset.\n\n    Parameters\n    ----------\n    X : iterable\n        Each element must be an iterable with at most three features and at\n        least one. The first that is obligatory is a valid graph structure\n        (adjacency matrix or edge_dictionary) while the second is\n        node_labels and the third edge_labels (that fitting the given graph\n        format). If None the kernel matrix is calculated upon fit data.\n        The test samples.\n\n    return_embedding_only: bool\n        Whether to return the embedding of the graphs only, instead of computing the kernel all\n        the way to the end.\n    Returns\n    -------\n    K : numpy array, shape = [n_targets, n_input_graphs]\n        corresponding to the kernel matrix, a calculation between\n        all pairs of graphs between target an features\n\n    \"\"\"\n    self._method_calling = 3\n    # Check is fit had been called\n    check_is_fitted(self, [\"X\", \"_nx\", \"_inv_labels\"])\n\n    # Input validation and parsing\n    if X is None:\n        raise ValueError(\"transform input cannot be None\")\n    else:\n        if not isinstance(X, collections.abc.Iterable):\n            raise ValueError(\"input must be an iterable\\n\")\n        else:\n            nx = 0\n            distinct_values = set()\n            Gs_ed, L = dict(), dict()\n            for i, x in enumerate(iter(X)):\n                is_iter = isinstance(x, collections.abc.Iterable)\n                if is_iter:\n                    x = list(x)\n                if is_iter and len(x) in [0, 2, 3]:\n                    if len(x) == 0:\n                        warnings.warn(\"Ignoring empty element on index: \" + str(i))\n                        continue\n\n                    elif len(x) in [2, 3]:\n                        x = Graph(x[0], x[1], {}, self._graph_format)\n                elif isinstance(x, Graph):\n                    x.desired_format(\"dictionary\")\n                else:\n                    raise ValueError(\n                        \"each element of X must have at \"\n                        + \"least one and at most 3 elements\\n\"\n                    )\n                Gs_ed[nx] = x.get_edge_dictionary()\n                L[nx] = x.get_labels(purpose=\"dictionary\")\n\n                # Hold all the distinct values\n                distinct_values |= {\n                    v for v in L[nx].values() if v not in self._inv_labels[0]\n                }\n                nx += 1\n            if nx == 0:\n                raise ValueError(\"parsed input is empty\")\n\n    nl = len(self._inv_labels[0])\n    WL_labels_inverse = {\n        dv: idx for (idx, dv) in enumerate(sorted(list(distinct_values)), nl)\n    }\n    WL_labels_inverse = OrderedDict(WL_labels_inverse)\n\n    def generate_graphs_transform(WL_labels_inverse, nl):\n        # calculate the kernel matrix for the 0 iteration\n        new_graphs = list()\n        for j in range(nx):\n            new_labels = dict()\n            for k, v in L[j].items():\n                if v in self._inv_labels[0]:\n                    new_labels[k] = self._inv_labels[0][v]\n                else:\n                    new_labels[k] = WL_labels_inverse[v]\n            L[j] = new_labels\n            # produce the new graphs\n            new_graphs.append([Gs_ed[j], new_labels])\n        yield new_graphs\n\n        for i in range(1, self._h):\n            new_graphs = list()\n            L_temp, label_set = dict(), set()\n            nl += len(self._inv_labels[i])\n            for j in range(nx):\n                # Find unique labels and sort them for both graphs\n                # Keep for each node the temporary\n                L_temp[j] = dict()\n                for v in Gs_ed[j].keys():\n                    credential = (\n                        str(L[j][v])\n                        + \",\"\n                        + str(sorted(L[j][n] for n in Gs_ed[j][v].keys()))\n                    )\n                    L_temp[j][v] = credential\n                    if credential not in self._inv_labels[i]:\n                        label_set.add(credential)\n\n            # Calculate the new label_set\n            WL_labels_inverse = dict()\n            if len(label_set) &gt; 0:\n                for dv in sorted(list(label_set)):\n                    idx = len(WL_labels_inverse) + nl\n                    WL_labels_inverse[dv] = idx\n\n            # Recalculate labels\n            new_graphs = list()\n            for j in range(nx):\n                new_labels = dict()\n                for k, v in L_temp[j].items():\n                    if v in self._inv_labels[i]:\n                        new_labels[k] = self._inv_labels[i][v]\n                    else:\n                        new_labels[k] = WL_labels_inverse[v]\n                L[j] = new_labels\n                # Create the new graphs with the new labels.\n                new_graphs.append([Gs_ed[j], new_labels])\n            yield new_graphs\n\n    if return_embedding_only:\n        K = []\n        for i, g in enumerate(generate_graphs_transform(WL_labels_inverse, nl)):\n            K.append(\n                self.X[i].transform(\n                    g,\n                    label_start_idx=self.feature_dims[i],\n                    label_end_idx=self.feature_dims[i + 1],\n                    return_embedding_only=True,\n                )\n            )\n        return K\n\n    # Calculate the kernel matrix without parallelization\n    if self.as_tensor:\n        summand = [\n            self.layer_weights[i]\n            * self.X[i].transform(\n                g,\n                label_start_idx=self.feature_dims[i],\n                label_end_idx=self.feature_dims[i + 1],\n            )\n            for i, g in enumerate(generate_graphs_transform(WL_labels_inverse, nl))\n        ]\n        K = torch.stack(summand, dim=0).sum(dim=0)\n    else:\n        K = np.sum(\n            (\n                self.layer_weights[i]\n                * self.X[i].transform(\n                    g,\n                    label_start_idx=self.feature_dims[i],\n                    label_end_idx=self.feature_dims[i + 1],\n                )\n                for (i, g) in enumerate(\n                    generate_graphs_transform(WL_labels_inverse, nl)\n                )\n            ),\n            axis=0,\n        )\n\n    self._is_transformed = True\n    if self.normalize:\n        X_diag, Y_diag = self.diagonal()\n        if self.as_tensor:\n            div_ = torch.sqrt(torch.ger(Y_diag, X_diag))\n            K /= div_\n        else:\n            old_settings = np.seterr(divide=\"ignore\")\n            K = np.nan_to_num(np.divide(K, np.sqrt(np.outer(Y_diag, X_diag))))\n            np.seterr(**old_settings)\n\n    return K\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.WeisfeilerLehman.translate_label","title":"translate_label  <code>staticmethod</code>","text":"<pre><code>translate_label(\n    curr_layer: dict, h: int, prev_layer: dict = None\n)\n</code></pre> <p>Translate the label to be in terms of the node attributes curr_layer: the WL_label_inverse object. A dictionary with element of the format of</p> return <p>label_in_node_attr: in terms of {encoding: pattern}, but pattern is always in term of the node attribute inv_label_in_node_attr: in terms of {pattern: encoding}</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>@staticmethod\ndef translate_label(curr_layer: dict, h: int, prev_layer: dict = None):\n    \"\"\"Translate the label to be in terms of the node attributes\n    curr_layer: the WL_label_inverse object. A dictionary with element of the format of\n    {pattern: encoding}\n\n    return:\n       label_in_node_attr: in terms of {encoding: pattern}, but pattern is always in term of the node attribute\n       inv_label_in_node_attr: in terms of {pattern: encoding}\n\n    \"\"\"\n    if h == 0:\n        return {v: str(k) for k, v in curr_layer.items()}, curr_layer\n    else:\n        assert prev_layer is not None\n        label_in_node_attr, inv_label_in_node_attr = OrderedDict(), OrderedDict()\n        for pattern, encoding in curr_layer.items():\n            # current pattern is in terms of the encoding previous layer. Find the pattern from the prev_layer\n            root, leaf = literal_eval(pattern)\n            root_ = prev_layer[root]\n            leaf_ = [prev_layer[i] for i in leaf]\n            label_in_node_attr.update({encoding: \"~\".join([root_] + leaf_)})\n            inv_label_in_node_attr.update({\"~\".join([root_] + leaf_): encoding})\n        return label_in_node_attr, inv_label_in_node_attr\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.efit","title":"efit","text":"<pre><code>efit(obj, data)\n</code></pre> <p>Fit an object on data.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def efit(obj, data):\n    \"\"\"Fit an object on data.\"\"\"\n    obj.fit(data)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.efit_transform","title":"efit_transform","text":"<pre><code>efit_transform(obj, data)\n</code></pre> <p>Fit-Transform an object on data.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def efit_transform(obj, data):\n    \"\"\"Fit-Transform an object on data.\"\"\"\n    return obj.fit_transform(data)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman/#neps.optimizers.bayesian_optimization.kernels.grakel_replace.weisfeiler_lehman.etransform","title":"etransform","text":"<pre><code>etransform(obj, data)\n</code></pre> <p>Transform an object on data.</p> Source code in <code>neps/optimizers/bayesian_optimization/kernels/grakel_replace/weisfeiler_lehman.py</code> <pre><code>def etransform(obj, data):\n    \"\"\"Transform an object on data.\"\"\"\n    return obj.transform(data)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/","title":"deepGP","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP","title":"neps.optimizers.bayesian_optimization.models.deepGP","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP.DeepGP","title":"DeepGP","text":"<pre><code>DeepGP(\n    pipeline_space: SearchSpace,\n    neural_network_args: dict | None = None,\n    logger=None,\n    surrogate_model_fit_args: dict | None = None,\n    checkpointing: bool = False,\n    root_directory: Path | str | None = None,\n    checkpoint_file: (\n        Path | str\n    ) = \"surrogate_checkpoint.pth\",\n    refine_epochs: int = 50,\n    **kwargs\n)\n</code></pre> <p>Gaussian process with a deep kernel</p> Source code in <code>neps/optimizers/bayesian_optimization/models/deepGP.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    neural_network_args: dict | None = None,\n    logger=None,\n    surrogate_model_fit_args: dict | None = None,\n    # IMPORTANT: Checkpointing does not use file locking,\n    # IMPORTANT: hence, it is not suitable for multiprocessing settings\n    checkpointing: bool = False,\n    root_directory: Path | str | None = None,\n    checkpoint_file: Path | str = \"surrogate_checkpoint.pth\",\n    refine_epochs: int = 50,\n    **kwargs,\n):\n    self.surrogate_model_fit_args = (\n        surrogate_model_fit_args if surrogate_model_fit_args is not None else {}\n    )\n\n    self.checkpointing = checkpointing\n    self.refine_epochs = refine_epochs\n    if checkpointing:\n        assert (\n            root_directory is not None\n        ), \"neps root_directory must be provided for the checkpointing\"\n        self.root_dir = Path(os.getcwd(), root_directory)\n        self.checkpoint_path = Path(os.getcwd(), root_directory, checkpoint_file)\n\n    super().__init__()\n    self.__preprocess_search_space(pipeline_space)\n    # set the categories array for the encoder\n    self.categories_array = np.array(self.categories)\n\n    if neural_network_args is None:\n        neural_network_args = {}\n    self.nn_args = neural_network_args\n\n    self.device = (\n        torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    )\n    # self.device = torch.device(\"cpu\")\n\n    # Save the NN args, necessary for preprocessing\n    self.cnn_kernel_size = neural_network_args.get(\"cnn_kernel_size\", 3)\n    self.model, self.likelihood, self.mll = self.__initialize_gp_model(\n        neural_network_args.get(\"n_layers\", 2)\n    )\n\n    # build the neural network\n    self.nn = NeuralFeatureExtractor(self.input_size, **neural_network_args)\n\n    self.logger = logger or logging.getLogger(\"neps\")\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP.DeepGP.__initialize_gp_model","title":"__initialize_gp_model","text":"<pre><code>__initialize_gp_model(\n    train_size: int,\n) -&gt; tuple[\n    GPRegressionModel,\n    GaussianLikelihood,\n    ExactMarginalLogLikelihood,\n]\n</code></pre> <p>Called when the surrogate is first initialized or restarted.</p> PARAMETER DESCRIPTION <code>train_size</code> <p>The size of the current training set.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tuple[GPRegressionModel, GaussianLikelihood, ExactMarginalLogLikelihood]</code> <p>model, likelihood, mll - The GP model, the likelihood and     the marginal likelihood.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/deepGP.py</code> <pre><code>def __initialize_gp_model(\n    self,\n    train_size: int,\n) -&gt; tuple[\n    GPRegressionModel,\n    gpytorch.likelihoods.GaussianLikelihood,\n    gpytorch.mlls.ExactMarginalLogLikelihood,\n]:\n    \"\"\"\n    Called when the surrogate is first initialized or restarted.\n\n    Args:\n        train_size: The size of the current training set.\n\n    Returns:\n        model, likelihood, mll - The GP model, the likelihood and\n            the marginal likelihood.\n    \"\"\"\n    train_x = torch.ones(train_size, train_size).to(self.device)\n    train_y = torch.ones(train_size).to(self.device)\n\n    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.device)\n    model = GPRegressionModel(\n        train_x=train_x, train_y=train_y, likelihood=likelihood\n    ).to(self.device)\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.device)\n    return model, likelihood, mll\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP.DeepGP.get_state","title":"get_state","text":"<pre><code>get_state() -&gt; dict[str, dict]\n</code></pre> <p>Get the current state of the surrogate.</p> RETURNS DESCRIPTION <code>current_state</code> <p>A dictionary that represents     the current state of the surrogate model.</p> <p> TYPE: <code>dict[str, dict]</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/models/deepGP.py</code> <pre><code>def get_state(self) -&gt; dict[str, dict]:\n    \"\"\"\n    Get the current state of the surrogate.\n\n    Returns:\n        current_state: A dictionary that represents\n            the current state of the surrogate model.\n    \"\"\"\n    current_state = {\n        \"gp_state_dict\": deepcopy(self.model.state_dict()),\n        \"nn_state_dict\": deepcopy(self.nn.state_dict()),\n        \"likelihood_state_dict\": deepcopy(self.likelihood.state_dict()),\n    }\n\n    return current_state\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP.DeepGP.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(state: dict | None = None)\n</code></pre> <p>Load the state from a previous checkpoint.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/deepGP.py</code> <pre><code>def load_checkpoint(self, state: dict | None = None):\n    \"\"\"\n    Load the state from a previous checkpoint.\n    \"\"\"\n    if state is None:\n        checkpoint = torch.load(self.checkpoint_path)\n    else:\n        checkpoint = state\n    self.model.load_state_dict(checkpoint[\"gp_state_dict\"])\n    self.nn.load_state_dict(checkpoint[\"nn_state_dict\"])\n    self.likelihood.load_state_dict(checkpoint[\"likelihood_state_dict\"])\n\n    self.model.to(self.device)\n    self.likelihood.to(self.device)\n    self.nn.to(self.device)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP.DeepGP.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(state: dict | None = None)\n</code></pre> <p>Save the given state or the current state in a checkpoint file.</p> PARAMETER DESCRIPTION <code>checkpoint_path</code> <p>path to the checkpoint file</p> <p> </p> <code>state</code> <p>The state to save, if none, it will</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/models/deepGP.py</code> <pre><code>def save_checkpoint(self, state: dict | None = None):\n    \"\"\"\n    Save the given state or the current state in a\n    checkpoint file.\n\n    Args:\n        checkpoint_path: path to the checkpoint file\n        state: The state to save, if none, it will\n        save the current state.\n    \"\"\"\n\n    if state is None:\n        torch.save(\n            self.get_state(),\n            self.checkpoint_path,\n        )\n    else:\n        torch.save(\n            state,\n            self.checkpoint_path,\n        )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP.GPRegressionModel","title":"GPRegressionModel","text":"<pre><code>GPRegressionModel(\n    train_x: Tensor,\n    train_y: Tensor,\n    likelihood: GaussianLikelihood,\n)\n</code></pre> <p>               Bases: <code>ExactGP</code></p> <p>A simple GP model.</p> PARAMETER DESCRIPTION <code>train_x</code> <p>The initial train examples for the GP.</p> <p> TYPE: <code>Tensor</code> </p> <code>train_y</code> <p>The initial train labels for the GP.</p> <p> TYPE: <code>Tensor</code> </p> <code>likelihood</code> <p>The likelihood to be used.</p> <p> TYPE: <code>GaussianLikelihood</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/models/deepGP.py</code> <pre><code>def __init__(\n    self,\n    train_x: torch.Tensor,\n    train_y: torch.Tensor,\n    likelihood: gpytorch.likelihoods.GaussianLikelihood,\n):\n    \"\"\"\n    Constructor of the GPRegressionModel.\n\n    Args:\n        train_x: The initial train examples for the GP.\n        train_y: The initial train labels for the GP.\n        likelihood: The likelihood to be used.\n    \"\"\"\n    super().__init__(train_x, train_y, likelihood)\n\n    self.mean_module = gpytorch.means.ConstantMean()\n    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/deepGP/#neps.optimizers.bayesian_optimization.models.deepGP.NeuralFeatureExtractor","title":"NeuralFeatureExtractor","text":"<pre><code>NeuralFeatureExtractor(input_size: int, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Neural network to be used in the DeepGP</p> Source code in <code>neps/optimizers/bayesian_optimization/models/deepGP.py</code> <pre><code>def __init__(self, input_size: int, **kwargs):\n    super().__init__()\n\n    # Set number of hyperparameters\n    self.input_size = input_size\n\n    self.n_layers = kwargs.get(\"n_layers\", 2)\n    self.activation = nn.LeakyReLU()\n\n    layer1_units = kwargs.get(\"layer1_units\", 128)\n    self.fc1 = nn.Linear(input_size, layer1_units)\n    self.bn1 = nn.BatchNorm1d(layer1_units)\n\n    previous_layer_units = layer1_units\n    for i in range(2, self.n_layers):\n        next_layer_units = kwargs.get(f\"layer{i}_units\", 256)\n        setattr(\n            self,\n            f\"fc{i}\",\n            nn.Linear(previous_layer_units, next_layer_units),\n        )\n        setattr(\n            self,\n            f\"bn{i}\",\n            nn.BatchNorm1d(next_layer_units),\n        )\n        previous_layer_units = next_layer_units\n\n    setattr(\n        self,\n        f\"fc{self.n_layers}\",\n        nn.Linear(\n            previous_layer_units + kwargs.get(\"cnn_nr_channels\", 4),\n            # accounting for the learning curve features\n            kwargs.get(f\"layer{self.n_layers}_units\", 256),\n        ),\n    )\n    self.cnn = nn.Sequential(\n        nn.Conv1d(\n            in_channels=1,\n            kernel_size=(kwargs.get(\"cnn_kernel_size\", 3),),\n            out_channels=4,\n        ),\n        nn.AdaptiveMaxPool1d(1),\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/","title":"Gp","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp","title":"neps.optimizers.bayesian_optimization.models.gp","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.ComprehensiveGP","title":"ComprehensiveGP","text":"<pre><code>ComprehensiveGP(\n    graph_kernels: Iterable,\n    hp_kernels: Iterable,\n    likelihood: float = 0.001,\n    weights=None,\n    vectorial_features: list = None,\n    combined_kernel: str = \"sum\",\n    logger=None,\n    surrogate_model_fit_args: dict = None,\n)\n</code></pre> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def __init__(\n    self,\n    graph_kernels: Iterable,\n    hp_kernels: Iterable,\n    likelihood: float = 1e-3,\n    weights=None,\n    vectorial_features: list = None,\n    combined_kernel: str = \"sum\",\n    logger=None,\n    surrogate_model_fit_args: dict = None,\n):\n    self.likelihood = likelihood\n    self.surrogate_model_fit_args = surrogate_model_fit_args or {}\n\n    self.domain_kernels: list = []\n    if bool(graph_kernels):\n        self.domain_kernels += list(graph_kernels)\n    if bool(hp_kernels):\n        self.domain_kernels += list(hp_kernels)\n\n    self.n_kernels: int = len(self.domain_kernels)\n    self.n_graph_kernels: int = len(\n        [i for i in self.domain_kernels if isinstance(i, GraphKernels)]\n    )\n    self.n_vector_kernels: int = self.n_kernels - self.n_graph_kernels\n\n    self.vectorial_features = vectorial_features\n\n    if weights is not None:\n        self.fixed_weights = True\n        if weights is not None:\n            assert len(weights) == len(self.n_kernels), (\n                \"the weights vector, if supplied, needs to have the same length as \"\n                \"the number of kernel_operators!\"\n            )\n        self.init_weights = (\n            weights\n            if isinstance(weights, torch.Tensor)\n            else torch.tensor(weights).flatten()\n        )\n    else:\n        self.fixed_weights = False\n        # Initialise the domain kernel weights to uniform\n        self.init_weights = torch.tensor(\n            [1.0 / self.n_kernels] * self.n_kernels,\n        )\n    self.weights = self.init_weights.clone()\n\n    if combined_kernel == \"product\":\n        self.combined_kernel = ProductKernel(\n            *self.domain_kernels, weights=self.weights\n        )\n    elif combined_kernel == \"sum\":\n        self.combined_kernel = SumKernel(*self.domain_kernels, weights=self.weights)\n    else:\n        raise NotImplementedError(\n            f'Combining kernel {combined_kernel} is not yet implemented! Only \"sum\" '\n            f'or \"product\" are currently supported. '\n        )\n\n    self.logger = logger or logging.getLogger(\"neps\")\n    # Cache the Gram matrix inverse and its log-determinant\n    self.K, self.K_i, self.logDetK = [None] * 3\n    self.theta_vector = None\n    self.layer_weights = None\n    self.nlml = None\n\n    self.x_configs: list = None\n    self.y: torch.Tensor = None\n    self.y_: torch.Tensor = None\n    self.y_mean: torch.Tensor = None\n    self.y_std: torch.Tensor = None\n    self.n: int = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.ComprehensiveGP.dmu_dphi","title":"dmu_dphi","text":"<pre><code>dmu_dphi(\n    X_s=None,\n    average_across_features=True,\n    average_across_occurrences=False,\n)\n</code></pre> <p>Compute the derivative of the GP posterior mean at the specified input location with respect to the vector embedding of the graph (e.g., if using WL-subtree, this function computes the gradient wrt each subtree pattern)</p> <p>The derivative is given by $ \\frac{\\partial \\mu^}{\\partial \\phi ^} = \\frac{\\partial K(\\phi, \\phi^)}{\\partial \\phi ^ }K(\\phi, \\phi)^{-1} \\mathbf{y} $</p> <p>which derives directly from the GP posterior mean formula, and since the term $K(\\phi, \\phi)^{-1} and \\mathbf{y} are both independent of the testing points (X_s, or \\phi^*}, the posterior gradient is simply the matrix produce of the kernel gradient with the inverse Gram and the training label vector.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.ComprehensiveGP.dmu_dphi--parameters","title":"Parameters","text":"<p>X_s: The locations on which the GP posterior mean derivatives should be evaluated. If left blank, the derivatives will be evaluated at the training points.</p> <p>compute_grad_var: bool. If true, also compute the gradient variance.</p> <p>The derivative of GP is also a GP, and thus the predictive distribution of the posterior gradient is Gaussian. The posterior mean is given above, and the posterior variance is: $ \\mathbb{V}[\\frac{\\partial f^}{\\partial \\phi^}]= \\frac{\\partial^2k(\\phi^, \\phi^)}{\\partial \\phi^^2} - \\frac{\\partial k(\\phi^, \\Phi)}{\\partial \\phi^}K(X, X)^{-1}\\frac{\\partial k{(\\Phi, \\phi^)}}{\\partial \\phi^*} $</p>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.ComprehensiveGP.dmu_dphi--returns","title":"Returns","text":"<p>list of K torch.Tensor of the shape N x2 D, where N is the length of the X_s list (each element of which is a networkx graph), K is the number of kernel_operators in the combined kernel and D is the dimensionality of the feature vector (this is determined by the specific graph kernel.</p> <p>OR</p> <p>list of K torch.Tensor of shape D, if averaged_over_samples flag is enabled.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def dmu_dphi(\n    self,\n    X_s=None,\n    # compute_grad_var=False,\n    average_across_features=True,\n    average_across_occurrences=False,\n):\n    r\"\"\"\n    Compute the derivative of the GP posterior mean at the specified input location with respect to the\n    *vector embedding* of the graph (e.g., if using WL-subtree, this function computes the gradient wrt\n    each subtree pattern)\n\n    The derivative is given by\n    $\n    \\frac{\\partial \\mu^*}{\\partial \\phi ^*} = \\frac{\\partial K(\\phi, \\phi^*)}{\\partial \\phi ^ *}K(\\phi, \\phi)^{-1}\n    \\mathbf{y}\n    $\n\n    which derives directly from the GP posterior mean formula, and since the term $K(\\phi, \\phi)^{-1} and \\mathbf{y}\n    are both independent of the testing points (X_s, or \\phi^*}, the posterior gradient is simply the matrix\n    produce of the kernel gradient with the inverse Gram and the training label vector.\n\n    Parameters\n    ----------\n    X_s: The locations on which the GP posterior mean derivatives should be evaluated. If left blank, the\n    derivatives will be evaluated at the training points.\n\n    compute_grad_var: bool. If true, also compute the gradient variance.\n\n    The derivative of GP is also a GP, and thus the predictive distribution of the posterior gradient is Gaussian.\n    The posterior mean is given above, and the posterior variance is:\n    $\n    \\mathbb{V}[\\frac{\\partial f^*}{\\partial \\phi^*}]= \\frac{\\partial^2k(\\phi^*, \\phi^*)}{\\partial \\phi^*^2} -\n    \\frac{\\partial k(\\phi^*, \\Phi)}{\\partial \\phi^*}K(X, X)^{-1}\\frac{\\partial k{(\\Phi, \\phi^*)}}{\\partial \\phi^*}\n    $\n\n    Returns\n    -------\n    list of K torch.Tensor of the shape N x2 D, where N is the length of the X_s list (each element of which is a\n    networkx graph), K is the number of kernel_operators in the combined kernel and D is the dimensionality of the\n    feature vector (this is determined by the specific graph kernel.\n\n    OR\n\n    list of K torch.Tensor of shape D, if averaged_over_samples flag is enabled.\n    \"\"\"\n    if self.K_i is None or self.logDetK is None:\n        raise ValueError(\n            \"Inverse of Gram matrix is not instantiated. Please call the optimize \"\n            \"function to fit on the training data first!\"\n        )\n    if self.n_vector_kernels:\n        if X_s is not None:\n            V_s = self._get_vectorial_features(X_s, self.vectorial_feactures)\n            V_s, _, _ = standardize_x(V_s, self.x_features_min, self.x_features_max)\n        else:\n            V_s = self.x_features\n            X_s = self.x[:]\n    else:\n        V_s = None\n        X_s = X_s if X_s is not None else self.x[:]\n\n    alpha = (self.K_i @ self.y).double().reshape(1, -1)\n    dmu_dphi = []\n    # dmu_dphi_var = [] if compute_grad_var else None\n\n    Ks_handles = []\n    feature_matrix = []\n    for j, x_s in enumerate(X_s):\n        jacob_vecs = []\n        if V_s is None:\n            handles = self.combined_kernel.forward_t(\n                self.weights,\n                [x_s],\n            )\n        else:\n            handles = self.combined_kernel.forward_t(self.weights, [x_s], V_s[j])\n        Ks_handles.append(handles)\n        # Each handle is a 2-tuple. first element is the Gram matrix, second element is the leaf variable\n        feature_vectors = []\n        for handle in handles:\n            k_s, y, _ = handle\n            # k_s is output, leaf is input, alpha is the K_i @ y term which is constant.\n            # When compute_grad_var is not required, computational graphs do not need to be saved.\n            jacob_vecs.append(\n                torch.autograd.grad(\n                    outputs=k_s, inputs=y, grad_outputs=alpha, retain_graph=False\n                )[0]\n            )\n            feature_vectors.append(y)\n        feature_matrix.append(feature_vectors)\n        jacob_vecs = torch.cat(jacob_vecs)\n        dmu_dphi.append(jacob_vecs)\n\n    feature_matrix = torch.cat([f[0] for f in feature_matrix])\n    if average_across_features:\n        dmu_dphi = torch.cat(dmu_dphi)\n        # compute the weighted average of the gradient across N_t.\n        # feature matrix is of shape N_t x K x D\n        avg_mu, avg_var, incidences = get_grad(\n            dmu_dphi, feature_matrix, average_across_occurrences\n        )\n        return avg_mu, avg_var, incidences\n    return (\n        dmu_dphi,\n        None,\n        feature_matrix.sum(dim=0) if average_across_occurrences else feature_matrix,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.ComprehensiveGP.predict","title":"predict","text":"<pre><code>predict(x_configs, preserve_comp_graph: bool = False)\n</code></pre> <p>Kriging predictions</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def predict(self, x_configs, preserve_comp_graph: bool = False):\n    \"\"\"Kriging predictions\"\"\"\n\n    if not isinstance(x_configs, list):\n        # Convert a single input X_s to a singleton list\n        x_configs = [x_configs]\n\n    if self.K_i is None or self.logDetK is None:\n        raise ValueError(\n            \"Inverse of Gram matrix is not instantiated. Please call the optimize \"\n            \"function to fit on the training data first!\"\n        )\n\n    # Concatenate the full list\n    X_configs_all = self.x_configs + x_configs\n\n    # Make a copy of the sum_kernels for this step, to avoid breaking the autodiff\n    # if grad guided mutation is used\n    if preserve_comp_graph:\n        combined_kernel_copy = deepcopy(self.combined_kernel)\n    else:\n        combined_kernel_copy = self.combined_kernel\n\n    K_full = combined_kernel_copy.fit_transform(\n        self.weights,\n        X_configs_all,\n        layer_weights=self.layer_weights,\n        feature_lengthscale=self.theta_vector,\n        rebuild_model=True,\n        save_gram_matrix=False,\n        gp_fit=False,\n    )\n\n    K_s = K_full[: self.n :, self.n :]\n\n    K_ss = K_full[self.n :, self.n :] + self.likelihood * torch.eye(\n        len(x_configs),\n    )\n\n    mu_s = K_s.t() @ self.K_i @ self.y\n    cov_s = K_ss - K_s.t() @ self.K_i @ K_s\n    cov_s = torch.clamp(cov_s, self.likelihood, np.inf)\n    mu_s = unnormalize_y(mu_s, self.y_mean, self.y_std)\n    std_s = torch.sqrt(cov_s)\n    std_s = unnormalize_y(std_s, None, self.y_std, True)\n    cov_s = std_s**2\n    if preserve_comp_graph:\n        del combined_kernel_copy\n    return mu_s, cov_s\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.compute_log_marginal_likelihood","title":"compute_log_marginal_likelihood","text":"<pre><code>compute_log_marginal_likelihood(\n    K_i: Tensor,\n    logDetK: Tensor,\n    y: Tensor,\n    normalize: bool = True,\n    log_prior_dist=None,\n)\n</code></pre> <p>Compute the zero mean Gaussian process log marginal likelihood given the inverse of Gram matrix K(x2,x2), its log determinant, and the training label vector y. Option:</p> <p>normalize: normalize the log marginal likelihood by the length of the label vector, as per the gpytorch routine.</p> <p>prior: A pytorch distribution object. If specified, the hyperparameter prior will be taken into consideration and we use Type-II MAP instead of Type-II MLE (compute log_posterior instead of log_evidence)</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def compute_log_marginal_likelihood(\n    K_i: torch.Tensor,\n    logDetK: torch.Tensor,\n    y: torch.Tensor,\n    normalize: bool = True,\n    log_prior_dist=None,\n):\n    \"\"\"Compute the zero mean Gaussian process log marginal likelihood given the inverse of Gram matrix K(x2,x2), its\n    log determinant, and the training label vector y.\n    Option:\n\n    normalize: normalize the log marginal likelihood by the length of the label vector, as per the gpytorch\n    routine.\n\n    prior: A pytorch distribution object. If specified, the hyperparameter prior will be taken into consideration and\n    we use Type-II MAP instead of Type-II MLE (compute log_posterior instead of log_evidence)\n    \"\"\"\n    lml = (\n        -0.5 * y.t() @ K_i @ y\n        + 0.5 * logDetK\n        - y.shape[0]\n        / 2.0\n        * torch.log(\n            2\n            * torch.tensor(\n                np.pi,\n            )\n        )\n    )\n    if log_prior_dist is not None:\n        lml -= log_prior_dist\n    return lml / y.shape[0] if normalize else lml\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.compute_pd_inverse","title":"compute_pd_inverse","text":"<pre><code>compute_pd_inverse(K: tensor, jitter: float = 1e-06)\n</code></pre> <p>Compute the inverse of a postive-(semi)definite matrix K using Cholesky inversion.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def compute_pd_inverse(K: torch.tensor, jitter: float = 1e-6):\n    \"\"\"Compute the inverse of a postive-(semi)definite matrix K using Cholesky inversion.\"\"\"\n    n = K.shape[0]\n    assert (\n        isinstance(jitter, float) or jitter.ndim == 0\n    ), \"only homoscedastic noise variance is allowed here!\"\n    is_successful = False\n    fail_count = 0\n    max_fail = 3\n    while fail_count &lt; max_fail and not is_successful:\n        try:\n            jitter_diag = jitter * torch.eye(n, device=K.device) * 10**fail_count\n            K_ = K + jitter_diag\n            try:\n                Kc = torch.linalg.cholesky(K_)\n            except AttributeError:  # For torch &lt; 1.8.0\n                Kc = torch.cholesky(K_)\n            is_successful = True\n        except RuntimeError:\n            fail_count += 1\n    if not is_successful:\n        raise RuntimeError(f\"Gram matrix not positive definite despite of jitter:\\n{K}\")\n    logDetK = -2 * torch.sum(torch.log(torch.diag(Kc)))\n    K_i = torch.cholesky_inverse(Kc)\n    return K_i.to(torch.get_default_dtype()), logDetK.to(torch.get_default_dtype())\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.get_grad","title":"get_grad","text":"<pre><code>get_grad(\n    grad_matrix, feature_matrix, average_occurrences=False\n)\n</code></pre> <p>Average across the samples via a Monte Carlo sampling scheme. Also estimates the empirical variance. :param average_occurrences: if True, do a weighted summation based on the frequency distribution of the occurrence to compute a gradient per each feature. Otherwise, each different occurrence (\\phi_i = k) will get a different gradient estimate.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def get_grad(grad_matrix, feature_matrix, average_occurrences=False):\n    r\"\"\"\n    Average across the samples via a Monte Carlo sampling scheme. Also estimates the\n    empirical variance. :param average_occurrences: if True, do a weighted summation\n    based on the frequency distribution of the occurrence to compute a gradient *per\n    each feature*. Otherwise, each different occurrence (\\phi_i = k) will get a\n    different gradient estimate.\n    \"\"\"\n    assert grad_matrix.shape == feature_matrix.shape\n    # Prune out the all-zero columns that pop up sometimes\n    valid_cols = []\n    for col_idx in range(feature_matrix.size(1)):\n        if not torch.all(feature_matrix[:, col_idx] == 0):\n            valid_cols.append(col_idx)\n    feature_matrix = feature_matrix[:, valid_cols]\n    grad_matrix = grad_matrix[:, valid_cols]\n\n    _, D = feature_matrix.shape\n    if average_occurrences:\n        avg_grad = torch.zeros(D)\n        avg_grad_var = torch.zeros(D)\n        for d in range(D):\n            current_feature = feature_matrix[:, d].clone().detach()\n            instances, indices, counts = torch.unique(\n                current_feature, return_inverse=True, return_counts=True\n            )\n            weight_vector = torch.tensor([counts[i] for i in indices]).type(torch.float)\n            weight_vector /= weight_vector.sum()\n            mean = torch.sum(weight_vector * grad_matrix[:, d])\n            # Compute the empirical variance of gradients\n            variance = torch.sum(weight_vector * grad_matrix[:, d] ** 2) - mean**2\n            avg_grad[d] = mean\n            avg_grad_var[d] = variance\n        return avg_grad, avg_grad_var, feature_matrix.sum(dim=0)\n    else:\n        # The maximum number possible occurrences -- 7 is an example, if problem occurs, maybe we can increase this\n        # number. But for now, for both NAS-Bench datasets, this should be more than enough!\n        max_occur = 7\n        avg_grad = torch.zeros(D, max_occur)\n        avg_grad_var = torch.zeros(D, max_occur)\n        incidences = torch.zeros(D, max_occur)\n        for d in range(D):\n            current_feature = feature_matrix[:, d].clone().detach()\n            instances, indices, counts = torch.unique(\n                current_feature, return_inverse=True, return_counts=True\n            )\n            for i, val in enumerate(instances):\n                # Find index of all feature counts that are equal to the current val\n                feature_at_val = grad_matrix[current_feature == val]\n                avg_grad[d, int(val)] = torch.mean(feature_at_val)\n                avg_grad_var[d, int(val)] = torch.var(feature_at_val)\n                incidences[d, int(val)] = counts[i]\n        return avg_grad, avg_grad_var, incidences\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.standardize_x","title":"standardize_x","text":"<pre><code>standardize_x(\n    x: Tensor, x_min: Tensor = None, x_max: Tensor = None\n)\n</code></pre> <p>Standardize the vectorial input into a d-dimensional hypercube [0, 1]^d, where d is the number of features. if x_min ond x_max are supplied, x2 will be standardised using these instead. This is used when standardising the validation/test inputs.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def standardize_x(\n    x: torch.Tensor, x_min: torch.Tensor = None, x_max: torch.Tensor = None\n):\n    \"\"\"Standardize the vectorial input into a d-dimensional hypercube [0, 1]^d, where d is the number of features.\n    if x_min ond x_max are supplied, x2 will be standardised using these instead. This is used when standardising the\n    validation/test inputs.\n    \"\"\"\n    if (x_min is not None and x_max is None) or (x_min is None and x_max is not None):\n        raise ValueError(\n            \"Either *both* or *neither* of x_min, x_max need to be supplied!\"\n        )\n    if x_min is None:\n        x_min = torch.min(x, 0)[0]\n        x_max = torch.max(x, 0)[0]\n    x = (x - x_min) / (x_max - x_min)\n    return x, x_min, x_max\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.unnormalize_y","title":"unnormalize_y","text":"<pre><code>unnormalize_y(y, y_mean, y_std, scale_std=False)\n</code></pre> <p>Similar to the undoing of the pre-processing step above, but on the output predictions</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def unnormalize_y(y, y_mean, y_std, scale_std=False):\n    \"\"\"Similar to the undoing of the pre-processing step above, but on the output predictions\"\"\"\n    if not scale_std:\n        return y * y_std + y_mean\n    else:\n        return y * y_std\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/","title":"Gp hierarchy","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy","title":"neps.optimizers.bayesian_optimization.models.gp_hierarchy","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.ComprehensiveGPHierarchy","title":"ComprehensiveGPHierarchy","text":"<pre><code>ComprehensiveGPHierarchy(\n    graph_kernels: Iterable,\n    hp_kernels: Iterable,\n    likelihood: float = 0.001,\n    weights=None,\n    learn_all_h=False,\n    graph_feature_ard=True,\n    d_graph_features: int = 0,\n    normalize_combined_kernel=True,\n    hierarchy_consider: list = None,\n    vectorial_features: list = None,\n    combined_kernel: str = \"sum\",\n    verbose: bool = False,\n    surrogate_model_fit_args: dict = None,\n    gpytorch_kinv: bool = False,\n)\n</code></pre> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def __init__(\n    self,\n    graph_kernels: Iterable,\n    hp_kernels: Iterable,\n    likelihood: float = 1e-3,\n    weights=None,\n    learn_all_h=False,\n    graph_feature_ard=True,\n    d_graph_features: int = 0,\n    normalize_combined_kernel=True,\n    hierarchy_consider: list = None,  # or a list of integers e.g. [0,1,2,3]\n    vectorial_features: list = None,\n    combined_kernel: str = \"sum\",\n    verbose: bool = False,\n    surrogate_model_fit_args: dict = None,\n    gpytorch_kinv: bool = False,\n):\n    self.likelihood = likelihood\n    self.surrogate_model_fit_args = surrogate_model_fit_args or {}\n    self.learn_all_h = learn_all_h\n    self.hierarchy_consider = hierarchy_consider\n    self.normalize_combined_kernel = normalize_combined_kernel\n    if self.hierarchy_consider is None:\n        self.learn_all_h = False\n    self.domain_kernels: list = []\n    if bool(graph_kernels):\n        self.domain_kernels += list(graph_kernels)\n    if bool(hp_kernels):\n        self.domain_kernels += list(hp_kernels)\n\n    self.hp_kernels = hp_kernels  # impose on scalar graph features\n    self.n_kernels: int = len(self.domain_kernels)\n    self.n_graph_kernels: int = len(\n        [i for i in self.domain_kernels if isinstance(i, GraphKernels)]\n    )\n    self.n_vector_kernels: int = self.n_kernels - self.n_graph_kernels\n    self.graph_feature_ard = graph_feature_ard\n    self.vectorial_features = vectorial_features\n    self.d_graph_features = d_graph_features\n\n    if weights is not None:\n        self.fixed_weights = True\n        if weights is not None:\n            assert len(weights) == self.n_kernels, (\n                \"the weights vector, if supplied, needs to have the same length as \"\n                \"the number of kernel_operators!\"\n            )\n        self.init_weights = (\n            weights\n            if isinstance(weights, torch.Tensor)\n            else torch.tensor(weights).flatten()\n        )\n    else:\n        self.fixed_weights = False\n        # Initialise the domain kernel weights to uniform\n        self.init_weights = torch.tensor(\n            [1.0 / self.n_kernels] * self.n_kernels,\n        )\n\n    self.weights = self.init_weights.clone()\n\n    if combined_kernel == \"product\":\n        self.combined_kernel = ProductKernel(\n            *self.domain_kernels,\n            weights=self.weights,\n            hierarchy_consider=self.hierarchy_consider,\n            d_graph_features=self.d_graph_features,\n        )\n    elif combined_kernel == \"sum\":\n        self.combined_kernel = SumKernel(\n            *self.domain_kernels,\n            weights=self.weights,\n            hierarchy_consider=self.hierarchy_consider,\n            d_graph_features=self.d_graph_features,\n        )\n    else:\n        raise NotImplementedError(\n            f'Combining kernel {combined_kernel} is not yet implemented! Only \"sum\" '\n            f'or \"product\" are currently supported. '\n        )\n    # Verbose mode\n    self.verbose = verbose\n    # Cache the Gram matrix inverse and its log-determinant\n    self.K, self.K_i, self.logDetK = [None] * 3\n    self.layer_weights = None\n    self.nlml = None\n\n    self.x_configs: list = None  # type: ignore[assignment]\n    self.y: torch.Tensor = None\n    self.y_: torch.Tensor = None\n    self.y_mean: torch.Tensor = None\n    self.y_std: torch.Tensor = None\n    self.n: int = None  # type: ignore[assignment]\n\n    self.gpytorch_kinv = gpytorch_kinv\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.ComprehensiveGPHierarchy.dmu_dphi","title":"dmu_dphi","text":"<pre><code>dmu_dphi(\n    X_s=None,\n    average_across_features=True,\n    average_across_occurrences=False,\n)\n</code></pre> <p>Compute the derivative of the GP posterior mean at the specified input location with respect to the vector embedding of the graph (e.g., if using WL-subtree, this function computes the gradient wrt each subtree pattern)</p> <p>The derivative is given by $ \\frac{\\partial \\mu^}{\\partial \\phi ^} = \\frac{\\partial K(\\phi, \\phi^)}{\\partial \\phi ^ }K(\\phi, \\phi)^{-1} \\mathbf{y} $</p> <p>which derives directly from the GP posterior mean formula, and since the term $K(\\phi, \\phi)^{-1} and \\mathbf{y} are both independent of the testing points (X_s, or \\phi^*}, the posterior gradient is simply the matrix produce of the kernel gradient with the inverse Gram and the training label vector.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.ComprehensiveGPHierarchy.dmu_dphi--parameters","title":"Parameters","text":"<p>X_s: The locations on which the GP posterior mean derivatives should be evaluated. If left blank, the derivatives will be evaluated at the training points.</p> <p>compute_grad_var: bool. If true, also compute the gradient variance.</p> <p>The derivative of GP is also a GP, and thus the predictive distribution of the posterior gradient is Gaussian. The posterior mean is given above, and the posterior variance is: $ \\mathbb{V}[\\frac{\\partial f^}{\\partial \\phi^}]= \\frac{\\partial^2k(\\phi^, \\phi^)}{\\partial \\phi^^2} - \\frac{\\partial k(\\phi^, \\Phi)}{\\partial \\phi^}K(X, X)^{-1}\\frac{\\partial k{(\\Phi, \\phi^)}}{\\partial \\phi^*} $</p>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.ComprehensiveGPHierarchy.dmu_dphi--returns","title":"Returns","text":"<p>list of K torch.Tensor of the shape N x2 D, where N is the length of the X_s list (each element of which is a networkx graph), K is the number of kernel_operators in the combined kernel and D is the dimensionality of the feature vector (this is determined by the specific graph kernel.</p> <p>OR</p> <p>list of K torch.Tensor of shape D, if averaged_over_samples flag is enabled.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def dmu_dphi(\n    self,\n    X_s=None,\n    # compute_grad_var=False,\n    average_across_features=True,\n    average_across_occurrences=False,\n):\n    r\"\"\"\n    Compute the derivative of the GP posterior mean at the specified input location with respect to the\n    *vector embedding* of the graph (e.g., if using WL-subtree, this function computes the gradient wrt\n    each subtree pattern)\n\n    The derivative is given by\n    $\n    \\frac{\\partial \\mu^*}{\\partial \\phi ^*} = \\frac{\\partial K(\\phi, \\phi^*)}{\\partial \\phi ^ *}K(\\phi, \\phi)^{-1}\n    \\mathbf{y}\n    $\n\n    which derives directly from the GP posterior mean formula, and since the term $K(\\phi, \\phi)^{-1} and \\mathbf{y}\n    are both independent of the testing points (X_s, or \\phi^*}, the posterior gradient is simply the matrix\n    produce of the kernel gradient with the inverse Gram and the training label vector.\n\n    Parameters\n    ----------\n    X_s: The locations on which the GP posterior mean derivatives should be evaluated. If left blank, the\n    derivatives will be evaluated at the training points.\n\n    compute_grad_var: bool. If true, also compute the gradient variance.\n\n    The derivative of GP is also a GP, and thus the predictive distribution of the posterior gradient is Gaussian.\n    The posterior mean is given above, and the posterior variance is:\n    $\n    \\mathbb{V}[\\frac{\\partial f^*}{\\partial \\phi^*}]= \\frac{\\partial^2k(\\phi^*, \\phi^*)}{\\partial \\phi^*^2} -\n    \\frac{\\partial k(\\phi^*, \\Phi)}{\\partial \\phi^*}K(X, X)^{-1}\\frac{\\partial k{(\\Phi, \\phi^*)}}{\\partial \\phi^*}\n    $\n\n    Returns\n    -------\n    list of K torch.Tensor of the shape N x2 D, where N is the length of the X_s list (each element of which is a\n    networkx graph), K is the number of kernel_operators in the combined kernel and D is the dimensionality of the\n    feature vector (this is determined by the specific graph kernel.\n\n    OR\n\n    list of K torch.Tensor of shape D, if averaged_over_samples flag is enabled.\n    \"\"\"\n    if self.K_i is None or self.logDetK is None:\n        raise ValueError(\n            \"Inverse of Gram matrix is not instantiated. Please call the optimize \"\n            \"function to fit on the training data first!\"\n        )\n    if self.n_vector_kernels:\n        if X_s is not None:\n            V_s = self._get_vectorial_features(X_s, self.vectorial_feactures)\n            V_s, _, _ = standardize_x(V_s, self.x_features_min, self.x_features_max)\n        else:\n            V_s = self.x_features\n            X_s = self.x[:]\n    else:\n        V_s = None\n        X_s = X_s if X_s is not None else self.x[:]\n\n    alpha = (self.K_i @ self.y).double().reshape(1, -1)\n    dmu_dphi = []\n    # dmu_dphi_var = [] if compute_grad_var else None\n\n    Ks_handles = []\n    feature_matrix = []\n    for j, x_s in enumerate(X_s):\n        jacob_vecs = []\n        if V_s is None:\n            handles = self.combined_kernel.forward_t(\n                self.weights,\n                [x_s],\n            )\n        else:\n            handles = self.combined_kernel.forward_t(self.weights, [x_s], V_s[j])\n        Ks_handles.append(handles)\n        # Each handle is a 2-tuple. first element is the Gram matrix, second element is the leaf variable\n        feature_vectors = []\n        for handle in handles:\n            k_s, y, _ = handle\n            # k_s is output, leaf is input, alpha is the K_i @ y term which is constant.\n            # When compute_grad_var is not required, computational graphs do not need to be saved.\n            jacob_vecs.append(\n                torch.autograd.grad(\n                    outputs=k_s, inputs=y, grad_outputs=alpha, retain_graph=False\n                )[0]\n            )\n            feature_vectors.append(y)\n        feature_matrix.append(feature_vectors)\n        jacob_vecs = torch.cat(jacob_vecs)\n        dmu_dphi.append(jacob_vecs)\n\n    feature_matrix = torch.cat([f[0] for f in feature_matrix])\n    if average_across_features:\n        dmu_dphi = torch.cat(dmu_dphi)\n        # compute the weighted average of the gradient across N_t.\n        # feature matrix is of shape N_t x K x D\n        avg_mu, avg_var, incidences = get_grad(\n            dmu_dphi, feature_matrix, average_across_occurrences\n        )\n        return avg_mu, avg_var, incidences\n    return (\n        dmu_dphi,\n        None,\n        feature_matrix.sum(dim=0) if average_across_occurrences else feature_matrix,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.ComprehensiveGPHierarchy.predict","title":"predict","text":"<pre><code>predict(x_configs, preserve_comp_graph: bool = False)\n</code></pre> <p>Kriging predictions</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def predict(self, x_configs, preserve_comp_graph: bool = False):\n    \"\"\"Kriging predictions\"\"\"\n\n    if not isinstance(x_configs, list):\n        # Convert a single input X_s to a singleton list\n        x_configs = [x_configs]\n\n    if self.K_i is None or self.logDetK is None:\n        raise ValueError(\n            \"Inverse of Gram matrix is not instantiated. Please call the optimize \"\n            \"function to fit on the training data first!\"\n        )\n\n    # Concatenate the full list\n    X_configs_all = self.x_configs + x_configs\n\n    # Make a copy of the sum_kernels for this step, to avoid breaking the autodiff\n    # if grad guided mutation is used\n    if preserve_comp_graph:\n        combined_kernel_copy = deepcopy(self.combined_kernel)\n    else:\n        combined_kernel_copy = self.combined_kernel\n\n    K_full = combined_kernel_copy.fit_transform(\n        self.weights,\n        X_configs_all,\n        layer_weights=self.layer_weights,\n        normalize=self.normalize_combined_kernel,\n        feature_lengthscale=torch.exp(self.theta_vector),\n        rebuild_model=True,\n        save_gram_matrix=False,\n        gp_fit=False,\n    )\n\n    K_s = K_full[: self.n :, self.n :]\n\n    K_ss = K_full[self.n :, self.n :] + self.likelihood * torch.eye(\n        len(x_configs),\n    )\n\n    mu_s = K_s.t() @ self.K_i @ self.y\n    cov_s = K_ss - K_s.t() @ self.K_i @ K_s\n    # TODO not taking the diag?\n    cov_s = torch.clamp(cov_s, self.likelihood, np.inf)\n    mu_s = unnormalize_y(mu_s, self.y_mean, self.y_std)\n    std_s = torch.sqrt(cov_s)\n    std_s = unnormalize_y(std_s, None, self.y_std, True)\n    cov_s = std_s**2\n    if preserve_comp_graph:\n        del combined_kernel_copy\n    return mu_s, cov_s\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.ComprehensiveGPHierarchy.predict_single_hierarchy","title":"predict_single_hierarchy","text":"<pre><code>predict_single_hierarchy(\n    x_configs,\n    hierarchy_id=0,\n    preserve_comp_graph: bool = False,\n)\n</code></pre> <p>Kriging predictions</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def predict_single_hierarchy(\n    self, x_configs, hierarchy_id=0, preserve_comp_graph: bool = False\n):\n    \"\"\"Kriging predictions\"\"\"\n\n    if not isinstance(x_configs, list):\n        # Convert a single input X_s to a singleton list\n        x_configs = [x_configs]\n\n    if self.K_i is None or self.logDetK is None:\n        raise ValueError(\n            \"Inverse of Gram matrix is not instantiated. Please call the optimize function to \"\n            \"fit on the training data first!\"\n        )\n\n    # Concatenate the full list\n    X_configs_all = self.x_configs + x_configs\n\n    # Make a copy of the sum_kernels for this step, to avoid breaking the autodiff if grad guided mutation is used\n    if preserve_comp_graph:\n        combined_kernel_copy = deepcopy(self.combined_kernel)\n    else:\n        combined_kernel_copy = self.combined_kernel\n\n    K_sub_full = combined_kernel_copy.fit_transform_single_hierarchy(\n        self.weights,\n        X_configs_all,\n        normalize=self.normalize_combined_kernel,\n        hierarchy_id=hierarchy_id,\n        feature_lengthscale=torch.exp(self.theta_vector),\n        layer_weights=self.layer_weights,\n        rebuild_model=True,\n        save_gram_matrix=False,\n        gp_fit=False,\n    )\n\n    K_s = K_sub_full[: self.n :, self.n :]\n    K_ss = K_sub_full[self.n :, self.n :]\n    mu_s = K_s.t() @ self.K_i @ self.y\n    cov_s_full = K_ss - K_s.t() @ self.K_i @ K_s\n    cov_s = torch.clamp(cov_s_full, self.likelihood, np.inf)\n    mu_s = unnormalize_y(mu_s, self.y_mean, self.y_std)\n    std_s = torch.sqrt(cov_s)\n    std_s = unnormalize_y(std_s, None, self.y_std, True)\n    cov_s = std_s**2\n    if preserve_comp_graph:\n        del combined_kernel_copy\n    return mu_s, cov_s\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.NumericalWarning","title":"NumericalWarning","text":"<p>               Bases: <code>RuntimeWarning</code></p> <p>Warning thrown when convergence criteria are not met, or when comptuations require extra stability.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.cholesky_jitter","title":"cholesky_jitter","text":"<pre><code>cholesky_jitter(float=None, double=None, half=None)\n</code></pre> <p>               Bases: <code>_dtype_value_context</code></p> <p>The jitter value used by <code>psd_safe_cholesky</code> when using cholesky solves. - Default for <code>float</code>: 1e-6 - Default for <code>double</code>: 1e-8</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def __init__(\n    self, float=None, double=None, half=None\n):\n    self._orig_float_value = (\n        self.__class__.value()\n    )\n    self._instance_float_value = float\n    self._orig_double_value = (\n        self.__class__.value()\n    )\n    self._instance_double_value = double\n    self._orig_half_value = (\n        self.__class__.value()\n    )\n    self._instance_half_value = half\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.cholesky_max_tries","title":"cholesky_max_tries","text":"<pre><code>cholesky_max_tries(value)\n</code></pre> <p>               Bases: <code>_value_context</code></p> <p>The max_tries value used by <code>psd_safe_cholesky</code> when using cholesky solves. (Default: 3)</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def __init__(self, value):\n    self._orig_value = self.__class__.value()\n    self._instance_value = value\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.verbose_linalg","title":"verbose_linalg","text":"<pre><code>verbose_linalg(state=True)\n</code></pre> <p>               Bases: <code>_feature_flag</code></p> <p>Print out information whenever running an expensive linear algebra routine (e.g. Cholesky, CG, Lanczos, CIQ, etc.) (Default: False)</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def __init__(self, state=True):\n    self.prev = self.__class__._state\n    self.state = state\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.compute_log_marginal_likelihood","title":"compute_log_marginal_likelihood","text":"<pre><code>compute_log_marginal_likelihood(\n    K_i: Tensor,\n    logDetK: Tensor,\n    y: Tensor,\n    normalize: bool = True,\n    log_prior_dist=None,\n)\n</code></pre> <p>Compute the zero mean Gaussian process log marginal likelihood given the inverse of Gram matrix K(x2,x2), its log determinant, and the training label vector y. Option:</p> <p>normalize: normalize the log marginal likelihood by the length of the label vector, as per the gpytorch routine.</p> <p>prior: A pytorch distribution object. If specified, the hyperparameter prior will be taken into consideration and we use Type-II MAP instead of Type-II MLE (compute log_posterior instead of log_evidence)</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def compute_log_marginal_likelihood(\n    K_i: torch.Tensor,\n    logDetK: torch.Tensor,\n    y: torch.Tensor,\n    normalize: bool = True,\n    log_prior_dist=None,\n):\n    \"\"\"Compute the zero mean Gaussian process log marginal likelihood given the inverse of Gram matrix K(x2,x2), its\n    log determinant, and the training label vector y.\n    Option:\n\n    normalize: normalize the log marginal likelihood by the length of the label vector, as per the gpytorch\n    routine.\n\n    prior: A pytorch distribution object. If specified, the hyperparameter prior will be taken into consideration and\n    we use Type-II MAP instead of Type-II MLE (compute log_posterior instead of log_evidence)\n    \"\"\"\n    lml = (\n        -0.5 * y.t() @ K_i @ y\n        + 0.5 * logDetK\n        - y.shape[0]\n        / 2.0\n        * torch.log(\n            2\n            * torch.tensor(\n                np.pi,\n            )\n        )\n    )\n    if log_prior_dist is not None:\n        lml -= log_prior_dist\n    return lml / y.shape[0] if normalize else lml\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.compute_pd_inverse","title":"compute_pd_inverse","text":"<pre><code>compute_pd_inverse(\n    K: tensor,\n    jitter: float = 1e-05,\n    gpytorch_kinv: bool = False,\n)\n</code></pre> <p>Compute the inverse of a postive-(semi)definite matrix K using Cholesky inversion.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def compute_pd_inverse(\n    K: torch.tensor, jitter: float = 1e-5, gpytorch_kinv: bool = False\n):\n    \"\"\"Compute the inverse of a postive-(semi)definite matrix K using Cholesky inversion.\"\"\"\n    if gpytorch_kinv:\n        Kc = psd_safe_cholesky(K)\n        try:\n            Kc.required_grad = True\n        except Exception:\n            Kc = torch.Tensor(Kc)\n    else:\n        n = K.shape[0]\n        assert (\n            isinstance(jitter, float) or jitter.ndim == 0\n        ), \"only homoscedastic noise variance is allowed here!\"\n        is_successful = False\n        fail_count = 0\n        max_fail = 3\n        while fail_count &lt; max_fail and not is_successful:\n            try:\n                jitter_diag = jitter * torch.eye(n, device=K.device) * 10**fail_count\n                K_ = K + jitter_diag\n                Kc = torch.linalg.cholesky(K_)\n                is_successful = True\n            except RuntimeError:\n                fail_count += 1\n        if not is_successful:\n            raise RuntimeError(\n                f\"Gram matrix not positive definite despite of jitter:\\n{K}\"\n            )\n\n    logDetK = -2 * torch.sum(torch.log(torch.diag(Kc)))\n    K_i = torch.cholesky_inverse(Kc)\n    return K_i.to(torch.get_default_dtype()), logDetK.to(torch.get_default_dtype())\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.get_grad","title":"get_grad","text":"<pre><code>get_grad(\n    grad_matrix, feature_matrix, average_occurrences=False\n)\n</code></pre> <p>Average across the samples via a Monte Carlo sampling scheme. Also estimates the empirical variance. :param average_occurrences: if True, do a weighted summation based on the frequency distribution of the occurrence to compute a gradient per each feature. Otherwise, each different occurrence (\\phi_i = k) will get a different gradient estimate.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def get_grad(grad_matrix, feature_matrix, average_occurrences=False):\n    r\"\"\"\n    Average across the samples via a Monte Carlo sampling scheme. Also estimates the\n    empirical variance. :param average_occurrences: if True, do a weighted summation\n    based on the frequency distribution of the occurrence to compute a gradient *per\n    each feature*. Otherwise, each different occurrence (\\phi_i = k) will get a\n    different gradient estimate.\n    \"\"\"\n    assert grad_matrix.shape == feature_matrix.shape\n    # Prune out the all-zero columns that pop up sometimes\n    valid_cols = []\n    for col_idx in range(feature_matrix.size(1)):\n        if not torch.all(feature_matrix[:, col_idx] == 0):\n            valid_cols.append(col_idx)\n    feature_matrix = feature_matrix[:, valid_cols]\n    grad_matrix = grad_matrix[:, valid_cols]\n\n    _, D = feature_matrix.shape\n    if average_occurrences:\n        avg_grad = torch.zeros(D)\n        avg_grad_var = torch.zeros(D)\n        for d in range(D):\n            current_feature = feature_matrix[:, d].clone().detach()\n            instances, indices, counts = torch.unique(\n                current_feature, return_inverse=True, return_counts=True\n            )\n            weight_vector = torch.tensor([counts[i] for i in indices]).type(torch.float)\n            weight_vector /= weight_vector.sum()\n            mean = torch.sum(weight_vector * grad_matrix[:, d])\n            # Compute the empirical variance of gradients\n            variance = torch.sum(weight_vector * grad_matrix[:, d] ** 2) - mean**2\n            avg_grad[d] = mean\n            avg_grad_var[d] = variance\n        return avg_grad, avg_grad_var, feature_matrix.sum(dim=0)\n    else:\n        # The maximum number possible occurrences -- 7 is an example, if problem occurs, maybe we can increase this\n        # number. But for now, for both NAS-Bench datasets, this should be more than enough!\n        max_occur = 7\n        avg_grad = torch.zeros(D, max_occur)\n        avg_grad_var = torch.zeros(D, max_occur)\n        incidences = torch.zeros(D, max_occur)\n        for d in range(D):\n            current_feature = feature_matrix[:, d].clone().detach()\n            instances, indices, counts = torch.unique(\n                current_feature, return_inverse=True, return_counts=True\n            )\n            for i, val in enumerate(instances):\n                # Find index of all feature counts that are equal to the current val\n                feature_at_val = grad_matrix[current_feature == val]\n                avg_grad[d, int(val)] = torch.mean(feature_at_val)\n                avg_grad_var[d, int(val)] = torch.var(feature_at_val)\n                incidences[d, int(val)] = counts[i]\n        return avg_grad, avg_grad_var, incidences\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.psd_safe_cholesky","title":"psd_safe_cholesky","text":"<pre><code>psd_safe_cholesky(\n    A, upper=False, out=None, jitter=None, max_tries=None\n)\n</code></pre> <p>Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal. Args:     A (Tensor):         The tensor to compute the Cholesky decomposition of     upper (bool, optional):         See torch.cholesky     out (Tensor, optional):         See torch.cholesky     jitter (float, optional):         The jitter to add to the diagonal of A in case A is only p.s.d. If omitted,         uses settings.cholesky_jitter.value()     max_tries (int, optional):         Number of attempts (with successively increasing jitter) to make before raising an error.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def psd_safe_cholesky(A, upper=False, out=None, jitter=None, max_tries=None):\n    \"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\n    Args:\n        A (Tensor):\n            The tensor to compute the Cholesky decomposition of\n        upper (bool, optional):\n            See torch.cholesky\n        out (Tensor, optional):\n            See torch.cholesky\n        jitter (float, optional):\n            The jitter to add to the diagonal of A in case A is only p.s.d. If omitted,\n            uses settings.cholesky_jitter.value()\n        max_tries (int, optional):\n            Number of attempts (with successively increasing jitter) to make before raising an error.\n    \"\"\"\n    L = _psd_safe_cholesky(A, out=out, jitter=jitter, max_tries=max_tries)\n    if upper:\n        if out is not None:\n            out = out.transpose_(-1, -2)\n        else:\n            L = L.transpose(-1, -2)\n    return L\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.standardize_x","title":"standardize_x","text":"<pre><code>standardize_x(\n    x: Tensor, x_min: Tensor = None, x_max: Tensor = None\n)\n</code></pre> <p>Standardize the vectorial input into a d-dimensional hypercube [0, 1]^d, where d is the number of features. if x_min ond x_max are supplied, x2 will be standardised using these instead. This is used when standardising the validation/test inputs.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def standardize_x(\n    x: torch.Tensor, x_min: torch.Tensor = None, x_max: torch.Tensor = None\n):\n    \"\"\"Standardize the vectorial input into a d-dimensional hypercube [0, 1]^d, where d is the number of features.\n    if x_min ond x_max are supplied, x2 will be standardised using these instead. This is used when standardising the\n    validation/test inputs.\n    \"\"\"\n    if (x_min is not None and x_max is None) or (x_min is None and x_max is not None):\n        raise ValueError(\n            \"Either *both* or *neither* of x_min, x_max need to be supplied!\"\n        )\n    if x_min is None:\n        x_min = torch.min(x, 0)[0]\n        x_max = torch.max(x, 0)[0]\n    x = (x - x_min) / (x_max - x_min)\n    return x, x_min, x_max\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp_hierarchy/#neps.optimizers.bayesian_optimization.models.gp_hierarchy.unnormalize_y","title":"unnormalize_y","text":"<pre><code>unnormalize_y(y, y_mean, y_std, scale_std=False)\n</code></pre> <p>Similar to the undoing of the pre-processing step above, but on the output predictions</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp_hierarchy.py</code> <pre><code>def unnormalize_y(y, y_mean, y_std, scale_std=False):\n    \"\"\"Similar to the undoing of the pre-processing step above, but on the output predictions\"\"\"\n    if not scale_std:\n        y = y * y_std + y_mean\n    else:\n        y *= y_std\n    return y\n</code></pre>"},{"location":"api/neps/optimizers/grid_search/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/optimizers/grid_search/optimizer/#neps.optimizers.grid_search.optimizer","title":"neps.optimizers.grid_search.optimizer","text":""},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/","title":"Dyhpo","text":""},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo","title":"neps.optimizers.multi_fidelity.dyhpo","text":""},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO","title":"MFEIBO","text":"<pre><code>MFEIBO(\n    pipeline_space: SearchSpace,\n    budget: int | None = None,\n    step_size: int | float = 1,\n    optimal_assignment: bool = False,\n    use_priors: bool = False,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    patience: int = 100,\n    ignore_errors: bool = False,\n    logger=None,\n    surrogate_model: str | Any = \"deep_gp\",\n    surrogate_model_args: dict | None = None,\n    domain_se_kernel: str | None = None,\n    graph_kernels: list | None = None,\n    hp_kernels: list | None = None,\n    acquisition: str | BaseAcquisition = acquisition,\n    acquisition_args: dict | None = None,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"freeze-thaw\",\n    acquisition_sampler_args: dict | None = None,\n    model_policy: Any = FreezeThawModel,\n    initial_design_fraction: float = 0.75,\n    initial_design_size: int = 10,\n    initial_design_budget: int | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Base class for MF-BO algorithms that use DyHPO-like acquisition and budgeting.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>use_priors</code> <p>Allows random samples to be generated from a default Samples generated from a Gaussian centered around the default value</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sampling_policy</code> <p>The type of sampling procedure to use</p> <p> </p> <code>promotion_policy</code> <p>The type of promotion procedure to use</p> <p> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>logger object, or None to use the neps logger</p> <p> DEFAULT: <code>None</code> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/multi_fidelity/dyhpo.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int | None = None,\n    step_size: int | float = 1,\n    optimal_assignment: bool = False,\n    use_priors: bool = False,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    patience: int = 100,\n    ignore_errors: bool = False,\n    logger=None,\n    # arguments for model\n    surrogate_model: str | Any = \"deep_gp\",\n    surrogate_model_args: dict | None = None,\n    domain_se_kernel: str | None = None,\n    graph_kernels: list | None = None,\n    hp_kernels: list | None = None,\n    acquisition: str | BaseAcquisition = acquisition,\n    acquisition_args: dict | None = None,\n    acquisition_sampler: str | AcquisitionSampler = \"freeze-thaw\",\n    acquisition_sampler_args: dict | None = None,\n    model_policy: Any = FreezeThawModel,\n    initial_design_fraction: float = 0.75,\n    initial_design_size: int = 10,\n    initial_design_budget: int | None = None,\n):\n    \"\"\"Initialise\n\n    Args:\n        pipeline_space: Space in which to search\n        budget: Maximum budget\n        use_priors: Allows random samples to be generated from a default\n            Samples generated from a Gaussian centered around the default value\n        sampling_policy: The type of sampling procedure to use\n        promotion_policy: The type of promotion procedure to use\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        logger: logger object, or None to use the neps logger\n        sample_default_first: Whether to sample the default configuration first\n    \"\"\"\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        patience=patience,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n    )\n    self.raw_tabular_space = None  # placeholder, can be populated using pre_load_hook\n    self._budget_list: list[int | float] = []\n    self.step_size: int | float = step_size\n    self.min_budget = self.pipeline_space.fidelity.lower\n    # TODO: generalize this to work with real data (not benchmarks)\n    self.max_budget = self.pipeline_space.fidelity.upper\n\n    self._initial_design_fraction = initial_design_fraction\n    (\n        self._initial_design_size,\n        self._initial_design_budget,\n    ) = self._set_initial_design(\n        initial_design_size, initial_design_budget, self._initial_design_fraction\n    )\n    # TODO: Write use cases for these parameters\n    self._model_update_failed = False\n    self.sample_default_first = sample_default_first\n    self.sample_default_at_target = sample_default_at_target\n\n    self.surrogate_model_name = surrogate_model\n\n    self.use_priors = use_priors\n    self.total_fevals: int = 0\n\n    self.observed_configs = MFObservedData(\n        columns=[\"config\", \"perf\", \"learning_curves\"],\n        index_names=[\"config_id\", \"budget_id\"],\n    )\n\n    # Preparing model\n    self.graph_kernels, self.hp_kernels = get_kernels(\n        pipeline_space=pipeline_space,\n        domain_se_kernel=domain_se_kernel,\n        graph_kernels=graph_kernels,\n        hp_kernels=hp_kernels,\n        optimal_assignment=optimal_assignment,\n    )\n    self.surrogate_model_args = (\n        {} if surrogate_model_args is None else surrogate_model_args\n    )\n    self._prep_model_args(self.hp_kernels, self.graph_kernels, pipeline_space)\n\n    # TODO: Better solution than branching based on the surrogate name is needed\n    if surrogate_model in [\"deep_gp\", \"gp\"]:\n        model_policy = FreezeThawModel\n    elif surrogate_model == \"pfn\":\n        model_policy = PFNSurrogate\n    else:\n        raise ValueError(\"Invalid model option selected!\")\n\n    # The surrogate model is initalized here\n    self.model_policy = model_policy(\n        pipeline_space=pipeline_space,\n        surrogate_model=surrogate_model,\n        surrogate_model_args=self.surrogate_model_args,\n    )\n    self.acquisition_args = {} if acquisition_args is None else acquisition_args\n    self.acquisition_args.update(\n        {\n            \"pipeline_space\": self.pipeline_space,\n            \"surrogate_model_name\": self.surrogate_model_name,\n        }\n    )\n    self.acquisition = instance_from_map(\n        AcquisitionMapping,\n        acquisition,\n        name=\"acquisition function\",\n        kwargs=self.acquisition_args,\n    )\n    self.acquisition_sampler_args = (\n        {} if acquisition_sampler_args is None else acquisition_sampler_args\n    )\n    self.acquisition_sampler_args.update(\n        {\"patience\": self.patience, \"pipeline_space\": self.pipeline_space}\n    )\n    self.acquisition_sampler = instance_from_map(\n        AcquisitionSamplerMapping,\n        acquisition_sampler,\n        name=\"acquisition sampler function\",\n        kwargs=self.acquisition_sampler_args,\n    )\n    self.count = 0\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/dyhpo.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    config_id = None\n    previous_config_id = None\n    if self.is_init_phase(budget_based=False):\n        # sample a new config till initial design size is satisfied\n        self.logger.info(\"sampling...\")\n        config = self.pipeline_space.sample(\n            patience=self.patience, user_priors=True, ignore_fidelity=False\n        )\n        assert config.fidelity is not None\n        config.fidelity.set_value(self.min_budget)\n\n        _config_id = self.observed_configs.next_config_id()\n    elif self.is_init_phase(budget_based=True) or self._model_update_failed:\n        # promote a config randomly if initial design size is satisfied but the\n        # initial design budget has not been exhausted\n        self.logger.info(\"promoting...\")\n        config, _config_id = self._randomly_promote()\n    else:\n        if self.count == 0:\n            self.logger.info(\"\\nPartial learning curves as initial design:\\n\")\n            self.logger.info(f\"{self.observed_configs.get_learning_curves()}\\n\")\n        self.count += 1\n        # main acquisition call here after initial design is turned off\n        self.logger.info(\"acquiring...\")\n        # generates candidate samples for acquisition calculation\n        assert self.pipeline_space.fidelity is not None\n        samples = self.acquisition_sampler.sample(\n            set_new_sample_fidelity=self.pipeline_space.fidelity.lower\n        )  # fidelity values here should be the observations or min. fidelity\n        # calculating acquisition function values for the candidate samples\n        acq, _samples = self.acquisition.eval(  # type: ignore[attr-defined]\n            x=samples, asscalar=True\n        )\n        # maximizing acquisition function\n        _idx = np.argsort(acq)[-1]\n        # extracting the config ID for the selected maximizer\n        _config_id = samples.index[_samples.index.values[_idx]]\n        # `_samples` should have new configs with fidelities set to as required\n        # NOTE: len(samples) need not be equal to len(_samples) as `samples` contain\n        # all (partials + new) configurations obtained from the sampler, but\n        # in `_samples`, configs are removed that have reached maximum epochs allowed\n        # NOTE: `samples` and `_samples` should share the same index values, hence,\n        # avoid using `.iloc` and work with `.loc` on pandas DataFrame/Series\n\n        # Is this \"config = _samples.loc[_config_id]\"?\n        config = samples.loc[_config_id]\n        config.fidelity.set_value(_samples.loc[_config_id].fidelity.value)\n    # generating correct IDs\n    if _config_id in self.observed_configs.seen_config_ids:\n        config_id = f\"{_config_id}_{self.get_budget_level(config)}\"\n        previous_config_id = f\"{_config_id}_{self.get_budget_level(config) - 1}\"\n    else:\n        config_id = f\"{self.observed_configs.next_config_id()}_{self.get_budget_level(config)}\"\n\n    return config.hp_values(), config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO.load_results","title":"load_results","text":"<pre><code>load_results(\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None\n</code></pre> <p>This is basically the fit method.</p> PARAMETER DESCRIPTION <code>previous_results</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> <code>pending_evaluations</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/dyhpo.py</code> <pre><code>def load_results(\n    self,\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None:\n    \"\"\"This is basically the fit method.\n\n    Args:\n        previous_results (dict[str, ConfigResult]): [description]\n        pending_evaluations (dict[str, ConfigResult]): [description]\n    \"\"\"\n    self.observed_configs = MFObservedData(\n        columns=[\"config\", \"perf\", \"learning_curves\"],\n        index_names=[\"config_id\", \"budget_id\"],\n    )\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(previous_results)\n    self.total_fevals = len(previous_results) + len(pending_evaluations)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending_evaluations)\n\n    # an aesthetic choice more than a functional choice\n    self.observed_configs.df.sort_index(\n        level=self.observed_configs.df.index.names, inplace=True\n    )\n\n    # TODO: can we do better than keeping a copy of the observed configs?\n    # TODO: can we not hide this in load_results and have something that pops out\n    #   more, like a set_state or policy_args\n    self.model_policy.observed_configs = self.observed_configs\n    # fit any model/surrogates\n    init_phase = self.is_init_phase()\n    if not init_phase:\n        self._fit_models()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/dyhpo/#neps.optimizers.multi_fidelity.dyhpo.MFEIBO.total_budget_spent","title":"total_budget_spent","text":"<pre><code>total_budget_spent() -&gt; int | float\n</code></pre> <p>Calculates the toal budget spent so far.</p> <p>This is calculated as a function of the fidelity range provided, that takes into account the minimum budget and the step size.</p> Source code in <code>neps/optimizers/multi_fidelity/dyhpo.py</code> <pre><code>def total_budget_spent(self) -&gt; int | float:\n    \"\"\"Calculates the toal budget spent so far.\n\n    This is calculated as a function of the fidelity range provided, that takes into\n    account the minimum budget and the step size.\n    \"\"\"\n    if len(self.observed_configs.df) == 0:\n        return 0\n\n    n_configs = len(self.observed_configs.seen_config_ids)\n    total_budget_level = sum(self.observed_configs.seen_budget_levels)\n    total_initial_budget_spent = n_configs * self.pipeline_space.fidelity.lower\n    total_budget_spent = (\n        total_initial_budget_spent + total_budget_level * self.step_size\n    )\n\n    return total_budget_spent\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/","title":"Hyperband","text":""},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband","title":"neps.optimizers.multi_fidelity.hyperband","text":""},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband","title":"AsynchronousHyperband","text":"<pre><code>AsynchronousHyperband(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>HyperbandBase</code></p> <p>Implements ASHA but as Hyperband.</p> <p>Implements the Promotion variant of ASHA as used in Mobster.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: typing.Any = RandomUniformPolicy,\n    promotion_policy: typing.Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    args = dict(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        use_priors=use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    super().__init__(**args)\n    # overwrite parent class SH brackets with Async SH brackets\n    self.sh_brackets = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        # key difference from vanilla HB where it runs synchronous SH brackets\n        self.sh_brackets[s] = AsynchronousSuccessiveHalving(**args)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\"\"\"\n    # unlike synchronous SH, the state is not reset at each rung and a configuration\n    # is promoted if the rung has eta configs if it is the top performing\n    # base class allows for retaining the whole optimization state\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    # the rung to sample at\n    bracket_to_run = self._get_bracket_to_run()\n    config, config_id, previous_config_id = self.sh_brackets[\n        bracket_to_run\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors","title":"AsynchronousHyperbandWithPriors","text":"<pre><code>AsynchronousHyperbandWithPriors(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>AsynchronousHyperband</code></p> <p>Implements ASHA but as Hyperband.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = FixedPriorPolicy,\n    promotion_policy: typing.Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,  # key change to the base Async HB class\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\"\"\"\n    # unlike synchronous SH, the state is not reset at each rung and a configuration\n    # is promoted if the rung has eta configs if it is the top performing\n    # base class allows for retaining the whole optimization state\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    # the rung to sample at\n    bracket_to_run = self._get_bracket_to_run()\n    config, config_id, previous_config_id = self.sh_brackets[\n        bracket_to_run\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband","title":"Hyperband","text":"<pre><code>Hyperband(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>HyperbandBase</code></p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: typing.Any = RandomUniformPolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    args = dict(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=self.early_stopping_rate,  # HB subsumes this param of SH\n        initial_design_type=initial_design_type,\n        use_priors=use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    super().__init__(**args)\n    # stores the flattened sequence of SH brackets to loop over - the HB heuristic\n    # for (n,r) pairing, i.e., (num. configs, fidelity)\n    self.full_rung_trace = []\n    self.sh_brackets = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        self.sh_brackets[s] = SuccessiveHalving(**args)\n        # `full_rung_trace` contains the index of SH bracket to run sequentially\n        self.full_rung_trace.extend([s] * len(self.sh_brackets[s].full_rung_trace))\n    # book-keeping variables\n    self.current_sh_bracket = None  # type: ignore\n    self.old_history_len = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    config, config_id, previous_config_id = self.sh_brackets[\n        self.current_sh_bracket  # type: ignore\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase","title":"HyperbandBase","text":"<pre><code>HyperbandBase(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalvingBase</code></p> <p>Implements a Hyperband procedure with a sampling and promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: typing.Any = RandomUniformPolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    args = dict(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=self.early_stopping_rate,  # HB subsumes this param of SH\n        initial_design_type=initial_design_type,\n        use_priors=use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    super().__init__(**args)\n    # stores the flattened sequence of SH brackets to loop over - the HB heuristic\n    # for (n,r) pairing, i.e., (num. configs, fidelity)\n    self.full_rung_trace = []\n    self.sh_brackets = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        self.sh_brackets[s] = SuccessiveHalving(**args)\n        # `full_rung_trace` contains the index of SH bracket to run sequentially\n        self.full_rung_trace.extend([s] * len(self.sh_brackets[s].full_rung_trace))\n    # book-keeping variables\n    self.current_sh_bracket = None  # type: ignore\n    self.old_history_len = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\"\"\"\n    # unlike synchronous SH, the state is not reset at each rung and a configuration\n    # is promoted if the rung has eta configs if it is the top performing\n    # base class allows for retaining the whole optimization state\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault","title":"HyperbandCustomDefault","text":"<pre><code>HyperbandCustomDefault(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>HyperbandWithPriors</code></p> <p>If prior specified, does 50% times priors and 50% random search like vanilla-HB.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = EnsemblePolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    self.sampling_args = {\n        \"inc\": None,\n        \"weights\": {\n            \"prior\": 0.5,\n            \"inc\": 0,\n            \"random\": 0.5,\n        },\n    }\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_args = self.sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    config, config_id, previous_config_id = self.sh_brackets[\n        self.current_sh_bracket  # type: ignore\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors","title":"HyperbandWithPriors","text":"<pre><code>HyperbandWithPriors(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>Hyperband</code></p> <p>Implements a Hyperband procedure with a sampling and promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = FixedPriorPolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,  # key change to the base HB class\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    config, config_id, previous_config_id = self.sh_brackets[\n        self.current_sh_bracket  # type: ignore\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/","title":"Mf bo","text":""},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo","title":"neps.optimizers.multi_fidelity.mf_bo","text":""},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.FreezeThawModel","title":"FreezeThawModel","text":"<pre><code>FreezeThawModel(\n    pipeline_space,\n    surrogate_model: str = \"deep_gp\",\n    surrogate_model_args: dict = None,\n)\n</code></pre> <p>Designed to work with model search in unit step multi-fidelity algorithms.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space,\n    surrogate_model: str = \"deep_gp\",\n    surrogate_model_args: dict = None,\n):\n    self.observed_configs = None\n    self.pipeline_space = pipeline_space\n    self.surrogate_model_name = surrogate_model\n    self.surrogate_model_args = (\n        surrogate_model_args if surrogate_model_args is not None else {}\n    )\n    if self.surrogate_model_name in [\"deep_gp\", \"pfn\"]:\n        self.surrogate_model_args.update({\"pipeline_space\": pipeline_space})\n\n    # instantiate the surrogate model\n    self.surrogate_model = instance_from_map(\n        SurrogateModelMapping,\n        self.surrogate_model_name,\n        name=\"surrogate model\",\n        kwargs=self.surrogate_model_args,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.MFBOBase","title":"MFBOBase","text":"<p>Designed to work with model-based search on SH-based multi-fidelity algorithms.</p> <p>Requires certain strict assumptions about fidelities and rung maps.</p>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.MFBOBase.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        resources /= self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.MFBOBase.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(rung: int = None, **kwargs)\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int = None,\n    **kwargs,\n):\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            fidelity = active_max_fidelity = None\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = self.pipeline_space.sample(\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.PFNSurrogate","title":"PFNSurrogate","text":"<pre><code>PFNSurrogate(*args, **kwargs)\n</code></pre> <p>               Bases: <code>FreezeThawModel</code></p> <p>Special class to deal with PFN surrogate model and freeze-thaw acquisition.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.train_x = None\n    self.train_y = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/","title":"Promotion policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy","title":"neps.optimizers.multi_fidelity.promotion_policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.AsyncPromotionPolicy","title":"AsyncPromotionPolicy","text":"<pre><code>AsyncPromotionPolicy(eta, **kwargs)\n</code></pre> <p>               Bases: <code>PromotionPolicy</code></p> <p>Implements an asynchronous promotion from lower to higher fidelity.</p> <p>Promotes whenever a higher fidelity has at least eta configurations.</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def __init__(self, eta, **kwargs):\n    super().__init__(eta, **kwargs)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.AsyncPromotionPolicy.retrieve_promotions","title":"retrieve_promotions","text":"<pre><code>retrieve_promotions() -&gt; dict\n</code></pre> <p>Returns the top 1/eta configurations per rung if enough configurations seen</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def retrieve_promotions(self) -&gt; dict:\n    \"\"\"Returns the top 1/eta configurations per rung if enough configurations seen\"\"\"\n    for rung in range(self.max_rung + 1):\n        if rung == self.max_rung:\n            # cease promotions for the highest rung (configs at max budget)\n            continue\n        # if less than eta configurations seen, no promotions occur as top_k=0\n        top_k = len(self.rung_members_performance[rung]) // self.eta\n        _ordered_idx = np.argsort(self.rung_members_performance[rung])\n        self.rung_promotions[rung] = np.array(self.rung_members[rung])[_ordered_idx][\n            :top_k\n        ].tolist()\n    return self.rung_promotions\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.PromotionPolicy","title":"PromotionPolicy","text":"<pre><code>PromotionPolicy(eta: int)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for implementing a sampling straregy for SH and its subclasses</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def __init__(self, eta: int):\n    self.rung_members: dict = {}\n    self.rung_members_performance: dict = {}\n    self.rung_promotions: dict = {}\n    self.eta = eta  # type: int\n    self.max_rung: int = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.SyncPromotionPolicy","title":"SyncPromotionPolicy","text":"<pre><code>SyncPromotionPolicy(eta, **kwargs)\n</code></pre> <p>               Bases: <code>PromotionPolicy</code></p> <p>Implements a synchronous promotion from lower to higher fidelity.</p> <p>Promotes only when all predefined number of config slots are full.</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def __init__(self, eta, **kwargs):\n    super().__init__(eta, **kwargs)\n    self.config_map: dict = None\n    self.rung_promotions = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.SyncPromotionPolicy.retrieve_promotions","title":"retrieve_promotions","text":"<pre><code>retrieve_promotions() -&gt; dict\n</code></pre> <p>Returns the top 1/eta configurations per rung if enough configurations seen</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def retrieve_promotions(self) -&gt; dict:\n    \"\"\"Returns the top 1/eta configurations per rung if enough configurations seen\"\"\"\n    assert self.config_map is not None\n\n    self.rung_promotions = {rung: [] for rung in self.config_map.keys()}\n    total_rung_evals = 0\n    for rung in reversed(sorted(self.config_map.keys())):\n        total_rung_evals += len(self.rung_members[rung])\n        if (\n            total_rung_evals &gt;= self.config_map[rung]\n            and np.isnan(self.rung_members_performance[rung]).sum()\n        ):\n            # if rung is full but incomplete evaluations, pause on promotions, wait\n            return self.rung_promotions\n        if rung == self.max_rung:\n            # cease promotions for the highest rung (configs at max budget)\n            continue\n        if (\n            total_rung_evals &gt;= self.config_map[rung]\n            and np.isnan(self.rung_members_performance[rung]).sum() == 0\n        ):\n            # if rung is full and no incomplete evaluations, find promotions\n            top_k = (self.config_map[rung] // self.eta) - (\n                self.config_map[rung] - len(self.rung_members[rung])\n            )\n            selected_idx = np.argsort(self.rung_members_performance[rung])[:top_k]\n            self.rung_promotions[rung] = self.rung_members[rung][selected_idx]\n    return self.rung_promotions\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/","title":"Sampling policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy","title":"neps.optimizers.multi_fidelity.sampling_policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.EnsemblePolicy","title":"EnsemblePolicy","text":"<pre><code>EnsemblePolicy(\n    pipeline_space: SearchSpace,\n    inc_type: str = \"mutation\",\n    logger=None,\n)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>Ensemble of sampling policies including sampling randomly, from prior &amp; incumbent.</p> PARAMETER DESCRIPTION <code>SamplingPolicy</code> <p>[description]</p> <p> TYPE: <code>[type]</code> </p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>inc_type</code> <p>str if \"hypersphere\", uniformly samples from around the incumbent within its     distance from the nearest neighbour in history if \"gaussian\", samples from a gaussian around the incumbent if \"crossover\", generates a config by crossover between a random sample     and the incumbent if \"mutation\", generates a config by perturbing each hyperparameter with     50% (mutation_rate=0.5) probability of selecting each hyperparmeter     for perturbation, sampling a deviation N(value, mutation_std=0.5))</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mutation'</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    inc_type: str = \"mutation\",\n    logger=None,\n):\n    \"\"\"Samples a policy as per its weights and performs the selected sampling.\n\n    Args:\n        pipeline_space: Space in which to search\n        inc_type: str\n            if \"hypersphere\", uniformly samples from around the incumbent within its\n                distance from the nearest neighbour in history\n            if \"gaussian\", samples from a gaussian around the incumbent\n            if \"crossover\", generates a config by crossover between a random sample\n                and the incumbent\n            if \"mutation\", generates a config by perturbing each hyperparameter with\n                50% (mutation_rate=0.5) probability of selecting each hyperparmeter\n                for perturbation, sampling a deviation N(value, mutation_std=0.5))\n    \"\"\"\n    super().__init__(pipeline_space=pipeline_space, logger=logger)\n    self.inc_type = inc_type\n    # setting all probabilities uniformly\n    self.policy_map = {\"random\": 0.33, \"prior\": 0.34, \"inc\": 0.33}\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.EnsemblePolicy.sample","title":"sample","text":"<pre><code>sample(\n    inc: SearchSpace = None,\n    weights: dict[str, float] = None,\n    *args,\n    **kwargs\n) -&gt; SearchSpace\n</code></pre> <p>Samples from the prior with a certain probability</p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>[description]</p> <p> TYPE: <code>SearchSpace</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample(\n    self, inc: SearchSpace = None, weights: dict[str, float] = None, *args, **kwargs\n) -&gt; SearchSpace:\n    \"\"\"Samples from the prior with a certain probability\n\n    Returns:\n        SearchSpace: [description]\n    \"\"\"\n    if weights is not None:\n        for key, value in sorted(weights.items()):\n            self.policy_map[key] = value\n    else:\n        self.logger.info(f\"Using default policy weights: {self.policy_map}\")\n    prob_weights = [v for _, v in sorted(self.policy_map.items())]\n    policy_idx = np.random.choice(range(len(prob_weights)), p=prob_weights)\n    policy = sorted(self.policy_map.keys())[policy_idx]\n\n    self.logger.info(\n        f\"Sampling from {policy} with weights (i, p, r)={prob_weights}\"\n    )\n\n    if policy == \"prior\":\n        config = self.pipeline_space.sample(\n            patience=self.patience, user_priors=True, ignore_fidelity=True\n        )\n    elif policy == \"inc\":\n\n        if (\n            hasattr(self.pipeline_space, \"has_prior\")\n            and self.pipeline_space.has_prior\n        ):\n            user_priors = True\n        else:\n            user_priors = False\n\n        if inc is None:\n            inc = self.pipeline_space.sample_default_configuration().clone()\n            self.logger.warning(\n                \"No incumbent config found, using default as the incumbent.\"\n            )\n\n        if self.inc_type == \"hypersphere\":\n            distance = kwargs[\"distance\"]\n            config = self.sample_neighbour(inc, distance)\n        elif self.inc_type == \"gaussian\":\n            # use inc to set the defaults of the configuration\n            _inc = inc.clone()\n            _inc.set_defaults_to_current_values()\n            # then sample with prior=True from that configuration\n            # since the defaults are treated as the prior\n            config = _inc.sample(\n                patience=self.patience,\n                user_priors=user_priors,\n                ignore_fidelity=True,\n            )\n        elif self.inc_type == \"crossover\":\n            # choosing the configuration for crossover with incumbent\n            # the weight distributed across prior adnd inc\n            _w_priors = 1 - self.policy_map[\"random\"]\n            # re-calculate normalized score ratio for prior-inc\n            w_prior = np.clip(\n                self.policy_map[\"prior\"] / _w_priors, a_min=0, a_max=1\n            )\n            w_inc = np.clip(self.policy_map[\"inc\"] / _w_priors, a_min=0, a_max=1)\n            # calculating difference of prior and inc score\n            score_diff = np.abs(w_prior - w_inc)\n            # using the difference in score as the weight of what to sample when\n            # if the score difference is small, crossover between incumbent and prior\n            # if the score difference is large, crossover between incumbent and random\n            probs = [1 - score_diff, score_diff]  # the order is [prior, random]\n            user_priors = np.random.choice([True, False], p=probs)\n            if (\n                hasattr(self.pipeline_space, \"has_prior\")\n                and not self.pipeline_space.has_prior\n            ):\n                user_priors = False\n            self.logger.info(\n                f\"Crossing over with user_priors={user_priors} with p={probs}\"\n            )\n            # sampling a configuration either randomly or from a prior\n            _config = self.pipeline_space.sample(\n                patience=self.patience,\n                user_priors=user_priors,\n                ignore_fidelity=True,\n            )\n            # injecting hyperparameters from the sampled config into the incumbent\n            # TODO: ideally lower crossover prob overtime\n            config = custom_crossover(inc, _config, crossover_prob=0.5)\n        elif self.inc_type == \"mutation\":\n            if \"inc_mutation_rate\" in kwargs:\n                config = local_mutation(\n                    inc,\n                    mutation_rate=kwargs[\"inc_mutation_rate\"],\n                    std=kwargs[\"inc_mutation_std\"],\n                )\n            else:\n                config = local_mutation(inc)\n        else:\n            raise ValueError(\n                f\"{self.inc_type} is not in \"\n                f\"{{'mutation', 'crossover', 'hypersphere', 'gaussian'}}\"\n            )\n    else:\n        # random\n        config = self.pipeline_space.sample(\n            patience=self.patience, user_priors=False, ignore_fidelity=True\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.EnsemblePolicy.sample_neighbour","title":"sample_neighbour","text":"<pre><code>sample_neighbour(incumbent, distance, tolerance=TOLERANCE)\n</code></pre> <p>Samples a config from around the <code>incumbent</code> within radius as <code>distance</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample_neighbour(self, incumbent, distance, tolerance=TOLERANCE):\n    \"\"\"Samples a config from around the `incumbent` within radius as `distance`.\"\"\"\n    # TODO: how does tolerance affect optimization on landscapes of different scale\n    sample_counter = 0\n    while True:\n        # sampling a config\n        config = self.pipeline_space.sample(\n            patience=self.patience, user_priors=False, ignore_fidelity=False\n        )\n        # computing distance from incumbent\n        d = compute_config_dist(config, incumbent)\n        # checking if sample is within the hypersphere around the incumbent\n        if d &lt; max(distance, tolerance):\n            # accept sample\n            break\n        sample_counter += 1\n        if sample_counter &gt; SAMPLE_THRESHOLD:\n            # reset counter for next increased radius for hypersphere\n            sample_counter = 0\n            # if no sample falls within the radius, increase the threshold radius 1%\n            distance += distance * DELTA_THRESHOLD\n    # end of while\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.FixedPriorPolicy","title":"FixedPriorPolicy","text":"<pre><code>FixedPriorPolicy(\n    pipeline_space: SearchSpace,\n    fraction_from_prior: float = 1,\n    logger=None,\n)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>A random policy for sampling configuration, i.e. the default for SH but samples a fixed fraction from the prior.</p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(\n    self, pipeline_space: SearchSpace, fraction_from_prior: float = 1, logger=None\n):\n    super().__init__(pipeline_space=pipeline_space, logger=logger)\n    assert 0 &lt;= fraction_from_prior &lt;= 1\n    self.fraction_from_prior = fraction_from_prior\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.FixedPriorPolicy.sample","title":"sample","text":"<pre><code>sample(*args, **kwargs) -&gt; SearchSpace\n</code></pre> <p>Samples from the prior with a certain probabiliyu</p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>[description]</p> <p> TYPE: <code>SearchSpace</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample(self, *args, **kwargs) -&gt; SearchSpace:\n    \"\"\"Samples from the prior with a certain probabiliyu\n\n    Returns:\n        SearchSpace: [description]\n    \"\"\"\n    user_priors = False\n    if np.random.uniform() &lt; self.fraction_from_prior:\n        user_priors = True\n    config = self.pipeline_space.sample(\n        patience=self.patience, user_priors=user_priors, ignore_fidelity=True\n    )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.ModelPolicy","title":"ModelPolicy","text":"<pre><code>ModelPolicy(\n    pipeline_space: SearchSpace,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str = None,\n    graph_kernels: list = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"random\",\n    patience: int = 100,\n    logger=None,\n)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>A policy for sampling configuration, i.e. the default for SH / hyperband</p> PARAMETER DESCRIPTION <code>SamplingPolicy</code> <p>[description]</p> <p> TYPE: <code>[type]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str = None,\n    graph_kernels: list = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str | AcquisitionSampler = \"random\",\n    patience: int = 100,\n    logger=None,\n):\n    super().__init__(pipeline_space=pipeline_space, logger=logger)\n\n    surrogate_model_args = surrogate_model_args or {}\n\n    graph_kernels, hp_kernels = get_kernels(\n        pipeline_space=pipeline_space,\n        domain_se_kernel=domain_se_kernel,\n        graph_kernels=graph_kernels,\n        hp_kernels=hp_kernels,\n        optimal_assignment=False,\n    )\n    if \"graph_kernels\" not in surrogate_model_args:\n        surrogate_model_args[\"graph_kernels\"] = None\n    if \"hp_kernels\" not in surrogate_model_args:\n        surrogate_model_args[\"hp_kernels\"] = hp_kernels\n    if not surrogate_model_args[\"hp_kernels\"]:\n        raise ValueError(\"No kernels are provided!\")\n    if \"vectorial_features\" not in surrogate_model_args:\n        surrogate_model_args[\n            \"vectorial_features\"\n        ] = pipeline_space.get_vectorial_dim()\n\n    self.surrogate_model = instance_from_map(\n        SurrogateModelMapping,\n        surrogate_model,\n        name=\"surrogate model\",\n        kwargs=surrogate_model_args,\n    )\n\n    self.acquisition = instance_from_map(\n        AcquisitionMapping,\n        acquisition,\n        name=\"acquisition function\",\n    )\n\n    # TODO: Enable only when a flag exists to toggle prior-based decaying of AF\n    # if pipeline_space.has_prior:\n    #     self.acquisition = DecayingPriorWeightedAcquisition(\n    #         self.acquisition, log=log_prior_weighted\n    #     )\n\n    self.acquisition_sampler = instance_from_map(\n        AcquisitionSamplerMapping,\n        acquisition_sampler,\n        name=\"acquisition sampler function\",\n        kwargs={\"patience\": patience, \"pipeline_space\": pipeline_space},\n    )\n\n    self.sampling_args: dict = {}\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.ModelPolicy.sample","title":"sample","text":"<pre><code>sample(\n    active_max_fidelity: int = None,\n    fidelity: int = None,\n    **kwargs\n) -&gt; SearchSpace\n</code></pre> <p>Performs the equivalent of optimizing the acquisition function.</p> Performs 2 strategies as per the arguments passed <ul> <li>If fidelity is not None, triggers the case when the surrogate has been   trained jointly with the fidelity dimension, i.e., all observations ever   recorded. In this case, the EI for random samples is evaluated at the   <code>fidelity</code> where the new sample will be evaluated. The top-10 are selected,   and the EI for them is evaluated at the target/mmax fidelity.</li> <li>If active_max_fidelity is not None, triggers the case when a surrogate is   trained per fidelity. In this case, all samples have their fidelity   variable set to the same value. This value is same as that of the fidelity   value of the configs in the training data.</li> </ul> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample(\n    self, active_max_fidelity: int = None, fidelity: int = None, **kwargs\n) -&gt; SearchSpace:\n    \"\"\"Performs the equivalent of optimizing the acquisition function.\n\n    Performs 2 strategies as per the arguments passed:\n        * If fidelity is not None, triggers the case when the surrogate has been\n          trained jointly with the fidelity dimension, i.e., all observations ever\n          recorded. In this case, the EI for random samples is evaluated at the\n          `fidelity` where the new sample will be evaluated. The top-10 are selected,\n          and the EI for them is evaluated at the target/mmax fidelity.\n        * If active_max_fidelity is not None, triggers the case when a surrogate is\n          trained per fidelity. In this case, all samples have their fidelity\n          variable set to the same value. This value is same as that of the fidelity\n          value of the configs in the training data.\n    \"\"\"\n    self.logger.info(\"Acquiring...\")\n\n    # sampling random configurations\n    samples = [\n        self.pipeline_space.sample(user_priors=False, ignore_fidelity=True)\n        for _ in range(SAMPLE_THRESHOLD)\n    ]\n\n    if fidelity is not None:\n        # w/o setting this flag, the AF eval will set all fidelities to max\n        self.acquisition.optimize_on_max_fidelity = False\n        _inc_copy = self.acquisition.incumbent\n        # TODO: better design required, for example, not import torch\n        #  right now this case handles the 2-step acquisition in `sample`\n        if \"incumbent\" in kwargs:\n            # sets the incumbent to the best score at the required fidelity for\n            # correct computation of EI scores\n            self.acquisition.incumbent = torch.tensor(kwargs[\"incumbent\"])\n        # updating the fidelity of the sampled configurations\n        samples = list(map(update_fidelity, samples, [fidelity] * len(samples)))\n        # computing EI at the given `fidelity`\n        eis = self.acquisition.eval(x=samples, asscalar=True)\n        # extracting the 10 highest scores\n        _ids = np.argsort(eis)[-TOP_EI_SAMPLE_COUNT:]\n        samples = pd.Series(samples).iloc[_ids].values.tolist()\n        # setting the fidelity to the maximum fidelity\n        self.acquisition.optimize_on_max_fidelity = True\n        self.acquisition.incumbent = _inc_copy\n\n    if active_max_fidelity is not None:\n        # w/o setting this flag, the AF eval will set all fidelities to max\n        self.acquisition.optimize_on_max_fidelity = False\n        fidelity = active_max_fidelity\n        samples = list(map(update_fidelity, samples, [fidelity] * len(samples)))\n\n    # computes the EI for all `samples`\n    eis = self.acquisition.eval(x=samples, asscalar=True)\n    # extracting the highest scored sample\n    config = samples[np.argmax(eis)]\n    # TODO: can generalize s.t. sampler works for all types, currently,\n    #  random sampler in NePS does not do what is required here\n    # return self.acquisition_sampler.sample(self.acquisition)\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.RandomUniformPolicy","title":"RandomUniformPolicy","text":"<pre><code>RandomUniformPolicy(\n    pipeline_space: SearchSpace, logger=None\n)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>A random policy for sampling configuration, i.e. the default for SH / hyperband</p> PARAMETER DESCRIPTION <code>SamplingPolicy</code> <p>[description]</p> <p> TYPE: <code>[type]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    logger=None,\n):\n    super().__init__(pipeline_space=pipeline_space, logger=logger)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.SamplingPolicy","title":"SamplingPolicy","text":"<pre><code>SamplingPolicy(\n    pipeline_space: SearchSpace,\n    patience: int = 100,\n    logger=None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for implementing a sampling strategy for SH and its subclasses</p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(self, pipeline_space: SearchSpace, patience: int = 100, logger=None):\n    self.pipeline_space = pipeline_space\n    self.patience = patience\n    self.logger = logger or logging.getLogger(\"neps\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/","title":"Successive halving","text":""},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving","title":"neps.optimizers.multi_fidelity.successive_halving","text":""},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving","title":"AsynchronousSuccessiveHalving","text":"<pre><code>AsynchronousSuccessiveHalving(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalvingBase</code></p> <p>Implements ASHA with a sampling and asynchronous promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: typing.Any = RandomUniformPolicy,\n    promotion_policy: typing.Any = AsyncPromotionPolicy,  # key difference from SH\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        use_priors=use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n        config.fidelity.set_value(self.rung_map[rung])\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                self.logger.info(\"Next config will be evaluated at target fidelity.\")\n            self.logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.sample_default_configuration()\n\n        elif random.random() &lt; self.random_interleave_prob:\n            config = self.pipeline_space.sample(\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config.fidelity.set_value(fidelity_value)\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config.hp_values(), config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.load_results","title":"load_results","text":"<pre><code>load_results(\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None\n</code></pre> <p>This is basically the fit method.</p> PARAMETER DESCRIPTION <code>previous_results</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> <code>pending_evaluations</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def load_results(\n    self,\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None:\n    \"\"\"This is basically the fit method.\n\n    Args:\n        previous_results (dict[str, ConfigResult]): [description]\n        pending_evaluations (dict[str, ConfigResult]): [description]\n    \"\"\"\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(previous_results)\n    self.total_fevals = len(previous_results) + len(pending_evaluations)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending_evaluations)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors","title":"AsynchronousSuccessiveHalvingWithPriors","text":"<pre><code>AsynchronousSuccessiveHalvingWithPriors(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>AsynchronousSuccessiveHalving</code></p> <p>Implements ASHA with a sampling and asynchronous promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = FixedPriorPolicy,\n    promotion_policy: typing.Any = AsyncPromotionPolicy,  # key difference from SH\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n        config.fidelity.set_value(self.rung_map[rung])\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                self.logger.info(\"Next config will be evaluated at target fidelity.\")\n            self.logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.sample_default_configuration()\n\n        elif random.random() &lt; self.random_interleave_prob:\n            config = self.pipeline_space.sample(\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config.fidelity.set_value(fidelity_value)\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config.hp_values(), config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.load_results","title":"load_results","text":"<pre><code>load_results(\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None\n</code></pre> <p>This is basically the fit method.</p> PARAMETER DESCRIPTION <code>previous_results</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> <code>pending_evaluations</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def load_results(\n    self,\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None:\n    \"\"\"This is basically the fit method.\n\n    Args:\n        previous_results (dict[str, ConfigResult]): [description]\n        pending_evaluations (dict[str, ConfigResult]): [description]\n    \"\"\"\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(previous_results)\n    self.total_fevals = len(previous_results) + len(pending_evaluations)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending_evaluations)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving","title":"SuccessiveHalving","text":"<pre><code>SuccessiveHalving(\n    pipeline_space: SearchSpace,\n    budget: int = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalvingBase</code></p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>eta</code> <p>The reduction factor used by SH</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>early_stopping_rate</code> <p>Determines the number of rungs in an SH bracket Choosing 0 creates maximal rungs given the fidelity bounds</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>initial_design_type</code> <p>Type of initial design to switch to BO Legacy parameter from NePS BO design. Could be used to extend to MF-BO.</p> <p> TYPE: <code>Literal['max_budget', 'unique_configs']</code> DEFAULT: <code>'max_budget'</code> </p> <code>use_priors</code> <p>Allows random samples to be generated from a default Samples generated from a Gaussian centered around the default value</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sampling_policy</code> <p>The type of sampling procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>RandomUniformPolicy</code> </p> <code>promotion_policy</code> <p>The type of promotion procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>SyncPromotionPolicy</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>logger object, or None to use the neps logger</p> <p> DEFAULT: <code>None</code> </p> <code>prior_confidence</code> <p>The range of confidence to have on the prior The higher the confidence, the smaller is the standard deviation of the prior distribution centered around the default</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>None</code> </p> <code>random_interleave_prob</code> <p>Chooses the fraction of samples from random vs prior</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sample_default_at_target</code> <p>Whether to evaluate the default configuration at the target fidelity or max budget</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: typing.Any = RandomUniformPolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    \"\"\"Initialise an SH bracket.\n\n    Args:\n        pipeline_space: Space in which to search\n        budget: Maximum budget\n        eta: The reduction factor used by SH\n        early_stopping_rate: Determines the number of rungs in an SH bracket\n            Choosing 0 creates maximal rungs given the fidelity bounds\n        initial_design_type: Type of initial design to switch to BO\n            Legacy parameter from NePS BO design. Could be used to extend to MF-BO.\n        use_priors: Allows random samples to be generated from a default\n            Samples generated from a Gaussian centered around the default value\n        sampling_policy: The type of sampling procedure to use\n        promotion_policy: The type of promotion procedure to use\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        logger: logger object, or None to use the neps logger\n        prior_confidence: The range of confidence to have on the prior\n            The higher the confidence, the smaller is the standard deviation of the\n            prior distribution centered around the default\n        random_interleave_prob: Chooses the fraction of samples from random vs prior\n        sample_default_first: Whether to sample the default configuration first\n        sample_default_at_target: Whether to evaluate the default configuration at\n            the target fidelity or max budget\n    \"\"\"\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n    )\n    if random_interleave_prob &lt; 0 or random_interleave_prob &gt; 1:\n        raise ValueError(\"random_interleave_prob should be in [0.0, 1.0]\")\n    self.random_interleave_prob = random_interleave_prob\n    self.sample_default_first = sample_default_first\n    self.sample_default_at_target = sample_default_at_target\n\n    self.min_budget = self.pipeline_space.fidelity.lower\n    self.max_budget = self.pipeline_space.fidelity.upper\n    self.eta = eta\n    # SH implicitly sets early_stopping_rate to 0\n    # the parameter is exposed to allow HB to call SH with different stopping rates\n    self.early_stopping_rate = early_stopping_rate\n    self.sampling_policy = sampling_policy(\n        pipeline_space=self.pipeline_space, logger=self.logger\n    )\n    self.promotion_policy = promotion_policy(self.eta)\n\n    # `max_budget_init` checks for the number of configurations that have been\n    # evaluated at the target budget\n    self.initial_design_type = initial_design_type\n    self.use_priors = use_priors\n\n    # check to ensure no rung ID is negative\n    # equivalent to s_max in https://arxiv.org/pdf/1603.06560.pdf\n    self.stopping_rate_limit = np.floor(\n        np.log(self.max_budget / self.min_budget) / np.log(self.eta)\n    ).astype(int)\n    assert self.early_stopping_rate &lt;= self.stopping_rate_limit\n\n    # maps rungs to a fidelity value for an SH bracket with `early_stopping_rate`\n    self.rung_map = self._get_rung_map(self.early_stopping_rate)\n    self.config_map = self._get_config_map(self.early_stopping_rate)\n\n    self.min_rung = min(list(self.rung_map.keys()))\n    self.max_rung = max(list(self.rung_map.keys()))\n\n    # placeholder args for varying promotion and sampling policies\n    self.promotion_policy_kwargs: dict = {}\n    self.promotion_policy_kwargs.update({\"config_map\": self.config_map})\n    self.sampling_args: dict = {}\n\n    self.fidelities = list(self.rung_map.values())\n    # stores the observations made and the corresponding fidelity explored\n    # crucial data structure used for determining promotion candidates\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n    # stores which configs occupy each rung at any time\n    self.rung_members: dict = dict()  # stores config IDs per rung\n    self.rung_members_performance: dict = dict()  # performances recorded per rung\n    self.rung_promotions: dict = dict()  # records a promotable config per rung\n    self.total_fevals = 0\n\n    # setup SH state counter\n    self._counter = 0\n    self.full_rung_trace = SuccessiveHalving._get_rung_trace(\n        self.rung_map, self.config_map\n    )\n\n    #############################\n    # Setting prior confidences #\n    #############################\n    # the std. dev or peakiness of distribution\n    self.prior_confidence = prior_confidence\n    self._enhance_priors()\n    self.rung_histories = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. The key to simulating reset of rungs like in vanilla SH is by subsetting only the relevant part of the observation history that corresponds to one SH bracket. Under a parallel run, multiple SH brackets can be spawned. The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket is spawned. There are no waiting or blocking calls.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    The key to simulating reset of rungs like in vanilla SH is by subsetting only the\n    relevant part of the observation history that corresponds to one SH bracket.\n    Under a parallel run, multiple SH brackets can be spawned. The oldest, active,\n    incomplete SH bracket is searched for to choose the next evaluation. If either\n    all brackets are over or waiting, a new SH bracket is spawned.\n    There are no waiting or blocking calls.\n    \"\"\"\n    # indexes to mark separate brackets\n    start = 0\n    end = self.config_map[self.min_rung]  # length of lowest rung in a bracket\n    if self.sample_default_at_target and self.sample_default_first:\n        start += 1\n        end += 1\n    # iterates over the different SH brackets which span start-end by index\n    while end &lt;= len(self.observed_configs):\n        # for the SH bracket in start-end, calculate total SH budget used\n\n        # TODO(eddiebergman): Not idea what the type is of the stuff in the deepcopy\n        # but should work on removing the deepcopy\n        bracket_budget_used = self._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than a SH bracket budget then still an active bracket\n        if bracket_budget_used &lt; sum(self.full_rung_trace):\n            # subsetting only this SH bracket from the history\n            self._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, and signals to starts a new SH bracket\n            self._handle_promotions()\n            promotion_count = 0\n            for _, promotions in self.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found\n                return\n        # else move to next SH bracket recorded by an offset (= lowest rung length)\n        start = end\n        end = start + self.config_map[self.min_rung]\n\n    # updates rung info with the latest active, incomplete bracket\n    self._get_rungs_state(self.observed_configs.iloc[start:end])\n    # _handle_promotion() need not be called as it is called by load_results()\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n        config.fidelity.set_value(self.rung_map[rung])\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                self.logger.info(\"Next config will be evaluated at target fidelity.\")\n            self.logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.sample_default_configuration()\n\n        elif random.random() &lt; self.random_interleave_prob:\n            config = self.pipeline_space.sample(\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config.fidelity.set_value(fidelity_value)\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config.hp_values(), config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.load_results","title":"load_results","text":"<pre><code>load_results(\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None\n</code></pre> <p>This is basically the fit method.</p> PARAMETER DESCRIPTION <code>previous_results</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> <code>pending_evaluations</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def load_results(\n    self,\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None:\n    \"\"\"This is basically the fit method.\n\n    Args:\n        previous_results (dict[str, ConfigResult]): [description]\n        pending_evaluations (dict[str, ConfigResult]): [description]\n    \"\"\"\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(previous_results)\n    self.total_fevals = len(previous_results) + len(pending_evaluations)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending_evaluations)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase","title":"SuccessiveHalvingBase","text":"<pre><code>SuccessiveHalvingBase(\n    pipeline_space: SearchSpace,\n    budget: int = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Implements a SuccessiveHalving procedure with a sampling and promotion policy.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>eta</code> <p>The reduction factor used by SH</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>early_stopping_rate</code> <p>Determines the number of rungs in an SH bracket Choosing 0 creates maximal rungs given the fidelity bounds</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>initial_design_type</code> <p>Type of initial design to switch to BO Legacy parameter from NePS BO design. Could be used to extend to MF-BO.</p> <p> TYPE: <code>Literal['max_budget', 'unique_configs']</code> DEFAULT: <code>'max_budget'</code> </p> <code>use_priors</code> <p>Allows random samples to be generated from a default Samples generated from a Gaussian centered around the default value</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sampling_policy</code> <p>The type of sampling procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>RandomUniformPolicy</code> </p> <code>promotion_policy</code> <p>The type of promotion procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>SyncPromotionPolicy</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>logger object, or None to use the neps logger</p> <p> DEFAULT: <code>None</code> </p> <code>prior_confidence</code> <p>The range of confidence to have on the prior The higher the confidence, the smaller is the standard deviation of the prior distribution centered around the default</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>None</code> </p> <code>random_interleave_prob</code> <p>Chooses the fraction of samples from random vs prior</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sample_default_at_target</code> <p>Whether to evaluate the default configuration at the target fidelity or max budget</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: typing.Any = RandomUniformPolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    \"\"\"Initialise an SH bracket.\n\n    Args:\n        pipeline_space: Space in which to search\n        budget: Maximum budget\n        eta: The reduction factor used by SH\n        early_stopping_rate: Determines the number of rungs in an SH bracket\n            Choosing 0 creates maximal rungs given the fidelity bounds\n        initial_design_type: Type of initial design to switch to BO\n            Legacy parameter from NePS BO design. Could be used to extend to MF-BO.\n        use_priors: Allows random samples to be generated from a default\n            Samples generated from a Gaussian centered around the default value\n        sampling_policy: The type of sampling procedure to use\n        promotion_policy: The type of promotion procedure to use\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        logger: logger object, or None to use the neps logger\n        prior_confidence: The range of confidence to have on the prior\n            The higher the confidence, the smaller is the standard deviation of the\n            prior distribution centered around the default\n        random_interleave_prob: Chooses the fraction of samples from random vs prior\n        sample_default_first: Whether to sample the default configuration first\n        sample_default_at_target: Whether to evaluate the default configuration at\n            the target fidelity or max budget\n    \"\"\"\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n    )\n    if random_interleave_prob &lt; 0 or random_interleave_prob &gt; 1:\n        raise ValueError(\"random_interleave_prob should be in [0.0, 1.0]\")\n    self.random_interleave_prob = random_interleave_prob\n    self.sample_default_first = sample_default_first\n    self.sample_default_at_target = sample_default_at_target\n\n    self.min_budget = self.pipeline_space.fidelity.lower\n    self.max_budget = self.pipeline_space.fidelity.upper\n    self.eta = eta\n    # SH implicitly sets early_stopping_rate to 0\n    # the parameter is exposed to allow HB to call SH with different stopping rates\n    self.early_stopping_rate = early_stopping_rate\n    self.sampling_policy = sampling_policy(\n        pipeline_space=self.pipeline_space, logger=self.logger\n    )\n    self.promotion_policy = promotion_policy(self.eta)\n\n    # `max_budget_init` checks for the number of configurations that have been\n    # evaluated at the target budget\n    self.initial_design_type = initial_design_type\n    self.use_priors = use_priors\n\n    # check to ensure no rung ID is negative\n    # equivalent to s_max in https://arxiv.org/pdf/1603.06560.pdf\n    self.stopping_rate_limit = np.floor(\n        np.log(self.max_budget / self.min_budget) / np.log(self.eta)\n    ).astype(int)\n    assert self.early_stopping_rate &lt;= self.stopping_rate_limit\n\n    # maps rungs to a fidelity value for an SH bracket with `early_stopping_rate`\n    self.rung_map = self._get_rung_map(self.early_stopping_rate)\n    self.config_map = self._get_config_map(self.early_stopping_rate)\n\n    self.min_rung = min(list(self.rung_map.keys()))\n    self.max_rung = max(list(self.rung_map.keys()))\n\n    # placeholder args for varying promotion and sampling policies\n    self.promotion_policy_kwargs: dict = {}\n    self.promotion_policy_kwargs.update({\"config_map\": self.config_map})\n    self.sampling_args: dict = {}\n\n    self.fidelities = list(self.rung_map.values())\n    # stores the observations made and the corresponding fidelity explored\n    # crucial data structure used for determining promotion candidates\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n    # stores which configs occupy each rung at any time\n    self.rung_members: dict = dict()  # stores config IDs per rung\n    self.rung_members_performance: dict = dict()  # performances recorded per rung\n    self.rung_promotions: dict = dict()  # records a promotable config per rung\n    self.total_fevals = 0\n\n    # setup SH state counter\n    self._counter = 0\n    self.full_rung_trace = SuccessiveHalving._get_rung_trace(\n        self.rung_map, self.config_map\n    )\n\n    #############################\n    # Setting prior confidences #\n    #############################\n    # the std. dev or peakiness of distribution\n    self.prior_confidence = prior_confidence\n    self._enhance_priors()\n    self.rung_histories = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n        config.fidelity.set_value(self.rung_map[rung])\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                self.logger.info(\"Next config will be evaluated at target fidelity.\")\n            self.logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.sample_default_configuration()\n\n        elif random.random() &lt; self.random_interleave_prob:\n            config = self.pipeline_space.sample(\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config.fidelity.set_value(fidelity_value)\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config.hp_values(), config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.load_results","title":"load_results","text":"<pre><code>load_results(\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None\n</code></pre> <p>This is basically the fit method.</p> PARAMETER DESCRIPTION <code>previous_results</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> <code>pending_evaluations</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def load_results(\n    self,\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None:\n    \"\"\"This is basically the fit method.\n\n    Args:\n        previous_results (dict[str, ConfigResult]): [description]\n        pending_evaluations (dict[str, ConfigResult]): [description]\n    \"\"\"\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(previous_results)\n    self.total_fevals = len(previous_results) + len(pending_evaluations)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending_evaluations)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors","title":"SuccessiveHalvingWithPriors","text":"<pre><code>SuccessiveHalvingWithPriors(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalving</code></p> <p>Implements a SuccessiveHalving procedure with a sampling and promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = FixedPriorPolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",  # medium = 0.25\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. The key to simulating reset of rungs like in vanilla SH is by subsetting only the relevant part of the observation history that corresponds to one SH bracket. Under a parallel run, multiple SH brackets can be spawned. The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket is spawned. There are no waiting or blocking calls.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    The key to simulating reset of rungs like in vanilla SH is by subsetting only the\n    relevant part of the observation history that corresponds to one SH bracket.\n    Under a parallel run, multiple SH brackets can be spawned. The oldest, active,\n    incomplete SH bracket is searched for to choose the next evaluation. If either\n    all brackets are over or waiting, a new SH bracket is spawned.\n    There are no waiting or blocking calls.\n    \"\"\"\n    # indexes to mark separate brackets\n    start = 0\n    end = self.config_map[self.min_rung]  # length of lowest rung in a bracket\n    if self.sample_default_at_target and self.sample_default_first:\n        start += 1\n        end += 1\n    # iterates over the different SH brackets which span start-end by index\n    while end &lt;= len(self.observed_configs):\n        # for the SH bracket in start-end, calculate total SH budget used\n\n        # TODO(eddiebergman): Not idea what the type is of the stuff in the deepcopy\n        # but should work on removing the deepcopy\n        bracket_budget_used = self._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than a SH bracket budget then still an active bracket\n        if bracket_budget_used &lt; sum(self.full_rung_trace):\n            # subsetting only this SH bracket from the history\n            self._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, and signals to starts a new SH bracket\n            self._handle_promotions()\n            promotion_count = 0\n            for _, promotions in self.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found\n                return\n        # else move to next SH bracket recorded by an offset (= lowest rung length)\n        start = end\n        end = start + self.config_map[self.min_rung]\n\n    # updates rung info with the latest active, incomplete bracket\n    self._get_rungs_state(self.observed_configs.iloc[start:end])\n    # _handle_promotion() need not be called as it is called by load_results()\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n        config.fidelity.set_value(self.rung_map[rung])\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                self.logger.info(\"Next config will be evaluated at target fidelity.\")\n            self.logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.sample_default_configuration()\n\n        elif random.random() &lt; self.random_interleave_prob:\n            config = self.pipeline_space.sample(\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config.fidelity.set_value(fidelity_value)\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config.hp_values(), config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.load_results","title":"load_results","text":"<pre><code>load_results(\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None\n</code></pre> <p>This is basically the fit method.</p> PARAMETER DESCRIPTION <code>previous_results</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> <code>pending_evaluations</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def load_results(\n    self,\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None:\n    \"\"\"This is basically the fit method.\n\n    Args:\n        previous_results (dict[str, ConfigResult]): [description]\n        pending_evaluations (dict[str, ConfigResult]): [description]\n    \"\"\"\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(previous_results)\n    self.total_fevals = len(previous_results) + len(pending_evaluations)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending_evaluations)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/","title":"Utils","text":""},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils","title":"neps.optimizers.multi_fidelity.utils","text":""},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData","title":"MFObservedData","text":"<pre><code>MFObservedData(\n    columns: list[str] | None = None,\n    index_names: list[str] | None = None,\n)\n</code></pre> <p>(Under development)</p> <p>This module is used to unify the data access across different Multi-Fidelity optimizers. It stores column names and index names. Possible optimizations and extensions of the observed data should be handled by this class.</p> <p>So far this is just a draft class containing the DataFrame and some properties.</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def __init__(\n    self,\n    columns: list[str] | None = None,\n    index_names: list[str] | None = None,\n):\n    if columns is None:\n        columns = [self.default_config_col, self.default_perf_col]\n    if index_names is None:\n        index_names = [self.default_config_idx, self.default_budget_idx]\n\n    self.config_col = columns[0]\n    self.perf_col = columns[1]\n\n    if len(columns) &gt; 2:\n        self.lc_col_name = columns[2]\n    else:\n        self.lc_col_name = self.default_lc_col\n\n    if len(index_names) == 1:\n        index_names += [\"budget_id\"]\n\n    self.config_idx = index_names[0]\n    self.budget_idx = index_names[1]\n\n    index = pd.MultiIndex.from_tuples([], names=index_names)\n\n    self.df = pd.DataFrame([], columns=columns, index=index)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.add_data","title":"add_data","text":"<pre><code>add_data(\n    data: list[Any] | list[list[Any]],\n    index: (\n        tuple[int, ...]\n        | Sequence[tuple[int, ...]]\n        | Sequence[int]\n        | int\n    ),\n    error: bool = False,\n)\n</code></pre> <p>Add data only if none of the indices are already existing in the DataFrame</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def add_data(\n    self,\n    data: list[Any] | list[list[Any]],\n    index: tuple[int, ...] | Sequence[tuple[int, ...]] | Sequence[int] | int,\n    error: bool = False,\n):\n    \"\"\"\n    Add data only if none of the indices are already existing in the DataFrame\n    \"\"\"\n    # TODO: If index is only config_id extend it\n    if not isinstance(index, list):\n        index_list = [index]\n        data_list = [data]\n    else:\n        index_list = index\n        data_list = data\n\n    if not self.df.index.isin(index_list).any():\n        _df = pd.DataFrame(data_list, columns=self.df.columns, index=index_list)\n        self.df = pd.concat((self.df, _df))\n    elif error:\n        raise ValueError(\n            f\"Data with at least one of the given indices already \"\n            f\"exists: {self.df[self.df.index.isin(index_list)]}\\n\"\n            f\"Given indices: {index_list}\"\n        )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.get_best_learning_curve_id","title":"get_best_learning_curve_id","text":"<pre><code>get_best_learning_curve_id(maximize: bool = False)\n</code></pre> <p>Returns a single configuration id of the best observed performance</p> this will always return the single best lowest ID <p>if two configurations has the same performance</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def get_best_learning_curve_id(self, maximize: bool = False):\n    \"\"\"\n    Returns a single configuration id of the best observed performance\n\n    Note: this will always return the single best lowest ID\n          if two configurations has the same performance\n    \"\"\"\n    learning_curves = self.get_learning_curves()\n    if maximize:\n        return learning_curves.max(axis=1).idxmax()\n    else:\n        return learning_curves.min(axis=1).idxmin()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.get_best_performance_for_each_budget","title":"get_best_performance_for_each_budget","text":"<pre><code>get_best_performance_for_each_budget(\n    maximize: bool = False,\n)\n</code></pre> <p>Returns a series object with the best partial configuration for each budget id</p> this will always map the best lowest ID if two configurations <p>has the same performance at the same fidelity</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def get_best_performance_for_each_budget(self, maximize: bool = False):\n    \"\"\"\n    Returns a series object with the best partial configuration for each budget id\n\n    Note: this will always map the best lowest ID if two configurations\n          has the same performance at the same fidelity\n    \"\"\"\n    learning_curves = self.get_learning_curves()\n    if maximize:\n        performance = learning_curves.max(axis=0)\n    else:\n        performance = learning_curves.min(axis=0)\n\n    return performance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.get_incumbents_for_budgets","title":"get_incumbents_for_budgets","text":"<pre><code>get_incumbents_for_budgets(maximize: bool = False)\n</code></pre> <p>Returns a series object with the best partial configuration for each budget id</p> this will always map the best lowest ID if two configurations <p>has the same performance at the same fidelity</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def get_incumbents_for_budgets(self, maximize: bool = False):\n    \"\"\"\n    Returns a series object with the best partial configuration for each budget id\n\n    Note: this will always map the best lowest ID if two configurations\n          has the same performance at the same fidelity\n    \"\"\"\n    learning_curves = self.get_learning_curves()\n    if maximize:\n        config_ids = learning_curves.idxmax(axis=0)\n    else:\n        config_ids = learning_curves.idxmin(axis=0)\n\n    indices = list(zip(config_ids.values.tolist(), config_ids.index.to_list()))\n    partial_configs = self.df.loc[indices, self.config_col].to_list()\n    return pd.Series(partial_configs, index=config_ids.index, name=self.config_col)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.tokenize","title":"tokenize","text":"<pre><code>tokenize(df: DataFrame, as_tensor: bool = False)\n</code></pre> <p>Function to format data for PFN.</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def tokenize(self, df: pd.DataFrame, as_tensor: bool = False):\n    \"\"\"Function to format data for PFN.\"\"\"\n    configs = np.array([normalize_vectorize_config(c) for c in df])\n    fidelity = np.array([c.fidelity.value for c in df]).reshape(-1, 1)\n    idx = df.index.values.reshape(-1, 1)\n\n    data = np.hstack([idx, fidelity, configs])\n\n    if as_tensor:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        data = torch.Tensor(data).to(device)\n    return data\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.update_data","title":"update_data","text":"<pre><code>update_data(\n    data_dict: dict[str, list[Any]],\n    index: (\n        tuple[int, ...]\n        | Sequence[tuple[int, ...]]\n        | Sequence[int]\n        | int\n    ),\n    error: bool = False,\n)\n</code></pre> <p>Update data if all the indices already exist in the DataFrame</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def update_data(\n    self,\n    data_dict: dict[str, list[Any]],\n    index: tuple[int, ...] | Sequence[tuple[int, ...]] | Sequence[int] | int,\n    error: bool = False,\n):\n    \"\"\"\n    Update data if all the indices already exist in the DataFrame\n    \"\"\"\n    if not isinstance(index, list):\n        index_list = [index]\n    else:\n        index_list = index\n    if self.df.index.isin(index_list).sum() == len(index_list):\n        column_names, data = zip(*data_dict.items())\n        data = list(zip(*data))\n        self.df.loc[index_list, list(column_names)] = data\n\n    elif error:\n        raise ValueError(\n            f\"Data with at least one of the given indices doesn't \"\n            f\"exist.\\n Existing indices: {self.df.index}\\n\"\n            f\"Given indices: {index_list}\"\n        )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.continuous_to_tabular","title":"continuous_to_tabular","text":"<pre><code>continuous_to_tabular(\n    config: SearchSpace, categorical_space: SearchSpace\n) -&gt; SearchSpace\n</code></pre> <p>Convert the continuous parameters in the config into categorical ones based on the categorical_space provided</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def continuous_to_tabular(\n    config: SearchSpace, categorical_space: SearchSpace\n) -&gt; SearchSpace:\n    \"\"\"\n    Convert the continuous parameters in the config into categorical ones based on\n    the categorical_space provided\n    \"\"\"\n    result = config.clone()\n    for hp_name, _ in config.items():\n        if hp_name in categorical_space.keys():\n            choices = np.array(categorical_space[hp_name].choices)\n            diffs = choices - config[hp_name].value\n            # NOTE: in case of a tie the first value in the choices array will be returned\n            closest = choices[np.abs(diffs).argmin()]\n            result[hp_name].set_value(closest)\n\n    return result\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/","title":"Async priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband","title":"neps.optimizers.multi_fidelity_prior.async_priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha","title":"PriorBandAsha","text":"<pre><code>PriorBandAsha(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",\n    inc_sample_type: str = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",\n    model_based: bool = False,\n    modelling_type: str = \"joint\",\n    initial_design_size: int = None,\n    model_policy: Any = ModelPolicy,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"random\",\n)\n</code></pre> <p>               Bases: <code>MFBOBase</code>, <code>PriorBandBase</code>, <code>AsynchronousSuccessiveHalvingWithPriors</code></p> <p>Implements a PriorBand on top of ASHA.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = EnsemblePolicy,  # key difference to ASHA\n    promotion_policy: typing.Any = AsyncPromotionPolicy,  # key difference from SH\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",  # could also be {\"linear\", \"50-50\"}\n    inc_sample_type: str = \"mutation\",  # or {\"crossover\", \"gaussian\", \"hypersphere\"}\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",  # could also be {\"decay\", \"constant\"}\n    # arguments for model\n    model_based: bool = False,  # crucial argument to set to allow model-search\n    modelling_type: str = \"joint\",  # could also be {\"rung\"}\n    initial_design_size: int = None,\n    model_policy: typing.Any = ModelPolicy,\n    surrogate_model: str | typing.Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str | AcquisitionSampler = \"random\",\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    self.prior_weight_type = prior_weight_type\n    self.inc_sample_type = inc_sample_type\n    self.inc_mutation_rate = inc_mutation_rate\n    self.inc_mutation_std = inc_mutation_std\n    self.sampling_policy = sampling_policy(\n        pipeline_space=pipeline_space, inc_type=self.inc_sample_type\n    )\n    # determines the kind of trade-off between incumbent and prior weightage\n    self.inc_style = inc_style  # used by PriorBandBase\n    self.sampling_args = {\n        \"inc\": None,\n        \"weights\": {\n            \"prior\": 1,  # begin with only prior sampling\n            \"inc\": 0,\n            \"random\": 0,\n        },\n    }\n\n    bo_args = dict(\n        surrogate_model=surrogate_model,\n        domain_se_kernel=domain_se_kernel,\n        hp_kernels=hp_kernels,\n        surrogate_model_args=surrogate_model_args,\n        acquisition=acquisition,\n        log_prior_weighted=log_prior_weighted,\n        acquisition_sampler=acquisition_sampler,\n    )\n    self.model_based = model_based\n    self.modelling_type = modelling_type\n    self.initial_design_size = initial_design_size\n    # counting non-fidelity dimensions in search space\n    ndims = sum(\n        1\n        for _, hp in self.pipeline_space.hyperparameters.items()\n        if not hp.is_fidelity\n    )\n    n_min = ndims + 1\n    self.init_size = n_min + 1  # in BOHB: init_design &gt;= N_dim + 2\n    if self.modelling_type == \"joint\" and self.initial_design_size is not None:\n        self.init_size = self.initial_design_size\n    self.model_policy = model_policy(pipeline_space, **bo_args)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    sampling_args = {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n    return sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    distance = min(distances)\n    return distance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    distances = [d for d in distances if d &gt; 0]\n    return distances\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            self.logger.warn(f\"{rung} not in {np.unique(idxs)}\")\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            self.logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.sample_default_configuration()\n        self.logger.warn(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def get_config_and_ids(\n    self,\n) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        rung = rung_to_promote + 1\n    else:\n        rung = self.min_rung\n    self.set_sampling_weights_and_inc(rung=rung)\n    # performs standard ASHA but sampling happens as per the EnsemblePolicy\n    return super().get_config_and_ids()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    if hasattr(self, \"sh_brackets\") and len(self.sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = self.sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(list(bracket.rung_map.keys()), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    resources_used = calc_total_resources_spent(self.observed_configs, self.rung_map)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        resources /= self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.load_results","title":"load_results","text":"<pre><code>load_results(\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None\n</code></pre> <p>This is basically the fit method.</p> PARAMETER DESCRIPTION <code>previous_results</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> <code>pending_evaluations</code> <p>[description]</p> <p> TYPE: <code>dict[str, ConfigResult]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def load_results(\n    self,\n    previous_results: dict[str, ConfigResult],\n    pending_evaluations: dict[str, SearchSpace],\n) -&gt; None:\n    \"\"\"This is basically the fit method.\n\n    Args:\n        previous_results (dict[str, ConfigResult]): [description]\n        pending_evaluations (dict[str, ConfigResult]): [description]\n    \"\"\"\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(previous_results)\n    self.total_fevals = len(previous_results) + len(pending_evaluations)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending_evaluations)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; float | float\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; float | float:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    elif self.inc_style == \"decay\":\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    elif self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    else:\n        raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(rung: int = None, **kwargs)\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int = None,\n    **kwargs,\n):\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            fidelity = active_max_fidelity = None\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = self.pipeline_space.sample(\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB","title":"PriorBandAshaHB","text":"<pre><code>PriorBandAshaHB(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",\n    inc_sample_type: str = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",\n    model_based: bool = False,\n    modelling_type: str = \"joint\",\n    initial_design_size: int = None,\n    model_policy: Any = ModelPolicy,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"random\",\n)\n</code></pre> <p>               Bases: <code>PriorBandAsha</code></p> <p>Implements a PriorBand on top of ASHA-HB (Mobster).</p> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = EnsemblePolicy,  # key difference to ASHA\n    promotion_policy: typing.Any = AsyncPromotionPolicy,  # key difference from PB\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",  # could also be {\"linear\", \"50-50\"}\n    inc_sample_type: str = \"mutation\",  # or {\"crossover\", \"gaussian\", \"hypersphere\"}\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",  # could also be {\"decay\", \"constant\"}\n    # arguments for model\n    model_based: bool = False,  # crucial argument to set to allow model-search\n    modelling_type: str = \"joint\",  # could also be {\"rung\"}\n    initial_design_size: int = None,\n    model_policy: typing.Any = ModelPolicy,\n    surrogate_model: str | typing.Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str | AcquisitionSampler = \"random\",\n):\n    # collecting arguments required by ASHA\n    args = dict(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=self.early_stopping_rate,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    bo_args = dict(\n        surrogate_model=surrogate_model,\n        domain_se_kernel=domain_se_kernel,\n        hp_kernels=hp_kernels,\n        surrogate_model_args=surrogate_model_args,\n        acquisition=acquisition,\n        log_prior_weighted=log_prior_weighted,\n        acquisition_sampler=acquisition_sampler,\n    )\n    super().__init__(\n        **args,\n        prior_weight_type=prior_weight_type,\n        inc_sample_type=inc_sample_type,\n        inc_mutation_rate=inc_mutation_rate,\n        inc_mutation_std=inc_mutation_std,\n        inc_style=inc_style,\n        model_based=model_based,\n        modelling_type=modelling_type,\n        initial_design_size=initial_design_size,\n        model_policy=model_policy,\n        **bo_args,\n    )\n\n    # Creating the ASHA (SH) brackets that Hyperband iterates over\n    self.sh_brackets = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        # key difference from vanilla HB where it runs synchronous SH brackets\n        self.sh_brackets[s] = AsynchronousSuccessiveHalvingWithPriors(**args)\n        self.sh_brackets[s].sampling_policy = self.sampling_policy\n        self.sh_brackets[s].sampling_args = self.sampling_args\n        self.sh_brackets[s].model_policy = self.model_policy\n        self.sh_brackets[s].sample_new_config = self.sample_new_config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    sampling_args = {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n    return sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    distance = min(distances)\n    return distance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    distances = [d for d in distances if d &gt; 0]\n    return distances\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            self.logger.warn(f\"{rung} not in {np.unique(idxs)}\")\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            self.logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.sample_default_configuration()\n        self.logger.warn(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    # the rung to sample at\n    bracket_to_run = self._get_bracket_to_run()\n\n    self.set_sampling_weights_and_inc(rung=bracket_to_run)\n    self.sh_brackets[bracket_to_run].sampling_args = self.sampling_args\n    config, config_id, previous_config_id = self.sh_brackets[\n        bracket_to_run\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    if hasattr(self, \"sh_brackets\") and len(self.sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = self.sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(list(bracket.rung_map.keys()), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    resources_used = calc_total_resources_spent(self.observed_configs, self.rung_map)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        resources /= self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; float | float\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; float | float:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    elif self.inc_style == \"decay\":\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    elif self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    else:\n        raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(rung: int = None, **kwargs)\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int = None,\n    **kwargs,\n):\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            fidelity = active_max_fidelity = None\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = self.pipeline_space.sample(\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/","title":"Priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband","title":"neps.optimizers.multi_fidelity_prior.priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand","title":"PriorBand","text":"<pre><code>PriorBand(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",\n    inc_sample_type: str = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",\n    model_based: bool = False,\n    modelling_type: str = \"joint\",\n    initial_design_size: int = None,\n    model_policy: Any = ModelPolicy,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"random\",\n)\n</code></pre> <p>               Bases: <code>MFBOBase</code>, <code>HyperbandCustomDefault</code>, <code>PriorBandBase</code></p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = EnsemblePolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",  # could also be {\"linear\", \"50-50\"}\n    inc_sample_type: str = \"mutation\",  # or {\"crossover\", \"gaussian\", \"hypersphere\"}\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",  # could also be {\"decay\", \"constant\"}\n    # arguments for model\n    model_based: bool = False,  # crucial argument to set to allow model-search\n    modelling_type: str = \"joint\",  # could also be {\"rung\"}\n    initial_design_size: int = None,\n    model_policy: typing.Any = ModelPolicy,\n    surrogate_model: str | typing.Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str | AcquisitionSampler = \"random\",\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    self.prior_weight_type = prior_weight_type\n    self.inc_sample_type = inc_sample_type\n    self.inc_mutation_rate = inc_mutation_rate\n    self.inc_mutation_std = inc_mutation_std\n    self.sampling_policy = sampling_policy(\n        pipeline_space=pipeline_space, inc_type=self.inc_sample_type\n    )\n    # determines the kind of trade-off between incumbent and prior weightage\n    self.inc_style = inc_style  # used by PriorBandBase\n    self.sampling_args = {\n        \"inc\": None,\n        \"weights\": {\n            \"prior\": 1,  # begin with only prior sampling\n            \"inc\": 0,\n            \"random\": 0,\n        },\n    }\n\n    bo_args = dict(\n        surrogate_model=surrogate_model,\n        domain_se_kernel=domain_se_kernel,\n        hp_kernels=hp_kernels,\n        surrogate_model_args=surrogate_model_args,\n        acquisition=acquisition,\n        log_prior_weighted=log_prior_weighted,\n        acquisition_sampler=acquisition_sampler,\n    )\n    self.model_based = model_based\n    self.modelling_type = modelling_type\n    self.initial_design_size = initial_design_size\n    # counting non-fidelity dimensions in search space\n    ndims = sum(\n        1\n        for _, hp in self.pipeline_space.hyperparameters.items()\n        if not hp.is_fidelity\n    )\n    n_min = ndims + 1\n    self.init_size = n_min + 1  # in BOHB: init_design &gt;= N_min + 2\n    if self.modelling_type == \"joint\" and self.initial_design_size is not None:\n        self.init_size = self.initial_design_size\n    self.model_policy = model_policy(pipeline_space, **bo_args)\n\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_policy = self.sampling_policy\n        sh.sampling_args = self.sampling_args\n        sh.model_policy = self.model_policy\n        sh.sample_new_config = self.sample_new_config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    sampling_args = {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n    return sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    distance = min(distances)\n    return distance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    distances = [d for d in distances if d &gt; 0]\n    return distances\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            self.logger.warn(f\"{rung} not in {np.unique(idxs)}\")\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            self.logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.sample_default_configuration()\n        self.logger.warn(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    self.set_sampling_weights_and_inc(rung=self.current_sh_bracket)\n\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_args = self.sampling_args\n    return super().get_config_and_ids()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    if hasattr(self, \"sh_brackets\") and len(self.sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = self.sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(list(bracket.rung_map.keys()), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    resources_used = calc_total_resources_spent(self.observed_configs, self.rung_map)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        resources /= self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; float | float\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; float | float:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    elif self.inc_style == \"decay\":\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    elif self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    else:\n        raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(rung: int = None, **kwargs)\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int = None,\n    **kwargs,\n):\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            fidelity = active_max_fidelity = None\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = self.pipeline_space.sample(\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase","title":"PriorBandBase","text":"<p>Class that defines essential properties needed by PriorBand.</p> <p>Designed to work with the topmost parent class as SuccessiveHalvingBase.</p>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    sampling_args = {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n    return sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    distance = min(distances)\n    return distance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    distances = [d for d in distances if d &gt; 0]\n    return distances\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            self.logger.warn(f\"{rung} not in {np.unique(idxs)}\")\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            self.logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.sample_default_configuration()\n        self.logger.warn(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    if hasattr(self, \"sh_brackets\") and len(self.sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = self.sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(list(bracket.rung_map.keys()), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    resources_used = calc_total_resources_spent(self.observed_configs, self.rung_map)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; float | float\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; float | float:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    elif self.inc_style == \"decay\":\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    elif self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    else:\n        raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior","title":"PriorBandNoIncToPrior","text":"<pre><code>PriorBandNoIncToPrior(\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",\n    inc_sample_type: str = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",\n    model_based: bool = False,\n    modelling_type: str = \"joint\",\n    initial_design_size: int = None,\n    model_policy: Any = ModelPolicy,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: (\n        str | AcquisitionSampler\n    ) = \"random\",\n)\n</code></pre> <p>               Bases: <code>PriorBand</code></p> <p>Disables incumbent sampling to replace with prior-based sampling.</p> <p>This is equivalent to running HyperBand with Prior and Random sampling, where their relationship is controlled by the <code>prior_weight_type</code> argument.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: typing.Any = EnsemblePolicy,\n    promotion_policy: typing.Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    logger=None,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: str = \"geometric\",  # could also be {\"linear\", \"50-50\"}\n    inc_sample_type: str = \"mutation\",  # or {\"crossover\", \"gaussian\", \"hypersphere\"}\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: str = \"dynamic\",  # could also be {\"decay\", \"constant\"}\n    # arguments for model\n    model_based: bool = False,  # crucial argument to set to allow model-search\n    modelling_type: str = \"joint\",  # could also be {\"rung\"}\n    initial_design_size: int = None,\n    model_policy: typing.Any = ModelPolicy,\n    surrogate_model: str | typing.Any = \"gp\",\n    domain_se_kernel: str = None,\n    hp_kernels: list = None,\n    surrogate_model_args: dict = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str | AcquisitionSampler = \"random\",\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        logger=logger,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    self.prior_weight_type = prior_weight_type\n    self.inc_sample_type = inc_sample_type\n    self.inc_mutation_rate = inc_mutation_rate\n    self.inc_mutation_std = inc_mutation_std\n    self.sampling_policy = sampling_policy(\n        pipeline_space=pipeline_space, inc_type=self.inc_sample_type\n    )\n    # determines the kind of trade-off between incumbent and prior weightage\n    self.inc_style = inc_style  # used by PriorBandBase\n    self.sampling_args = {\n        \"inc\": None,\n        \"weights\": {\n            \"prior\": 1,  # begin with only prior sampling\n            \"inc\": 0,\n            \"random\": 0,\n        },\n    }\n\n    bo_args = dict(\n        surrogate_model=surrogate_model,\n        domain_se_kernel=domain_se_kernel,\n        hp_kernels=hp_kernels,\n        surrogate_model_args=surrogate_model_args,\n        acquisition=acquisition,\n        log_prior_weighted=log_prior_weighted,\n        acquisition_sampler=acquisition_sampler,\n    )\n    self.model_based = model_based\n    self.modelling_type = modelling_type\n    self.initial_design_size = initial_design_size\n    # counting non-fidelity dimensions in search space\n    ndims = sum(\n        1\n        for _, hp in self.pipeline_space.hyperparameters.items()\n        if not hp.is_fidelity\n    )\n    n_min = ndims + 1\n    self.init_size = n_min + 1  # in BOHB: init_design &gt;= N_min + 2\n    if self.modelling_type == \"joint\" and self.initial_design_size is not None:\n        self.init_size = self.initial_design_size\n    self.model_policy = model_policy(pipeline_space, **bo_args)\n\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_policy = self.sampling_policy\n        sh.sampling_args = self.sampling_args\n        sh.model_policy = self.model_policy\n        sh.sample_new_config = self.sample_new_config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    sampling_args = {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n    return sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    distance = min(distances)\n    return distance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    distances = [d for d in distances if d &gt; 0]\n    return distances\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            self.logger.warn(f\"{rung} not in {np.unique(idxs)}\")\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            self.logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.sample_default_configuration()\n        self.logger.warn(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    self.set_sampling_weights_and_inc(rung=self.current_sh_bracket)\n\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_args = self.sampling_args\n    return super().get_config_and_ids()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    if hasattr(self, \"sh_brackets\") and len(self.sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = self.sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(list(bracket.rung_map.keys()), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    resources_used = calc_total_resources_spent(self.observed_configs, self.rung_map)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        resources /= self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; float | float\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; float | float:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    elif self.inc_style == \"decay\":\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    elif self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    else:\n        raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoIncToPrior.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(rung: int = None, **kwargs)\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int = None,\n    **kwargs,\n):\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            fidelity = active_max_fidelity = None\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = self.pipeline_space.sample(\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc","title":"PriorBandNoPriorToInc","text":"<pre><code>PriorBandNoPriorToInc(**kwargs)\n</code></pre> <p>               Bases: <code>PriorBand</code></p> <p>Disables prior based sampling to replace with incumbent-based sampling.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n    # cannot use prior in this version\n    self.pipeline_space.has_prior = False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    sampling_args = {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n    return sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets()\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self):\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    distance = min(distances)\n    return distance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(incumbent)\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent):\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    distances = [d for d in distances if d &gt; 0]\n    return distances\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            self.logger.warn(f\"{rung} not in {np.unique(idxs)}\")\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            self.logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.sample_default_configuration()\n        self.logger.warn(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    self.set_sampling_weights_and_inc(rung=self.current_sh_bracket)\n\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_args = self.sampling_args\n    return super().get_config_and_ids()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.get_cost","title":"get_cost","text":"<pre><code>get_cost(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\"\"\"\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.get_learning_curve","title":"get_learning_curve","text":"<pre><code>get_learning_curve(\n    result: str | dict | float,\n) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_learning_curve(self, result: str | dict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_learning_curve(\n        result,\n        learning_curve_on_error=self.learning_curve_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.get_loss","title":"get_loss","text":"<pre><code>get_loss(result: ERROR | ResultDict | float) -&gt; float | Any\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float) -&gt; float | Any:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\"\"\"\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    if hasattr(self, \"sh_brackets\") and len(self.sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = self.sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(list(bracket.rung_map.keys()), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    resources_used = calc_total_resources_spent(self.observed_configs, self.rung_map)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        resources /= self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.is_out_of_budget","title":"is_out_of_budget","text":"<pre><code>is_out_of_budget() -&gt; bool\n</code></pre> <p>Check if the optimizer has used all of its budget, if any.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def is_out_of_budget(self) -&gt; bool:\n    \"\"\"Check if the optimizer has used all of its budget, if any.\"\"\"\n    return self.budget is not None and self.used_budget &gt;= self.budget\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; float | float\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; float | float:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    elif self.inc_style == \"decay\":\n        resources = calc_total_resources_spent(self.observed_configs, self.rung_map)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    elif self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    else:\n        raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandNoPriorToInc.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(rung: int = None, **kwargs)\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int = None,\n    **kwargs,\n):\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            fidelity = active_max_fidelity = None\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = self.pipeline_space.sample(\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/","title":"Utils","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils","title":"neps.optimizers.multi_fidelity_prior.utils","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.compute_config_dist","title":"compute_config_dist","text":"<pre><code>compute_config_dist(\n    config1: SearchSpace, config2: SearchSpace\n) -&gt; float\n</code></pre> <p>Computes distance between two configurations.</p> <p>Divides the search space into continuous and categorical subspaces. Normalizes all the continuous values while gives numerical encoding to categories. Distance returned is the sum of the Euclidean distance of the continous subspace and the Hamming distance of the categorical subspace.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def compute_config_dist(config1: SearchSpace, config2: SearchSpace) -&gt; float:\n    \"\"\"Computes distance between two configurations.\n\n    Divides the search space into continuous and categorical subspaces.\n    Normalizes all the continuous values while gives numerical encoding to categories.\n    Distance returned is the sum of the Euclidean distance of the continous subspace and\n    the Hamming distance of the categorical subspace.\n    \"\"\"\n    config1 = config1.get_normalized_hp_categories(ignore_fidelity=True)\n    config2 = config2.get_normalized_hp_categories(ignore_fidelity=True)\n\n    # adding a dim with 0 to all subspaces in case the search space is not mixed type\n\n    # computing euclidean distance over the continuous subspace\n    diff = np.array(config1[\"continuous\"] + [0]) - np.array(config2[\"continuous\"] + [0])\n    d_cont = np.linalg.norm(diff, ord=2)\n\n    # TODO: can we consider the number of choices per dimension\n    # computing hamming distance over the categorical subspace\n    d_cat = scipy.spatial.distance.hamming(\n        config1[\"categorical\"] + [0], config2[\"categorical\"] + [0]\n    )\n\n    distance = d_cont + d_cat\n    return distance\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.compute_scores","title":"compute_scores","text":"<pre><code>compute_scores(\n    config: SearchSpace,\n    prior: SearchSpace,\n    inc: SearchSpace,\n) -&gt; tuple[float, float]\n</code></pre> <p>Scores the config by a Gaussian around the prior and the incumbent.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def compute_scores(\n    config: SearchSpace,\n    prior: SearchSpace,\n    inc: SearchSpace,\n) -&gt; tuple[float, float]:\n    \"\"\"Scores the config by a Gaussian around the prior and the incumbent.\"\"\"\n    _prior = prior.clone()\n    _prior.set_hyperparameters_from_dict(config.hp_values(), defaults=False)\n    # compute the score of config if it was sampled from the prior (as the default)\n    prior_score = _prior.compute_prior()\n\n    _inc = inc.clone()\n    # setting the default to be the incumbent\n    _inc.set_defaults_to_current_values()\n    _inc.set_hyperparameters_from_dict(config.hp_values(), defaults=False)\n    # compute the score of config if it was sampled from the inc (as the default)\n    inc_score = _inc.compute_prior()\n\n    return prior_score, inc_score\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.custom_crossover","title":"custom_crossover","text":"<pre><code>custom_crossover(\n    config1: SearchSpace,\n    config2: SearchSpace,\n    crossover_prob: float = 0.5,\n    patience: int = 50,\n) -&gt; SearchSpace\n</code></pre> <p>Performs a crossover of config2 into config1.</p> <p>Returns a configuration where each HP in config1 has <code>crossover_prob</code>% chance of getting config2's value of the corresponding HP. By default, crossover rate is 50%.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def custom_crossover(\n    config1: SearchSpace,\n    config2: SearchSpace,\n    crossover_prob: float = 0.5,\n    patience: int = 50,\n) -&gt; SearchSpace:\n    \"\"\"Performs a crossover of config2 into config1.\n\n    Returns a configuration where each HP in config1 has `crossover_prob`% chance of\n    getting config2's value of the corresponding HP. By default, crossover rate is 50%.\n    \"\"\"\n    for _ in range(patience):\n\n        child_config = config1.clone()\n        for key, hyperparameter in config1.items():\n            if not hyperparameter.is_fidelity and np.random.random() &lt; crossover_prob:\n                child_config[key].set_value(config2[key].value)\n\n        if not child_config.is_equal_value(config1):\n            return SearchSpace(**child_config)\n\n    # fail safe check to handle edge cases where config1=config2 or\n    # config1 extremely local to config2 such that crossover fails to\n    # generate new config in a discrete (sub-)space\n    return config1.sample(\n        patience=patience,\n        user_priors=False,\n        ignore_fidelity=True,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.get_prior_weight_for_decay","title":"get_prior_weight_for_decay","text":"<pre><code>get_prior_weight_for_decay(\n    resources_used: float, eta: int, min_budget, max_budget\n) -&gt; float\n</code></pre> <p>Creates a step function schedule for the prior weight decay.</p> <p>The prior weight ratio is decayed every time the total resources used is equivalent to the cost of one successive halving bracket within the HB schedule. This is approximately eta   imes max_budget resources for one evaluation.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def get_prior_weight_for_decay(\n    resources_used: float, eta: int, min_budget, max_budget\n) -&gt; float:\n    \"\"\"Creates a step function schedule for the prior weight decay.\n\n    The prior weight ratio is decayed every time the total resources used is\n    equivalent to the cost of one successive halving bracket within the HB schedule.\n    This is approximately eta \\times max_budget resources for one evaluation.\n    \"\"\"\n    # decay factor for the prior\n    decay = 2\n    unit_resources = eta * max_budget\n    idx = resources_used // unit_resources\n    weight = 1 / decay**idx\n    return weight\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.local_mutation","title":"local_mutation","text":"<pre><code>local_mutation(\n    config: SearchSpace,\n    std: float = 0.25,\n    mutation_rate: float = 0.5,\n    patience: int = 50,\n    mutate_categoricals: bool = True,\n    mutate_graphs: bool = True,\n) -&gt; SearchSpace\n</code></pre> <p>Performs a local search by mutating randomly chosen hyperparameters.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def local_mutation(\n    config: SearchSpace,\n    std: float = 0.25,\n    mutation_rate: float = 0.5,\n    patience: int = 50,\n    mutate_categoricals: bool = True,\n    mutate_graphs: bool = True,\n) -&gt; SearchSpace:\n    \"\"\"Performs a local search by mutating randomly chosen hyperparameters.\"\"\"\n    for _ in range(patience):\n        new_config: dict[str, Parameter] = {}\n\n        for hp_name, hp in config.items():\n\n            if hp.is_fidelity or np.random.uniform() &gt; mutation_rate:\n                new_config[hp_name] = hp.clone()\n\n            elif isinstance(hp, CategoricalParameter):\n                if mutate_categoricals:\n                    new_config[hp_name] = hp.mutate(mutation_strategy=\"local_search\")\n                else:\n                    new_config[hp_name] = hp.clone()\n\n            elif isinstance(hp, GraphParameter):\n                if mutate_graphs:\n                    new_config[hp_name] = hp.mutate(mutation_strategy=\"bananas\")\n                else:\n                    new_config[hp_name] = hp.clone()\n\n            elif isinstance(hp, NumericalParameter):\n                new_config[hp_name] = hp.mutate(\n                    mutation_strategy=\"local_search\",\n                    std=std,\n                )\n            elif isinstance(hp, ConstantParameter):\n                new_config[hp_name] = hp.clone()\n\n            else:\n                raise NotImplementedError(f\"Unknown hp type for {hp_name}: {type(hp)}\")\n\n        # if the new config doesn't differ from the original config then regenerate\n        _new_ss = SearchSpace(**new_config)\n        if not config.is_equal_value(_new_ss, include_fidelity=False):\n            return _new_ss\n\n    return config.clone()\n</code></pre>"},{"location":"api/neps/optimizers/multiple_knowledge_sources/prototype_optimizer/","title":"Prototype optimizer","text":""},{"location":"api/neps/optimizers/multiple_knowledge_sources/prototype_optimizer/#neps.optimizers.multiple_knowledge_sources.prototype_optimizer","title":"neps.optimizers.multiple_knowledge_sources.prototype_optimizer","text":""},{"location":"api/neps/optimizers/random_search/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/optimizers/random_search/optimizer/#neps.optimizers.random_search.optimizer","title":"neps.optimizers.random_search.optimizer","text":""},{"location":"api/neps/optimizers/regularized_evolution/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/optimizers/regularized_evolution/optimizer/#neps.optimizers.regularized_evolution.optimizer","title":"neps.optimizers.regularized_evolution.optimizer","text":""},{"location":"api/neps/plot/plot/","title":"Plot","text":""},{"location":"api/neps/plot/plot/#neps.plot.plot","title":"neps.plot.plot","text":"<p>Plot results of a neural pipeline search run.</p>"},{"location":"api/neps/plot/plot/#neps.plot.plot.plot","title":"plot","text":"<pre><code>plot(\n    root_directory: str | Path,\n    *,\n    scientific_mode: bool = False,\n    key_to_extract: str | None = None,\n    benchmarks: list[str] | None = None,\n    algorithms: list[str] | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n    x_range: tuple | None = None,\n    log_x: bool = False,\n    log_y: bool = True,\n    filename: str = \"incumbent_trajectory\",\n    extension: str = \"png\",\n    dpi: int = 100\n) -&gt; None\n</code></pre> <p>Plot results of a neural pipeline search run.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The directory with neps results (see below).</p> <p> TYPE: <code>str | Path</code> </p> <code>scientific_mode</code> <p>If true, plot from a tree-structured root_directory: benchmark={}/algorithm={}/seed={}</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>key_to_extract</code> <p>The metric to be used on the x-axis (if active, make sure run_pipeline returns the metric in the info_dict)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>benchmarks</code> <p>List of benchmarks to plot</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>algorithms</code> <p>List of algorithms to plot</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>consider_continuations</code> <p>If true, toggle calculation of continuation costs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_workers</code> <p>Number of parallel processes of neps.run</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>x_range</code> <p>Bound x-axis (e.g. 1 10)</p> <p> TYPE: <code>tuple | None</code> DEFAULT: <code>None</code> </p> <code>log_x</code> <p>If true, toggle logarithmic scale on the x-axis</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>log_y</code> <p>If true, toggle logarithmic scale on the y-axis</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>filename</code> <p>Filename</p> <p> TYPE: <code>str</code> DEFAULT: <code>'incumbent_trajectory'</code> </p> <code>extension</code> <p>Image format</p> <p> TYPE: <code>str</code> DEFAULT: <code>'png'</code> </p> <code>dpi</code> <p>Image resolution</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the data to be plotted is not present.</p> Source code in <code>neps/plot/plot.py</code> <pre><code>def plot(  # noqa: C901, PLR0913\n    root_directory: str | Path,\n    *,\n    scientific_mode: bool = False,\n    key_to_extract: str | None = None,\n    benchmarks: list[str] | None = None,\n    algorithms: list[str] | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n    x_range: tuple | None = None,\n    log_x: bool = False,\n    log_y: bool = True,\n    filename: str = \"incumbent_trajectory\",\n    extension: str = \"png\",\n    dpi: int = 100,\n) -&gt; None:\n    \"\"\"Plot results of a neural pipeline search run.\n\n    Args:\n        root_directory: The directory with neps results (see below).\n        scientific_mode: If true, plot from a tree-structured root_directory:\n            benchmark={}/algorithm={}/seed={}\n        key_to_extract: The metric to be used on the x-axis\n            (if active, make sure run_pipeline returns the metric in the info_dict)\n        benchmarks: List of benchmarks to plot\n        algorithms: List of algorithms to plot\n        consider_continuations: If true, toggle calculation of continuation costs\n        n_workers: Number of parallel processes of neps.run\n        x_range: Bound x-axis (e.g. 1 10)\n        log_x: If true, toggle logarithmic scale on the x-axis\n        log_y: If true, toggle logarithmic scale on the y-axis\n        filename: Filename\n        extension: Image format\n        dpi: Image resolution\n\n    Raises:\n        FileNotFoundError: If the data to be plotted is not present.\n    \"\"\"\n    logger = logging.getLogger(\"neps\")\n    logger.info(f\"Starting neps.plot using working directory {root_directory}\")\n\n    if benchmarks is None:\n        benchmarks = [\"example\"]\n    if algorithms is None:\n        algorithms = [\"neps\"]\n\n    logger.info(\n        f\"Processing {len(benchmarks)} benchmark(s) \"\n        f\"and {len(algorithms)} algorithm(s)...\"\n    )\n\n    ncols = 1 if len(benchmarks) == 1 else 2\n    nrows = np.ceil(len(benchmarks) / ncols).astype(int)\n\n    fig, axs = _get_fig_and_axs(nrows=nrows, ncols=ncols)\n\n    base_path = Path(root_directory)\n\n    for benchmark_idx, benchmark in enumerate(benchmarks):\n        if scientific_mode:\n            _base_path = base_path / f\"benchmark={benchmark}\"\n            if not _base_path.is_dir():\n                raise FileNotFoundError(\n                    errno.ENOENT, os.strerror(errno.ENOENT), _base_path\n                )\n        else:\n            _base_path = None\n\n        for algorithm in algorithms:\n            seeds = [None]\n            if _base_path is not None:\n                assert scientific_mode\n                _path = _base_path / f\"algorithm={algorithm}\"\n                if not _path.is_dir():\n                    raise FileNotFoundError(\n                        errno.ENOENT, os.strerror(errno.ENOENT), _path\n                    )\n\n                seeds = sorted(os.listdir(_path))  # type: ignore\n            else:\n                _path = None\n\n            incumbents = []\n            costs = []\n            max_costs = []\n            for seed in seeds:\n                incumbent, cost, max_cost = process_seed(\n                    path=_path if _path is not None else base_path,\n                    seed=seed,\n                    key_to_extract=key_to_extract,\n                    consider_continuations=consider_continuations,\n                    n_workers=n_workers,\n                )\n                incumbents.append(incumbent)\n                costs.append(cost)\n                max_costs.append(max_cost)\n\n            is_last_row = benchmark_idx &gt;= (nrows - 1) * ncols\n            is_first_column = benchmark_idx % ncols == 0\n            xlabel = \"Evaluations\" if key_to_extract is None else key_to_extract.upper()\n            _plot_incumbent(\n                ax=_map_axs(\n                    axs,\n                    benchmark_idx,\n                    len(benchmarks),\n                    ncols,\n                ),\n                x=costs,\n                y=incumbents,\n                scale_x=max(max_costs) if key_to_extract == \"fidelity\" else None,\n                title=benchmark if scientific_mode else None,\n                xlabel=xlabel if is_last_row else None,\n                ylabel=\"Best error\" if is_first_column else None,\n                log_x=log_x,\n                log_y=log_y,\n                x_range=x_range,\n                label=algorithm,\n            )\n\n    if scientific_mode:\n        _set_legend(\n            fig,\n            axs,\n            benchmarks=benchmarks,\n            algorithms=algorithms,\n            nrows=nrows,\n            ncols=ncols,\n        )\n    _save_fig(fig, output_dir=base_path, filename=filename, extension=extension, dpi=dpi)\n    logger.info(f\"Saved to '{base_path}/{filename}.{extension}'\")\n</code></pre>"},{"location":"api/neps/plot/plotting/","title":"Plotting","text":""},{"location":"api/neps/plot/plotting/#neps.plot.plotting","title":"neps.plot.plotting","text":"<p>Plotting functions for incumbent trajectory plots.</p>"},{"location":"api/neps/plot/read_results/","title":"Read results","text":""},{"location":"api/neps/plot/read_results/#neps.plot.read_results","title":"neps.plot.read_results","text":"<p>Utility functions for reading and processing results.</p>"},{"location":"api/neps/plot/read_results/#neps.plot.read_results.process_seed","title":"process_seed","text":"<pre><code>process_seed(\n    *,\n    path: str | Path,\n    seed: str | int | None,\n    key_to_extract: str | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1\n) -&gt; tuple[list[float], list[float], float]\n</code></pre> <p>Reads and processes data per seed.</p> Source code in <code>neps/plot/read_results.py</code> <pre><code>def process_seed(\n    *,\n    path: str | Path,\n    seed: str | int | None,\n    key_to_extract: str | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n) -&gt; tuple[list[float], list[float], float]:\n    \"\"\"Reads and processes data per seed.\"\"\"\n    path = Path(path)\n    if seed is not None:\n        path = path / str(seed) / \"neps_root_directory\"\n\n    stats, _ = neps.status(path, print_summary=False)\n    sorted_stats = sorted(sorted(stats.items()), key=lambda x: len(x[0]))\n    stats = OrderedDict(sorted_stats)\n\n    # max_cost only relevant for scaling x-axis when using fidelity on the x-axis\n    max_cost: float = -1.0\n    if key_to_extract == \"fidelity\":\n        # TODO(eddiebergman): This can crash for a number of reasons, namely if the config\n        # crased and it's result is an error, or if the `\"info_dict\"` and/or\n        # `key_to_extract` doesn't exist\n        max_cost = max(s.result[\"info_dict\"][key_to_extract] for s in stats.values())  # type: ignore\n\n    global_start = stats[min(stats.keys())].metadata[\"time_sampled\"]\n\n    def get_cost(idx: str) -&gt; float:\n        if key_to_extract is not None:\n            # TODO(eddiebergman): This can crash for a number of reasons, namely if the\n            # config crased and it's result is an error, or if the `\"info_dict\"` and/or\n            # `key_to_extract` doesn't exist\n            return float(stats[idx].result[\"info_dict\"][key_to_extract])  # type: ignore\n\n        return 1.0\n\n    losses = []\n    costs = []\n\n    for config_id, config_result in stats.items():\n        config_cost = get_cost(config_id)\n        if consider_continuations:\n            if n_workers == 1:\n                # calculates continuation costs for MF algorithms NOTE: assumes that\n                # all recorded evaluations are black-box evaluations where\n                # continuations or freeze-thaw was not accounted for during optimization\n                if \"previous_config_id\" in config_result.metadata:\n                    previous_config_id = config_result.metadata[\"previous_config_id\"]\n                    config_cost -= get_cost(previous_config_id)\n            else:\n                config_cost = config_result.metadata[\"time_end\"] - global_start\n\n        # TODO(eddiebergman): Assumes it never crashed and there's a loss available,\n        # not fixing now but it should be addressed\n        losses.append(config_result.result[\"loss\"])  # type: ignore\n        costs.append(config_cost)\n\n    return list(np.minimum.accumulate(losses)), costs, max_cost\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/","title":"Tensorboard eval","text":""},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval","title":"neps.plot.tensorboard_eval","text":"<p>The tblogger module provides a simplified interface for logging to TensorBoard.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.SummaryWriter_","title":"SummaryWriter_","text":"<p>               Bases: <code>SummaryWriter</code></p> <p>This class inherits from the base SummaryWriter class and provides modifications to improve the logging. It simplifies the logging structure and ensures consistent tag formatting for metrics.</p> <p>Changes Made: - Avoids creating unnecessary subfolders in the log directory. - Ensures all logs are stored in the same 'tfevent' directory for   better organization. - Updates metric keys to have a consistent 'Summary/' prefix for clarity. - Improves the display of 'Loss' or 'Accuracy' on the Summary file.</p> <p>Methods: - add_hparams: Overrides the base method to log hyperparameters and metrics with better formatting.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.SummaryWriter_.add_hparams","title":"add_hparams","text":"<pre><code>add_hparams(\n    hparam_dict: dict[str, Any],\n    metric_dict: dict[str, Any],\n    global_step: int,\n) -&gt; None\n</code></pre> <p>Add a set of hyperparameters to be logged.</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@override\ndef add_hparams(  # type: ignore\n    self,\n    hparam_dict: dict[str, Any],\n    metric_dict: dict[str, Any],\n    global_step: int,\n) -&gt; None:\n    \"\"\"Add a set of hyperparameters to be logged.\"\"\"\n    if not isinstance(hparam_dict, dict) or not isinstance(metric_dict, dict):\n        raise TypeError(\"hparam_dict and metric_dict should be dictionary.\")\n    updated_metric = {f\"Summary/{key}\": val for key, val in metric_dict.items()}\n    exp, ssi, sei = hparams(hparam_dict, updated_metric)\n\n    assert self.file_writer is not None\n    self.file_writer.add_summary(exp)\n    self.file_writer.add_summary(ssi)\n    self.file_writer.add_summary(sei)\n    for k, v in updated_metric.items():\n        self.add_scalar(tag=k, scalar_value=v, global_step=global_step)\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger","title":"tblogger","text":"<p>The tblogger class provides a simplified interface for logging to tensorboard.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.logger_bool","title":"logger_bool  <code>class-attribute</code>","text":"<pre><code>logger_bool: bool = False\n</code></pre> <p>logger_bool is true only if tblogger.log is used by the user, this allows to always capturing the configuration data.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.disable","title":"disable  <code>staticmethod</code>","text":"<pre><code>disable() -&gt; None\n</code></pre> <p>The function allows for disabling the logger functionality. When the logger is disabled, it will not perform logging operations.</p> <p>By default tblogger is enabled when used.</p> Example Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef disable() -&gt; None:\n    \"\"\"The function allows for disabling the logger functionality.\n    When the logger is disabled, it will not perform logging operations.\n\n    By default tblogger is enabled when used.\n\n    Example:\n        # Disable the logger\n        tblogger.disable()\n    \"\"\"\n    tblogger.disable_logging = True\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.disable--disable-the-logger","title":"Disable the logger","text":"<p>tblogger.disable()</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.enable","title":"enable  <code>staticmethod</code>","text":"<pre><code>enable() -&gt; None\n</code></pre> <p>The function allows for enabling the logger functionality. When the logger is enabled, it will perform the logging operations.</p> <p>By default this is enabled.</p> Example Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef enable() -&gt; None:\n    \"\"\"The function allows for enabling the logger functionality.\n    When the logger is enabled, it will perform the logging operations.\n\n    By default this is enabled.\n\n    Example:\n        # Enable the logger\n        tblogger.enable()\n    \"\"\"\n    tblogger.disable_logging = False\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.enable--enable-the-logger","title":"Enable the logger","text":"<p>tblogger.enable()</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.end_of_config","title":"end_of_config  <code>staticmethod</code>","text":"<pre><code>end_of_config() -&gt; None\n</code></pre> <p>Closes the writer.</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef end_of_config() -&gt; None:\n    \"\"\"Closes the writer.\"\"\"\n    if tblogger.config_writer:\n        # Close and reset previous config writers for consistent logging.\n        # Prevent conflicts by reinitializing writers when logging ongoing.\n        tblogger.config_writer.close()\n        tblogger.config_writer = None\n\n    if tblogger.write_incumbent:\n        tblogger._tracking_incumbent_api()\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.get_status","title":"get_status  <code>staticmethod</code>","text":"<pre><code>get_status() -&gt; bool\n</code></pre> <p>Returns the currect state of tblogger ie. whether the logger is enabled or not.</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef get_status() -&gt; bool:\n    \"\"\"Returns the currect state of tblogger ie. whether the logger is\n    enabled or not.\n    \"\"\"\n    return not tblogger.disable_logging\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.image_logging","title":"image_logging  <code>staticmethod</code>","text":"<pre><code>image_logging(\n    image: Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | RandomState | None = None\n) -&gt; tuple[\n    str,\n    Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | RandomState | None,\n]\n</code></pre> <p>Prepare an image tensor for logging.</p> PARAMETER DESCRIPTION <code>image</code> <p>Image tensor to be logged.</p> <p> TYPE: <code>Tensor</code> </p> <code>counter</code> <p>Counter value associated with the images.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>resize_images</code> <p>List of integers for image sizes after resizing.</p> <p> TYPE: <code>list[None | int] | None</code> DEFAULT: <code>None</code> </p> <code>random_images</code> <p>Images are randomly selected if True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_images</code> <p>Number of images to log.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>seed</code> <p>Seed value or RandomState instance to control randomness.</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[str, Tensor, int, list[None | int] | None, bool, int, int | RandomState | None]</code> <p>A tuple containing the logging mode and all the necessary parameters for image logging. Tuple: (logging_mode, img_tensor, counter, resize_images,                 random_images, num_images, seed).</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef image_logging(\n    image: torch.Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | np.random.RandomState | None = None,\n) -&gt; tuple[\n    str,\n    torch.Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | np.random.RandomState | None,\n]:\n    \"\"\"Prepare an image tensor for logging.\n\n    Args:\n        image: Image tensor to be logged.\n        counter: Counter value associated with the images.\n        resize_images: List of integers for image sizes after resizing.\n        random_images: Images are randomly selected if True.\n        num_images: Number of images to log.\n        seed: Seed value or RandomState instance to control randomness.\n\n    Returns:\n        A tuple containing the logging mode and all the necessary parameters for\n        image logging.\n        Tuple: (logging_mode, img_tensor, counter, resize_images,\n                        random_images, num_images, seed).\n    \"\"\"\n    logging_mode = \"image\"\n    return (\n        logging_mode,\n        image,\n        counter,\n        resize_images,\n        random_images,\n        num_images,\n        seed,\n    )\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.log","title":"log  <code>staticmethod</code>","text":"<pre><code>log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None\n) -&gt; None\n</code></pre> <p>Log experiment data to the logger, including scalar values, hyperparameters, and images.</p> PARAMETER DESCRIPTION <code>loss</code> <p>Current loss value.</p> <p> TYPE: <code>float</code> </p> <code>current_epoch</code> <p>Current epoch of the experiment (used as the global step).</p> <p> TYPE: <code>int</code> </p> <code>writer_config_scalar</code> <p>Displaying the loss or accuracy curve on tensorboard (default: True)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>writer_config_hparam</code> <p>Write hyperparameters logging of the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>write_summary_incumbent</code> <p>Set to <code>True</code> for a live incumbent trajectory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>extra_data</code> <p>Additional experiment data for logging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None,\n) -&gt; None:\n    \"\"\"Log experiment data to the logger, including scalar values,\n    hyperparameters, and images.\n\n    Args:\n        loss: Current loss value.\n        current_epoch: Current epoch of the experiment (used as the global step).\n        writer_config_scalar: Displaying the loss or accuracy\n            curve on tensorboard (default: True)\n        writer_config_hparam: Write hyperparameters logging of the configs.\n        write_summary_incumbent: Set to `True` for a live incumbent trajectory.\n        extra_data: Additional experiment data for logging.\n    \"\"\"\n    if tblogger.disable_logging:\n        tblogger.logger_bool = False\n        return\n\n    tblogger.logger_bool = True\n\n    tblogger.current_epoch = current_epoch\n    tblogger.loss = loss\n    tblogger.write_incumbent = write_summary_incumbent\n\n    tblogger._initiate_internal_configurations()\n\n    if writer_config_scalar:\n        tblogger._write_scalar_config(tag=\"Loss\", value=loss)\n\n    if writer_config_hparam:\n        tblogger._write_hparam_config()\n\n    if extra_data is not None:\n        for key in extra_data:\n            if extra_data[key][0] == \"scalar\":\n                tblogger._write_scalar_config(tag=str(key), value=extra_data[key][1])\n\n            elif extra_data[key][0] == \"image\":\n                tblogger._write_image_config(\n                    tag=str(key),\n                    image=extra_data[key][1],\n                    counter=extra_data[key][2],\n                    resize_images=extra_data[key][3],\n                    random_images=extra_data[key][4],\n                    num_images=extra_data[key][5],\n                    seed=extra_data[key][6],\n                )\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.scalar_logging","title":"scalar_logging  <code>staticmethod</code>","text":"<pre><code>scalar_logging(value: float) -&gt; tuple[str, float]\n</code></pre> <p>Prepare a scalar value for logging.</p> PARAMETER DESCRIPTION <code>value</code> <p>The scalar value to be logged.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tuple</code> <p>A tuple containing the logging mode and the value for logging.     The tuple format is (logging_mode, value).</p> <p> TYPE: <code>tuple[str, float]</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef scalar_logging(value: float) -&gt; tuple[str, float]:\n    \"\"\"Prepare a scalar value for logging.\n\n    Args:\n        value (float): The scalar value to be logged.\n\n    Returns:\n        Tuple: A tuple containing the logging mode and the value for logging.\n            The tuple format is (logging_mode, value).\n    \"\"\"\n    logging_mode = \"scalar\"\n    return (logging_mode, value)\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/","title":"Parameter","text":""},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter","title":"neps.search_spaces.parameter","text":"<p>The base <code>Parameter</code> class.</p> <p>The <code>Parameter</code> refers to both the hyperparameter definition but also holds a <code>.value</code> which can be set or empty, in which case it is <code>None</code>.</p> <p>Tip</p> <p>A <code>Parameter</code> which allows for mutations and crossovers should implement the <code>MutatableParameter</code> protocol.</p> <p>Tip</p> <p>A <code>Parameter</code> which allows for defining a <code>.default</code> and some prior, i.e. some default value along with a confidence that this is a good setting, should implement the <code>ParameterWithPrior</code> class.</p> <p>This is utilized by certain optimization routines to inform the search process.</p>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.MutatableParameter","title":"MutatableParameter","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for hyperparameters that can be mutated.</p> <p>Particpating parameters must implement the <code>mutate()</code> method and the <code>crossover()</code> method.</p>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.MutatableParameter.crossover","title":"crossover","text":"<pre><code>crossover(\n    parent1: Self, parent2: Self | None = None\n) -&gt; tuple[Self, Self]\n</code></pre> <p>Crossover the hyperparameter with another hyperparameter.</p> PARAMETER DESCRIPTION <code>parent1</code> <p>the first parent hyperparameter.</p> <p> TYPE: <code>Self</code> </p> <code>parent2</code> <p>the second parent hyperparameter. If left as <code>None</code>, this hyperparameter will be used as the second parent to crossover with.</p> <p> TYPE: <code>Self | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Self, Self]</code> <p>A tuple of the two crossovered hyperparameters.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def crossover(self, parent1: Self, parent2: Self | None = None) -&gt; tuple[Self, Self]:\n    \"\"\"Crossover the hyperparameter with another hyperparameter.\n\n    Args:\n        parent1: the first parent hyperparameter.\n        parent2: the second parent hyperparameter.\n            If left as `None`, this hyperparameter will be used as the second parent\n            to crossover with.\n\n    Returns:\n        A tuple of the two crossovered hyperparameters.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.MutatableParameter.mutate","title":"mutate","text":"<pre><code>mutate(parent: Self | None = None) -&gt; Self\n</code></pre> <p>Mutate the hyperparameter.</p> PARAMETER DESCRIPTION <code>parent</code> <p>the parent hyperparameter to mutate from.</p> <p> TYPE: <code>Self | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The mutated hyperparameter.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def mutate(self, parent: Self | None = None) -&gt; Self:\n    \"\"\"Mutate the hyperparameter.\n\n    Args:\n        parent: the parent hyperparameter to mutate from.\n\n    Returns:\n        The mutated hyperparameter.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter","title":"Parameter","text":"<pre><code>Parameter(\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool\n)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[ValueT, SerializedT]</code></p> <p>A base class for hyperparameters.</p> ATTRIBUTE DESCRIPTION <code>default</code> <p>default value for the hyperparameter. This value is used as a prior to inform algorithms about a decent default value for the hyperparameter, as well as use attributes from <code>ParameterWithPrior</code>, to aid in optimization.</p> <p> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> </p> <code>value</code> <p>value for the hyperparameter, if any.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>normalized_value</code> <p>normalized value for the hyperparameter.</p> <p> </p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool,\n):\n    \"\"\"Create a new `Parameter`.\n\n    Args:\n        value: value for the hyperparameter.\n        default: default value for the hyperparameter.\n        is_fidelity: whether the hyperparameter is fidelity.\n    \"\"\"\n    self.default = default\n    self.is_fidelity = is_fidelity\n\n    # TODO(eddiebergman): The reason to have this not as a straight alone\n    # attribute is that the graph parameters currently expose there own\n    # way of calculating a value on demand.\n    # To fix this would mean to essentially decouple GraphParameter entirely\n    # from Parameter as it's less of a heirarchy and more of just a small overlap\n    # of functionality.\n    self._value = value\n    self.normalized_value = (\n        self.value_to_normalized(value) if value is not None else None\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.clone","title":"clone  <code>abstractmethod</code>","text":"<pre><code>clone() -&gt; Self\n</code></pre> <p>Create a copy of the <code>Parameter</code>.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef clone(self) -&gt; Self:\n    \"\"\"Create a copy of the `Parameter`.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.deserialize_value","title":"deserialize_value  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>deserialize_value(value: SerializedT) -&gt; ValueT\n</code></pre> <p>Deserialize a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to deserialize.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@classmethod\n@abstractmethod\ndef deserialize_value(cls, value: SerializedT) -&gt; ValueT:\n    \"\"\"Deserialize a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to deserialize.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.normalized_to_value","title":"normalized_to_value  <code>abstractmethod</code>","text":"<pre><code>normalized_to_value(normalized_value: float) -&gt; ValueT\n</code></pre> <p>Convert a normalized value back to value in the defined hyperparameter range.</p> PARAMETER DESCRIPTION <code>normalized_value</code> <p>normalized value to convert.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef normalized_to_value(self, normalized_value: float) -&gt; ValueT:\n    \"\"\"Convert a normalized value back to value in the defined hyperparameter range.\n\n    Args:\n        normalized_value: normalized value to convert.\n\n    Returns:\n        The value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.sample","title":"sample","text":"<pre><code>sample() -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Will set the <code>.value</code> to the sampled value.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Will set the [`.value`][neps.search_spaces.Parameter.value] to the\n    sampled value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value()\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.sample_value","title":"sample_value  <code>abstractmethod</code>","text":"<pre><code>sample_value() -&gt; ValueT\n</code></pre> <p>Sample a new value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef sample_value(self) -&gt; ValueT:\n    \"\"\"Sample a new value.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.serialize_value","title":"serialize_value  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>serialize_value(value: ValueT) -&gt; SerializedT\n</code></pre> <p>Ensure the hyperparameter value is in a serializable format.</p> RETURNS DESCRIPTION <code>SerializedT</code> <p>A serializable version of the hyperparameter value</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@classmethod\n@abstractmethod\ndef serialize_value(cls, value: ValueT) -&gt; SerializedT:\n    \"\"\"Ensure the hyperparameter value is in a serializable format.\n\n\n    Returns:\n        A serializable version of the hyperparameter value\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.set_default","title":"set_default  <code>abstractmethod</code>","text":"<pre><code>set_default(default: ValueT | None) -&gt; None\n</code></pre> <p>Set the default value for the hyperparameter.</p> <p>The <code>default=</code> is used as a prior and used to inform algorithms about a decent default value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_default(self, default: ValueT | None) -&gt; None:\n    \"\"\"Set the default value for the hyperparameter.\n\n    The `default=` is used as a prior and used to inform\n    algorithms about a decent default value for the hyperparameter.\n\n    Args:\n        default: default value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.set_value","title":"set_value  <code>abstractmethod</code>","text":"<pre><code>set_value(value: ValueT | None) -&gt; None\n</code></pre> <p>Set the value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_value(self, value: ValueT | None) -&gt; None:\n    \"\"\"Set the value for the hyperparameter.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.value_to_normalized","title":"value_to_normalized  <code>abstractmethod</code>","text":"<pre><code>value_to_normalized(value: ValueT) -&gt; float\n</code></pre> <p>Convert a value to a normalized value.</p> <p>Normalization is different per hyperparameter type, but roughly refers to numeric values.</p> <ul> <li><code>(0, 1)</code> scaling in the case of     a <code>NumericalParameter</code>,</li> <li><code>{0.0, 1.0}</code> for a <code>ConstantParameter</code>,</li> <li><code>[0, 1, ..., n]</code> for a     <code>Categorical</code>.</li> </ul> PARAMETER DESCRIPTION <code>value</code> <p>value to convert.</p> <p> TYPE: <code>ValueT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The normalized value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef value_to_normalized(self, value: ValueT) -&gt; float:\n    \"\"\"Convert a value to a normalized value.\n\n    Normalization is different per hyperparameter type,\n    but roughly refers to numeric values.\n\n    * `(0, 1)` scaling in the case of\n        a [`NumericalParameter`][neps.search_spaces.NumericalParameter],\n    * `{0.0, 1.0}` for a [`ConstantParameter`][neps.search_spaces.ConstantParameter],\n    * `[0, 1, ..., n]` for a\n        [`Categorical`][neps.search_spaces.CategoricalParameter].\n\n    Args:\n        value: value to convert.\n\n    Returns:\n        The normalized value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior","title":"ParameterWithPrior","text":"<pre><code>ParameterWithPrior(\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool\n)\n</code></pre> <p>               Bases: <code>Parameter[ValueT, SerializedT]</code></p> <p>A base class for hyperparameters with priors.</p> ATTRIBUTE DESCRIPTION <code>default_confidence_choice</code> <p>The choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> <code>default_confidence_score</code> <p>A score used by algorithms to utilize the default value.</p> <p> TYPE: <code>float</code> </p> <code>has_prior</code> <p>whether the hyperparameter has a prior that can be used by an algorithm. In many cases, this refers to having a default value.</p> <p> TYPE: <code>bool</code> </p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool,\n):\n    \"\"\"Create a new `Parameter`.\n\n    Args:\n        value: value for the hyperparameter.\n        default: default value for the hyperparameter.\n        is_fidelity: whether the hyperparameter is fidelity.\n    \"\"\"\n    self.default = default\n    self.is_fidelity = is_fidelity\n\n    # TODO(eddiebergman): The reason to have this not as a straight alone\n    # attribute is that the graph parameters currently expose there own\n    # way of calculating a value on demand.\n    # To fix this would mean to essentially decouple GraphParameter entirely\n    # from Parameter as it's less of a heirarchy and more of just a small overlap\n    # of functionality.\n    self._value = value\n    self.normalized_value = (\n        self.value_to_normalized(value) if value is not None else None\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.clone","title":"clone  <code>abstractmethod</code>","text":"<pre><code>clone() -&gt; Self\n</code></pre> <p>Create a copy of the <code>Parameter</code>.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef clone(self) -&gt; Self:\n    \"\"\"Create a copy of the `Parameter`.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.compute_prior","title":"compute_prior  <code>abstractmethod</code>","text":"<pre><code>compute_prior(*, log: bool = True) -&gt; float\n</code></pre> <p>Compute the likelihood of the currently set value from the sampling distribution of the hyperparameter.</p> PARAMETER DESCRIPTION <code>log</code> <p>whether to return the log likelihood.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef compute_prior(self, *, log: bool = True) -&gt; float:\n    \"\"\"Compute the likelihood of the currently set value from\n    the sampling distribution of the hyperparameter.\n\n    Args:\n        log: whether to return the log likelihood.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.deserialize_value","title":"deserialize_value  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>deserialize_value(value: SerializedT) -&gt; ValueT\n</code></pre> <p>Deserialize a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to deserialize.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@classmethod\n@abstractmethod\ndef deserialize_value(cls, value: SerializedT) -&gt; ValueT:\n    \"\"\"Deserialize a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to deserialize.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.normalized_to_value","title":"normalized_to_value  <code>abstractmethod</code>","text":"<pre><code>normalized_to_value(normalized_value: float) -&gt; ValueT\n</code></pre> <p>Convert a normalized value back to value in the defined hyperparameter range.</p> PARAMETER DESCRIPTION <code>normalized_value</code> <p>normalized value to convert.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef normalized_to_value(self, normalized_value: float) -&gt; ValueT:\n    \"\"\"Convert a normalized value back to value in the defined hyperparameter range.\n\n    Args:\n        normalized_value: normalized value to convert.\n\n    Returns:\n        The value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.sample_value","title":"sample_value  <code>abstractmethod</code>","text":"<pre><code>sample_value(*, user_priors: bool = False) -&gt; ValueT\n</code></pre> <p>Sample a new value.</p> <p>Similar to <code>Parameter.sample_value()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef sample_value(self, *, user_priors: bool = False) -&gt; ValueT:\n    \"\"\"Sample a new value.\n\n    Similar to\n    [`Parameter.sample_value()`][neps.search_spaces.Parameter.sample_value],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        The sampled value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.serialize_value","title":"serialize_value  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>serialize_value(value: ValueT) -&gt; SerializedT\n</code></pre> <p>Ensure the hyperparameter value is in a serializable format.</p> RETURNS DESCRIPTION <code>SerializedT</code> <p>A serializable version of the hyperparameter value</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@classmethod\n@abstractmethod\ndef serialize_value(cls, value: ValueT) -&gt; SerializedT:\n    \"\"\"Ensure the hyperparameter value is in a serializable format.\n\n\n    Returns:\n        A serializable version of the hyperparameter value\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.set_default","title":"set_default  <code>abstractmethod</code>","text":"<pre><code>set_default(default: ValueT | None) -&gt; None\n</code></pre> <p>Set the default value for the hyperparameter.</p> <p>The <code>default=</code> is used as a prior and used to inform algorithms about a decent default value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_default(self, default: ValueT | None) -&gt; None:\n    \"\"\"Set the default value for the hyperparameter.\n\n    The `default=` is used as a prior and used to inform\n    algorithms about a decent default value for the hyperparameter.\n\n    Args:\n        default: default value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.set_default_confidence_score","title":"set_default_confidence_score","text":"<pre><code>set_default_confidence_score(\n    default_confidence: str,\n) -&gt; None\n</code></pre> <p>Set the default confidence score for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default_confidence</code> <p>the choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the confidence score is not a valid choice.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def set_default_confidence_score(self, default_confidence: str) -&gt; None:\n    \"\"\"Set the default confidence score for the hyperparameter.\n\n    Args:\n        default_confidence: the choice of how confident any algorithm should\n            be in the default value being a good value.\n\n    Raises:\n        ValueError: if the confidence score is not a valid choice.\n    \"\"\"\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        cls_name = self.__class__.__name__\n        raise ValueError(\n            f\"Invalid default confidence score: {default_confidence}\"\n            f\" for {cls_name}. Expected one of:\"\n            f\" {list(self.DEFAULT_CONFIDENCE_SCORES.keys())}\"\n        )\n\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.set_value","title":"set_value  <code>abstractmethod</code>","text":"<pre><code>set_value(value: ValueT | None) -&gt; None\n</code></pre> <p>Set the value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_value(self, value: ValueT | None) -&gt; None:\n    \"\"\"Set the value for the hyperparameter.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.value_to_normalized","title":"value_to_normalized  <code>abstractmethod</code>","text":"<pre><code>value_to_normalized(value: ValueT) -&gt; float\n</code></pre> <p>Convert a value to a normalized value.</p> <p>Normalization is different per hyperparameter type, but roughly refers to numeric values.</p> <ul> <li><code>(0, 1)</code> scaling in the case of     a <code>NumericalParameter</code>,</li> <li><code>{0.0, 1.0}</code> for a <code>ConstantParameter</code>,</li> <li><code>[0, 1, ..., n]</code> for a     <code>Categorical</code>.</li> </ul> PARAMETER DESCRIPTION <code>value</code> <p>value to convert.</p> <p> TYPE: <code>ValueT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The normalized value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef value_to_normalized(self, value: ValueT) -&gt; float:\n    \"\"\"Convert a value to a normalized value.\n\n    Normalization is different per hyperparameter type,\n    but roughly refers to numeric values.\n\n    * `(0, 1)` scaling in the case of\n        a [`NumericalParameter`][neps.search_spaces.NumericalParameter],\n    * `{0.0, 1.0}` for a [`ConstantParameter`][neps.search_spaces.ConstantParameter],\n    * `[0, 1, ..., n]` for a\n        [`Categorical`][neps.search_spaces.CategoricalParameter].\n\n    Args:\n        value: value to convert.\n\n    Returns:\n        The normalized value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/","title":"Search space","text":""},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space","title":"neps.search_spaces.search_space","text":"<p>Contains the <code>SearchSpace</code> class which is a container for hyperparameters that can be sampled, mutated, and crossed over.</p>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace","title":"SearchSpace","text":"<pre><code>SearchSpace(**hyperparameters: Parameter)\n</code></pre> <p>               Bases: <code>Mapping[str, Any]</code></p> <p>A container for hyperparameters that can be sampled, mutated, and crossed over.</p> <p>Provides operations for operating on and generating new configurations from the hyperparameters.</p> <p>Note</p> <p>The <code>SearchSpace</code> class is both the definition of the search space and also a configuration at the same time.</p> <p>When refering to the <code>SearchSpace</code> as a configuration, the documentation will refer to it as a <code>configuration</code> or <code>config</code>. Otherwise, it will be referred to as a <code>search space</code>.</p> <p>TODO</p> <p>This documentation is WIP. If you have any questions, please reach out so we can know better what to document.</p> PARAMETER DESCRIPTION <code>**hyperparameters</code> <p>The hyperparameters that define the search space.</p> <p> TYPE: <code>Parameter</code> DEFAULT: <code>{}</code> </p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def __init__(self, **hyperparameters: Parameter):\n    \"\"\"Initialize the SearchSpace with hyperparameters.\n\n    Args:\n        **hyperparameters: The hyperparameters that define the search space.\n    \"\"\"\n    # Ensure a consistent ordering for uses throughout the lib\n    _hyperparameters = sorted(hyperparameters.items(), key=lambda x: x[0])\n    _fidelity_param: NumericalParameter | None = None\n    _fidelity_name: str | None = None\n    _has_prior: bool = False\n\n    for name, hp in _hyperparameters:\n        if hp.is_fidelity:\n            if _fidelity_param is not None:\n                raise ValueError(\n                    \"neps only supports one fidelity parameter in the pipeline space,\"\n                    \" but multiple were given. (Hint: check you pipeline space for \"\n                    \"multiple is_fidelity=True)\"\n                )\n\n            if not isinstance(hp, NumericalParameter):\n                raise ValueError(\n                    \"neps only suport float and integer fidelity parameters\"\n                )\n\n            _fidelity_param = hp\n            _fidelity_name = name\n\n        if isinstance(hp, ParameterWithPrior) and hp.has_prior:\n            _has_prior = True\n\n    self.hyperparameters: dict[str, Parameter] = dict(_hyperparameters)\n    self.fidelity: NumericalParameter | None = _fidelity_param\n    self.fidelity_name: str | None = _fidelity_name\n    self.has_prior: bool = _has_prior\n\n    # TODO(eddiebergman): This should be a seperate thing most likely and not\n    # in a `SearchSpace`.\n    # Variables for tabular bookkeeping\n    self.custom_grid_table: pd.Series | pd.DataFrame | None = None\n    self.raw_tabular_space: SearchSpace | None = None\n    self.has_tabular: bool = False\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.has_fidelity","title":"has_fidelity  <code>property</code>","text":"<pre><code>has_fidelity: bool\n</code></pre> <p>Check if the search space has a fidelity parameter.</p>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.add_hyperparameter","title":"add_hyperparameter","text":"<pre><code>add_hyperparameter(name: str, hp: Parameter) -&gt; None\n</code></pre> <p>Add a hyperparameter to the search space.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the hyperparameter.</p> <p> TYPE: <code>str</code> </p> <code>hp</code> <p>The hyperparameter to add.</p> <p> TYPE: <code>Parameter</code> </p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def add_hyperparameter(self, name: str, hp: Parameter) -&gt; None:\n    \"\"\"Add a hyperparameter to the search space.\n\n    Args:\n        name: The name of the hyperparameter.\n        hp: The hyperparameter to add.\n    \"\"\"\n    self.hyperparameters[str(name)] = hp\n    self.hyperparameters = dict(\n        sorted(self.hyperparameters.items(), key=lambda x: x[0])\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.clone","title":"clone","text":"<pre><code>clone(*, _with_tabular: bool = False) -&gt; SearchSpace\n</code></pre> <p>Create a copy of the search space.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def clone(self, *, _with_tabular: bool = False) -&gt; SearchSpace:\n    \"\"\"Create a copy of the search space.\"\"\"\n    new_copy = self.__class__(\n        **{k: v.clone() for k, v in self.hyperparameters.items()}\n    )\n    if _with_tabular and self.has_tabular:\n        assert self.custom_grid_table is not None\n        assert self.raw_tabular_space is not None\n        new_copy.set_custom_grid_space(\n            grid_table=self.custom_grid_table,\n            raw_space=self.raw_tabular_space,\n        )\n\n    return new_copy\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.compute_prior","title":"compute_prior","text":"<pre><code>compute_prior(\n    *, log: bool = False, ignore_fidelity: bool = False\n) -&gt; float\n</code></pre> <p>Compute the prior probability of the search space.</p> <p>This is better know as the <code>pdf</code> of the configuraiton in the search space, or a relative measure of how likely this configuration is under the search space.</p> PARAMETER DESCRIPTION <code>log</code> <p>Whether to compute the log of the prior.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when computing the prior.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The likelihood of the configuration in the search space.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def compute_prior(self, *, log: bool = False, ignore_fidelity: bool = False) -&gt; float:\n    \"\"\"Compute the prior probability of the search space.\n\n    This is better know as the `pdf` of the configuraiton in the search space, or a\n    relative measure of how likely this configuration is under the search space.\n\n    Args:\n        log: Whether to compute the log of the prior.\n        ignore_fidelity: Whether to ignore the fidelity parameter when\n            computing the prior.\n\n\n    Returns:\n        The likelihood of the configuration in the search space.\n    \"\"\"\n    density_value = 0.0 if log else 1.0\n    op = operator.add if log else operator.mul\n\n    prior_hps = (\n        hp\n        for hp in self.hyperparameters.values()\n        if isinstance(hp, ParameterWithPrior) and hp.has_prior\n    )\n\n    for hyperparameter in prior_hps:\n        if ignore_fidelity and hyperparameter.is_fidelity:\n            continue\n\n        hp_prior = hyperparameter.compute_prior(log=log)\n        density_value = op(density_value, hp_prior)\n\n    return density_value\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.crossover","title":"crossover","text":"<pre><code>crossover(\n    config2: SearchSpace,\n    crossover_probability_per_hyperparameter: float = 1.0,\n    patience: int = 50,\n    crossover_strategy: str = \"simple\",\n) -&gt; tuple[SearchSpace, SearchSpace]\n</code></pre> <p>Crossover this configuration with another.</p> PARAMETER DESCRIPTION <code>config2</code> <p>The other search space to crossover with.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>crossover_probability_per_hyperparameter</code> <p>The probability of crossing over each hyperparameter.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>patience</code> <p>The number of times to try to crossover a valid value for a hyperparameter.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>crossover_strategy</code> <p>The strategy to use for crossover.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'simple'</code> </p> RETURNS DESCRIPTION <code>tuple[SearchSpace, SearchSpace]</code> <p>A tuple of the two new configurations.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def crossover(\n    self,\n    config2: SearchSpace,\n    crossover_probability_per_hyperparameter: float = 1.0,\n    patience: int = 50,\n    crossover_strategy: str = \"simple\",\n) -&gt; tuple[SearchSpace, SearchSpace]:\n    \"\"\"Crossover this configuration with another.\n\n    Args:\n        config2: The other search space to crossover with.\n        crossover_probability_per_hyperparameter: The probability of crossing over\n            each hyperparameter.\n        patience: The number of times to try to crossover a valid value for a\n            hyperparameter.\n        crossover_strategy: The strategy to use for crossover.\n\n    Returns:\n        A tuple of the two new configurations.\n    \"\"\"\n    if crossover_strategy == \"simple\":\n        new_config1, new_config2 = self._simple_crossover(\n            config2=config2,\n            crossover_probability_per_hyperparameter=crossover_probability_per_hyperparameter,\n            patience=patience,\n        )\n    else:\n        raise NotImplementedError(\"No such crossover strategy!\")\n\n    if len(self.hyperparameters.keys()) != len(new_config1):\n        raise Exception(\"Cannot crossover\")\n\n    return SearchSpace(**new_config1), SearchSpace(**new_config2)\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.get_normalized_hp_categories","title":"get_normalized_hp_categories","text":"<pre><code>get_normalized_hp_categories(\n    *, ignore_fidelity: bool = False\n) -&gt; dict[\n    Literal[\"continuous\", \"categorical\", \"graphs\"],\n    list[Any],\n]\n</code></pre> <p>Get the normalized values for each hyperparameter in the configuraiton.</p> PARAMETER DESCRIPTION <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when getting the normalized values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict[Literal['continuous', 'categorical', 'graphs'], list[Any]]</code> <p>A dictionary of the normalized values for each hyperparameter, separated by type.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def get_normalized_hp_categories(\n    self,\n    *,\n    ignore_fidelity: bool = False,\n) -&gt; dict[Literal[\"continuous\", \"categorical\", \"graphs\"], list[Any]]:\n    \"\"\"Get the normalized values for each hyperparameter in the configuraiton.\n\n    Args:\n        ignore_fidelity: Whether to ignore the fidelity parameter when getting the\n            normalized values.\n\n    Returns:\n        A dictionary of the normalized values for each hyperparameter,\n        separated by type.\n    \"\"\"\n    hps: dict[Literal[\"continuous\", \"categorical\", \"graphs\"], list[Any]] = {\n        \"continuous\": [],\n        \"categorical\": [],\n        \"graphs\": [],\n    }\n    for hp in self.values():\n        if ignore_fidelity and hp.is_fidelity:\n            continue\n\n        if isinstance(hp, ConstantParameter):\n            continue\n\n        # TODO(eddiebergman): Not sure this covers all graph parameters but a search\n        # for `def value` that have a property decorator is all that could have\n        # worked previously for graphs\n        if isinstance(hp, GraphParameter):\n            hps[\"graphs\"].append(hp.value)\n\n        elif isinstance(hp, CategoricalParameter):\n            assert hp.value is not None\n            hp_value = hp.value_to_normalized(hp.value)\n            hps[\"categorical\"].append(hp_value)\n\n        # TODO(eddiebergman): Technically integer is not continuous\n        elif isinstance(hp, NumericalParameter):\n            assert hp.value is not None\n            hp_value = hp.value_to_normalized(hp.value)\n            hps[\"continuous\"].append(hp_value)\n        else:\n            raise NotImplementedError(f\"Unknown Parameter type: {type(hp)}\\n{hp}\")\n\n    return hps\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.get_search_space_grid","title":"get_search_space_grid","text":"<pre><code>get_search_space_grid(\n    *,\n    size_per_numerical_hp: int = 10,\n    include_endpoints: bool = True\n) -&gt; list[SearchSpace]\n</code></pre> <p>Get a grid of configurations from the search space.</p> <p>For <code>NumericalParameter</code> hyperparameters, the parameter <code>size_per_numerical_hp=</code> is used to determine a grid. If there are any duplicates, e.g. for an <code>IntegerParameter</code>, then we will remove duplicates.</p> <p>For <code>CategoricalParameter</code> hyperparameters, we include all the choices in the grid.</p> <p>For <code>ConstantParameter</code> hyperparameters, we include the constant value in the grid.</p> <p>TODO</p> <p>Does not support graph parameters currently.</p> PARAMETER DESCRIPTION <code>size_per_numerical_hp</code> <p>The size of the grid for each numerical hyperparameter.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>include_endpoints</code> <p>Whether to include the endpoints of the grid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list[SearchSpace]</code> <p>A list of configurations from the search space.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def get_search_space_grid(\n    self,\n    *,\n    size_per_numerical_hp: int = 10,\n    include_endpoints: bool = True,\n) -&gt; list[SearchSpace]:\n    \"\"\"Get a grid of configurations from the search space.\n\n    For [`NumericalParameter`][neps.search_spaces.NumericalParameter] hyperparameters,\n    the parameter `size_per_numerical_hp=` is used to determine a grid. If there are\n    any duplicates, e.g. for an\n    [`IntegerParameter`][neps.search_spaces.IntegerParameter], then we will\n    remove duplicates.\n\n    For [`CategoricalParameter`][neps.search_spaces.CategoricalParameter]\n    hyperparameters, we include all the choices in the grid.\n\n    For [`ConstantParameter`][neps.search_spaces.ConstantParameter] hyperparameters,\n    we include the constant value in the grid.\n\n    !!! note \"TODO\"\n\n        Does not support graph parameters currently.\n\n    Args:\n        size_per_numerical_hp: The size of the grid for each numerical hyperparameter.\n        include_endpoints: Whether to include the endpoints of the grid.\n\n    Returns:\n        A list of configurations from the search space.\n    \"\"\"\n    param_ranges = []\n    for hp in self.hyperparameters.values():\n        # NOTE(eddiebergman): This is a temporary fix to avoid graphs\n        # If this is resolved, please update the docstring!\n        if isinstance(hp, GraphParameter):\n            raise ValueError(\"Trying to create a grid for graphs!\")\n\n        if isinstance(hp, CategoricalParameter):\n            param_ranges.append(hp.choices)\n            continue\n\n        if isinstance(hp, ConstantParameter):\n            param_ranges.append([hp.value])\n            continue\n\n        if isinstance(hp, NumericalParameter):\n            grid = hp.grid(\n                size=size_per_numerical_hp,\n                include_endpoint=include_endpoints,\n            )\n            _grid = np.clip(grid, hp.lower, hp.upper).astype(np.float64)\n            _grid = (\n                _grid.astype(np.int64) if isinstance(hp, IntegerParameter) else _grid\n            )\n            _grid = np.unique(grid).tolist()\n            param_ranges.append(grid)\n            continue\n\n        raise NotImplementedError(f\"Unknown Parameter type: {type(hp)}\\n{hp}\")\n\n    full_grid = product(*param_ranges)\n\n    return [\n        SearchSpace(\n            **{\n                name: ConstantParameter(value=value)  # type: ignore\n                for name, value in zip(self.hyperparameters.keys(), config_values)\n            }\n        )\n        for config_values in full_grid\n    ]\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.get_vectorial_dim","title":"get_vectorial_dim","text":"<pre><code>get_vectorial_dim() -&gt; (\n    dict[Literal[\"continuous\", \"categorical\"], int] | None\n)\n</code></pre> <p>Get the vectorial dimension of the search space.</p> <p>The count of <code>NumericalParameter</code> are put under the key <code>\"continuous\"</code> and the count of <code>CategoricalParameter</code> are put under the key <code>\"categorical\"</code> in the return dict.</p> <p>If there are no numerical or categorical hyperparameters or constant parameters, then <code>None</code> is returned.</p> RETURNS DESCRIPTION <code>dict[Literal['continuous', 'categorical'], int] | None</code> <p>The vectorial dimension</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def get_vectorial_dim(self) -&gt; dict[Literal[\"continuous\", \"categorical\"], int] | None:\n    \"\"\"Get the vectorial dimension of the search space.\n\n    The count of [`NumericalParameter`][neps.search_spaces.NumericalParameter]\n    are put under the key `#!python \"continuous\"` and the count of\n    [`CategoricalParameter`][neps.search_spaces.CategoricalParameter] are put under\n    the key `#!python \"categorical\"` in the return dict.\n\n    If there are no numerical or categorical hyperparameters **or constant**\n    parameters, then `None` is returned.\n\n    Returns:\n        The vectorial dimension\n    \"\"\"\n    if not any(\n        isinstance(hp, (NumericalParameter, CategoricalParameter, ConstantParameter))\n        for hp in self.values()\n    ):\n        return None\n\n    features: dict[Literal[\"continuous\", \"categorical\"], int] = {\n        \"continuous\": 0,\n        \"categorical\": 0,\n    }\n    for hp in self.values():\n        if isinstance(hp, ConstantParameter):\n            pass\n        elif isinstance(hp, GraphParameter):\n            # TODO(eddiebergman): This was what the old behaviour would do...\n            pass\n        elif isinstance(hp, CategoricalParameter):\n            features[\"categorical\"] += 1\n        elif isinstance(hp, NumericalParameter):\n            features[\"continuous\"] += 1\n        else:\n            raise NotImplementedError(f\"Unknown Parameter type: {type(hp)}\\n{hp}\")\n\n    return features\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.hp_values","title":"hp_values","text":"<pre><code>hp_values() -&gt; dict[str, Any]\n</code></pre> <p>Get the values for each hyperparameter in this configuration.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def hp_values(self) -&gt; dict[str, Any]:\n    \"\"\"Get the values for each hyperparameter in this configuration.\"\"\"\n    return {\n        hp_name: hp if isinstance(hp, GraphParameter) else hp.value\n        for hp_name, hp in self.hyperparameters.items()\n    }\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.is_equal_value","title":"is_equal_value","text":"<pre><code>is_equal_value(\n    other: SearchSpace,\n    *,\n    include_fidelity: bool = True,\n    on_decimal: int = 8\n) -&gt; bool\n</code></pre> <p>Check if the configuration is equal to another configuration.</p> <p>Warning</p> <p>This does NOT check that the entire <code>SearchSpace</code> is equal (and thus it is not a dunder method), but only checks the configuration values.</p> PARAMETER DESCRIPTION <code>other</code> <p>The other configuration to compare to.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>include_fidelity</code> <p>Whether to include the fidelity parameter in the comparison.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_decimal</code> <p>The decimal to round to when comparing float values.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the configuration values are equal.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def is_equal_value(\n    self,\n    other: SearchSpace,\n    *,\n    include_fidelity: bool = True,\n    on_decimal: int = 8,\n) -&gt; bool:\n    \"\"\"Check if the configuration is equal to another configuration.\n\n    !!! warning\n\n        This does **NOT** check that the entire `SearchSpace` is equal (and thus it is\n        not a dunder method), but only checks the configuration values.\n\n    Args:\n        other: The other configuration to compare to.\n        include_fidelity: Whether to include the fidelity parameter in the comparison.\n        on_decimal: The decimal to round to when comparing float values.\n\n    Returns:\n        Whether the configuration values are equal.\n    \"\"\"\n    if self.hyperparameters.keys() != other.hyperparameters.keys():\n        return False\n\n    for hp_key, this_hp in self.hyperparameters.items():\n        if this_hp.is_fidelity and (not include_fidelity):\n            continue\n\n        other_hp = other.hyperparameters[hp_key]\n        if not isinstance(other_hp, type(this_hp)):\n            return False\n\n        if isinstance(this_hp.value, float):\n            this_norm = this_hp.value_to_normalized(this_hp.value)\n            other_norm = other_hp.value_to_normalized(other_hp.value)  # type: ignore\n            if np.round(this_norm - other_norm, on_decimal) != 0:\n                return False\n        elif this_hp.value != other_hp.value:\n            return False\n\n    return True\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.load_from","title":"load_from","text":"<pre><code>load_from(\n    config: Mapping[str, Any | GraphParameter]\n) -&gt; None\n</code></pre> <p>Load a configuration from a dictionary, setting all the values.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def load_from(self, config: Mapping[str, Any | GraphParameter]) -&gt; None:\n    \"\"\"Load a configuration from a dictionary, setting all the values.\"\"\"\n    for name, val in config.items():\n        self.hyperparameters[name].load_from(val)\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.mutate","title":"mutate","text":"<pre><code>mutate(\n    *,\n    parent: SearchSpace | None = None,\n    mutation_rate: float = 1.0,\n    mutation_strategy: Literal[\"smbo\"] = \"smbo\",\n    patience: int = 50,\n    **kwargs: Any\n) -&gt; SearchSpace\n</code></pre> <p>Mutate the search space.</p> PARAMETER DESCRIPTION <code>parent</code> <p>The parent configuration to mutate from.</p> <p> TYPE: <code>SearchSpace | None</code> DEFAULT: <code>None</code> </p> <code>mutation_rate</code> <p>The rate at which to mutate the search space.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>mutation_strategy</code> <p>The strategy to use for mutation.</p> <p> TYPE: <code>Literal['smbo']</code> DEFAULT: <code>'smbo'</code> </p> <code>patience</code> <p>The number of times to try to mutate a valid value for a hyperparameter.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the mutation strategy.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>The mutated search space.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def mutate(\n    self,\n    *,\n    parent: SearchSpace | None = None,\n    mutation_rate: float = 1.0,\n    mutation_strategy: Literal[\"smbo\"] = \"smbo\",\n    patience: int = 50,\n    **kwargs: Any,\n) -&gt; SearchSpace:\n    \"\"\"Mutate the search space.\n\n    Args:\n        parent: The parent configuration to mutate from.\n        mutation_rate: The rate at which to mutate the search space.\n        mutation_strategy: The strategy to use for mutation.\n        patience: The number of times to try to mutate a valid value for a\n            hyperparameter.\n        **kwargs: Additional keyword arguments to pass to the mutation strategy.\n\n    Returns:\n        The mutated search space.\n    \"\"\"\n    if mutation_strategy == \"smbo\":\n        args = {\n            \"parent\": parent,\n            \"mutation_rate\": mutation_rate,\n            \"mutation_strategy\": \"local_search\",  # fixing property for SMBO mutation\n        }\n        kwargs.update(args)\n        new_config = self._smbo_mutation(patience=patience, **kwargs)\n    else:\n        raise NotImplementedError(\"No such mutation strategy!\")\n\n    return SearchSpace(**new_config)\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.sample","title":"sample","text":"<pre><code>sample(\n    *,\n    user_priors: bool = False,\n    patience: int = 1,\n    ignore_fidelity: bool = True\n) -&gt; SearchSpace\n</code></pre> <p>Sample a configuration from the search space.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>Whether to use user priors when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patience</code> <p>The number of times to try to sample a valid value for a hyperparameter.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>A sampled configuration from the search space.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def sample(\n    self,\n    *,\n    user_priors: bool = False,\n    patience: int = 1,\n    ignore_fidelity: bool = True,\n) -&gt; SearchSpace:\n    \"\"\"Sample a configuration from the search space.\n\n    Args:\n        user_priors: Whether to use user priors when sampling.\n        patience: The number of times to try to sample a valid value for a\n            hyperparameter.\n        ignore_fidelity: Whether to ignore the fidelity parameter when sampling.\n\n    Returns:\n        A sampled configuration from the search space.\n    \"\"\"\n    sampled_hps: dict[str, Parameter] = {}\n\n    for name, hp in self.hyperparameters.items():\n        if hp.is_fidelity and ignore_fidelity:\n            sampled_hps[name] = hp.clone()\n            continue\n\n        for _ in range(patience):\n            try:\n                if user_priors and isinstance(hp, ParameterWithPrior):\n                    sampled_hps[name] = hp.sample(user_priors=user_priors)\n                else:\n                    sampled_hps[name] = hp.sample()\n                break\n            except ValueError:\n                logger.warning(\n                    f\"Could not sample valid value for hyperparameter {name}!\"\n                )\n        else:\n            raise ValueError(\n                f\"Could not sample valid value for hyperparameter {name}\"\n                f\" in {patience} tries!\"\n            )\n\n    return SearchSpace(**sampled_hps)\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.sample_default_configuration","title":"sample_default_configuration","text":"<pre><code>sample_default_configuration(\n    *,\n    patience: int = 1,\n    ignore_fidelity: bool = True,\n    ignore_missing_defaults: bool = False\n) -&gt; SearchSpace\n</code></pre> <p>Sample the default configuration from the search space.</p> <p>By default, if there is no default set for a hyperparameter, an error will be raised. If <code>ignore_missing_defaults=True</code>, then a sampled value will be used instead.</p> PARAMETER DESCRIPTION <code>patience</code> <p>The number of times to try to sample a valid value for a hyperparameter.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_missing_defaults</code> <p>Whether to ignore missing defaults when setting the default configuration.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>The default configuration.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def sample_default_configuration(\n    self,\n    *,\n    patience: int = 1,\n    ignore_fidelity: bool = True,\n    ignore_missing_defaults: bool = False,\n) -&gt; SearchSpace:\n    \"\"\"Sample the default configuration from the search space.\n\n    By default, if there is no default set for a hyperparameter, an error will be\n    raised. If `ignore_missing_defaults=True`, then a sampled value will be used\n    instead.\n\n    Args:\n        patience: The number of times to try to sample a valid value for a\n            hyperparameter.\n        ignore_fidelity: Whether to ignore the fidelity parameter when sampling.\n        ignore_missing_defaults: Whether to ignore missing defaults when setting\n            the default configuration.\n\n    Returns:\n        The default configuration.\n    \"\"\"\n    # Sample a random config and then set the defaults if there are any\n    config = self.sample(patience=patience, ignore_fidelity=ignore_fidelity)\n    for hp_name, hp in self.hyperparameters.items():\n        if hp.is_fidelity and ignore_fidelity:\n            continue\n\n        if hp.default is None:\n            if not ignore_missing_defaults:\n                raise ValueError(f\"No defaults specified for {hp} in the space.\")\n\n            # Use the sampled value instead\n        else:\n            config[hp_name].set_value(hp.default)\n\n    return config\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.serialize","title":"serialize","text":"<pre><code>serialize() -&gt; dict[str, Hashable]\n</code></pre> <p>Serialize the configuration to a dictionary that can be written to disk.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def serialize(self) -&gt; dict[str, Hashable]:\n    \"\"\"Serialize the configuration to a dictionary that can be written to disk.\"\"\"\n    serialized_config = {}\n    for name, hp in self.hyperparameters.items():\n        if hp.value is None:\n            raise ValueError(\n                f\"Hyperparameter {name} has no value set and can't\" \" be serialized!\"\n            )\n        serialized_config[name] = hp.serialize_value(hp.value)\n    return serialized_config\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.set_custom_grid_space","title":"set_custom_grid_space","text":"<pre><code>set_custom_grid_space(\n    grid_table: Series | DataFrame,\n    raw_space: SearchSpace | ConfigurationSpace,\n) -&gt; None\n</code></pre> <p>Set a custom grid space for the search space.</p> <p>This function is used to set a custom grid space for the pipeline space.</p> <p>Warning</p> <p>The type check and the table format requirement is loose and can break certain components.</p> Note <p>Only to be used if a custom set of hyperparameters from the search space is to be sampled or used for acquisition functions.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def set_custom_grid_space(\n    self,\n    grid_table: pd.Series | pd.DataFrame,\n    raw_space: SearchSpace | CS.ConfigurationSpace,\n) -&gt; None:\n    \"\"\"Set a custom grid space for the search space.\n\n    This function is used to set a custom grid space for the pipeline space.\n\n    !!! warning\n\n        The type check and the table format requirement is loose and\n        can break certain components.\n\n    Note:\n        Only to be used if a custom set of hyperparameters from the search space\n        is to be sampled or used for acquisition functions.\n    \"\"\"\n    if grid_table is None or raw_space is None:\n        raise ValueError(\n            \"Both grid_table and raw_space must be set!\\n\"\n            \"A table or list of fixed configs must be supported with a \"\n            \"continuous space representing the type and bounds of each \"\n            \"hyperparameter for accurate modeling.\"\n        )\n\n    self.custom_grid_table = grid_table\n    self.raw_tabular_space = (\n        SearchSpace(**raw_space)\n        if not isinstance(raw_space, SearchSpace)\n        else raw_space\n    )\n    self.has_tabular = True\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.set_defaults_to_current_values","title":"set_defaults_to_current_values","text":"<pre><code>set_defaults_to_current_values() -&gt; None\n</code></pre> <p>Update the configuration/search space to use the current values as defaults.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def set_defaults_to_current_values(self) -&gt; None:\n    \"\"\"Update the configuration/search space to use the current values as defaults.\"\"\"\n    for hp in self.hyperparameters.values():\n        if isinstance(hp, NumericalParameter):\n            hp.set_default(hp.value)\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.set_hyperparameters_from_dict","title":"set_hyperparameters_from_dict","text":"<pre><code>set_hyperparameters_from_dict(\n    hyperparameters: Mapping[str, Any],\n    *,\n    defaults: bool = True,\n    values: bool = True,\n    confidence: str = \"low\",\n    delete_previous_defaults: bool = False,\n    delete_previous_values: bool = False,\n    overwrite_constants: bool = False\n) -&gt; None\n</code></pre> <p>Set the hyperparameters from a dictionary of values.</p> <p>Constant Hyperparameters</p> <p><code>ConstantParameter</code> hyperparameters have only a single possible value and hence only a single possible default. If <code>overwrite_constants=</code> is <code>False</code>, then it will remain unchanged and ignore the new value.</p> <p>If <code>overwrite_constants=</code> is <code>True</code>, then the constant hyperparameter will be updated, requiring both <code>defaults=True</code> and <code>values=True</code> to be set.</p> <p>The arguments <code>delete_previous_defaults</code> and <code>delete_previous_values</code> are ignored for <code>ConstantParameter</code>.</p> PARAMETER DESCRIPTION <code>hyperparameters</code> <p>The dictionary of hyperparameters to set with values.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>defaults</code> <p>Whether to set the defaults to these values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>values</code> <p>Whether to set the value of the hyperparameters to these values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>confidence</code> <p>The confidence score to use when setting the default. Only applies if <code>defaults=True</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'low'</code> </p> <code>delete_previous_defaults</code> <p>Whether to delete the previous defaults.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>delete_previous_values</code> <p>Whether to delete the previous values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>overwrite_constants</code> <p>Whether to overwrite constant hyperparameters.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the value is invalid for the hyperparameter.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def set_hyperparameters_from_dict(  # noqa: C901\n    self,\n    hyperparameters: Mapping[str, Any],\n    *,\n    defaults: bool = True,\n    values: bool = True,\n    # TODO(eddiebergman): The existence of this makes me think\n    # all hyperparameters that accept confidence should use the same keys\n    confidence: str = \"low\",\n    delete_previous_defaults: bool = False,\n    delete_previous_values: bool = False,\n    overwrite_constants: bool = False,\n) -&gt; None:\n    \"\"\"Set the hyperparameters from a dictionary of values.\n\n    !!! note \"Constant Hyperparameters\"\n\n        [`ConstantParameter`][neps.search_spaces.ConstantParameter] hyperparameters\n        have only a single possible value and hence only a single possible default.\n        If `overwrite_constants=` is `False`, then it will remain unchanged and\n        ignore the new value.\n\n        If `overwrite_constants=` is `True`, then the constant hyperparameter will\n        be updated, requiring both `defaults=True` and `values=True` to be set.\n\n        The arguments `delete_previous_defaults` and `delete_previous_values` are\n        ignored for [`ConstantParameter`][neps.search_spaces.ConstantParameter].\n\n    Args:\n        hyperparameters: The dictionary of hyperparameters to set with values.\n        defaults: Whether to set the defaults to these values.\n        values: Whether to set the value of the hyperparameters to these values.\n        confidence: The confidence score to use when setting the default.\n            Only applies if `defaults=True`.\n        delete_previous_defaults: Whether to delete the previous defaults.\n        delete_previous_values: Whether to delete the previous values.\n        overwrite_constants: Whether to overwrite constant hyperparameters.\n\n    Raises:\n        ValueError: If the value is invalid for the hyperparameter.\n    \"\"\"\n    if values is False and defaults is False:\n        raise ValueError(\"At least one of `values` or `defaults` must be True.\")\n\n    for hp_key, current_hp in self.hyperparameters.items():\n        new_hp_value = hyperparameters.get(hp_key, NotSet)\n        if isinstance(new_hp_value, _NotSet):\n            continue\n\n        # Handle constants specially as they have particular logic which\n        # is different from the other hyperparameters\n        if isinstance(current_hp, ConstantParameter):\n            if not overwrite_constants:\n                continue\n\n            if not (defaults and values):\n                raise ValueError(\n                    \"Cannot have a constant parameter with a seperate default and\"\n                    \" and value. Please provide both `values=True` and\"\n                    \" `defaults=True` if passing `overwrite_constants=True`\"\n                    f\" with a new value for the constant '{hp_key}'.\"\n                )\n\n            current_hp.set_constant_value(new_hp_value)\n            continue\n\n        if delete_previous_defaults:\n            current_hp.set_default(None)\n\n        if delete_previous_values:\n            current_hp.set_value(None)\n\n        if defaults:\n            current_hp.set_default(new_hp_value)\n            if isinstance(current_hp, ParameterWithPrior):\n                current_hp.set_default_confidence_score(confidence)\n\n        if values:\n            current_hp.set_value(new_hp_value)\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.set_to_max_fidelity","title":"set_to_max_fidelity","text":"<pre><code>set_to_max_fidelity() -&gt; None\n</code></pre> <p>Set the configuration to the maximum fidelity.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def set_to_max_fidelity(self) -&gt; None:\n    \"\"\"Set the configuration to the maximum fidelity.\"\"\"\n    if self.fidelity is None:\n        raise ValueError(\"No fidelity parameter in the search space!\")\n\n    self.fidelity.set_value(self.fidelity.upper)\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.pipeline_space_from_configspace","title":"pipeline_space_from_configspace","text":"<pre><code>pipeline_space_from_configspace(\n    configspace: ConfigurationSpace,\n) -&gt; dict[str, Parameter]\n</code></pre> <p>Constructs the <code>Parameter</code> objects from a <code>ConfigurationSpace</code>.</p> PARAMETER DESCRIPTION <code>configspace</code> <p>The configuration space to construct the pipeline space from.</p> <p> TYPE: <code>ConfigurationSpace</code> </p> RETURNS DESCRIPTION <code>dict[str, Parameter]</code> <p>A dictionary where keys are parameter names and values are parameter objects.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def pipeline_space_from_configspace(\n    configspace: CS.ConfigurationSpace,\n) -&gt; dict[str, Parameter]:\n    \"\"\"Constructs the [`Parameter`][neps.search_spaces.parameter.Parameter] objects\n    from a [`ConfigurationSpace`][ConfigSpace.configuration_space.ConfigurationSpace].\n\n    Args:\n        configspace: The configuration space to construct the pipeline space from.\n\n    Returns:\n        A dictionary where keys are parameter names and values are parameter objects.\n    \"\"\"\n    pipeline_space = {}\n    parameter: Parameter\n    if any(configspace.get_conditions()) or any(configspace.get_forbiddens()):\n        raise NotImplementedError(\n            \"The ConfigurationSpace has conditions or forbidden clauses, \"\n            \"which are not supported by neps.\"\n        )\n\n    for hyperparameter in configspace.get_hyperparameters():\n        if isinstance(hyperparameter, CS.Constant):\n            parameter = ConstantParameter(value=hyperparameter.value)\n        elif isinstance(hyperparameter, CS.CategoricalHyperparameter):\n            parameter = CategoricalParameter(\n                hyperparameter.choices,\n                default=hyperparameter.default_value,\n            )\n        elif isinstance(hyperparameter, CS.OrdinalHyperparameter):\n            parameter = CategoricalParameter(\n                hyperparameter.sequence,\n                default=hyperparameter.default_value,\n            )\n        elif isinstance(hyperparameter, CS.UniformIntegerHyperparameter):\n            parameter = IntegerParameter(\n                lower=hyperparameter.lower,\n                upper=hyperparameter.upper,\n                log=hyperparameter.log,\n                default=hyperparameter.default_value,\n            )\n        elif isinstance(hyperparameter, CS.UniformFloatHyperparameter):\n            parameter = FloatParameter(\n                lower=hyperparameter.lower,\n                upper=hyperparameter.upper,\n                log=hyperparameter.log,\n                default=hyperparameter.default_value,\n            )\n        else:\n            raise ValueError(f\"Unknown hyperparameter type {hyperparameter}\")\n        pipeline_space[hyperparameter.name] = parameter\n    return pipeline_space\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.pipeline_space_from_yaml","title":"pipeline_space_from_yaml","text":"<pre><code>pipeline_space_from_yaml(\n    config: str | Path | dict,\n) -&gt; dict[str, Parameter]\n</code></pre> <p>Reads configuration details from a YAML file or a dictionary and constructs a pipeline space dictionary.</p> PARAMETER DESCRIPTION <code>config</code> <p>Path to the YAML file or a dictionary containing</p> <p> TYPE: <code>str | Path | dict</code> </p> RETURNS DESCRIPTION <code>dict[str, Parameter]</code> <p>dict[str, Parameter]: A dictionary where keys are parameter names and values are parameter objects.</p> RAISES DESCRIPTION <code>SearchSpaceFromYamlFileError</code> <p>Raised if there are issues with the YAML file's</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def pipeline_space_from_yaml(  # noqa: C901, PLR0912\n    config: str | Path | dict,\n) -&gt; dict[str, Parameter]:\n    \"\"\"Reads configuration details from a YAML file or a dictionary and constructs a\n    pipeline space dictionary.\n\n    Args:\n        config (str | Path | dict): Path to the YAML file or a dictionary containing\n        parameter configurations.\n\n    Returns:\n        dict[str, Parameter]: A dictionary where keys are parameter names and values\n        are parameter objects.\n\n    Raises:\n        SearchSpaceFromYamlFileError: Raised if there are issues with the YAML file's\n        format, contents, or if the dictionary is invalid.\n    \"\"\"\n    try:\n        if isinstance(config, (str, Path)):\n            # try to load the YAML file\n            try:\n                yaml_file_path = Path(config)\n                with yaml_file_path.open(\"r\") as file:\n                    config = yaml.safe_load(file)\n                if not isinstance(config, dict):\n                    raise ValueError(\n                        \"The loaded pipeline_space is not a valid dictionary. Please \"\n                        \"ensure that you use a proper structure. See the documentation \"\n                        \"for more details.\"\n                    )\n            except FileNotFoundError as e:\n                raise FileNotFoundError(\n                    f\"Unable to find the specified file for 'pipeline_space' at \"\n                    f\"'{config}'. Please verify the path specified in the \"\n                    f\"'pipeline_space' argument and try again.\"\n                ) from e\n            except yaml.YAMLError as e:\n                raise ValueError(f\"The file at {config} is not a valid YAML file.\") from e\n\n        # Initialize the pipeline space\n        pipeline_space: dict[str, Parameter] = {}\n\n        # Iterate over the items in the YAML configuration\n        for name, details in config.items():\n            # get parameter type\n            param_type = deduce_type(name, details)\n\n            # init parameter by checking type\n            if param_type in (\"int\", \"integer\"):\n                # Integer Parameter\n                formatted_details = formatting_int(name, details)\n                pipeline_space[name] = IntegerParameter(**formatted_details)\n            elif param_type == \"float\":\n                # Float Parameter\n                formatted_details = formatting_float(name, details)\n                pipeline_space[name] = FloatParameter(**formatted_details)\n            elif param_type in (\"cat\", \"categorical\"):\n                # Categorical parameter\n                formatted_details = formatting_cat(name, details)\n                pipeline_space[name] = CategoricalParameter(**formatted_details)\n            elif param_type == \"const\":\n                # Constant parameter\n                formatted_details = formatting_const(details)  # type: ignore\n                pipeline_space[name] = ConstantParameter(formatted_details)\n            else:\n                # Handle unknown parameter type\n                raise TypeError(\n                    f\"Unsupported parameter with details: {details} for '{name}'.\\n\"\n                    f\"Supported Types for argument type are:\\n\"\n                    \"For integer parameter: int, integer\\n\"\n                    \"For float parameter: float\\n\"\n                    \"For categorical parameter: cat, categorical\\n\"\n                    \"Constant parameter was not detect\\n\"\n                )\n    except (KeyError, TypeError, ValueError, FileNotFoundError) as e:\n        raise SearchSpaceFromYamlFileError(e) from e\n\n    return pipeline_space\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/","title":"Yaml search space utils","text":""},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils","title":"neps.search_spaces.yaml_search_space_utils","text":""},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.SearchSpaceFromYamlFileError","title":"SearchSpaceFromYamlFileError","text":"<pre><code>SearchSpaceFromYamlFileError(exception)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised for errors occurring during the initialization of the search space from a YAML file.</p> ATTRIBUTE DESCRIPTION <code>exception_type</code> <p>The type of the original exception.</p> <p> TYPE: <code>str</code> </p> <code>message</code> <p>A detailed message that includes the type of the original exception            and the error description.</p> <p> TYPE: <code>str</code> </p> PARAMETER DESCRIPTION <code>exception</code> <p>The original exception that was raised during the                     initialization of the search space from the YAML file.</p> <p> TYPE: <code>Exception</code> </p> Example Usage <p>try:     # Code to initialize search space from YAML file except (KeyError, TypeError, ValueError) as e:     raise SearchSpaceFromYamlFileError(e)</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def __init__(self, exception):\n    self.exception_type = type(exception).__name__\n    self.message = (\n        f\"Error occurred during initialization of search space from \"\n        f\"YAML file.\\n {self.exception_type}: {exception}\"\n    )\n    super().__init__(self.message)\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.convert_scientific_notation","title":"convert_scientific_notation","text":"<pre><code>convert_scientific_notation(\n    value: str | int | float, show_usage_flag=False\n) -&gt; float | (float, bool)\n</code></pre> <p>Convert a given value to a float if it's a string that matches scientific e notation. This is especially useful for numbers like \"3.3e-5\" which YAML parsers may not directly interpret as floats.</p> <p>If the 'show_usage_flag' is set to True, the function returns a tuple of the float conversion and a boolean flag indicating whether scientific notation was detected.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to convert. Can be an integer, float,                        or a string representing a number, possibly in                        scientific notation.</p> <p> TYPE: <code>str | int | float</code> </p> <code>show_usage_flag</code> <p>Optional; defaults to False. If True, the function                     also returns a flag indicating whether scientific                     notation was detected in the string.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The value converted to float if 'show_usage_flag' is False. (float, bool): A tuple containing the value converted to float and a flag                indicating scientific notation detection if 'show_usage_flag'                is True.</p> <p> TYPE: <code>float | (float, bool)</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the value is a string and does not represent a valid number.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def convert_scientific_notation(value: str | int | float, show_usage_flag=False) \\\n        -&gt; float | (float, bool):\n    \"\"\"\n    Convert a given value to a float if it's a string that matches scientific e notation.\n    This is especially useful for numbers like \"3.3e-5\" which YAML parsers may not\n    directly interpret as floats.\n\n    If the 'show_usage_flag' is set to True, the function returns a tuple of the float\n    conversion and a boolean flag indicating whether scientific notation was detected.\n\n    Args:\n        value (str | int | float): The value to convert. Can be an integer, float,\n                                   or a string representing a number, possibly in\n                                   scientific notation.\n        show_usage_flag (bool): Optional; defaults to False. If True, the function\n                                also returns a flag indicating whether scientific\n                                notation was detected in the string.\n\n    Returns:\n        float: The value converted to float if 'show_usage_flag' is False.\n        (float, bool): A tuple containing the value converted to float and a flag\n                       indicating scientific notation detection if 'show_usage_flag'\n                       is True.\n\n    Raises:\n        ValueError: If the value is a string and does not represent a valid number.\n    \"\"\"\n\n    e_notation_pattern = r\"^-?\\d+(\\.\\d+)?[eE]-?\\d+$\"\n\n    flag = False  # Flag if e notation was detected\n\n    if isinstance(value, str):\n        # Remove all whitespace from the string\n        value_no_space = value.replace(\" \", \"\")\n\n        # check for e notation\n        if re.match(e_notation_pattern, value_no_space):\n            flag = True\n\n    if show_usage_flag is True:\n        return float(value), flag\n    else:\n        return float(value)\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.deduce_param_type","title":"deduce_param_type","text":"<pre><code>deduce_param_type(\n    name: str, details: dict[str, int | str | float]\n) -&gt; str\n</code></pre> <p>Deduces the parameter type based on the provided details.</p> <p>The function interprets the 'details' dictionary to determine the parameter type. The dictionary should include key-value pairs that describe the parameter's characteristics, such as lower, upper and choices.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing parameter</p> <p> TYPE: <code>dict[str, int | str | float]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The deduced parameter type ('int', 'float' or 'categorical').</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the parameter type cannot be deduced from the details, or if the</p> Example <p>param_type = deduce_param_type('example_param', {'lower': 0, 'upper': 10})</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def deduce_param_type(name: str, details: dict[str, int | str | float]) -&gt; str:\n    \"\"\"Deduces the parameter type based on the provided details.\n\n    The function interprets the 'details' dictionary to determine the parameter type.\n    The dictionary should include key-value pairs that describe the parameter's\n    characteristics, such as lower, upper and choices.\n\n\n    Args:\n        name (str): The name of the parameter.\n        details ((dict[str, int | str | float])): A dictionary containing parameter\n        specifications.\n\n    Returns:\n        str: The deduced parameter type ('int', 'float' or 'categorical').\n\n    Raises:\n        TypeError: If the parameter type cannot be deduced from the details, or if the\n        provided details have inconsistent types for expected keys.\n\n    Example:\n        param_type = deduce_param_type('example_param', {'lower': 0, 'upper': 10})\"\"\"\n    # Logic to deduce type from details\n\n    # check for int and float conditions\n    if \"lower\" in details and \"upper\" in details:\n        # Determine if it's an integer or float range parameter\n        if isinstance(details[\"lower\"], int) and isinstance(details[\"upper\"], int):\n            param_type = \"int\"\n        elif isinstance(details[\"lower\"], float) and isinstance(details[\"upper\"], float):\n            param_type = \"float\"\n        else:\n            try:\n                details[\"lower\"], flag_lower = convert_scientific_notation(\n                    details[\"lower\"], show_usage_flag=True\n                )\n                details[\"upper\"], flag_upper = convert_scientific_notation(\n                    details[\"upper\"], show_usage_flag=True\n                )\n            except ValueError as e:\n                raise TypeError(\n                    f\"Inconsistent types for 'lower' and 'upper' in '{name}'. \"\n                    f\"Both must be either integers or floats.\"\n                ) from e\n\n            # check if one value is e notation and if so convert it to float\n            if flag_lower or flag_upper:\n                logger.info(\n                    f\"Because of e notation, Parameter {name} gets \"\n                    f\"interpreted as float\"\n                )\n                param_type = \"float\"\n            else:\n                raise TypeError(\n                    f\"Inconsistent types for 'lower' and 'upper' in '{name}'. \"\n                    f\"Both must be either integers or floats.\"\n                )\n    # check for categorical condition\n    elif \"choices\" in details:\n        param_type = \"categorical\"\n    else:\n        raise KeyError(\n            f\"Unable to deduce parameter type from {name} \"\n            f\"with details {details}\\n\"\n            \"Supported parameters:\\n\"\n            \"Float and Integer: Expected keys: 'lower', 'upper'\\n\"\n            \"Categorical: Expected keys: 'choices'\\n\"\n        )\n    return param_type\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.deduce_type","title":"deduce_type","text":"<pre><code>deduce_type(\n    name: str,\n    details: (\n        dict[str, str | int | float] | str | int | float\n    ),\n) -&gt; str\n</code></pre> <p>Deduces the parameter type from details.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing parameter     specifications or a direct value (string, integer, or float).</p> <p> TYPE: <code>dict | str | int | float</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The deduced parameter type ('int', 'float', 'categorical', or 'constant').</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the type cannot be deduced or the details don't align with expected     constraints.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def deduce_type(\n    name: str, details: dict[str, str | int | float] | str | int | float\n) -&gt; str:\n    \"\"\"\n    Deduces the parameter type from details.\n\n    Args:\n        name (str): The name of the parameter.\n        details (dict | str | int | float): A dictionary containing parameter\n                specifications or a direct value (string, integer, or float).\n\n    Returns:\n        str: The deduced parameter type ('int', 'float', 'categorical', or 'constant').\n\n    Raises:\n        TypeError: If the type cannot be deduced or the details don't align with expected\n                constraints.\n        \"\"\"\n    if isinstance(details, (str,  int, float)):\n        param_type = \"const\"\n    elif isinstance(details, dict):\n        if \"type\" in details:\n            param_type = details.pop(\"type\").lower()\n        else:\n            param_type = deduce_param_type(name, details)\n    else:\n        raise TypeError(\n            f\"Unable to deduce parameter type for '{name}' with details '{details}'.\")\n\n    return param_type\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_cat","title":"formatting_cat","text":"<pre><code>formatting_cat(\n    name: str, details: dict[str, str | int | float]\n) -&gt; dict\n</code></pre> <p>This function ensures that the 'choices' key in the details is a list and attempts to convert any elements expressed in scientific notation to floats. It also handles the 'default' value, converting it from scientific notation if necessary.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the categorical parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing the parameter's specifications. The required key      is 'choices', which must be a list. The 'default' key is optional.</p> <p> TYPE: <code>dict[str, str | int | float]</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If 'choices' is not a list.</p> RETURNS DESCRIPTION <code>dict</code> <p>The validated and possibly converted categorical parameter details.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_cat(name: str, details: dict[str, str | int | float]) -&gt; dict:\n    \"\"\"\n    This function ensures that the 'choices' key in the details is a list and attempts\n    to convert any elements expressed in scientific notation to floats. It also handles\n    the 'default' value, converting it from scientific notation if necessary.\n\n    Args:\n        name: The name of the categorical parameter.\n        details: A dictionary containing the parameter's specifications. The required key\n                 is 'choices', which must be a list. The 'default' key is optional.\n\n    Raises:\n        TypeError: If 'choices' is not a list.\n\n    Returns:\n        The validated and possibly converted categorical parameter details.\n    \"\"\"\n    if not isinstance(details[\"choices\"], list):\n        raise TypeError(f\"The 'choices' for '{name}' must be a list.\")\n    for i, element in enumerate(details[\"choices\"]):\n        try:\n            converted_value, e_flag = convert_scientific_notation(\n                element, show_usage_flag=True\n            )\n            if e_flag:\n                details[\"choices\"][\n                    i\n                ] = converted_value  # Replace the element at the same position\n        except ValueError:\n            pass  # If a ValueError occurs, simply continue to the next element\n    if \"default\" in details:\n        e_flag = False\n        try:\n            # check if e notation, if then convert to number\n            default, e_flag = convert_scientific_notation(\n                details[\"default\"], show_usage_flag=True\n            )\n        except ValueError:\n            pass  # if default value is not in a numeric format, Value Error occurs\n        if e_flag is True:\n            details[\"default\"] = default\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_const","title":"formatting_const","text":"<pre><code>formatting_const(\n    details: str | int | float,\n) -&gt; str | int | float\n</code></pre> <p>Validates and converts a constant parameter.</p> <p>This function checks if the 'details' parameter contains a value expressed in scientific notation and converts it to a float. It ensures that the input is appropriately formatted, either as a string, integer, or float.</p> PARAMETER DESCRIPTION <code>details</code> <p>A constant parameter that can be a string, integer, or float.      If the value is in scientific notation, it will be converted to a float.</p> <p> TYPE: <code>str | int | float</code> </p> RETURNS DESCRIPTION <code>str | int | float</code> <p>The validated and possibly converted constant parameter.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_const(details: str | int | float) -&gt; str | int | float:\n    \"\"\"\n    Validates and converts a constant parameter.\n\n    This function checks if the 'details' parameter contains a value expressed in\n    scientific notation and converts it to a float. It ensures that the input\n    is appropriately formatted, either as a string, integer, or float.\n\n    Args:\n        details: A constant parameter that can be a string, integer, or float.\n                 If the value is in scientific notation, it will be converted to a float.\n\n    Returns:\n        The validated and possibly converted constant parameter.\n    \"\"\"\n\n    # check for e notation and convert it to float\n    e_flag = False\n    try:\n        converted_value, e_flag = convert_scientific_notation(\n            details, show_usage_flag=True\n        )\n    except ValueError:\n        # if the value is not able to convert to float a ValueError get raised by\n        # convert_scientific_notation function\n        pass\n    if e_flag:\n        details = converted_value\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_float","title":"formatting_float","text":"<pre><code>formatting_float(\n    name: str, details: dict[str, str | int | float]\n) -&gt; dict\n</code></pre> <p>Converts scientific notation values to floats.</p> <p>This function converts the 'lower' and 'upper' bounds, as well as the 'default' value (if present), from scientific notation to floats.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the float parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing the parameter's specifications. Expected keys      include 'lower', 'upper', and optionally 'default'.</p> <p> TYPE: <code>dict[str, str | int | float]</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If 'lower', 'upper', or 'default' cannot be converted from scientific        notation to floats.</p> RETURNS DESCRIPTION <code>dict</code> <p>The dictionary with the converted float parameter details.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_float(name: str, details: dict[str, str | int | float]) -&gt; dict:\n    \"\"\"\n    Converts scientific notation values to floats.\n\n    This function converts the 'lower' and 'upper' bounds, as well as the 'default'\n    value (if present), from scientific notation to floats.\n\n    Args:\n        name: The name of the float parameter.\n        details: A dictionary containing the parameter's specifications. Expected keys\n                 include 'lower', 'upper', and optionally 'default'.\n\n    Raises:\n        TypeError: If 'lower', 'upper', or 'default' cannot be converted from scientific\n                   notation to floats.\n\n    Returns:\n        The dictionary with the converted float parameter details.\n    \"\"\"\n\n    if not isinstance(details[\"lower\"], float) or not isinstance(details[\"upper\"], float):\n        try:\n            # for numbers like 1e-5 and 10^\n            details[\"lower\"] = convert_scientific_notation(details[\"lower\"])\n            details[\"upper\"] = convert_scientific_notation(details[\"upper\"])\n        except ValueError as e:\n            raise TypeError(\n                f\"'lower' and 'upper' must be float for \" f\"float parameter '{name}'.\"\n            ) from e\n    if \"default\" in details:\n        if not isinstance(details[\"default\"], float):\n            try:\n                details[\"default\"] = convert_scientific_notation(details[\"default\"])\n            except ValueError as e:\n                raise TypeError(\n                    f\" default'{details['default']}' must be float for float \"\n                    f\"parameter {name} \"\n                ) from e\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_int","title":"formatting_int","text":"<pre><code>formatting_int(\n    name: str, details: dict[str, str | int | float]\n) -&gt; dict\n</code></pre> <p>Converts scientific notation values to integers.</p> <p>This function converts the 'lower' and 'upper' bounds, as well as the 'default' value (if present), from scientific notation to integers.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the integer parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing the parameter's                                     specifications. Expected keys include                                     'lower', 'upper', and optionally 'default'.</p> <p> TYPE: <code>dict[str, str | int | float]</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If 'lower', 'upper', or 'default' cannot be converted from scientific        notation to integers.</p> RETURNS DESCRIPTION <code>dict</code> <p>The dictionary with the converted integer parameter details.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_int(name: str, details: dict[str, str | int | float]) -&gt; dict:\n    \"\"\"\n     Converts scientific notation values to integers.\n\n    This function converts the 'lower' and 'upper' bounds, as well as the 'default'\n    value (if present), from scientific notation to integers.\n\n    Args:\n        name (str): The name of the integer parameter.\n        details (dict[str, str | int | float]): A dictionary containing the parameter's\n                                                specifications. Expected keys include\n                                                'lower', 'upper', and optionally 'default'.\n\n    Raises:\n        TypeError: If 'lower', 'upper', or 'default' cannot be converted from scientific\n                   notation to integers.\n\n    Returns:\n        The dictionary with the converted integer parameter details.\n    \"\"\"\n    if not isinstance(details[\"lower\"], int) or not isinstance(details[\"upper\"], int):\n        try:\n            # for numbers like 1e2 and 10^\n            lower, flag_lower = convert_scientific_notation(\n                details[\"lower\"], show_usage_flag=True\n            )\n            upper, flag_upper = convert_scientific_notation(\n                details[\"upper\"], show_usage_flag=True\n            )\n            # check if one value format is e notation and if it's an integer\n            if flag_lower or flag_upper:\n                if lower == int(lower) and upper == int(upper):\n                    details[\"lower\"] = int(lower)\n                    details[\"upper\"] = int(upper)\n                else:\n                    raise TypeError()\n            else:\n                raise TypeError()\n        except (ValueError, TypeError) as e:\n            raise TypeError(\n                f\"'lower' and 'upper' must be integer for \" f\"integer parameter '{name}'.\"\n            ) from e\n    if \"default\" in details:\n        if not isinstance(details[\"default\"], int):\n            try:\n                # convert value can raise ValueError\n                default = convert_scientific_notation(details[\"default\"])\n                if default == int(default):\n                    details[\"default\"] = int(default)\n                else:\n                    raise TypeError()  # type of value is not int\n            except (ValueError, TypeError) as e:\n                raise TypeError(\n                    f\"default value {details['default']} \"\n                    f\"must be integer for integer parameter {name}\"\n                ) from e\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/api/","title":"Api","text":""},{"location":"api/neps/search_spaces/architecture/api/#neps.search_spaces.architecture.api","title":"neps.search_spaces.architecture.api","text":""},{"location":"api/neps/search_spaces/architecture/api/#neps.search_spaces.architecture.api.ArchitectureParameter","title":"ArchitectureParameter","text":"<pre><code>ArchitectureParameter(**kwargs)\n</code></pre> <p>Factory function</p> Source code in <code>neps/search_spaces/architecture/api.py</code> <pre><code>def ArchitectureParameter(**kwargs):\n    \"\"\"Factory function\"\"\"\n\n    if \"structure\" not in kwargs:\n        raise ValueError(\"Factory function requires structure\")\n    if not isinstance(kwargs[\"structure\"], list) or len(kwargs[\"structure\"]) == 1:\n        base = GraphGrammar\n    else:\n        base = GraphGrammarMultipleRepetitive\n\n    class _FunctionParameter(base):\n        def __init__(\n            self,\n            structure: Grammar\n            | list[Grammar]\n            | ConstrainedGrammar\n            | list[ConstrainedGrammar]\n            | str\n            | list[str]\n            | dict\n            | list[dict],\n            primitives: dict,\n            constraint_kwargs: dict | None = None,\n            name: str = \"ArchitectureParameter\",\n            set_recursive_attribute: Callable | None = None,\n            **kwargs,\n        ):\n            local_vars = locals()\n            self.input_kwargs = {\n                args: local_vars[args]\n                for args in inspect.getfullargspec(self.__init__).args  # type: ignore[misc]\n                if args != \"self\"\n            }\n            self.input_kwargs.update(**kwargs)\n\n            if isinstance(structure, list):\n                structures = [\n                    _dict_structure_to_str(\n                        st,\n                        primitives,\n                        repetitive_mapping=kwargs[\"terminal_to_sublanguage_map\"]\n                        if \"terminal_to_sublanguage_map\" in kwargs\n                        else None,\n                    )\n                    if isinstance(st, dict)\n                    else st\n                    for st in structure\n                ]\n                _structures = []\n                for st in structures:\n                    if isinstance(st, str):\n                        if constraint_kwargs is None:\n                            _st = Grammar.fromstring(st)\n                        else:\n                            _st = ConstrainedGrammar.fromstring(st)\n                            _st.set_constraints(**constraint_kwargs)\n                    _structures.append(_st)  # type: ignore[has-type]\n                structures = _structures\n\n                super().__init__(\n                    grammars=structures,\n                    terminal_to_op_names=primitives,\n                    edge_attr=False,\n                    **kwargs,\n                )\n            else:\n                if isinstance(structure, dict):\n                    structure = _dict_structure_to_str(structure, primitives)\n\n                if isinstance(structure, str):\n                    if constraint_kwargs is None:\n                        structure = Grammar.fromstring(structure)\n                    else:\n                        structure = ConstrainedGrammar.fromstring(structure)\n                        structure.set_constraints(**constraint_kwargs)  # type: ignore[union-attr]\n\n                super().__init__(\n                    grammar=structure,  # type: ignore[arg-type]\n                    terminal_to_op_names=primitives,\n                    edge_attr=False,\n                    **kwargs,\n                )\n\n            self._set_recursive_attribute = set_recursive_attribute\n            self.name: str = name\n\n        def to_pytorch(self) -&gt; nn.Module:\n            self.clear_graph()\n            if len(self.nodes()) == 0:\n                composed_function = self.compose_functions()\n                # part below is required since PyTorch has no standard functional API\n                self.graph_to_self(composed_function)\n                self.prune_graph()\n\n                if self._set_recursive_attribute:\n                    m = _build(\n                        self, self._set_recursive_attribute\n                    )\n\n                if m is not None:\n                    return m\n\n                self.compile()\n                self.update_op_names()\n            return super().to_pytorch()  # create PyTorch model\n\n        def to_tensorflow(self, inputs):\n            composed_function = self.compose_functions(flatten_graph=False)\n            return composed_function(inputs)\n\n        def create_new_instance_from_id(self, identifier: str):\n            g = ArchitectureParameter(**self.input_kwargs)  # type: ignore[arg-type]\n            g.load_from(identifier)\n            return g\n\n    return _FunctionParameter(**kwargs)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/","title":"Cfg","text":""},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg","title":"neps.search_spaces.architecture.cfg","text":""},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar","title":"Grammar","text":"<pre><code>Grammar(*args, **kwargs)\n</code></pre> <p>               Bases: <code>CFG</code></p> <p>Extended context free grammar (CFG) class from the NLTK python package We have provided functionality to sample from the CFG. We have included generation capability within the class (before it was an external function) Also allow sampling to return whole trees (not just the string of terminals)</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    # store some extra quantities needed later\n    non_unique_nonterminals = [str(prod.lhs()) for prod in self.productions()]\n    self.nonterminals = list(set(non_unique_nonterminals))\n    self.terminals = list(\n        {str(individual) for prod in self.productions() for individual in prod.rhs()}\n        - set(self.nonterminals)\n    )\n    # collect nonterminals that are worth swapping when doing genetic operations (i.e not those with a single production that leads to a terminal)\n    self.swappable_nonterminals = list(\n        {i for i in non_unique_nonterminals if non_unique_nonterminals.count(i) &gt; 1}\n    )\n\n    self.max_sampling_level = 2\n\n    self.convergent = False\n    self._prior = None\n\n    self.check_grammar()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar.compute_space_size","title":"compute_space_size  <code>property</code>","text":"<pre><code>compute_space_size: int\n</code></pre> <p>Computes the size of the space described by the grammar.</p> PARAMETER DESCRIPTION <code>primitive_nonterminal</code> <p>The primitive nonterminal of the grammar. Defaults to \"OPS\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>int</code> <p>size of space described by grammar.</p> <p> TYPE: <code>int</code> </p>"},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar.mutate","title":"mutate","text":"<pre><code>mutate(\n    parent: str,\n    subtree_index: int,\n    subtree_node: str,\n    patience: int = 50,\n) -&gt; str\n</code></pre> <p>Grammar-based mutation, i.e., we sample a new subtree from a nonterminal node in the parse tree.</p> PARAMETER DESCRIPTION <code>parent</code> <p>parent of the mutation.</p> <p> TYPE: <code>str</code> </p> <code>subtree_index</code> <p>index pointing to the node that is root of the subtree.</p> <p> TYPE: <code>int</code> </p> <code>subtree_node</code> <p>nonterminal symbol of the node.</p> <p> TYPE: <code>str</code> </p> <code>patience</code> <p>Number of tries. Defaults to 50.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> RETURNS DESCRIPTION <code>str</code> <p>mutated child from parent.</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>def mutate(\n    self, parent: str, subtree_index: int, subtree_node: str, patience: int = 50\n) -&gt; str:\n    \"\"\"Grammar-based mutation, i.e., we sample a new subtree from a nonterminal\n    node in the parse tree.\n\n    Args:\n        parent (str): parent of the mutation.\n        subtree_index (int): index pointing to the node that is root of the subtree.\n        subtree_node (str): nonterminal symbol of the node.\n        patience (int, optional): Number of tries. Defaults to 50.\n\n    Returns:\n        str: mutated child from parent.\n    \"\"\"\n    # chop out subtree\n    pre, _, post = self.remove_subtree(parent, subtree_index)\n    _patience = patience\n    while _patience &gt; 0:\n        # only sample subtree -&gt; avoids full sampling of large parse trees\n        new_subtree = self.sampler(1, start_symbol=subtree_node)[0]\n        child = pre + new_subtree + post\n        if parent != child:  # ensure that parent is really mutated\n            break\n        _patience -= 1\n\n    child = self._remove_empty_spaces(child)\n\n    return child\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar.rand_subtree","title":"rand_subtree","text":"<pre><code>rand_subtree(tree: str) -&gt; Tuple[str, int]\n</code></pre> <p>Helper function to choose a random subtree in a given parse tree. Runs a single pass through the tree (stored as string) to look for the location of swappable nonterminal symbols.</p> PARAMETER DESCRIPTION <code>tree</code> <p>parse tree.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Tuple[str, int]</code> <p>Tuple[str, int]: return the parent node of the subtree and its index.</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>def rand_subtree(self, tree: str) -&gt; Tuple[str, int]:\n    \"\"\"Helper function to choose a random subtree in a given parse tree.\n    Runs a single pass through the tree (stored as string) to look for\n    the location of swappable nonterminal symbols.\n\n    Args:\n        tree (str): parse tree.\n\n    Returns:\n        Tuple[str, int]: return the parent node of the subtree and its index.\n    \"\"\"\n    split_tree = tree.split(\" \")\n    swappable_indices = [\n        i\n        for i in range(0, len(split_tree))\n        if split_tree[i][1:] in self.swappable_nonterminals\n    ]\n    r = np.random.randint(1, len(swappable_indices))\n    chosen_non_terminal = split_tree[swappable_indices[r]][1:]\n    chosen_non_terminal_index = swappable_indices[r]\n    return chosen_non_terminal, chosen_non_terminal_index\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar.remove_subtree","title":"remove_subtree  <code>staticmethod</code>","text":"<pre><code>remove_subtree(\n    tree: str, index: int\n) -&gt; Tuple[str, str, str]\n</code></pre> <p>Helper functioon to remove a subtree from a parse tree given its index. E.g. '(S (S (T 2)) (ADD +) (T 1))' becomes '(S (S (T 2)) ', '(T 1))'  after removing (ADD +)</p> PARAMETER DESCRIPTION <code>tree</code> <p>parse tree</p> <p> TYPE: <code>str</code> </p> <code>index</code> <p>index of the subtree root node</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tuple[str, str, str]</code> <p>Tuple[str, str, str]: part before the subtree, subtree, part past subtree</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>@staticmethod\ndef remove_subtree(tree: str, index: int) -&gt; Tuple[str, str, str]:\n    \"\"\"Helper functioon to remove a subtree from a parse tree\n    given its index.\n    E.g. '(S (S (T 2)) (ADD +) (T 1))'\n    becomes '(S (S (T 2)) ', '(T 1))'  after removing (ADD +)\n\n    Args:\n        tree (str): parse tree\n        index (int): index of the subtree root node\n\n    Returns:\n        Tuple[str, str, str]: part before the subtree, subtree, part past subtree\n    \"\"\"\n    split_tree = tree.split(\" \")\n    pre_subtree = \" \".join(split_tree[:index]) + \" \"\n    #  get chars to the right of split\n    right = \" \".join(split_tree[index + 1 :])\n    # remove chosen subtree\n    # single pass to find the bracket matching the start of the split\n    counter, current_index = 1, 0\n    for char in right:\n        if char == \"(\":\n            counter += 1\n        elif char == \")\":\n            counter -= 1\n        if counter == 0:\n            break\n        current_index += 1\n    post_subtree = right[current_index + 1 :]\n    removed = \"\".join(split_tree[index]) + \" \" + right[: current_index + 1]\n    return (pre_subtree, removed, post_subtree)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/","title":"Core graph grammar","text":""},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar","title":"neps.search_spaces.architecture.core_graph_grammar","text":""},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar","title":"CoreGraphGrammar","text":"<pre><code>CoreGraphGrammar(\n    grammars: list[Grammar] | Grammar,\n    terminal_to_op_names: dict,\n    terminal_to_graph_edges: dict = None,\n    edge_attr: bool = True,\n    edge_label: str = \"op_name\",\n    zero_op: list = None,\n    identity_op: list = None,\n    name: str = None,\n    scope: str = None,\n    return_all_subgraphs: bool = False,\n    return_graph_per_hierarchy: bool = False,\n)\n</code></pre> <p>               Bases: <code>Graph</code></p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def __init__(\n    self,\n    grammars: list[Grammar] | Grammar,\n    terminal_to_op_names: dict,\n    terminal_to_graph_edges: dict = None,\n    edge_attr: bool = True,\n    edge_label: str = \"op_name\",\n    zero_op: list = None,\n    identity_op: list = None,\n    name: str = None,\n    scope: str = None,\n    return_all_subgraphs: bool = False,\n    return_graph_per_hierarchy: bool = False,\n):\n    super().__init__(name, scope)\n\n    self.grammars = [grammars] if isinstance(grammars, Grammar) else grammars\n\n    self.terminal_to_op_names = terminal_to_op_names\n\n    grammar_terminals = {\n        terminal for grammar in self.grammars for terminal in grammar.terminals\n    }\n    diff_terminals = grammar_terminals - set(self.terminal_to_op_names.keys())\n    if len(diff_terminals) != 0:\n        raise Exception(\n            f\"Terminals {diff_terminals} not defined in primitive mapping!\"\n        )\n\n    if terminal_to_graph_edges is None:  # only compute it once -&gt; more efficient\n        self.terminal_to_graph_edges = get_edge_lists_of_topologies(\n            self.terminal_to_op_names\n        )\n    else:\n        self.terminal_to_graph_edges = terminal_to_graph_edges\n    self.edge_attr = edge_attr\n    self.edge_label = edge_label\n\n    self.zero_op = zero_op if zero_op is not None else []\n    self.identity_op = identity_op if identity_op is not None else []\n\n    self.terminal_to_graph_nodes: dict = {}\n\n    self.return_all_subgraphs = return_all_subgraphs\n    self.return_graph_per_hierarchy = return_graph_per_hierarchy\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.OPTIMIZER_SCOPE","title":"OPTIMIZER_SCOPE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPTIMIZER_SCOPE = 'all'\n</code></pre> <p>Whether the search space has an interface to one of the tabular benchmarks which can then be used to query architecture performances.</p> <p>If this is set to true then <code>query()</code> should be implemented.</p>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>As it is very complicated to compare graphs (i.e. check all edge attributes, do the have shared attributes, ...) use just the name for comparison.</p> <p>This is used when determining whether two instances are copies.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __hash__(self):\n    \"\"\"\n    As it is very complicated to compare graphs (i.e. check all edge\n    attributes, do the have shared attributes, ...) use just the name\n    for comparison.\n\n    This is used when determining whether two instances are copies.\n    \"\"\"\n    h = 0\n    h += hash(self.name)\n    h += hash(self.scope) if self.scope else 0\n    h += hash(self._id)\n    return h\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.add_edges_densly","title":"add_edges_densly","text":"<pre><code>add_edges_densly()\n</code></pre> <p>Adds edges to get a fully connected DAG without cycles</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def add_edges_densly(self):\n    \"\"\"\n    Adds edges to get a fully connected DAG without cycles\n    \"\"\"\n    self.add_edges_from(self.get_dense_edges())\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.add_node","title":"add_node","text":"<pre><code>add_node(node_index, **attr)\n</code></pre> <p>Adds a node to the graph.</p> <p>Note that adding a node using an index that has been used already will override its attributes.</p> PARAMETER DESCRIPTION <code>node_index</code> <p>The index for the node. Expect to be &gt;= 1.</p> <p> TYPE: <code>int</code> </p> <code>**attr</code> <p>The attributes which can be added in a dict like form.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def add_node(self, node_index, **attr):\n    \"\"\"\n    Adds a node to the graph.\n\n    Note that adding a node using an index that has been used already\n    will override its attributes.\n\n    Args:\n        node_index (int): The index for the node. Expect to be &gt;= 1.\n        **attr: The attributes which can be added in a dict like form.\n    \"\"\"\n    assert node_index &gt;= 1, \"Expecting the node index to be greater or equal 1\"\n    nx.DiGraph.add_node(self, node_index, **attr)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.assemble_trees","title":"assemble_trees","text":"<pre><code>assemble_trees(\n    base_tree: str | DiGraph,\n    motif_trees: list[str] | list[DiGraph],\n    terminal_to_sublanguage_map: dict = None,\n    node_label: str = \"op_name\",\n) -&gt; str | DiGraph\n</code></pre> <p>Assembles the base parse tree with the motif parse trees</p> PARAMETER DESCRIPTION <code>base_tree</code> <p>Base parse tree</p> <p> TYPE: <code>DiGraph</code> </p> <code>motif_trees</code> <p>List of motif parse trees</p> <p> TYPE: <code>List[DiGraph]</code> </p> <code>node_label</code> <p>node label key. Defaults to \"op_name\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'op_name'</code> </p> RETURNS DESCRIPTION <code>str | DiGraph</code> <p>nx.DiGraph: Assembled parse tree</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def assemble_trees(\n    self,\n    base_tree: str | nx.DiGraph,\n    motif_trees: list[str] | list[nx.DiGraph],\n    terminal_to_sublanguage_map: dict = None,\n    node_label: str = \"op_name\",\n) -&gt; str | nx.DiGraph:\n    \"\"\"Assembles the base parse tree with the motif parse trees\n\n    Args:\n        base_tree (nx.DiGraph): Base parse tree\n        motif_trees (List[nx.DiGraph]): List of motif parse trees\n        node_label (str, optional): node label key. Defaults to \"op_name\".\n\n    Returns:\n        nx.DiGraph: Assembled parse tree\n    \"\"\"\n    if not all([isinstance(base_tree, type(tree)) for tree in motif_trees]):\n        raise ValueError(\"All trees must be of the same type!\")\n    if isinstance(base_tree, str):\n        ensembled_tree_string = base_tree\n        if terminal_to_sublanguage_map is None:\n            raise NotImplementedError\n\n        for motif, replacement in zip(\n            terminal_to_sublanguage_map.keys(), motif_trees\n        ):\n            if motif in ensembled_tree_string:\n                ensembled_tree_string = ensembled_tree_string.replace(\n                    motif, replacement\n                )\n        return ensembled_tree_string\n    elif isinstance(base_tree, nx.DiGraph):\n        raise NotImplementedError\n        leafnodes = self._find_leafnodes(base_tree)\n        root_nodes = [self._find_root(G) for G in motif_trees]\n        root_op_names = np.array(\n            [\n                motif_tree.nodes[root_node][node_label]\n                for motif_tree, root_node in zip(motif_trees, root_nodes)\n            ]\n        )\n        largest_node_number = max(base_tree.nodes())\n        # ensembled_tree = base_tree.copy()\n        # recreation is slightly faster\n        ensembled_tree: nx.DiGraph = nx.DiGraph()\n        ensembled_tree.add_nodes_from(base_tree.nodes(data=True))\n        ensembled_tree.add_edges_from(base_tree.edges())\n        for leafnode in leafnodes:\n            idx = np.where(base_tree.nodes[leafnode][node_label] == root_op_names)[0]\n            if len(idx) == 0:\n                continue\n            if len(idx) &gt; 1:\n                raise ValueError(\n                    \"More than two similar terminal/start symbols are not supported!\"\n                )\n\n            tree = motif_trees[idx[0]]\n            # generate mapping\n            mapping = {\n                n: n_new\n                for n, n_new in zip(\n                    tree.nodes(),\n                    range(\n                        largest_node_number + 1,\n                        largest_node_number + 1 + len(tree),\n                    ),\n                )\n            }\n            largest_node_number = largest_node_number + 1 + len(tree)\n            tree_relabeled = self._relabel_nodes(G=tree, mapping=mapping)\n\n            # compose trees\n            predecessor_in_base_tree = list(ensembled_tree.pred[leafnode])[0]\n            motif_tree_root_node = self._find_root(tree_relabeled)\n            successors_in_motif_tree = tree_relabeled.nodes[motif_tree_root_node][\n                \"children\"\n            ]\n\n            # delete unnecessary edges\n            ensembled_tree.remove_node(leafnode)\n            tree_relabeled.remove_node(motif_tree_root_node)\n            # add new edges\n            tree_relabeled.add_node(predecessor_in_base_tree)\n            for n in successors_in_motif_tree:\n                tree_relabeled.add_edge(predecessor_in_base_tree, n)\n\n            ensembled_tree.update(\n                edges=tree_relabeled.edges(data=True),\n                nodes=tree_relabeled.nodes(data=True),\n            )\n\n            idx = np.where(\n                np.array(ensembled_tree.nodes[predecessor_in_base_tree][\"children\"])\n                == leafnode\n            )[0][0]\n            old_children = ensembled_tree.nodes[predecessor_in_base_tree][\"children\"]\n            ensembled_tree.nodes[predecessor_in_base_tree][\"children\"] = (\n                old_children[: idx + 1]\n                + successors_in_motif_tree\n                + old_children[idx + 1 :]\n            )\n            ensembled_tree.nodes[predecessor_in_base_tree][\"children\"].remove(\n                leafnode\n            )\n        return ensembled_tree\n    else:\n        raise NotImplementedError(\n            f\"Assembling of trees of type {type(base_tree)} is not supported!\"\n        )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.build_graph_from_tree","title":"build_graph_from_tree","text":"<pre><code>build_graph_from_tree(\n    tree: DiGraph,\n    terminal_to_torch_map: dict,\n    node_label: str = \"op_name\",\n    flatten_graph: bool = True,\n    return_cell: bool = False,\n) -&gt; None | Graph\n</code></pre> <p>Builds the computational graph from a parse tree.</p> PARAMETER DESCRIPTION <code>tree</code> <p>parse tree.</p> <p> TYPE: <code>DiGraph</code> </p> <code>terminal_to_torch_map</code> <p>Mapping from terminal symbols to primitives or topologies.</p> <p> TYPE: <code>dict</code> </p> <code>node_label</code> <p>Key to access terminal symbol. Defaults to \"op_name\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'op_name'</code> </p> <code>return_cell</code> <p>Whether to return a cell. Is only needed if cell is repeated multiple times.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>None | Graph</code> <p>Tuple[Union[None, Graph]]: computational graph (self) or cell.</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def build_graph_from_tree(\n    self,\n    tree: nx.DiGraph,\n    terminal_to_torch_map: dict,\n    node_label: str = \"op_name\",\n    flatten_graph: bool = True,\n    return_cell: bool = False,\n) -&gt; None | Graph:\n    \"\"\"Builds the computational graph from a parse tree.\n\n    Args:\n        tree (nx.DiGraph): parse tree.\n        terminal_to_torch_map (dict): Mapping from terminal symbols to primitives or topologies.\n        node_label (str, optional): Key to access terminal symbol. Defaults to \"op_name\".\n        return_cell (bool, optional): Whether to return a cell. Is only needed if cell is repeated multiple times.\n        Defaults to False.\n\n    Returns:\n        Tuple[Union[None, Graph]]: computational graph (self) or cell.\n    \"\"\"\n\n    def _build_graph_from_tree(\n        visited: set,\n        tree: nx.DiGraph,\n        node: int,\n        terminal_to_torch_map: dict,\n        node_label: str,\n        is_primitive: bool = False,\n    ):\n        \"\"\"Recursive DFS-esque function to build computational graph from parse tree\n\n        Args:\n            visited (set): set of visited nodes.\n            tree (nx.DiGraph): parse tree.\n            node (int): node index.\n            terminal_to_torch_map (dict): mapping from terminal symbols to primitives or topologies.\n            node_label (str): key to access operation name\n\n        Raises:\n            Exception: primitive or topology is unknown, i.e., it is probably missing in the terminal to\n            torch mapping\n            Exception: leftmost children can only be primitive, topology or have one child\n\n        Returns:\n            [type]: computational graph.\n        \"\"\"\n        if node not in visited:\n            subgraphs = []\n            primitive_hps = []\n            if len(tree.out_edges(node)) == 0:\n                if is_primitive:\n                    return tree.nodes[node][node_label]\n                else:\n                    if (\n                        tree.nodes[node][node_label]\n                        not in terminal_to_torch_map.keys()\n                    ):\n                        raise Exception(\n                            f\"Unknown primitive or topology: {tree.nodes[node][node_label]}\"\n                        )\n                    return deepcopy(\n                        terminal_to_torch_map[tree.nodes[node][node_label]]\n                    )\n            if len(tree.out_edges(node)) == 1:\n                return _build_graph_from_tree(\n                    visited,\n                    tree,\n                    list(tree.neighbors(node))[0],\n                    terminal_to_torch_map,\n                    node_label,\n                    is_primitive,\n                )\n            # for idx, neighbor in enumerate(tree.neighbors(node)):\n            for idx, neighbor in enumerate(\n                self._get_neighbors_from_parse_tree(tree, node)\n            ):\n                if idx == 0:  # topology or primitive\n                    n = neighbor\n                    while not tree.nodes[n][\"terminal\"]:\n                        if len(tree.out_edges(n)) != 1:\n                            raise Exception(\n                                \"Leftmost Child can only be primitive, topology or recursively have one child!\"\n                            )\n                        n = next(tree.neighbors(n))\n                    if is_primitive:\n                        primitive_hp_key = tree.nodes[n][node_label]\n                        primitive_hp_dict = {primitive_hp_key: None}\n                        is_primitive_op = True\n                    else:\n                        if (\n                            tree.nodes[n][node_label]\n                            not in terminal_to_torch_map.keys()\n                        ):\n                            raise Exception(\n                                f\"Unknown primitive or topology: {tree.nodes[n][node_label]}\"\n                            )\n                        graph_el = terminal_to_torch_map[tree.nodes[n][node_label]]\n                        is_primitive_op = issubclass(\n                            graph_el.func\n                            if isinstance(graph_el, partial)\n                            else graph_el,\n                            AbstractPrimitive,\n                        )\n                elif not tree.nodes[neighbor][\n                    \"terminal\"\n                ]:  # exclude '[' ']' ... symbols\n                    if is_primitive:\n                        primitive_hp_dict[primitive_hp_key] = _build_graph_from_tree(\n                            visited,\n                            tree,\n                            neighbor,\n                            terminal_to_torch_map,\n                            node_label,\n                            is_primitive_op,\n                        )\n                    elif is_primitive_op:\n                        primitive_hps.append(\n                            _build_graph_from_tree(\n                                visited,\n                                tree,\n                                neighbor,\n                                terminal_to_torch_map,\n                                node_label,\n                                is_primitive_op,\n                            )\n                        )\n                    else:\n                        subgraphs.append(\n                            _build_graph_from_tree(\n                                visited,\n                                tree,\n                                neighbor,\n                                terminal_to_torch_map,\n                                node_label,\n                                is_primitive_op,\n                            )\n                        )\n                elif (\n                    tree.nodes[neighbor][node_label] in terminal_to_torch_map.keys()\n                ):  # exclude '[' ']' ... symbols\n                    # TODO check if there is a potential bug here?\n                    subgraphs.append(\n                        deepcopy(\n                            terminal_to_torch_map[tree.nodes[neighbor][node_label]]\n                        )\n                    )\n\n            if is_primitive:\n                return primitive_hp_dict\n            elif is_primitive_op:\n                return dict(\n                    collections.ChainMap(*([{\"op\": graph_el}] + primitive_hps))\n                )\n            else:\n                return graph_el(*subgraphs)\n\n    def _flatten_graph(\n        graph,\n        flattened_graph,\n        start_node: int = None,\n        end_node: int = None,\n    ):\n        nodes: dict = {}\n        for u, v, data in graph.edges(data=True):\n            if u in nodes.keys():\n                _u = nodes[u]\n            else:\n                _u = (\n                    1\n                    if len(flattened_graph.nodes.keys()) == 0\n                    else max(flattened_graph.nodes.keys()) + 1\n                )\n                _u = (\n                    start_node\n                    if graph.in_degree(u) == 0 and start_node is not None\n                    else _u\n                )\n                nodes[u] = _u\n                if _u not in flattened_graph.nodes.keys():\n                    flattened_graph.add_node(_u)\n\n            if v in nodes.keys():\n                _v = nodes[v]\n            else:\n                _v = max(flattened_graph.nodes.keys()) + 1\n                _v = (\n                    end_node\n                    if graph.out_degree(v) == 0 and end_node is not None\n                    else _v\n                )\n                nodes[v] = _v\n                if _v not in flattened_graph.nodes.keys():\n                    flattened_graph.add_node(_v)\n\n            if isinstance(data[\"op\"], Graph):\n                flattened_graph = _flatten_graph(\n                    data[\"op\"], flattened_graph, start_node=_u, end_node=_v\n                )\n            else:\n                flattened_graph.add_edge(_u, _v)\n                flattened_graph.edges[_u, _v].update(data)\n\n        return flattened_graph\n\n    root_node = self._find_root(tree)\n    graph = _build_graph_from_tree(\n        set(), tree, root_node, terminal_to_torch_map, node_label\n    )\n    self._check_graph(graph)\n    if return_cell:\n        cell = (\n            _flatten_graph(graph, flattened_graph=Graph()) if flatten_graph else graph\n        )\n        return cell\n    else:\n        if flatten_graph:\n            _flatten_graph(graph, flattened_graph=self)\n        else:\n            self.add_edge(0, 1)\n            self.edges[0, 1].set(\"op\", graph)\n        return None\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Deep copy of the current graph.</p> RETURNS DESCRIPTION <code>Graph</code> <p>Deep copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def clone(self):\n    \"\"\"\n    Deep copy of the current graph.\n\n    Returns:\n        Graph: Deep copy of the graph.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.compile","title":"compile","text":"<pre><code>compile()\n</code></pre> <p>Instanciates the ops at the edges using the arguments specified at the edges</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def compile(self):\n    \"\"\"\n    Instanciates the ops at the edges using the arguments specified at the edges\n    \"\"\"\n    for graph in self._get_child_graphs(single_instances=False) + [self]:\n        logger.debug(f\"Compiling graph {graph.name}\")\n        for _, v, edge_data in graph.edges.data():\n            if not edge_data.is_final():\n                attr = edge_data.to_dict()\n                op = attr.pop(\"op\")\n\n                if isinstance(op, list):\n                    compiled_ops = []\n                    for i, o in enumerate(op):\n                        if inspect.isclass(o):\n                            # get the relevant parameter if there are more.\n                            a = {\n                                k: v[i] if isinstance(v, list) else v\n                                for k, v in attr.items()\n                            }\n                            compiled_ops.append(o(**a))\n                        else:\n                            logger.debug(f\"op {o} already compiled. Skipping\")\n                    edge_data.set(\"op\", compiled_ops)\n                elif isinstance(op, AbstractPrimitive):\n                    logger.debug(f\"op {op} already compiled. Skipping\")\n                elif inspect.isclass(op) and issubclass(op, AbstractPrimitive):\n                    # Init the class\n                    if \"op_name\" in attr:\n                        del attr[\"op_name\"]\n                    edge_data.set(\"op\", op(**attr))\n                elif isinstance(op, Graph):\n                    pass  # This is already covered by _get_child_graphs\n                else:\n                    raise ValueError(f\"Unkown format of op: {op}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy as defined in networkx, i.e. a shallow copy.</p> <p>Just handling recursively nested graphs seperately.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Copy as defined in networkx, i.e. a shallow copy.\n\n    Just handling recursively nested graphs seperately.\n    \"\"\"\n\n    def copy_dict(d):\n        copied_dict = d.copy()\n        for k, v in d.items():\n            if isinstance(v, Graph):\n                copied_dict[k] = v.copy()\n            elif isinstance(v, list):\n                copied_dict[k] = [\n                    i.copy() if isinstance(i, Graph) else i for i in v\n                ]\n            elif isinstance(v, torch.nn.Module) or isinstance(v, AbstractPrimitive):\n                copied_dict[k] = copy.deepcopy(v)\n        return copied_dict\n\n    G = self.__class__()\n    G.graph.update(self.graph)\n    G.add_nodes_from((n, copy_dict(d)) for n, d in self._node.items())\n    G.add_edges_from(\n        (u, v, datadict.copy())\n        for u, nbrs in self._adj.items()\n        for v, datadict in nbrs.items()\n    )\n    G.scope = self.scope\n    G.name = self.name\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.forward","title":"forward","text":"<pre><code>forward(x, *args)\n</code></pre> <p>Forward some data through the graph. This is done recursively in case there are graphs defined on nodes or as 'op' on edges.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input. If the graph sits on a node the input can be a dict with {source_idx: Tensor} to be routed to the defined input nodes. If the graph sits on an edge, x is the feature tensor.</p> <p> TYPE: <code>Tensor or dict</code> </p> <code>args</code> <p>This is only required to handle cases where the graph sits on an edge and receives an EdgeData object which will be ignored</p> <p> DEFAULT: <code>()</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def forward(self, x, *args):\n    \"\"\"\n    Forward some data through the graph. This is done recursively\n    in case there are graphs defined on nodes or as 'op' on edges.\n\n    Args:\n        x (Tensor or dict): The input. If the graph sits on a node the\n            input can be a dict with {source_idx: Tensor} to be routed\n            to the defined input nodes. If the graph sits on an edge,\n            x is the feature tensor.\n        args: This is only required to handle cases where the graph sits\n            on an edge and receives an EdgeData object which will be ignored\n    \"\"\"\n    logger.debug(f\"Graph {self.name} called. Input {log_formats(x)}.\")\n\n    # Assign x to the corresponding input nodes\n    self._assign_x_to_nodes(x)\n\n    for node_idx in lexicographical_topological_sort(self):\n        node = self.nodes[node_idx]\n        logger.debug(\n            \"Node {}-{}, current data {}, start processing...\".format(\n                self.name, node_idx, log_formats(node)\n            )\n        )\n\n        # node internal: process input if necessary\n        if (\"subgraph\" in node and \"comb_op\" not in node) or (\n            \"comb_op\" in node and \"subgraph\" not in node\n        ):\n            log_first_n(\n                logging.WARN, \"Comb_op is ignored if subgraph is defined!\", n=1\n            )\n        # TODO: merge 'subgraph' and 'comb_op'. It is basicallly the same thing. Also in parse()\n        if \"subgraph\" in node:\n            x = node[\"subgraph\"].forward(node[\"input\"])\n        else:\n            if len(node[\"input\"].values()) == 1:\n                x = list(node[\"input\"].values())[0]\n            else:\n                x = node[\"comb_op\"](\n                    [node[\"input\"][k] for k in sorted(node[\"input\"].keys())]\n                )\n        node[\"input\"] = {}  # clear the input as we have processed it\n\n        if (\n            len(list(self.neighbors(node_idx))) == 0\n            and node_idx &lt; list(lexicographical_topological_sort(self))[-1]\n        ):\n            # We have more than one output node. This is e.g. the case for\n            # auxillary losses. Attach them to the graph, handling must done\n            # by the user.\n            logger.debug(\n                \"Graph {} has more then one output node. Storing output of non-maximum index node {} at graph dict\".format(\n                    self, node_idx\n                )\n            )\n            self.graph[f\"out_from_{node_idx}\"] = x\n        else:\n            # outgoing edges: process all outgoing edges\n            for neigbor_idx in self.neighbors(node_idx):\n                edge_data = self.get_edge_data(node_idx, neigbor_idx)\n                # inject edge data only for AbstractPrimitive, not Graphs\n                if isinstance(edge_data.op, Graph):\n                    edge_output = edge_data.op.forward(x)\n                elif isinstance(edge_data.op, AbstractPrimitive):\n                    logger.debug(\n                        \"Processing op {} at edge {}-{}\".format(\n                            edge_data.op, node_idx, neigbor_idx\n                        )\n                    )\n                    edge_output = edge_data.op.forward(x)\n                else:\n                    raise ValueError(\n                        \"Unknown class as op: {}. Expected either Graph or AbstactPrimitive\".format(\n                            edge_data.op\n                        )\n                    )\n                self.nodes[neigbor_idx][\"input\"].update({node_idx: edge_output})\n\n        logger.debug(f\"Node {self.name}-{node_idx}, processing done.\")\n\n    logger.debug(f\"Graph {self.name} exiting. Output {log_formats(x)}.\")\n    return x\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.from_nxTree_to_stringTree","title":"from_nxTree_to_stringTree","text":"<pre><code>from_nxTree_to_stringTree(\n    nxTree: DiGraph, node_label: str = \"op_name\"\n) -&gt; str\n</code></pre> <p>Transforms parse tree represented as NetworkX DAG to string representation.</p> PARAMETER DESCRIPTION <code>nxTree</code> <p>parse tree.</p> <p> TYPE: <code>DiGraph</code> </p> <code>node_label</code> <p>key to access operation names. Defaults to \"op_name\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'op_name'</code> </p> RETURNS DESCRIPTION <code>str</code> <p>parse tree represented as string.</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def from_nxTree_to_stringTree(\n    self, nxTree: nx.DiGraph, node_label: str = \"op_name\"\n) -&gt; str:\n    \"\"\"Transforms parse tree represented as NetworkX DAG to string representation.\n\n    Args:\n        nxTree (nx.DiGraph): parse tree.\n        node_label (str, optional): key to access operation names. Defaults to \"op_name\".\n\n    Returns:\n        str: parse tree represented as string.\n    \"\"\"\n\n    def dfs(visited, graph, node):\n        if node not in visited:\n            visited.add(node)\n            if graph.nodes[node][\"terminal\"]:\n                return f\"{graph.nodes[node][node_label]}\"\n            tmp_str = f\"{f'({graph.nodes[node][node_label]}'}\" + \" \"\n            # for neighbor in graph.neighbors(node):\n            for neighbor in self._get_neighbors_from_parse_tree(graph, node):\n                tmp_str += dfs(visited, graph, neighbor) + \" \"\n            tmp_str = tmp_str[:-1] + \")\"\n            return tmp_str\n        return \"\"\n\n    return dfs(set(), nxTree, node=self._find_root(nxTree))\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.from_stringTree_to_graph_repr","title":"from_stringTree_to_graph_repr","text":"<pre><code>from_stringTree_to_graph_repr(\n    string_tree: str,\n    grammar: Grammar,\n    valid_terminals: KeysView,\n    edge_attr: bool = True,\n    sym_name: str = \"op_name\",\n    prune: bool = True,\n    add_subtree_map: bool = False,\n    return_all_subgraphs: bool = None,\n    return_graph_per_hierarchy: bool = None,\n) -&gt; DiGraph | tuple[DiGraph, OrderedDict]\n</code></pre> <p>Generates graph from parse tree in string representation. Note that we ignore primitive HPs!</p> PARAMETER DESCRIPTION <code>string_tree</code> <p>parse tree.</p> <p> TYPE: <code>str</code> </p> <code>grammar</code> <p>underlying grammar.</p> <p> TYPE: <code>Grammar</code> </p> <code>valid_terminals</code> <p>list of keys.</p> <p> TYPE: <code>list</code> </p> <code>edge_attr</code> <p>Shoud graph be edge attributed (True) or node attributed (False). Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sym_name</code> <p>Attribute name of operation. Defaults to \"op_name\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'op_name'</code> </p> <code>prune</code> <p>Prune graph, e.g., None operations etc. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>add_subtree_map</code> <p>Add attribute indicating to which subtrees of the parse tree the specific part belongs to. Can only be true if you set prune=False! TODO: Check if we really need this constraint or can also allow pruning. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>return_all_subgraphs</code> <p>Additionally returns an hierarchical dictionary containing all subgraphs. Defaults to False. TODO: check if edge attr also works.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>return_graph_per_hierarchy</code> <p>Additionally returns a graph from each each hierarchy.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DiGraph | tuple[DiGraph, OrderedDict]</code> <p>nx.DiGraph: [description]</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def from_stringTree_to_graph_repr(\n    self,\n    string_tree: str,\n    grammar: Grammar,\n    valid_terminals: collections.abc.KeysView,\n    edge_attr: bool = True,\n    sym_name: str = \"op_name\",\n    prune: bool = True,\n    add_subtree_map: bool = False,\n    return_all_subgraphs: bool = None,\n    return_graph_per_hierarchy: bool = None,\n) -&gt; nx.DiGraph | tuple[nx.DiGraph, collections.OrderedDict]:\n    \"\"\"Generates graph from parse tree in string representation.\n    Note that we ignore primitive HPs!\n\n    Args:\n        string_tree (str): parse tree.\n        grammar (Grammar): underlying grammar.\n        valid_terminals (list): list of keys.\n        edge_attr (bool, optional): Shoud graph be edge attributed (True) or node attributed (False). Defaults to True.\n        sym_name (str, optional): Attribute name of operation. Defaults to \"op_name\".\n        prune (bool, optional): Prune graph, e.g., None operations etc. Defaults to True.\n        add_subtree_map (bool, optional): Add attribute indicating to which subtrees of\n            the parse tree the specific part belongs to. Can only be true if you set prune=False!\n            TODO: Check if we really need this constraint or can also allow pruning. Defaults to False.\n        return_all_subgraphs (bool, optional): Additionally returns an hierarchical dictionary\n            containing all subgraphs. Defaults to False.\n            TODO: check if edge attr also works.\n        return_graph_per_hierarchy (bool, optional): Additionally returns a graph from each\n            each hierarchy.\n\n    Returns:\n        nx.DiGraph: [description]\n    \"\"\"\n\n    def get_node_labels(graph: nx.DiGraph):\n        return [\n            (n, d[sym_name])\n            for n, d in graph.nodes(data=True)\n            if d[sym_name] != \"input\" and d[sym_name] != \"output\"\n        ]\n\n    def get_hierarchicy_dict(\n        string_tree: str,\n        subgraphs: dict,\n        hierarchy_dict: dict = None,\n        hierarchy_level_counter: int = 0,\n    ):\n        if hierarchy_dict is None:\n            hierarchy_dict = {}\n        if hierarchy_level_counter not in hierarchy_dict.keys():\n            hierarchy_dict[hierarchy_level_counter] = []\n        hierarchy_dict[hierarchy_level_counter].append(string_tree)\n        node_labels = get_node_labels(subgraphs[string_tree])\n        for _, node_label in node_labels:\n            if node_label in subgraphs.keys():\n                hierarchy_dict = get_hierarchicy_dict(\n                    node_label, subgraphs, hierarchy_dict, hierarchy_level_counter + 1\n                )\n        return hierarchy_dict\n\n    def get_graph_per_hierarchy(string_tree: str, subgraphs: dict):\n        hierarchy_dict = get_hierarchicy_dict(\n            string_tree=string_tree, subgraphs=subgraphs\n        )\n\n        graph_per_hierarchy = collections.OrderedDict()\n        for k, v in hierarchy_dict.items():\n            if k == 0:\n                graph_per_hierarchy[k] = subgraphs[v[0]]\n            else:\n                subgraph_ = graph_per_hierarchy[k - 1].copy()\n                node_labels = get_node_labels(subgraph_)\n                for node, node_label in node_labels:\n                    if node_label in list(subgraphs.keys()):\n                        in_nodes = list(subgraph_.predecessors(node))\n                        out_nodes = list(subgraph_.successors(node))\n                        node_offset = max(subgraph_.nodes) + 1\n\n                        new_subgraph = nx.relabel.relabel_nodes(\n                            subgraphs[node_label],\n                            mapping={\n                                n: n + node_offset\n                                for n in subgraphs[node_label].nodes\n                            },\n                            copy=True,\n                        )\n                        first_nodes = {e[0] for e in new_subgraph.edges}\n                        second_nodes = {e[1] for e in new_subgraph.edges}\n                        (begin_node,) = first_nodes - second_nodes\n                        (end_node,) = second_nodes - first_nodes\n                        successors = list(new_subgraph.successors(begin_node))\n                        predecessors = list(new_subgraph.predecessors(end_node))\n                        new_subgraph.remove_nodes_from([begin_node, end_node])\n                        edges = []\n                        added_identities = False\n                        for in_node in in_nodes:\n                            for succ in successors:\n                                if succ == end_node:\n                                    if not added_identities:\n                                        edges.extend(\n                                            [\n                                                (inn, onn)\n                                                for inn in in_nodes\n                                                for onn in out_nodes\n                                            ]\n                                        )\n                                    added_identities = True\n                                else:\n                                    edges.append((in_node, succ))\n                        for out_node in out_nodes:\n                            for pred in predecessors:\n                                if pred != begin_node:\n                                    edges.append((pred, out_node))\n\n                        subgraph_ = nx.compose(new_subgraph, subgraph_)\n                        subgraph_.add_edges_from(edges)\n                        subgraph_.remove_node(node)\n\n                graph_per_hierarchy[k] = subgraph_\n        return graph_per_hierarchy\n\n    def to_node_attributed_edge_list(\n        edge_list: list[tuple],\n    ) -&gt; tuple[list[tuple[int, int]], dict]:\n        node_offset = 2\n        edge_to_node_map = {e: i + node_offset for i, e in enumerate(edge_list)}\n        first_nodes = {e[0] for e in edge_list}\n        second_nodes = {e[1] for e in edge_list}\n        (src,) = first_nodes - second_nodes\n        (tgt,) = second_nodes - first_nodes\n        node_list = []\n        for e in edge_list:\n            ni = edge_to_node_map[e]\n            u, v = e\n            if u == src:\n                node_list.append((0, ni))\n            if v == tgt:\n                node_list.append((ni, 1))\n\n            for e_ in filter(\n                lambda e: (e[1] == u), edge_list\n            ):\n                node_list.append((edge_to_node_map[e_], ni))\n\n        return node_list, edge_to_node_map\n\n    def skip_char(char: str) -&gt; bool:\n        return True if char in [\" \", \"\\t\", \"\\n\", \"[\", \"]\"] else False\n\n    if prune:\n        add_subtree_map = False\n\n    if return_all_subgraphs is None:\n        return_all_subgraphs = self.return_all_subgraphs\n    if return_graph_per_hierarchy is None:\n        return_graph_per_hierarchy = self.return_graph_per_hierarchy\n    compute_subgraphs = return_all_subgraphs or return_graph_per_hierarchy\n\n    G = nx.DiGraph()\n    if add_subtree_map:\n        q_nonterminals: Deque = collections.deque()\n    if compute_subgraphs:\n        q_subtrees: Deque = collections.deque()\n        q_subgraphs: Deque = collections.deque()\n        subgraphs_dict = collections.OrderedDict()\n    if edge_attr:\n        node_offset = 0\n        q_el: Deque = collections.deque()  # edge-attr\n        terminal_to_graph = self.terminal_to_graph_edges\n    else:  # node-attributed\n        G.add_node(0, **{sym_name: \"input\"})\n        G.add_node(1, **{sym_name: \"output\"})\n        node_offset = 2\n        if bool(self.terminal_to_graph_nodes):\n            terminal_to_graph_nodes = self.terminal_to_graph_nodes\n        else:\n            terminal_to_graph_nodes = {\n                k: to_node_attributed_edge_list(edge_list) if edge_list else []\n                for k, edge_list in self.terminal_to_graph_edges.items()\n            }\n            self.terminal_to_graph_nodes = terminal_to_graph_nodes\n        terminal_to_graph = {\n            k: v[0] if v else [] for k, v in terminal_to_graph_nodes.items()\n        }\n        q_el = collections.deque()  # node-attr\n\n    # pre-compute stuff\n    begin_end_nodes = {}\n    for sym, g in terminal_to_graph.items():\n        if g:\n            first_nodes = {e[0] for e in g}\n            second_nodes = {e[1] for e in g}\n            (begin_node,) = first_nodes - second_nodes\n            (end_node,) = second_nodes - first_nodes\n            begin_end_nodes[sym] = (begin_node, end_node)\n        else:\n            begin_end_nodes[sym] = (None, None)\n\n    for split_idx, sym in enumerate(string_tree.split(\" \")):\n        is_nonterminal = False\n        if sym == \"\":\n            continue\n        if compute_subgraphs:\n            new_sym = True\n            sym_copy = sym[:]\n        if sym[0] == \"(\":\n            sym = sym[1:]\n            is_nonterminal = True\n        if sym[-1] == \")\":\n            if add_subtree_map:\n                for _ in range(sym.count(\")\")):\n                    q_nonterminals.pop()\n            if compute_subgraphs:\n                new_sym = False\n            while sym[-1] == \")\" and sym not in valid_terminals:\n                sym = sym[:-1]\n\n        if compute_subgraphs and new_sym:\n            if sym in grammar.nonterminals:\n                # need dict as a graph can have multiple subgraphs\n                q_subtrees.append(sym_copy[:])\n            else:\n                q_subtrees[-1] += f\" {sym_copy}\"\n\n        if len(sym) == 1 and skip_char(sym[0]):\n            continue\n\n        if add_subtree_map and sym in grammar.nonterminals:\n            q_nonterminals.append((sym, split_idx))\n        elif sym in valid_terminals and not is_nonterminal:  # terminal symbol\n            if sym in self.terminal_to_graph_edges:\n                if len(q_el) == 0:\n                    if edge_attr:\n                        edges = [\n                            tuple(t + node_offset for t in e)\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n                    else:  # node-attr\n                        edges = [\n                            tuple(t for t in e)\n                            for e in terminal_to_graph_nodes[sym][0]\n                        ]\n                        nodes = [\n                            terminal_to_graph_nodes[sym][1][e]\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n                    if add_subtree_map:\n                        subtrees = []\n                    first_nodes = {e[0] for e in edges}\n                    second_nodes = {e[1] for e in edges}\n                    (src_node,) = first_nodes - second_nodes\n                    (sink_node,) = second_nodes - first_nodes\n                else:\n                    begin_node, end_node = begin_end_nodes[sym]\n                    el = q_el.pop()\n                    if edge_attr:\n                        u, v = el\n                        if add_subtree_map:\n                            subtrees = G[u][v][\"subtrees\"]\n                        G.remove_edge(u, v)\n                        edges = [\n                            tuple(\n                                u\n                                if t == begin_node\n                                else v\n                                if t == end_node\n                                else t + node_offset\n                                for t in e\n                            )\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n                    else:  # node-attr\n                        n = el\n                        if add_subtree_map:\n                            subtrees = G.nodes[n][\"subtrees\"]\n                        in_nodes = list(G.predecessors(n))\n                        out_nodes = list(G.successors(n))\n                        G.remove_node(n)\n                        edges = []\n                        for e in terminal_to_graph_nodes[sym][0]:\n                            if not (e[0] == begin_node or e[1] == end_node):\n                                edges.append((e[0] + node_offset, e[1] + node_offset))\n                            elif e[0] == begin_node:\n                                for nin in in_nodes:\n                                    edges.append((nin, e[1] + node_offset))\n                            elif e[1] == end_node:\n                                for nout in out_nodes:\n                                    edges.append((e[0] + node_offset, nout))\n                        nodes = [\n                            terminal_to_graph_nodes[sym][1][e] + node_offset\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n\n                G.add_edges_from(edges)\n\n                if compute_subgraphs:\n                    subgraph = nx.DiGraph()\n                    subgraph.add_edges_from(edges)\n                    q_subgraphs.append(\n                        {\n                            \"graph\": subgraph,\n                            \"atoms\": collections.OrderedDict(\n                                (atom, None)\n                                for atom in (edges if edge_attr else nodes)\n                            ),\n                        }\n                    )\n\n                if add_subtree_map:\n                    if edge_attr:\n                        subtrees.append(q_nonterminals[-1])\n                        for u, v in edges:\n                            G[u][v][\"subtrees\"] = subtrees.copy()\n                    else:  # node-attr\n                        subtrees.append(q_nonterminals[-1])\n                        for n in nodes:\n                            G.nodes[n][\"subtrees\"] = subtrees.copy()\n\n                q_el.extend(reversed(edges if edge_attr else nodes))\n                if edge_attr:\n                    node_offset += max(max(self.terminal_to_graph_edges[sym]))\n                else:\n                    node_offset += max(terminal_to_graph_nodes[sym][1].values())\n            else:  # primitive operations\n                el = q_el.pop()\n                if edge_attr:\n                    u, v = el\n                    if prune and sym in self.zero_op:\n                        G.remove_edge(u, v)\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].remove_edge(u, v)\n                            del q_subgraphs[-1][\"atoms\"][(u, v)]\n                    else:\n                        G[u][v][sym_name] = sym\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"][u][v][sym_name] = sym\n                        if add_subtree_map:\n                            G[u][v][\"subtrees\"].append(q_nonterminals[-1])\n                            q_nonterminals.pop()\n                else:  # node-attr\n                    n = el\n                    if prune and sym in self.zero_op:\n                        G.remove_node(n)\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].remove_node(n)\n                            del q_subgraphs[-1][\"atoms\"][n]\n                    elif prune and sym in self.identity_op:\n                        G.add_edges_from(\n                            [\n                                (n_in, n_out)\n                                for n_in in G.predecessors(n)\n                                for n_out in G.successors(n)\n                            ]\n                        )\n                        G.remove_node(n)\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].add_edges_from(\n                                [\n                                    (n_in, n_out)\n                                    for n_in in q_subgraphs[-1][\"graph\"].predecessors(\n                                        n\n                                    )\n                                    for n_out in q_subgraphs[-1][\"graph\"].successors(\n                                        n\n                                    )\n                                ]\n                            )\n                            q_subgraphs[-1][\"graph\"].remove_node(n)\n                            del q_subgraphs[-1][\"atoms\"][n]\n                    else:\n                        G.nodes[n][sym_name] = sym\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].nodes[n][sym_name] = sym\n                            q_subgraphs[-1][\"atoms\"][\n                                next(\n                                    filter(\n                                        lambda x: x[1] is None,\n                                        q_subgraphs[-1][\"atoms\"].items(),\n                                    )\n                                )[0]\n                            ] = sym\n                        if add_subtree_map:\n                            G.nodes[n][\"subtrees\"].append(q_nonterminals[-1])\n                            q_nonterminals.pop()\n        if compute_subgraphs and sym_copy[-1] == \")\":\n            q_subtrees[-1] += f\" {sym_copy}\"\n            for _ in range(sym_copy.count(\")\")):\n                subtree_identifier = q_subtrees.pop()\n                if len(q_subtrees) &gt; 0:\n                    q_subtrees[-1] += f\" {subtree_identifier}\"\n                if len(q_subtrees) == len(q_subgraphs) - 1:\n                    difference = subtree_identifier.count(\n                        \"(\"\n                    ) - subtree_identifier.count(\")\")\n                    if difference &lt; 0:\n                        subtree_identifier = subtree_identifier[:difference]\n                    subgraph_dict = q_subgraphs.pop()\n                    subgraph = subgraph_dict[\"graph\"]\n                    atoms = subgraph_dict[\"atoms\"]\n                    if len(q_subtrees) &gt; 0:\n                        # subtree_identifier is subgraph graph at [-1]\n                        # (and sub-...-subgraph currently in q_subgraphs)\n                        q_subgraphs[-1][\"atoms\"][\n                            next(\n                                filter(\n                                    lambda x: x[1] is None,\n                                    q_subgraphs[-1][\"atoms\"].items(),\n                                )\n                            )[0]\n                        ] = subtree_identifier\n\n                    for atom in filter(lambda x: x[1] is not None, atoms.items()):\n                        if edge_attr:\n                            subgraph[atom[0][0]][atom[0][1]][sym_name] = atom[1]\n                        else:  # node-attr\n                            subgraph.nodes[atom[0]][sym_name] = atom[1]\n\n                    if not edge_attr:  # node-attr\n                        # ensure there is actually one input and output node\n                        first_nodes = {e[0] for e in subgraph.edges}\n                        second_nodes = {e[1] for e in subgraph.edges}\n                        new_src_node = max(subgraph.nodes) + 1\n                        src_nodes = first_nodes - second_nodes\n                        subgraph.add_edges_from(\n                            [\n                                (new_src_node, successor)\n                                for src_node in src_nodes\n                                for successor in subgraph.successors(src_node)\n                            ]\n                        )\n                        subgraph.add_node(new_src_node, **{sym_name: \"input\"})\n                        subgraph.remove_nodes_from(src_nodes)\n                        new_sink_node = max(subgraph.nodes) + 1\n                        sink_nodes = second_nodes - first_nodes\n                        subgraph.add_edges_from(\n                            [\n                                (predecessor, new_sink_node)\n                                for sink_node in sink_nodes\n                                for predecessor in subgraph.predecessors(sink_node)\n                            ]\n                        )\n                        subgraph.add_node(new_sink_node, **{sym_name: \"output\"})\n                        subgraph.remove_nodes_from(sink_nodes)\n                    subgraphs_dict[subtree_identifier] = subgraph\n\n    if len(q_el) != 0:\n        raise Exception(\"Invalid string_tree\")\n\n    if prune:\n        G = self.prune_unconnected_parts(G, src_node, sink_node)\n    self._check_graph(G)\n\n    if return_all_subgraphs or return_graph_per_hierarchy:\n        return_val = [G]\n        subgraphs_dict = collections.OrderedDict(\n            reversed(list(subgraphs_dict.items()))\n        )\n        if prune:\n            for v in subgraphs_dict.values():\n                first_nodes = {e[0] for e in v.edges}\n                second_nodes = {e[1] for e in v.edges}\n                (vG_src_node,) = first_nodes - second_nodes\n                (vG_sink_node,) = second_nodes - first_nodes\n                v = self.prune_unconnected_parts(v, vG_src_node, vG_sink_node)\n                self._check_graph(v)\n        if return_all_subgraphs:\n            return_val.append(subgraphs_dict)\n        if return_graph_per_hierarchy:\n            graph_per_hierarchy = get_graph_per_hierarchy(string_tree, subgraphs_dict)\n            _ = (\n                graph_per_hierarchy.popitem()\n            )  # remove last graph since it is equal to full graph\n            return_val.append(graph_per_hierarchy)\n        return return_val\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.from_stringTree_to_nxTree","title":"from_stringTree_to_nxTree  <code>staticmethod</code>","text":"<pre><code>from_stringTree_to_nxTree(\n    string_tree: str,\n    grammar: Grammar,\n    sym_name: str = \"op_name\",\n) -&gt; DiGraph\n</code></pre> <p>Transforms a parse tree from string representation to NetworkX representation.</p> PARAMETER DESCRIPTION <code>string_tree</code> <p>parse tree.</p> <p> TYPE: <code>str</code> </p> <code>grammar</code> <p>context-free grammar which generated the parse tree in string represenation.</p> <p> TYPE: <code>Grammar</code> </p> <code>sym_name</code> <p>Key to save the terminal symbols. Defaults to \"op_name\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'op_name'</code> </p> RETURNS DESCRIPTION <code>DiGraph</code> <p>nx.DiGraph: parse tree as NetworkX representation.</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>@staticmethod\ndef from_stringTree_to_nxTree(\n    string_tree: str, grammar: Grammar, sym_name: str = \"op_name\"\n) -&gt; nx.DiGraph:\n    \"\"\"Transforms a parse tree from string representation to NetworkX representation.\n\n    Args:\n        string_tree (str): parse tree.\n        grammar (Grammar): context-free grammar which generated the parse tree in string represenation.\n        sym_name (str, optional): Key to save the terminal symbols. Defaults to \"op_name\".\n\n    Returns:\n        nx.DiGraph: parse tree as NetworkX representation.\n    \"\"\"\n\n    def skip_char(char: str) -&gt; bool:\n        if char in [\" \", \"\\t\", \"\\n\"]:\n            return True\n        # special case: \"(\" is (part of) a terminal\n        if (\n            i != 0\n            and char == \"(\"\n            and string_tree[i - 1] == \" \"\n            and string_tree[i + 1] == \" \"\n        ):\n            return False\n        if char == \"(\":\n            return True\n        return False\n\n    def find_longest_match(\n        i: int, string_tree: str, symbols: list[str], max_match: int\n    ) -&gt; int:\n        # search for longest matching symbol and add it\n        # assumes that the longest match is the true match\n        j = min(i + max_match, len(string_tree) - 1)\n        while j &gt; i and j &lt; len(string_tree):\n            if string_tree[i:j] in symbols:\n                break\n            j -= 1\n        if j == i:\n            raise Exception(f\"Terminal or nonterminal at position {i} does not exist\")\n        return j\n\n    if isinstance(grammar, list) and len(grammar) &gt; 1:\n        full_grammar = deepcopy(grammar[0])\n        rules = full_grammar.productions()\n        nonterminals = full_grammar.nonterminals\n        terminals = full_grammar.terminals\n        for g in grammar[1:]:\n            rules.extend(g.productions())\n            nonterminals.extend(g.nonterminals)\n            terminals.extend(g.terminals)\n        grammar = full_grammar\n        raise NotImplementedError(\"TODO check implementation\")\n\n    symbols = grammar.nonterminals + grammar.terminals\n    max_match = max(map(len, symbols))\n    find_longest_match_func = partial(\n        find_longest_match,\n        string_tree=string_tree,\n        symbols=symbols,\n        max_match=max_match,\n    )\n\n    G = nx.DiGraph()\n    q: queue.LifoQueue = queue.LifoQueue()\n    q_children: queue.LifoQueue = queue.LifoQueue()\n    node_number = 0\n    i = 0\n    while i &lt; len(string_tree):\n        char = string_tree[i]\n        if skip_char(char):\n            pass\n        elif char == \")\" and not string_tree[i - 1] == \" \":\n            # closing symbol of production\n            _node_number = q.get(block=False)\n            _node_children = q_children.get(block=False)\n            G.nodes[_node_number][\"children\"] = _node_children\n        else:\n            j = find_longest_match_func(i)\n            sym = string_tree[i:j]\n            i = j - 1\n            node_number += 1\n            G.add_node(\n                node_number,\n                **{\n                    sym_name: sym,\n                    \"terminal\": sym in grammar.terminals,\n                    \"children\": [],\n                },\n            )\n            if not q.empty():\n                G.add_edge(q.queue[-1], node_number)\n                q_children.queue[-1].append(node_number)\n            if sym in grammar.nonterminals:\n                q.put(node_number)\n                q_children.put([])\n        i += 1\n\n    if len(q.queue) != 0:\n        raise Exception(\"Invalid string_tree\")\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.get_all_edge_data","title":"get_all_edge_data","text":"<pre><code>get_all_edge_data(\n    key: str, scope=\"all\", private_edge_data: bool = False\n) -&gt; list\n</code></pre> <p>Get edge attributes of this graph and all child graphs in one go.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key of the attribute</p> <p> TYPE: <code>str</code> </p> <code>scope</code> <p>The scope to be applied</p> <p> TYPE: <code>str</code> DEFAULT: <code>'all'</code> </p> <code>private_edge_data</code> <p>Whether to return data from graph copies as well.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list</code> <p>All data in a list.</p> <p> TYPE: <code>list</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def get_all_edge_data(\n    self, key: str, scope=\"all\", private_edge_data: bool = False\n) -&gt; list:\n    \"\"\"\n    Get edge attributes of this graph and all child graphs in one go.\n\n    Args:\n        key (str): The key of the attribute\n        scope (str): The scope to be applied\n        private_edge_data (bool): Whether to return data from graph copies as well.\n\n    Returns:\n        list: All data in a list.\n    \"\"\"\n    assert scope is not None\n    result = []\n    for graph in self._get_child_graphs(single_instances=not private_edge_data) + [\n        self\n    ]:\n        if (\n            scope == \"all\"\n            or graph.scope == scope\n            or (isinstance(scope, list) and graph.scope in scope)\n        ):\n            for _, _, edge_data in graph.edges.data():\n                if edge_data.has(key):\n                    result.append(edge_data[key])\n    return result\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.get_dense_edges","title":"get_dense_edges","text":"<pre><code>get_dense_edges()\n</code></pre> <p>Returns the edge indices (i, j) that would make a fully connected DAG without circles such that i &lt; j and i != j. Assumes nodes are already created.</p> RETURNS DESCRIPTION <code>list</code> <p>list of edge indices.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def get_dense_edges(self):\n    \"\"\"\n    Returns the edge indices (i, j) that would make a fully connected\n    DAG without circles such that i &lt; j and i != j. Assumes nodes are\n    already created.\n\n    Returns:\n        list: list of edge indices.\n    \"\"\"\n    edges = []\n    nodes = sorted(list(self.nodes()))\n    for i in nodes:\n        for j in nodes:\n            if i != j and j &gt; i:\n                edges.append((i, j))\n    return edges\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.get_graph_representation","title":"get_graph_representation","text":"<pre><code>get_graph_representation(\n    identifier: str, grammar: Grammar, edge_attr: bool\n) -&gt; DiGraph\n</code></pre> <p>This functions takes an identifier and constructs the (multi-variate) composition of the functions it describes. Args:     identifier (str): identifier     grammar (Grammar): grammar     flatten_graph (bool, optional): Whether to flatten the graph. Defaults to True. Returns:     nx.DiGraph: (multi-variate) composition of functions</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def get_graph_representation(\n    self,\n    identifier: str,\n    grammar: Grammar,\n    edge_attr: bool,\n) -&gt; nx.DiGraph:\n    \"\"\"This functions takes an identifier and constructs the\n    (multi-variate) composition of the functions it describes.\n    Args:\n        identifier (str): identifier\n        grammar (Grammar): grammar\n        flatten_graph (bool, optional): Whether to flatten the graph. Defaults to True.\n    Returns:\n        nx.DiGraph: (multi-variate) composition of functions\n    \"\"\"\n\n    def _skip_char(char: str) -&gt; bool:\n        return True if char in [\" \", \"\\t\", \"\\n\", \"[\", \"]\"] else False\n\n    def _get_sym_from_split(split: str) -&gt; str:\n        start_idx, end_idx = 0, len(split)\n        while start_idx &lt; end_idx and split[start_idx] == \"(\":\n            start_idx += 1\n        while start_idx &lt; end_idx and split[end_idx - 1] == \")\":\n            end_idx -= 1\n        return split[start_idx:end_idx]\n\n    def to_node_attributed_edge_list(\n        edge_list: list[tuple],\n    ) -&gt; tuple[list[tuple[int, int]], dict]:\n        first_nodes = {e[0] for e in edge_list}\n        second_nodes = {e[1] for e in edge_list}\n        src = first_nodes - second_nodes\n        tgt = second_nodes - first_nodes\n        node_offset = len(src)\n        edge_to_node_map = {e: i + node_offset for i, e in enumerate(edge_list)}\n        node_list = []\n        for e in edge_list:\n            ni = edge_to_node_map[e]\n            u, v = e\n            if u in src:\n                node_list.append((u, ni))\n            if v in tgt:\n                node_list.append((ni, v))\n\n            for e_ in filter(\n                lambda e: (e[1] == u), edge_list\n            ):\n                node_list.append((edge_to_node_map[e_], ni))\n\n        return node_list, edge_to_node_map\n\n    descriptor = self.id_to_string_tree(identifier)\n\n    if edge_attr:\n        terminal_to_graph = self.terminal_to_graph_edges\n    else:  # node-attr\n        terminal_to_graph_nodes = {\n            k: to_node_attributed_edge_list(edge_list) if edge_list else (None, None)\n            for k, edge_list in self.terminal_to_graph_edges.items()\n        }\n        terminal_to_graph = {k: v[0] for k, v in terminal_to_graph_nodes.items()}\n        # edge_to_node_map = {k: v[1] for k, v in terminal_to_graph_nodes.items()}\n\n    q_nonterminals: queue.LifoQueue = queue.LifoQueue()\n    q_topologies: queue.LifoQueue = queue.LifoQueue()\n    q_primitives: queue.LifoQueue = queue.LifoQueue()\n\n    G = nx.DiGraph()\n    for _, split in enumerate(descriptor.split(\" \")):\n        if _skip_char(split):\n            continue\n        sym = _get_sym_from_split(split)\n\n        if sym in grammar.terminals:\n            is_topology = False\n            if inspect.isclass(self.terminal_to_op_names[sym]) and issubclass(\n                self.terminal_to_op_names[sym], AbstractTopology\n            ):\n                is_topology = True\n            elif isinstance(self.terminal_to_op_names[sym], partial) and issubclass(\n                self.terminal_to_op_names[sym].func, AbstractTopology\n            ):\n                is_topology = True\n\n            if is_topology:\n                q_topologies.put([self.terminal_to_op_names[sym], 0])\n            else:  # is primitive operation\n                q_primitives.put(self.terminal_to_op_names[sym])\n                q_topologies.queue[-1][1] += 1  # count number of primitives\n        elif sym in grammar.nonterminals:\n            q_nonterminals.put(sym)\n        else:\n            raise Exception(f\"Unknown symbol {sym}\")\n\n        if \")\" in split:\n            # closing symbol of production\n            while \")\" in split:\n                if q_nonterminals.qsize() == q_topologies.qsize():\n                    topology, number_of_primitives = q_topologies.get(block=False)\n                    primitives = [\n                        q_primitives.get(block=False)\n                        for _ in range(number_of_primitives)\n                    ][::-1]\n                    if (\n                        topology in terminal_to_graph\n                        and terminal_to_graph[topology] is not None\n                    ):\n                        raise NotImplementedError\n                        # edges = terminal_to_graph[topology]\n                    elif isinstance(topology, partial):\n                        raise NotImplementedError\n                    else:\n                        composed_function = topology(*primitives)\n                        node_attr_dag = composed_function.get_node_list_and_ops()\n                        G = node_attr_dag  # TODO only works for DARTS for now\n\n                    if not q_topologies.empty():\n                        q_primitives.put(composed_function)\n                        q_topologies.queue[-1][1] += 1\n\n                _ = q_nonterminals.get(block=False)\n                split = split[:-1]\n\n    if not q_topologies.empty():\n        raise Exception(\"Invalid descriptor\")\n\n    # G = self.prune_unconnected_parts(G, src_node, sink_node)\n    # self._check_graph(G)\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.graph_to_self","title":"graph_to_self","text":"<pre><code>graph_to_self(\n    graph: DiGraph, clear_self: bool = True\n) -&gt; None\n</code></pre> <p>Copies graph to self</p> PARAMETER DESCRIPTION <code>graph</code> <p>graph</p> <p> TYPE: <code>DiGraph</code> </p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def graph_to_self(self, graph: nx.DiGraph, clear_self: bool = True) -&gt; None:\n    \"\"\"Copies graph to self\n\n    Args:\n        graph (nx.DiGraph): graph\n    \"\"\"\n    if clear_self:\n        self.clear()\n    for u, v, data in graph.edges(data=True):\n        self.add_edge(u, v)  # type: ignore[union-attr]\n        self.edges[u, v].update(data)  # type: ignore[union-attr]\n    for n, data in graph.nodes(data=True):\n        self.nodes[n].update(**data)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.modules_str","title":"modules_str","text":"<pre><code>modules_str()\n</code></pre> <p>Once the graph has been parsed, prints the modules as they appear in pytorch.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def modules_str(self):\n    \"\"\"\n    Once the graph has been parsed, prints the modules as they appear in pytorch.\n    \"\"\"\n    if self.is_parsed:\n        result = \"\"\n        for g in self._get_child_graphs(single_instances=True) + [self]:\n            result += \"Graph {}:\\n {}\\n==========\\n\".format(\n                g.name, torch.nn.Module.__repr__(g)\n            )\n        return result\n    else:\n        return self.__repr__()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.num_input_nodes","title":"num_input_nodes","text":"<pre><code>num_input_nodes() -&gt; int\n</code></pre> <p>The number of input nodes, i.e. the nodes without an incoming edge.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of input nodes.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def num_input_nodes(self) -&gt; int:\n    \"\"\"\n    The number of input nodes, i.e. the nodes without an\n    incoming edge.\n\n    Returns:\n        int: Number of input nodes.\n    \"\"\"\n    return sum(self.in_degree(n) == 0 for n in self.nodes)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.parse","title":"parse","text":"<pre><code>parse()\n</code></pre> <p>Convert the graph into a neural network which can then be optimized by pytorch.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def parse(self):\n    \"\"\"\n    Convert the graph into a neural network which can then\n    be optimized by pytorch.\n    \"\"\"\n    for node_idx in lexicographical_topological_sort(self):\n        if \"subgraph\" in self.nodes[node_idx]:\n            self.nodes[node_idx][\"subgraph\"].parse()\n            self.add_module(\n                f\"{self.name}-subgraph_at({node_idx})\",\n                self.nodes[node_idx][\"subgraph\"],\n            )\n        else:\n            if isinstance(self.nodes[node_idx][\"comb_op\"], torch.nn.Module):\n                self.add_module(\n                    f\"{self.name}-comb_op_at({node_idx})\",\n                    self.nodes[node_idx][\"comb_op\"],\n                )\n        for neigbor_idx in self.neighbors(node_idx):\n            edge_data = self.get_edge_data(node_idx, neigbor_idx)\n            if isinstance(edge_data.op, Graph):\n                edge_data.op.parse()\n            elif edge_data.op.get_embedded_ops():\n                for primitive in edge_data.op.get_embedded_ops():\n                    if isinstance(primitive, Graph):\n                        primitive.parse()\n            self.add_module(\n                f\"{self.name}-edge({node_idx},{neigbor_idx})\",\n                edge_data.op,\n            )\n    self.is_parsed = True\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.prepare_discretization","title":"prepare_discretization","text":"<pre><code>prepare_discretization()\n</code></pre> <p>In some cases the search space is manipulated before the final discretization is happening, e.g. DARTS. In such chases this should be defined in the search space, so all optimizers can call it.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def prepare_discretization(self):\n    \"\"\"\n    In some cases the search space is manipulated before the final\n    discretization is happening, e.g. DARTS. In such chases this should\n    be defined in the search space, so all optimizers can call it.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.prepare_evaluation","title":"prepare_evaluation","text":"<pre><code>prepare_evaluation()\n</code></pre> <p>In some cases the evaluation architecture does not match the searched one. An example is where the makro_model is extended to increase the parameters. This is done here.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def prepare_evaluation(self):\n    \"\"\"\n    In some cases the evaluation architecture does not match the searched\n    one. An example is where the makro_model is extended to increase the\n    parameters. This is done here.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.prune_tree","title":"prune_tree","text":"<pre><code>prune_tree(\n    tree: DiGraph,\n    terminal_to_torch_map_keys: KeysView,\n    node_label: str = \"op_name\",\n) -&gt; DiGraph\n</code></pre> <p>Prunes unnecessary parts of parse tree, i.e., only one child</p> PARAMETER DESCRIPTION <code>tree</code> <p>Parse tree</p> <p> TYPE: <code>DiGraph</code> </p> RETURNS DESCRIPTION <code>DiGraph</code> <p>nx.DiGraph: Pruned parse tree</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def prune_tree(\n    self,\n    tree: nx.DiGraph,\n    terminal_to_torch_map_keys: collections.abc.KeysView,\n    node_label: str = \"op_name\",\n) -&gt; nx.DiGraph:\n    \"\"\"Prunes unnecessary parts of parse tree, i.e., only one child\n\n    Args:\n        tree (nx.DiGraph): Parse tree\n\n    Returns:\n        nx.DiGraph: Pruned parse tree\n    \"\"\"\n\n    def dfs(visited: set, tree: nx.DiGraph, node: int) -&gt; nx.DiGraph:\n        if node not in visited:\n            visited.add(node)\n\n            i = 0\n            while i &lt; len(tree.nodes[node][\"children\"]):\n                former_len = len(tree.nodes[node][\"children\"])\n                child = tree.nodes[node][\"children\"][i]\n                tree = dfs(\n                    visited,\n                    tree,\n                    child,\n                )\n                if former_len == len(tree.nodes[node][\"children\"]):\n                    i += 1\n\n            if len(tree.nodes[node][\"children\"]) == 1:\n                predecessor = list(tree.pred[node])\n                if len(predecessor) &gt; 0:\n                    tree.add_edge(predecessor[0], tree.nodes[node][\"children\"][0])\n                    old_children = tree.nodes[predecessor[0]][\"children\"]\n                    idx = [i for i, c in enumerate(old_children) if c == node][0]\n                    tree.nodes[predecessor[0]][\"children\"] = (\n                        old_children[: idx + 1]\n                        + [tree.nodes[node][\"children\"][0]]\n                        + old_children[idx + 1 :]\n                    )\n                    tree.nodes[predecessor[0]][\"children\"].remove(node)\n\n                tree.remove_node(node)\n            elif (\n                tree.nodes[node][\"terminal\"]\n                and tree.nodes[node][node_label] not in terminal_to_torch_map_keys\n            ):\n                predecessor = list(tree.pred[node])[0]\n                tree.nodes[predecessor][\"children\"].remove(node)\n                tree.remove_node(node)\n        return tree\n\n    return dfs(set(), tree, self._find_root(tree))\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.reset_weights","title":"reset_weights","text":"<pre><code>reset_weights(inplace: bool = False)\n</code></pre> <p>Resets the weights for the 'op' at all edges.</p> PARAMETER DESCRIPTION <code>inplace</code> <p>Do the operation in place or return a modified copy.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Graph: Returns the modified version of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def reset_weights(self, inplace: bool = False):\n    \"\"\"\n    Resets the weights for the 'op' at all edges.\n\n    Args:\n        inplace (bool): Do the operation in place or\n            return a modified copy.\n    Returns:\n        Graph: Returns the modified version of the graph.\n    \"\"\"\n\n    def weight_reset(m):\n        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n            m.reset_parameters()\n\n    if inplace:\n        graph = self\n    else:\n        graph = self.clone()\n\n    graph.apply(weight_reset)\n\n    return graph\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.set_at_edges","title":"set_at_edges","text":"<pre><code>set_at_edges(key, value, shared=False)\n</code></pre> <p>Sets the attribute for all edges in this and any child graph</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_at_edges(self, key, value, shared=False):\n    \"\"\"\n    Sets the attribute for all edges in this and any child graph\n    \"\"\"\n    for graph in self._get_child_graphs(single_instances=shared) + [self]:\n        logger.debug(f\"Updating edges of graph {graph.name}\")\n        for _, _, edge_data in graph.edges.data():\n            if not edge_data.is_final():\n                edge_data.set(key, value, shared)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.set_input","title":"set_input","text":"<pre><code>set_input(node_idxs: list)\n</code></pre> <p>Route the input from specific parent edges to the input nodes of this subgraph. Inputs are assigned in lexicographical order.</p> <p>Example: - Parent node (i.e. node where <code>self</code> is located on) has two   incoming edges from nodes 3 and 5. - <code>self</code> has two input nodes 1 and 2 (i.e. nodes without   an incoming edge) - <code>node_idxs = [5, 3]</code> Then input of node 5 is routed to node 1 and input of node 3 is routed to node 2.</p> <p>Similarly, if <code>node_idxs = [5, 5]</code> then input of node 5 is routed to both node 1 and 2. Warning: In this case the output of another incoming edge is ignored!</p> <p>Should be used in a builder-like pattern: <code>'subgraph'=Graph().set_input([5, 3])</code></p> PARAMETER DESCRIPTION <code>node_idx</code> <p>The index of the nodes where the data is coming from.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Graph</code> <p>self with input node indices set.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_input(self, node_idxs: list):\n    \"\"\"\n    Route the input from specific parent edges to the input nodes of\n    this subgraph. Inputs are assigned in lexicographical order.\n\n    Example:\n    - Parent node (i.e. node where `self` is located on) has two\n      incoming edges from nodes 3 and 5.\n    - `self` has two input nodes 1 and 2 (i.e. nodes without\n      an incoming edge)\n    - `node_idxs = [5, 3]`\n    Then input of node 5 is routed to node 1 and input of node 3\n    is routed to node 2.\n\n    Similarly, if `node_idxs = [5, 5]` then input of node 5 is routed\n    to both node 1 and 2. Warning: In this case the output of another\n    incoming edge is ignored!\n\n    Should be used in a builder-like pattern: `'subgraph'=Graph().set_input([5, 3])`\n\n    Args:\n        node_idx (list): The index of the nodes where the data is coming from.\n\n    Returns:\n        Graph: self with input node indices set.\n\n    \"\"\"\n    num_innodes = sum(self.in_degree(n) == 0 for n in self.nodes)\n    assert num_innodes == len(\n        node_idxs\n    ), \"Expecting node index for every input node. Excpected {}, got {}\".format(\n        num_innodes, len(node_idxs)\n    )\n    self.input_node_idxs = node_idxs  # type: ignore[assignment]\n    return self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.set_scope","title":"set_scope","text":"<pre><code>set_scope(scope: str, recursively=True)\n</code></pre> <p>Sets the scope of this instance of the graph.</p> <p>The function should be used in a builder-like pattern <code>'subgraph'=Graph().set_scope(\"scope\")</code>.</p> PARAMETER DESCRIPTION <code>scope</code> <p>the scope</p> <p> TYPE: <code>str</code> </p> <code>recursively</code> <p>Also set the scope for all child graphs. default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Graph</code> <p>self with the setted scope.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_scope(self, scope: str, recursively=True):\n    \"\"\"\n    Sets the scope of this instance of the graph.\n\n    The function should be used in a builder-like pattern\n    `'subgraph'=Graph().set_scope(\"scope\")`.\n\n    Args:\n        scope (str): the scope\n        recursively (bool): Also set the scope for all child graphs.\n            default True\n\n    Returns:\n        Graph: self with the setted scope.\n    \"\"\"\n    self.scope = scope\n    if recursively:\n        for g in self._get_child_graphs(single_instances=False):\n            g.scope = scope\n    return self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.to_graph_repr","title":"to_graph_repr","text":"<pre><code>to_graph_repr(graph: Graph, edge_attr: bool) -&gt; DiGraph\n</code></pre> <p>Transforms NASLib-esque graph to NetworkX graph.</p> PARAMETER DESCRIPTION <code>graph</code> <p>NASLib-esque graph.</p> <p> TYPE: <code>Graph</code> </p> <code>edge_attr</code> <p>Transform to edge attribution or node attribution.</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>DiGraph</code> <p>nx.DiGraph: edge- or node-attributed representation of computational graph.</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def to_graph_repr(self, graph: Graph, edge_attr: bool) -&gt; nx.DiGraph:\n    \"\"\"Transforms NASLib-esque graph to NetworkX graph.\n\n    Args:\n        graph (Graph): NASLib-esque graph.\n        edge_attr (bool): Transform to edge attribution or node attribution.\n\n    Returns:\n        nx.DiGraph: edge- or node-attributed representation of computational graph.\n    \"\"\"\n    if edge_attr:\n        g = nx.DiGraph()\n        g.add_nodes_from(graph.nodes())\n        for u, v in graph.edges():\n            if isinstance(graph.edges[u, v][\"op\"], Graph):\n                g.add_edge(u, v, op_name=graph.edges[u, v][\"op\"].name)\n            else:\n                g.add_edge(\n                    u, v, **{self.edge_label: graph.edges[u, v][self.edge_label]}\n                )\n        g.graph_type = \"edge_attr\"\n    else:\n        g = nx.DiGraph()\n        src = [n for n in graph.nodes() if graph.in_degree(n) == 0][0]\n        tgt = [n for n in graph.nodes() if graph.out_degree(n) == 0][0]\n        nof_edges = graph.size()\n        g.add_nodes_from(\n            [\n                (0, {self.edge_label: \"input\"}),\n                (nof_edges + 1, {self.edge_label: \"output\"}),\n            ]\n        )\n        node_counter = 1\n        open_edge: dict = {}\n        for node in nx.topological_sort(graph):\n            for edge in graph.out_edges(node):\n                g.add_node(\n                    node_counter,\n                    **{self.edge_label: graph.edges[edge][self.edge_label]},\n                )\n\n                u, v = edge\n                if u == src:  # special case for input node\n                    g.add_edge(0, node_counter)\n                if v == tgt:  # special case of output node\n                    g.add_edge(node_counter, nof_edges + 1)\n                if (\n                    u in open_edge.keys()\n                ):  # add edge between already seen nodes and new node\n                    for node_count in open_edge[u]:\n                        g.add_edge(node_count, node_counter)\n\n                if v in open_edge.keys():\n                    open_edge[v].append(node_counter)\n                else:\n                    open_edge[v] = [node_counter]\n                node_counter += 1\n        g.graph_type = \"node_attr\"\n\n    self._check_graph(g)\n\n    return g\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.unparse","title":"unparse","text":"<pre><code>unparse()\n</code></pre> <p>Undo the pytorch parsing by reconstructing the graph uusing the networkx data structures.</p> <p>This is done recursively also for child graphs.</p> RETURNS DESCRIPTION <code>Graph</code> <p>An unparsed shallow copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def unparse(self):\n    \"\"\"\n    Undo the pytorch parsing by reconstructing the graph uusing the\n    networkx data structures.\n\n    This is done recursively also for child graphs.\n\n    Returns:\n        Graph: An unparsed shallow copy of the graph.\n    \"\"\"\n    g = self.__class__()\n    g.clear()\n\n    graph_nodes = self.nodes\n    graph_edges = self.edges\n\n    # unparse possible child graphs\n    # be careful with copying/deepcopying here cause of shared edge data\n    for _, data in graph_nodes.data():\n        if \"subgraph\" in data:\n            data[\"subgraph\"] = data[\"subgraph\"].unparse()\n    for _, _, data in graph_edges.data():\n        if isinstance(data.op, Graph):\n            data.set(\"op\", data.op.unparse())\n\n    # create the new graph\n    # Remember to add all members here to update. I know it is ugly but don't know better\n    g.add_nodes_from(graph_nodes.data())\n    g.add_edges_from(graph_edges.data())\n    g.graph.update(self.graph)\n    g.name = self.name\n    g.input_node_idxs = self.input_node_idxs\n    g.scope = self.scope\n    g.is_parsed = False\n    g._id = self._id\n    g.OPTIMIZER_SCOPE = self.OPTIMIZER_SCOPE\n    g.QUERYABLE = self.QUERYABLE\n\n    return g\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.update_edges","title":"update_edges","text":"<pre><code>update_edges(\n    update_func: Callable,\n    scope=\"all\",\n    private_edge_data: bool = False,\n)\n</code></pre> <p>This updates the edge data of this graph and all child graphs. This is the preferred way to manipulate the edges after the definition of the graph, e.g. by optimizers who want to insert their own op. <code>update_func(current_edge_data)</code>. This way optimizers can initialize and store necessary information at edges.</p> <p>Note that edges marked as 'final' will not be updated here.</p> PARAMETER DESCRIPTION <code>update_func</code> <p>Function which accepts one argument called <code>current_edge_data</code>. and returns the modified EdgeData object.</p> <p> TYPE: <code>callable</code> </p> <code>scope</code> <p>Can be \"all\" or list of scopes to be updated.</p> <p> TYPE: <code>str or list(str</code> DEFAULT: <code>'all'</code> </p> <code>private_edge_data</code> <p>If set to true, this means update_func will be applied to all edges. THIS IS NOT RECOMMENDED FOR SHARED ATTRIBUTES. Shared attributes should be set only once, we take care it is syncronized across all copies of this graph.</p> <p>The only usecase for setting it to true is when actually changing <code>op</code> during the initialization of the optimizer (e.g. replacing it with MixedOp or SampleOp)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def update_edges(\n    self, update_func: Callable, scope=\"all\", private_edge_data: bool = False\n):\n    \"\"\"\n    This updates the edge data of this graph and all child graphs.\n    This is the preferred way to manipulate the edges after the definition\n    of the graph, e.g. by optimizers who want to insert their own op.\n    `update_func(current_edge_data)`. This way optimizers\n    can initialize and store necessary information at edges.\n\n    Note that edges marked as 'final' will not be updated here.\n\n    Args:\n        update_func (callable): Function which accepts one argument called `current_edge_data`.\n            and returns the modified EdgeData object.\n        scope (str or list(str)): Can be \"all\" or list of scopes to be updated.\n        private_edge_data (bool): If set to true, this means update_func will be\n            applied to all edges. THIS IS NOT RECOMMENDED FOR SHARED\n            ATTRIBUTES. Shared attributes should be set only once, we\n            take care it is syncronized across all copies of this graph.\n\n            The only usecase for setting it to true is when actually changing\n            `op` during the initialization of the optimizer (e.g. replacing it\n            with MixedOp or SampleOp)\n    \"\"\"\n    Graph._verify_update_function(update_func, private_edge_data)\n    assert scope is not None\n    for graph in self._get_child_graphs(single_instances=not private_edge_data) + [\n        self\n    ]:\n        if (\n            scope == \"all\"\n            or scope == graph.scope\n            or (isinstance(scope, list) and graph.scope in scope)\n        ):\n            logger.debug(f\"Updating edges of graph {graph.name}\")\n            for u, v, edge_data in graph.edges.data():\n                if not edge_data.is_final():\n                    edge = AttrDict(head=u, tail=v, data=edge_data)\n                    update_func(edge=edge)\n    self._delete_flagged_edges()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.update_nodes","title":"update_nodes","text":"<pre><code>update_nodes(\n    update_func: Callable,\n    scope=\"all\",\n    single_instances: bool = True,\n)\n</code></pre> <p>Update the nodes of the graph and its incoming and outgoing edges by iterating over the graph and applying <code>update_func</code> to each of it. This is the preferred way to change the search space once it has been defined.</p> <p>Note that edges marked as 'final' will not be updated here.</p> PARAMETER DESCRIPTION <code>update_func</code> <p>Function that accepts three incoming parameters named <code>node, in_edges, out_edges</code>.     - <code>node</code> is a tuple (int, dict) containing the       index and the attributes of the current node.     - <code>in_edges</code> is a list of tuples with the index of       the tail of the edge and its EdgeData.     - `out_edges is a list of tuples with the index of       the head of the edge and its EdgeData.</p> <p> TYPE: <code>callable</code> </p> <code>scope</code> <p>Can be \"all\" or list of scopes to be updated. Only graphs and child graphs with the specified scope are considered</p> <p> TYPE: <code>str or list(str</code> DEFAULT: <code>'all'</code> </p> <code>single_instance</code> <p>If set to false, this means update_func will be applied to nodes of all copies of a graphs. THIS IS NOT RECOMMENDED FOR SHARED ATTRIBUTES, i.e. when manipulating the shared data of incoming or outgoing edges. Shared attributes should be set only once, we take care it is syncronized across all copies of this graph.</p> <p>The only usecase for setting it to true is when actually changing <code>op</code> during the initialization of the optimizer (e.g. replacing it with MixedOp or SampleOp)</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def update_nodes(\n    self, update_func: Callable, scope=\"all\", single_instances: bool = True\n):\n    \"\"\"\n    Update the nodes of the graph and its incoming and outgoing edges by iterating over the\n    graph and applying `update_func` to each of it. This is the\n    preferred way to change the search space once it has been defined.\n\n    Note that edges marked as 'final' will not be updated here.\n\n    Args:\n        update_func (callable): Function that accepts three incoming parameters named\n            `node, in_edges, out_edges`.\n                - `node` is a tuple (int, dict) containing the\n                  index and the attributes of the current node.\n                - `in_edges` is a list of tuples with the index of\n                  the tail of the edge and its EdgeData.\n                - `out_edges is a list of tuples with the index of\n                  the head of the edge and its EdgeData.\n        scope (str or list(str)): Can be \"all\" or list of scopes to be updated. Only graphs\n            and child graphs with the specified scope are considered\n        single_instance (bool): If set to false, this means update_func will be\n            applied to nodes of all copies of a graphs. THIS IS NOT RECOMMENDED FOR SHARED\n            ATTRIBUTES, i.e. when manipulating the shared data of incoming or outgoing edges.\n            Shared attributes should be set only once, we take care it is syncronized across\n            all copies of this graph.\n\n            The only usecase for setting it to true is when actually changing\n            `op` during the initialization of the optimizer (e.g. replacing it\n            with MixedOp or SampleOp)\n    \"\"\"\n    assert scope is not None\n    for graph in self._get_child_graphs(single_instances) + [self]:\n        if (\n            scope == \"all\"\n            or graph.scope == scope\n            or (isinstance(scope, list) and graph.scope in scope)\n        ):\n            logger.debug(f\"Updating nodes of graph {graph.name}\")\n            for node_idx in lexicographical_topological_sort(graph):\n                node = (node_idx, graph.nodes[node_idx])\n                in_edges = list(graph.in_edges(node_idx, data=True))  # (v, u, data)\n                in_edges = [\n                    (v, data) for v, u, data in in_edges if not data.is_final()\n                ]  # u is same for all\n                out_edges = list(\n                    graph.out_edges(node_idx, data=True)\n                )  # (v, u, data)\n                out_edges = [\n                    (u, data) for v, u, data in out_edges if not data.is_final()\n                ]  # v is same for all\n                update_func(node=node, in_edges=in_edges, out_edges=out_edges)\n    self._delete_flagged_edges()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/crossover/","title":"Crossover","text":""},{"location":"api/neps/search_spaces/architecture/crossover/#neps.search_spaces.architecture.crossover","title":"neps.search_spaces.architecture.crossover","text":""},{"location":"api/neps/search_spaces/architecture/graph/","title":"Graph","text":""},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph","title":"neps.search_spaces.architecture.graph","text":""},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData","title":"EdgeData","text":"<pre><code>EdgeData(data: dict = None)\n</code></pre> <p>Class that holds data for each edge. Data can be shared between instances of the graph where the edges lives in.</p> <p>Also defines the default key 'op', which is <code>Identity()</code>. It must be private always.</p> <p>Items can be accessed directly as attributes with <code>.key</code> or in a dict-like fashion with <code>[key]</code>. To set a new item use <code>.set()</code>.</p> PARAMETER DESCRIPTION <code>data</code> <p>Inject some initial data. Will be always private.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __init__(self, data: dict = None):\n    \"\"\"\n    Initializes a new EdgeData object.\n    'op' is set as Identity() and private by default\n\n    Args:\n        data (dict): Inject some initial data. Will be always private.\n    \"\"\"\n    if data is None:\n        data = {}\n    self._private = {}\n    self._shared = {}\n\n    # set internal attributes\n    self._shared[\"_deleted\"] = False\n    self._private[\"_final\"] = False\n\n    # set defaults and potential input\n    self.set(\"op\", Identity(), shared=False)\n    for k, v in data.items():\n        self.set(k, v, shared=False)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Return a true deep copy of EdgeData. Even shared items are not shared anymore.</p> RETURNS DESCRIPTION <code>EdgeData</code> <p>New independent instance.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def clone(self):\n    \"\"\"\n    Return a true deep copy of EdgeData. Even shared\n    items are not shared anymore.\n\n    Returns:\n        EdgeData: New independent instance.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>When a graph is copied to get multiple instances (e.g. when reusing subgraphs at more than one location) then this function will be called for all edges.</p> <p>It will create a deep copy for the private entries but only a shallow copy for the shared entries. E.g. architectural weights should be shared, but parameters of a 3x3 convolution not.</p> <p>Therefore 'op' must be always private.</p> RETURNS DESCRIPTION <code>EdgeData</code> <p>A new EdgeData object with independent private     items, but shallow shared items.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def copy(self):\n    \"\"\"\n    When a graph is copied to get multiple instances (e.g. when\n    reusing subgraphs at more than one location) then\n    this function will be called for all edges.\n\n    It will create a deep copy for the private entries but\n    only a shallow copy for the shared entries. E.g. architectural\n    weights should be shared, but parameters of a 3x3 convolution not.\n\n    Therefore 'op' must be always private.\n\n    Returns:\n        EdgeData: A new EdgeData object with independent private\n            items, but shallow shared items.\n    \"\"\"\n    new_self = EdgeData()\n    new_self._private = copy.deepcopy(self._private)\n    new_self._shared = self._shared\n\n    # we need to handle copy of graphs seperately\n    for k, v in self._private.items():\n        if isinstance(v, Graph):\n            new_self._private[k] = v.copy()\n        elif isinstance(v, list):\n            new_self._private[k] = [\n                i.copy() if isinstance(i, Graph) else i for i in v\n            ]\n\n    return new_self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.delete","title":"delete","text":"<pre><code>delete()\n</code></pre> <p>Flag to delete the edge where this instance is attached to.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Flag to delete the edge where this instance is attached to.\n    \"\"\"\n    self._shared[\"_deleted\"] = True\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Sets this edge as final. This means it cannot be changed anymore and will also not appear in the update functions of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Sets this edge as final. This means it cannot be changed\n    anymore and will also not appear in the update functions\n    of the graph.\n    \"\"\"\n    self._private[\"_final\"] = True\n    return self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.has","title":"has","text":"<pre><code>has(key: str)\n</code></pre> <p>Checks whether <code>key</code> exists.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if key exists, False otherwise.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def has(self, key: str):\n    \"\"\"\n    Checks whether `key` exists.\n\n    Args:\n        key (str): The key to check.\n\n    Returns:\n        bool: True if key exists, False otherwise.\n\n    \"\"\"\n    assert not key.startswith(\"_\"), \"Access to private keys not allowed!\"\n    return key in self._private.keys() or key in self._shared.keys()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.is_deleted","title":"is_deleted","text":"<pre><code>is_deleted()\n</code></pre> <p>Returns true if the edge is flagged to be deleted</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def is_deleted(self):\n    \"\"\"\n    Returns true if the edge is flagged to be deleted\n    \"\"\"\n    return self._shared[\"_deleted\"]\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.is_final","title":"is_final","text":"<pre><code>is_final()\n</code></pre> RETURNS DESCRIPTION <code>bool</code> <p>True if the edge was finalized, False else</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def is_final(self):\n    \"\"\"\n    Returns:\n        bool: True if the edge was finalized, False else\n    \"\"\"\n    return self._private[\"_final\"]\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.remove","title":"remove","text":"<pre><code>remove(key: str)\n</code></pre> <p>Removes an item from the EdgeData</p> PARAMETER DESCRIPTION <code>key</code> <p>The key for the item to be removed.</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def remove(self, key: str):\n    \"\"\"\n    Removes an item from the EdgeData\n\n    Args:\n        key (str): The key for the item to be removed.\n    \"\"\"\n    if key in self._private:\n        del self._private[key]\n    elif key in self._shared:\n        del self._shared[key]\n    else:\n        raise KeyError(f\"Tried to delete unkown key {key}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.set","title":"set","text":"<pre><code>set(key: str, value, shared=False)\n</code></pre> <p>Used to assign a new item to the EdgeData object.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The value to store</p> <p> TYPE: <code>object</code> </p> <code>shared</code> <p>Default: False. Whether the item should be a shallow copy between different instances of EdgeData (and consequently between different instances of Graph).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set(self, key: str, value, shared=False):\n    \"\"\"\n    Used to assign a new item to the EdgeData object.\n\n    Args:\n        key (str): The key.\n        value (object): The value to store\n        shared (bool): Default: False. Whether the item should\n            be a shallow copy between different instances of EdgeData\n            (and consequently between different instances of Graph).\n    \"\"\"\n    assert isinstance(key, str), \"Accepting only string keys, got {}\".format(\n        type(key)\n    )\n    assert not key.startswith(\"_\"), \"Access to private keys not allowed!\"\n    assert not self.is_final(), \"Trying to change finalized edge!\"\n    if shared:\n        if key in self._private:\n            raise ValueError(\"Key {} alredy defined as non-shared\")\n        else:\n            self._shared[key] = value\n    else:\n        if key in self._shared:\n            raise ValueError(f\"Key {key} alredy defined as shared\")\n        else:\n            self._private[key] = value\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.update","title":"update","text":"<pre><code>update(data)\n</code></pre> <p>Update the data in here. If the data is added as dict, then all variables will be handled as private.</p> PARAMETER DESCRIPTION <code>data</code> <p>If dict, then values will be set as private. If EdgeData then all entries will be replaced.</p> <p> TYPE: <code>EdgeData or dict</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def update(self, data):\n    \"\"\"\n    Update the data in here. If the data is added as dict,\n    then all variables will be handled as private.\n\n    Args:\n        data (EdgeData or dict): If dict, then values will be set as\n            private. If EdgeData then all entries will be replaced.\n    \"\"\"\n    if isinstance(data, dict):\n        for k, v in data.items():\n            self.set(k, v)\n    elif isinstance(data, EdgeData):\n        # TODO: do update and not replace!\n        self.__dict__.update(data.__dict__)\n    else:\n        raise ValueError(f\"Unsupported type {data}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph","title":"Graph","text":"<pre><code>Graph(name: str = None, scope: str = None)\n</code></pre> <p>               Bases: <code>Module</code>, <code>DiGraph</code></p> <p>Base class for defining a search space. Add nodes and edges as for a directed acyclic graph in <code>networkx</code>. Nodes can contain graphs as children, also edges can contain graphs as operations.</p> <p>Note, if a graph is copied, the shared attributes of its edges are shallow copies whereas the private attributes are deep copies.</p> <p>To differentiate copies of the same graph you can define a <code>scope</code> with <code>set_scope()</code>.</p> <p>Graph at nodes:</p> <p>graph = Graph() graph.add_node(1, subgraph=Graph())</p> <p>If the node has more than one input use <code>set_input()</code> to define the routing to the input nodes of the subgraph.</p> <p>Graph at edges:</p> <p>graph = Graph() graph.add_nodes_from([1, 2]) graph.add_edge(1, 2, EdgeData({'op': Graph()}))</p> <p>Modify the graph after definition</p> <p>If you want to modify the graph e.g. in an optimizer once it has been defined already use the function <code>update_edges()</code> or <code>update_nodes()</code>.</p> <p>Use as pytorch module If you want to learn the weights of the operations or any other parameters of the graph you have to parse it first.</p> <p>graph = getFancySearchSpace() graph.parse() logits = graph(data) optimizer.min(loss(logits, target))</p> <p>To update the pytorch module representation (e.g. after removing or adding some new edges), you have to unparse. Beware that this is not fast, so it should not be done on each batch or epoch, rather once after discretizising. If you want to change the representation of the graph use rather some shared operation indexing at the edges.</p> <p>graph.update(remove_random_edges) graph.unparse() graph.parse() logits = graph(data)</p> <p>is set as sum.</p> Note <p>When inheriting form <code>Graph</code> note that <code>__init__()</code> cannot take any parameters. This is due to the way how networkx is implemented, i.e. graphs are reconstructed internally and no parameters for init are considered.</p> <p>Our recommended solution is to create static attributes before initialization and then load them dynamically in <code>__init__()</code>.</p> <p>def init(self):     num_classes = self.NUM_CLASSES MyGraph.NUM_CLASSES = 42 my_graph_42_classes = MyGraph()</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __init__(self, name: str = None, scope: str = None):\n    \"\"\"\n    Initialise a graph. The edges are automatically filled with an EdgeData object\n    which defines the default operation as Identity. The default combination operation\n    is set as sum.\n\n    Note:\n        When inheriting form `Graph` note that `__init__()` cannot take any parameters.\n        This is due to the way how networkx is implemented, i.e. graphs are reconstructed\n        internally and no parameters for init are considered.\n\n        Our recommended solution is to create static attributes before initialization and\n        then load them dynamically in `__init__()`.\n\n        &gt;&gt;&gt; def __init__(self):\n        &gt;&gt;&gt;     num_classes = self.NUM_CLASSES\n        &gt;&gt;&gt; MyGraph.NUM_CLASSES = 42\n        &gt;&gt;&gt; my_graph_42_classes = MyGraph()\n\n    \"\"\"\n    # super().__init__()\n    nx.DiGraph.__init__(self)\n    torch.nn.Module.__init__(self)\n\n    # Make DiGraph a member and not inherit. This is because when inheriting from\n    # `Graph` note that `__init__()` cannot take any parameters. This is due to\n    # the way how networkx is implemented, i.e. graphs are reconstructed internally\n    # and no parameters for init are considered.\n    # Therefore __getattr__ and __iter__ forward the DiGraph methods for straight-forward\n    # usage as if we would inherit.\n\n    # self._nxgraph = nx.DiGraph()\n\n    # Replace the default dicts at the edges with `EdgeData` objects\n    # `EdgeData` can be easily customized and allow shared parameters\n    # across different Graph instances.\n\n    # self._nxgraph.edge_attr_dict_factory = lambda: EdgeData()\n    self.edge_attr_dict_factory = lambda: EdgeData()\n\n    # Replace the default dicts at the nodes to include `input` from the beginning.\n    # `input` is required for storing the results of incoming edges.\n\n    # self._nxgraph.node_attr_dict_factory = lambda: dict({'input': {}, 'comb_op': sum})\n    self.node_attr_dict_factory = lambda: dict({\"input\": {}, \"comb_op\": sum})\n\n    # remember to add all members also in `unparse()`\n    self.name = name\n    self.scope = scope\n    self.input_node_idxs = None\n    self.is_parsed = False\n    self._id = random.random()  # pytorch expects unique modules in `add_module()`\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.OPTIMIZER_SCOPE","title":"OPTIMIZER_SCOPE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPTIMIZER_SCOPE = 'all'\n</code></pre> <p>Whether the search space has an interface to one of the tabular benchmarks which can then be used to query architecture performances.</p> <p>If this is set to true then <code>query()</code> should be implemented.</p>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>As it is very complicated to compare graphs (i.e. check all edge attributes, do the have shared attributes, ...) use just the name for comparison.</p> <p>This is used when determining whether two instances are copies.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __hash__(self):\n    \"\"\"\n    As it is very complicated to compare graphs (i.e. check all edge\n    attributes, do the have shared attributes, ...) use just the name\n    for comparison.\n\n    This is used when determining whether two instances are copies.\n    \"\"\"\n    h = 0\n    h += hash(self.name)\n    h += hash(self.scope) if self.scope else 0\n    h += hash(self._id)\n    return h\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.add_edges_densly","title":"add_edges_densly","text":"<pre><code>add_edges_densly()\n</code></pre> <p>Adds edges to get a fully connected DAG without cycles</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def add_edges_densly(self):\n    \"\"\"\n    Adds edges to get a fully connected DAG without cycles\n    \"\"\"\n    self.add_edges_from(self.get_dense_edges())\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.add_node","title":"add_node","text":"<pre><code>add_node(node_index, **attr)\n</code></pre> <p>Adds a node to the graph.</p> <p>Note that adding a node using an index that has been used already will override its attributes.</p> PARAMETER DESCRIPTION <code>node_index</code> <p>The index for the node. Expect to be &gt;= 1.</p> <p> TYPE: <code>int</code> </p> <code>**attr</code> <p>The attributes which can be added in a dict like form.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def add_node(self, node_index, **attr):\n    \"\"\"\n    Adds a node to the graph.\n\n    Note that adding a node using an index that has been used already\n    will override its attributes.\n\n    Args:\n        node_index (int): The index for the node. Expect to be &gt;= 1.\n        **attr: The attributes which can be added in a dict like form.\n    \"\"\"\n    assert node_index &gt;= 1, \"Expecting the node index to be greater or equal 1\"\n    nx.DiGraph.add_node(self, node_index, **attr)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Deep copy of the current graph.</p> RETURNS DESCRIPTION <code>Graph</code> <p>Deep copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def clone(self):\n    \"\"\"\n    Deep copy of the current graph.\n\n    Returns:\n        Graph: Deep copy of the graph.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.compile","title":"compile","text":"<pre><code>compile()\n</code></pre> <p>Instanciates the ops at the edges using the arguments specified at the edges</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def compile(self):\n    \"\"\"\n    Instanciates the ops at the edges using the arguments specified at the edges\n    \"\"\"\n    for graph in self._get_child_graphs(single_instances=False) + [self]:\n        logger.debug(f\"Compiling graph {graph.name}\")\n        for _, v, edge_data in graph.edges.data():\n            if not edge_data.is_final():\n                attr = edge_data.to_dict()\n                op = attr.pop(\"op\")\n\n                if isinstance(op, list):\n                    compiled_ops = []\n                    for i, o in enumerate(op):\n                        if inspect.isclass(o):\n                            # get the relevant parameter if there are more.\n                            a = {\n                                k: v[i] if isinstance(v, list) else v\n                                for k, v in attr.items()\n                            }\n                            compiled_ops.append(o(**a))\n                        else:\n                            logger.debug(f\"op {o} already compiled. Skipping\")\n                    edge_data.set(\"op\", compiled_ops)\n                elif isinstance(op, AbstractPrimitive):\n                    logger.debug(f\"op {op} already compiled. Skipping\")\n                elif inspect.isclass(op) and issubclass(op, AbstractPrimitive):\n                    # Init the class\n                    if \"op_name\" in attr:\n                        del attr[\"op_name\"]\n                    edge_data.set(\"op\", op(**attr))\n                elif isinstance(op, Graph):\n                    pass  # This is already covered by _get_child_graphs\n                else:\n                    raise ValueError(f\"Unkown format of op: {op}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy as defined in networkx, i.e. a shallow copy.</p> <p>Just handling recursively nested graphs seperately.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Copy as defined in networkx, i.e. a shallow copy.\n\n    Just handling recursively nested graphs seperately.\n    \"\"\"\n\n    def copy_dict(d):\n        copied_dict = d.copy()\n        for k, v in d.items():\n            if isinstance(v, Graph):\n                copied_dict[k] = v.copy()\n            elif isinstance(v, list):\n                copied_dict[k] = [\n                    i.copy() if isinstance(i, Graph) else i for i in v\n                ]\n            elif isinstance(v, torch.nn.Module) or isinstance(v, AbstractPrimitive):\n                copied_dict[k] = copy.deepcopy(v)\n        return copied_dict\n\n    G = self.__class__()\n    G.graph.update(self.graph)\n    G.add_nodes_from((n, copy_dict(d)) for n, d in self._node.items())\n    G.add_edges_from(\n        (u, v, datadict.copy())\n        for u, nbrs in self._adj.items()\n        for v, datadict in nbrs.items()\n    )\n    G.scope = self.scope\n    G.name = self.name\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.forward","title":"forward","text":"<pre><code>forward(x, *args)\n</code></pre> <p>Forward some data through the graph. This is done recursively in case there are graphs defined on nodes or as 'op' on edges.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input. If the graph sits on a node the input can be a dict with {source_idx: Tensor} to be routed to the defined input nodes. If the graph sits on an edge, x is the feature tensor.</p> <p> TYPE: <code>Tensor or dict</code> </p> <code>args</code> <p>This is only required to handle cases where the graph sits on an edge and receives an EdgeData object which will be ignored</p> <p> DEFAULT: <code>()</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def forward(self, x, *args):\n    \"\"\"\n    Forward some data through the graph. This is done recursively\n    in case there are graphs defined on nodes or as 'op' on edges.\n\n    Args:\n        x (Tensor or dict): The input. If the graph sits on a node the\n            input can be a dict with {source_idx: Tensor} to be routed\n            to the defined input nodes. If the graph sits on an edge,\n            x is the feature tensor.\n        args: This is only required to handle cases where the graph sits\n            on an edge and receives an EdgeData object which will be ignored\n    \"\"\"\n    logger.debug(f\"Graph {self.name} called. Input {log_formats(x)}.\")\n\n    # Assign x to the corresponding input nodes\n    self._assign_x_to_nodes(x)\n\n    for node_idx in lexicographical_topological_sort(self):\n        node = self.nodes[node_idx]\n        logger.debug(\n            \"Node {}-{}, current data {}, start processing...\".format(\n                self.name, node_idx, log_formats(node)\n            )\n        )\n\n        # node internal: process input if necessary\n        if (\"subgraph\" in node and \"comb_op\" not in node) or (\n            \"comb_op\" in node and \"subgraph\" not in node\n        ):\n            log_first_n(\n                logging.WARN, \"Comb_op is ignored if subgraph is defined!\", n=1\n            )\n        # TODO: merge 'subgraph' and 'comb_op'. It is basicallly the same thing. Also in parse()\n        if \"subgraph\" in node:\n            x = node[\"subgraph\"].forward(node[\"input\"])\n        else:\n            if len(node[\"input\"].values()) == 1:\n                x = list(node[\"input\"].values())[0]\n            else:\n                x = node[\"comb_op\"](\n                    [node[\"input\"][k] for k in sorted(node[\"input\"].keys())]\n                )\n        node[\"input\"] = {}  # clear the input as we have processed it\n\n        if (\n            len(list(self.neighbors(node_idx))) == 0\n            and node_idx &lt; list(lexicographical_topological_sort(self))[-1]\n        ):\n            # We have more than one output node. This is e.g. the case for\n            # auxillary losses. Attach them to the graph, handling must done\n            # by the user.\n            logger.debug(\n                \"Graph {} has more then one output node. Storing output of non-maximum index node {} at graph dict\".format(\n                    self, node_idx\n                )\n            )\n            self.graph[f\"out_from_{node_idx}\"] = x\n        else:\n            # outgoing edges: process all outgoing edges\n            for neigbor_idx in self.neighbors(node_idx):\n                edge_data = self.get_edge_data(node_idx, neigbor_idx)\n                # inject edge data only for AbstractPrimitive, not Graphs\n                if isinstance(edge_data.op, Graph):\n                    edge_output = edge_data.op.forward(x)\n                elif isinstance(edge_data.op, AbstractPrimitive):\n                    logger.debug(\n                        \"Processing op {} at edge {}-{}\".format(\n                            edge_data.op, node_idx, neigbor_idx\n                        )\n                    )\n                    edge_output = edge_data.op.forward(x)\n                else:\n                    raise ValueError(\n                        \"Unknown class as op: {}. Expected either Graph or AbstactPrimitive\".format(\n                            edge_data.op\n                        )\n                    )\n                self.nodes[neigbor_idx][\"input\"].update({node_idx: edge_output})\n\n        logger.debug(f\"Node {self.name}-{node_idx}, processing done.\")\n\n    logger.debug(f\"Graph {self.name} exiting. Output {log_formats(x)}.\")\n    return x\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.get_all_edge_data","title":"get_all_edge_data","text":"<pre><code>get_all_edge_data(\n    key: str, scope=\"all\", private_edge_data: bool = False\n) -&gt; list\n</code></pre> <p>Get edge attributes of this graph and all child graphs in one go.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key of the attribute</p> <p> TYPE: <code>str</code> </p> <code>scope</code> <p>The scope to be applied</p> <p> TYPE: <code>str</code> DEFAULT: <code>'all'</code> </p> <code>private_edge_data</code> <p>Whether to return data from graph copies as well.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list</code> <p>All data in a list.</p> <p> TYPE: <code>list</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def get_all_edge_data(\n    self, key: str, scope=\"all\", private_edge_data: bool = False\n) -&gt; list:\n    \"\"\"\n    Get edge attributes of this graph and all child graphs in one go.\n\n    Args:\n        key (str): The key of the attribute\n        scope (str): The scope to be applied\n        private_edge_data (bool): Whether to return data from graph copies as well.\n\n    Returns:\n        list: All data in a list.\n    \"\"\"\n    assert scope is not None\n    result = []\n    for graph in self._get_child_graphs(single_instances=not private_edge_data) + [\n        self\n    ]:\n        if (\n            scope == \"all\"\n            or graph.scope == scope\n            or (isinstance(scope, list) and graph.scope in scope)\n        ):\n            for _, _, edge_data in graph.edges.data():\n                if edge_data.has(key):\n                    result.append(edge_data[key])\n    return result\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.get_dense_edges","title":"get_dense_edges","text":"<pre><code>get_dense_edges()\n</code></pre> <p>Returns the edge indices (i, j) that would make a fully connected DAG without circles such that i &lt; j and i != j. Assumes nodes are already created.</p> RETURNS DESCRIPTION <code>list</code> <p>list of edge indices.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def get_dense_edges(self):\n    \"\"\"\n    Returns the edge indices (i, j) that would make a fully connected\n    DAG without circles such that i &lt; j and i != j. Assumes nodes are\n    already created.\n\n    Returns:\n        list: list of edge indices.\n    \"\"\"\n    edges = []\n    nodes = sorted(list(self.nodes()))\n    for i in nodes:\n        for j in nodes:\n            if i != j and j &gt; i:\n                edges.append((i, j))\n    return edges\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.modules_str","title":"modules_str","text":"<pre><code>modules_str()\n</code></pre> <p>Once the graph has been parsed, prints the modules as they appear in pytorch.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def modules_str(self):\n    \"\"\"\n    Once the graph has been parsed, prints the modules as they appear in pytorch.\n    \"\"\"\n    if self.is_parsed:\n        result = \"\"\n        for g in self._get_child_graphs(single_instances=True) + [self]:\n            result += \"Graph {}:\\n {}\\n==========\\n\".format(\n                g.name, torch.nn.Module.__repr__(g)\n            )\n        return result\n    else:\n        return self.__repr__()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.num_input_nodes","title":"num_input_nodes","text":"<pre><code>num_input_nodes() -&gt; int\n</code></pre> <p>The number of input nodes, i.e. the nodes without an incoming edge.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of input nodes.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def num_input_nodes(self) -&gt; int:\n    \"\"\"\n    The number of input nodes, i.e. the nodes without an\n    incoming edge.\n\n    Returns:\n        int: Number of input nodes.\n    \"\"\"\n    return sum(self.in_degree(n) == 0 for n in self.nodes)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.parse","title":"parse","text":"<pre><code>parse()\n</code></pre> <p>Convert the graph into a neural network which can then be optimized by pytorch.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def parse(self):\n    \"\"\"\n    Convert the graph into a neural network which can then\n    be optimized by pytorch.\n    \"\"\"\n    for node_idx in lexicographical_topological_sort(self):\n        if \"subgraph\" in self.nodes[node_idx]:\n            self.nodes[node_idx][\"subgraph\"].parse()\n            self.add_module(\n                f\"{self.name}-subgraph_at({node_idx})\",\n                self.nodes[node_idx][\"subgraph\"],\n            )\n        else:\n            if isinstance(self.nodes[node_idx][\"comb_op\"], torch.nn.Module):\n                self.add_module(\n                    f\"{self.name}-comb_op_at({node_idx})\",\n                    self.nodes[node_idx][\"comb_op\"],\n                )\n        for neigbor_idx in self.neighbors(node_idx):\n            edge_data = self.get_edge_data(node_idx, neigbor_idx)\n            if isinstance(edge_data.op, Graph):\n                edge_data.op.parse()\n            elif edge_data.op.get_embedded_ops():\n                for primitive in edge_data.op.get_embedded_ops():\n                    if isinstance(primitive, Graph):\n                        primitive.parse()\n            self.add_module(\n                f\"{self.name}-edge({node_idx},{neigbor_idx})\",\n                edge_data.op,\n            )\n    self.is_parsed = True\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.prepare_discretization","title":"prepare_discretization","text":"<pre><code>prepare_discretization()\n</code></pre> <p>In some cases the search space is manipulated before the final discretization is happening, e.g. DARTS. In such chases this should be defined in the search space, so all optimizers can call it.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def prepare_discretization(self):\n    \"\"\"\n    In some cases the search space is manipulated before the final\n    discretization is happening, e.g. DARTS. In such chases this should\n    be defined in the search space, so all optimizers can call it.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.prepare_evaluation","title":"prepare_evaluation","text":"<pre><code>prepare_evaluation()\n</code></pre> <p>In some cases the evaluation architecture does not match the searched one. An example is where the makro_model is extended to increase the parameters. This is done here.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def prepare_evaluation(self):\n    \"\"\"\n    In some cases the evaluation architecture does not match the searched\n    one. An example is where the makro_model is extended to increase the\n    parameters. This is done here.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.reset_weights","title":"reset_weights","text":"<pre><code>reset_weights(inplace: bool = False)\n</code></pre> <p>Resets the weights for the 'op' at all edges.</p> PARAMETER DESCRIPTION <code>inplace</code> <p>Do the operation in place or return a modified copy.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Graph: Returns the modified version of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def reset_weights(self, inplace: bool = False):\n    \"\"\"\n    Resets the weights for the 'op' at all edges.\n\n    Args:\n        inplace (bool): Do the operation in place or\n            return a modified copy.\n    Returns:\n        Graph: Returns the modified version of the graph.\n    \"\"\"\n\n    def weight_reset(m):\n        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n            m.reset_parameters()\n\n    if inplace:\n        graph = self\n    else:\n        graph = self.clone()\n\n    graph.apply(weight_reset)\n\n    return graph\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.set_at_edges","title":"set_at_edges","text":"<pre><code>set_at_edges(key, value, shared=False)\n</code></pre> <p>Sets the attribute for all edges in this and any child graph</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_at_edges(self, key, value, shared=False):\n    \"\"\"\n    Sets the attribute for all edges in this and any child graph\n    \"\"\"\n    for graph in self._get_child_graphs(single_instances=shared) + [self]:\n        logger.debug(f\"Updating edges of graph {graph.name}\")\n        for _, _, edge_data in graph.edges.data():\n            if not edge_data.is_final():\n                edge_data.set(key, value, shared)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.set_input","title":"set_input","text":"<pre><code>set_input(node_idxs: list)\n</code></pre> <p>Route the input from specific parent edges to the input nodes of this subgraph. Inputs are assigned in lexicographical order.</p> <p>Example: - Parent node (i.e. node where <code>self</code> is located on) has two   incoming edges from nodes 3 and 5. - <code>self</code> has two input nodes 1 and 2 (i.e. nodes without   an incoming edge) - <code>node_idxs = [5, 3]</code> Then input of node 5 is routed to node 1 and input of node 3 is routed to node 2.</p> <p>Similarly, if <code>node_idxs = [5, 5]</code> then input of node 5 is routed to both node 1 and 2. Warning: In this case the output of another incoming edge is ignored!</p> <p>Should be used in a builder-like pattern: <code>'subgraph'=Graph().set_input([5, 3])</code></p> PARAMETER DESCRIPTION <code>node_idx</code> <p>The index of the nodes where the data is coming from.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Graph</code> <p>self with input node indices set.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_input(self, node_idxs: list):\n    \"\"\"\n    Route the input from specific parent edges to the input nodes of\n    this subgraph. Inputs are assigned in lexicographical order.\n\n    Example:\n    - Parent node (i.e. node where `self` is located on) has two\n      incoming edges from nodes 3 and 5.\n    - `self` has two input nodes 1 and 2 (i.e. nodes without\n      an incoming edge)\n    - `node_idxs = [5, 3]`\n    Then input of node 5 is routed to node 1 and input of node 3\n    is routed to node 2.\n\n    Similarly, if `node_idxs = [5, 5]` then input of node 5 is routed\n    to both node 1 and 2. Warning: In this case the output of another\n    incoming edge is ignored!\n\n    Should be used in a builder-like pattern: `'subgraph'=Graph().set_input([5, 3])`\n\n    Args:\n        node_idx (list): The index of the nodes where the data is coming from.\n\n    Returns:\n        Graph: self with input node indices set.\n\n    \"\"\"\n    num_innodes = sum(self.in_degree(n) == 0 for n in self.nodes)\n    assert num_innodes == len(\n        node_idxs\n    ), \"Expecting node index for every input node. Excpected {}, got {}\".format(\n        num_innodes, len(node_idxs)\n    )\n    self.input_node_idxs = node_idxs  # type: ignore[assignment]\n    return self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.set_scope","title":"set_scope","text":"<pre><code>set_scope(scope: str, recursively=True)\n</code></pre> <p>Sets the scope of this instance of the graph.</p> <p>The function should be used in a builder-like pattern <code>'subgraph'=Graph().set_scope(\"scope\")</code>.</p> PARAMETER DESCRIPTION <code>scope</code> <p>the scope</p> <p> TYPE: <code>str</code> </p> <code>recursively</code> <p>Also set the scope for all child graphs. default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Graph</code> <p>self with the setted scope.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_scope(self, scope: str, recursively=True):\n    \"\"\"\n    Sets the scope of this instance of the graph.\n\n    The function should be used in a builder-like pattern\n    `'subgraph'=Graph().set_scope(\"scope\")`.\n\n    Args:\n        scope (str): the scope\n        recursively (bool): Also set the scope for all child graphs.\n            default True\n\n    Returns:\n        Graph: self with the setted scope.\n    \"\"\"\n    self.scope = scope\n    if recursively:\n        for g in self._get_child_graphs(single_instances=False):\n            g.scope = scope\n    return self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.unparse","title":"unparse","text":"<pre><code>unparse()\n</code></pre> <p>Undo the pytorch parsing by reconstructing the graph uusing the networkx data structures.</p> <p>This is done recursively also for child graphs.</p> RETURNS DESCRIPTION <code>Graph</code> <p>An unparsed shallow copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def unparse(self):\n    \"\"\"\n    Undo the pytorch parsing by reconstructing the graph uusing the\n    networkx data structures.\n\n    This is done recursively also for child graphs.\n\n    Returns:\n        Graph: An unparsed shallow copy of the graph.\n    \"\"\"\n    g = self.__class__()\n    g.clear()\n\n    graph_nodes = self.nodes\n    graph_edges = self.edges\n\n    # unparse possible child graphs\n    # be careful with copying/deepcopying here cause of shared edge data\n    for _, data in graph_nodes.data():\n        if \"subgraph\" in data:\n            data[\"subgraph\"] = data[\"subgraph\"].unparse()\n    for _, _, data in graph_edges.data():\n        if isinstance(data.op, Graph):\n            data.set(\"op\", data.op.unparse())\n\n    # create the new graph\n    # Remember to add all members here to update. I know it is ugly but don't know better\n    g.add_nodes_from(graph_nodes.data())\n    g.add_edges_from(graph_edges.data())\n    g.graph.update(self.graph)\n    g.name = self.name\n    g.input_node_idxs = self.input_node_idxs\n    g.scope = self.scope\n    g.is_parsed = False\n    g._id = self._id\n    g.OPTIMIZER_SCOPE = self.OPTIMIZER_SCOPE\n    g.QUERYABLE = self.QUERYABLE\n\n    return g\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.update_edges","title":"update_edges","text":"<pre><code>update_edges(\n    update_func: Callable,\n    scope=\"all\",\n    private_edge_data: bool = False,\n)\n</code></pre> <p>This updates the edge data of this graph and all child graphs. This is the preferred way to manipulate the edges after the definition of the graph, e.g. by optimizers who want to insert their own op. <code>update_func(current_edge_data)</code>. This way optimizers can initialize and store necessary information at edges.</p> <p>Note that edges marked as 'final' will not be updated here.</p> PARAMETER DESCRIPTION <code>update_func</code> <p>Function which accepts one argument called <code>current_edge_data</code>. and returns the modified EdgeData object.</p> <p> TYPE: <code>callable</code> </p> <code>scope</code> <p>Can be \"all\" or list of scopes to be updated.</p> <p> TYPE: <code>str or list(str</code> DEFAULT: <code>'all'</code> </p> <code>private_edge_data</code> <p>If set to true, this means update_func will be applied to all edges. THIS IS NOT RECOMMENDED FOR SHARED ATTRIBUTES. Shared attributes should be set only once, we take care it is syncronized across all copies of this graph.</p> <p>The only usecase for setting it to true is when actually changing <code>op</code> during the initialization of the optimizer (e.g. replacing it with MixedOp or SampleOp)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def update_edges(\n    self, update_func: Callable, scope=\"all\", private_edge_data: bool = False\n):\n    \"\"\"\n    This updates the edge data of this graph and all child graphs.\n    This is the preferred way to manipulate the edges after the definition\n    of the graph, e.g. by optimizers who want to insert their own op.\n    `update_func(current_edge_data)`. This way optimizers\n    can initialize and store necessary information at edges.\n\n    Note that edges marked as 'final' will not be updated here.\n\n    Args:\n        update_func (callable): Function which accepts one argument called `current_edge_data`.\n            and returns the modified EdgeData object.\n        scope (str or list(str)): Can be \"all\" or list of scopes to be updated.\n        private_edge_data (bool): If set to true, this means update_func will be\n            applied to all edges. THIS IS NOT RECOMMENDED FOR SHARED\n            ATTRIBUTES. Shared attributes should be set only once, we\n            take care it is syncronized across all copies of this graph.\n\n            The only usecase for setting it to true is when actually changing\n            `op` during the initialization of the optimizer (e.g. replacing it\n            with MixedOp or SampleOp)\n    \"\"\"\n    Graph._verify_update_function(update_func, private_edge_data)\n    assert scope is not None\n    for graph in self._get_child_graphs(single_instances=not private_edge_data) + [\n        self\n    ]:\n        if (\n            scope == \"all\"\n            or scope == graph.scope\n            or (isinstance(scope, list) and graph.scope in scope)\n        ):\n            logger.debug(f\"Updating edges of graph {graph.name}\")\n            for u, v, edge_data in graph.edges.data():\n                if not edge_data.is_final():\n                    edge = AttrDict(head=u, tail=v, data=edge_data)\n                    update_func(edge=edge)\n    self._delete_flagged_edges()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.update_nodes","title":"update_nodes","text":"<pre><code>update_nodes(\n    update_func: Callable,\n    scope=\"all\",\n    single_instances: bool = True,\n)\n</code></pre> <p>Update the nodes of the graph and its incoming and outgoing edges by iterating over the graph and applying <code>update_func</code> to each of it. This is the preferred way to change the search space once it has been defined.</p> <p>Note that edges marked as 'final' will not be updated here.</p> PARAMETER DESCRIPTION <code>update_func</code> <p>Function that accepts three incoming parameters named <code>node, in_edges, out_edges</code>.     - <code>node</code> is a tuple (int, dict) containing the       index and the attributes of the current node.     - <code>in_edges</code> is a list of tuples with the index of       the tail of the edge and its EdgeData.     - `out_edges is a list of tuples with the index of       the head of the edge and its EdgeData.</p> <p> TYPE: <code>callable</code> </p> <code>scope</code> <p>Can be \"all\" or list of scopes to be updated. Only graphs and child graphs with the specified scope are considered</p> <p> TYPE: <code>str or list(str</code> DEFAULT: <code>'all'</code> </p> <code>single_instance</code> <p>If set to false, this means update_func will be applied to nodes of all copies of a graphs. THIS IS NOT RECOMMENDED FOR SHARED ATTRIBUTES, i.e. when manipulating the shared data of incoming or outgoing edges. Shared attributes should be set only once, we take care it is syncronized across all copies of this graph.</p> <p>The only usecase for setting it to true is when actually changing <code>op</code> during the initialization of the optimizer (e.g. replacing it with MixedOp or SampleOp)</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def update_nodes(\n    self, update_func: Callable, scope=\"all\", single_instances: bool = True\n):\n    \"\"\"\n    Update the nodes of the graph and its incoming and outgoing edges by iterating over the\n    graph and applying `update_func` to each of it. This is the\n    preferred way to change the search space once it has been defined.\n\n    Note that edges marked as 'final' will not be updated here.\n\n    Args:\n        update_func (callable): Function that accepts three incoming parameters named\n            `node, in_edges, out_edges`.\n                - `node` is a tuple (int, dict) containing the\n                  index and the attributes of the current node.\n                - `in_edges` is a list of tuples with the index of\n                  the tail of the edge and its EdgeData.\n                - `out_edges is a list of tuples with the index of\n                  the head of the edge and its EdgeData.\n        scope (str or list(str)): Can be \"all\" or list of scopes to be updated. Only graphs\n            and child graphs with the specified scope are considered\n        single_instance (bool): If set to false, this means update_func will be\n            applied to nodes of all copies of a graphs. THIS IS NOT RECOMMENDED FOR SHARED\n            ATTRIBUTES, i.e. when manipulating the shared data of incoming or outgoing edges.\n            Shared attributes should be set only once, we take care it is syncronized across\n            all copies of this graph.\n\n            The only usecase for setting it to true is when actually changing\n            `op` during the initialization of the optimizer (e.g. replacing it\n            with MixedOp or SampleOp)\n    \"\"\"\n    assert scope is not None\n    for graph in self._get_child_graphs(single_instances) + [self]:\n        if (\n            scope == \"all\"\n            or graph.scope == scope\n            or (isinstance(scope, list) and graph.scope in scope)\n        ):\n            logger.debug(f\"Updating nodes of graph {graph.name}\")\n            for node_idx in lexicographical_topological_sort(graph):\n                node = (node_idx, graph.nodes[node_idx])\n                in_edges = list(graph.in_edges(node_idx, data=True))  # (v, u, data)\n                in_edges = [\n                    (v, data) for v, u, data in in_edges if not data.is_final()\n                ]  # u is same for all\n                out_edges = list(\n                    graph.out_edges(node_idx, data=True)\n                )  # (v, u, data)\n                out_edges = [\n                    (u, data) for v, u, data in out_edges if not data.is_final()\n                ]  # v is same for all\n                update_func(node=node, in_edges=in_edges, out_edges=out_edges)\n    self._delete_flagged_edges()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.iter_flatten","title":"iter_flatten","text":"<pre><code>iter_flatten(iterable)\n</code></pre> <p>Flatten a potentially deeply nested python list</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def iter_flatten(iterable):\n    \"\"\"\n    Flatten a potentially deeply nested python list\n    \"\"\"\n    # taken from https://rightfootin.blogspot.com/2006/09/more-on-python-flatten.html\n    it = iter(iterable)\n    for e in it:\n        if isinstance(e, (list, tuple)):\n            yield from iter_flatten(e)\n        else:\n            yield e\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.log_first_n","title":"log_first_n","text":"<pre><code>log_first_n(lvl, msg, n=1, *, name=None, key='caller')\n</code></pre> <p>Log only for the first n times. Args:     lvl (int): the logging level     msg (str):     n (int):     name (str): name of the logger to use. Will use the caller's module by default.     key (str or tuple[str]): the string(s) can be one of \"caller\" or         \"message\", which defines how to identify duplicated logs.         For example, if called with <code>n=1, key=\"caller\"</code>, this function         will only log the first call from the same caller, regardless of         the message content.         If called with <code>n=1, key=\"message\"</code>, this function will log the         same content only once, even if they are called from different places.         If called with <code>n=1, key=(\"caller\", \"message\")</code>, this function         will not log only if the same caller has logged the same message before.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def log_first_n(lvl, msg, n=1, *, name=None, key=\"caller\"):\n    \"\"\"\n    Log only for the first n times.\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n        key (str or tuple[str]): the string(s) can be one of \"caller\" or\n            \"message\", which defines how to identify duplicated logs.\n            For example, if called with `n=1, key=\"caller\"`, this function\n            will only log the first call from the same caller, regardless of\n            the message content.\n            If called with `n=1, key=\"message\"`, this function will log the\n            same content only once, even if they are called from different places.\n            If called with `n=1, key=(\"caller\", \"message\")`, this function\n            will not log only if the same caller has logged the same message before.\n    \"\"\"\n    if isinstance(key, str):\n        key = (key,)\n    assert len(key) &gt; 0\n\n    caller_module, caller_key = _find_caller()\n    hash_key = ()\n    if \"caller\" in key:\n        hash_key = hash_key + caller_key\n    if \"message\" in key:\n        hash_key = hash_key + (msg,)\n\n    _LOG_COUNTER[hash_key] += 1\n    if _LOG_COUNTER[hash_key] &lt;= n:\n        logging.getLogger(name or caller_module).log(lvl, msg)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph_grammar/","title":"Graph grammar","text":""},{"location":"api/neps/search_spaces/architecture/graph_grammar/#neps.search_spaces.architecture.graph_grammar","title":"neps.search_spaces.architecture.graph_grammar","text":""},{"location":"api/neps/search_spaces/architecture/graph_grammar/#neps.search_spaces.architecture.graph_grammar.GraphParameter","title":"GraphParameter","text":"<pre><code>GraphParameter(\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool\n)\n</code></pre> <p>               Bases: <code>ParameterWithPrior[DiGraph, str]</code>, <code>MutatableParameter</code></p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool,\n):\n    \"\"\"Create a new `Parameter`.\n\n    Args:\n        value: value for the hyperparameter.\n        default: default value for the hyperparameter.\n        is_fidelity: whether the hyperparameter is fidelity.\n    \"\"\"\n    self.default = default\n    self.is_fidelity = is_fidelity\n\n    # TODO(eddiebergman): The reason to have this not as a straight alone\n    # attribute is that the graph parameters currently expose there own\n    # way of calculating a value on demand.\n    # To fix this would mean to essentially decouple GraphParameter entirely\n    # from Parameter as it's less of a heirarchy and more of just a small overlap\n    # of functionality.\n    self._value = value\n    self.normalized_value = (\n        self.value_to_normalized(value) if value is not None else None\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph_grammar/#neps.search_spaces.architecture.graph_grammar.GraphParameter.deserialize_value","title":"deserialize_value  <code>classmethod</code>","text":"<pre><code>deserialize_value(value: str) -&gt; DiGraph\n</code></pre> <p>Functionality relying on this for GraphParameters should special case for whever this is needed...</p> <p>Warning</p> <p>Graph parameters don't directly support serialization. Instead they rely on holding on to the original string value from which they were created from.</p> Source code in <code>neps/search_spaces/architecture/graph_grammar.py</code> <pre><code>@classmethod\ndef deserialize_value(cls, value: str) -&gt; nx.DiGraph:\n    \"\"\"Functionality relying on this for GraphParameters should\n    special case for whever this is needed...\n\n    !!! warning\n\n        Graph parameters don't directly support serialization.\n        Instead they rely on holding on to the original string value\n        from which they were created from.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph_grammar/#neps.search_spaces.architecture.graph_grammar.GraphParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph_grammar/#neps.search_spaces.architecture.graph_grammar.GraphParameter.serialize_value","title":"serialize_value  <code>classmethod</code>","text":"<pre><code>serialize_value(value: DiGraph) -&gt; str\n</code></pre> <p>Functionality relying on this for GraphParameters should special case and use <code>self.id</code>.</p> <p>Warning</p> <p>Graph parameters don't directly support serialization. Instead they rely on holding on to the original string value from which they were created from.</p> Source code in <code>neps/search_spaces/architecture/graph_grammar.py</code> <pre><code>@classmethod\ndef serialize_value(cls, value: nx.DiGraph) -&gt; str:\n    \"\"\"Functionality relying on this for GraphParameters should\n    special case and use `self.id`.\n\n    !!! warning\n\n        Graph parameters don't directly support serialization.\n        Instead they rely on holding on to the original string value\n        from which they were created from.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph_grammar/#neps.search_spaces.architecture.graph_grammar.GraphParameter.set_default_confidence_score","title":"set_default_confidence_score","text":"<pre><code>set_default_confidence_score(\n    default_confidence: str,\n) -&gt; None\n</code></pre> <p>Set the default confidence score for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default_confidence</code> <p>the choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the confidence score is not a valid choice.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def set_default_confidence_score(self, default_confidence: str) -&gt; None:\n    \"\"\"Set the default confidence score for the hyperparameter.\n\n    Args:\n        default_confidence: the choice of how confident any algorithm should\n            be in the default value being a good value.\n\n    Raises:\n        ValueError: if the confidence score is not a valid choice.\n    \"\"\"\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        cls_name = self.__class__.__name__\n        raise ValueError(\n            f\"Invalid default confidence score: {default_confidence}\"\n            f\" for {cls_name}. Expected one of:\"\n            f\" {list(self.DEFAULT_CONFIDENCE_SCORES.keys())}\"\n        )\n\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/mutations/","title":"Mutations","text":""},{"location":"api/neps/search_spaces/architecture/mutations/#neps.search_spaces.architecture.mutations","title":"neps.search_spaces.architecture.mutations","text":""},{"location":"api/neps/search_spaces/architecture/primitives/","title":"Primitives","text":""},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives","title":"neps.search_spaces.architecture.primitives","text":""},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.AvgPool","title":"AvgPool","text":"<pre><code>AvgPool(kernel_size, stride, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of Avergae Pooling.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, kernel_size, stride, **kwargs):\n    stride = int(stride)\n    super().__init__(locals())\n    self.avgpool = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.AvgPool1x1","title":"AvgPool1x1","text":"<pre><code>AvgPool1x1(\n    kernel_size, stride, C_in, C_out, affine=True, **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of Avergae Pooling with an optional 1x1 convolution afterwards. The convolution is required to increase the number of channels if stride &gt; 1.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(\n    self,\n    kernel_size,\n    stride,\n    C_in,\n    C_out,\n    affine=True,\n    **kwargs,\n):\n    super().__init__(locals())\n    stride = int(stride)\n    self.stride = int(stride)\n    self.avgpool = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)\n    if stride &gt; 1:\n        assert C_in is not None and C_out is not None\n        self.conv = nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Concat1x1","title":"Concat1x1","text":"<pre><code>Concat1x1(num_in_edges, C_out, affine=True, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of the channel-wise concatination followed by a 1x1 convolution to retain the channel dimension.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(\n    self, num_in_edges, C_out, affine=True, **kwargs\n):\n    super().__init__(locals())\n    self.conv = nn.Conv2d(\n        num_in_edges * C_out, C_out, kernel_size=1, stride=1, padding=0, bias=False\n    )\n    self.bn = nn.BatchNorm2d(C_out, affine=affine)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Concat1x1.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Expecting a list of input tensors. Stacking them channel-wise and applying 1x1 conv</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Expecting a list of input tensors. Stacking them channel-wise\n    and applying 1x1 conv\n    \"\"\"\n    x = torch.cat(x, dim=1)\n    x = self.conv(x)\n    x = self.bn(x)\n    return x\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.ConvBn","title":"ConvBn","text":"<pre><code>ConvBn(\n    C_in,\n    C_out,\n    kernel_size,\n    stride=1,\n    affine=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of 2d convolution, followed by 2d batch normalization and ReLU activation.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, C_in, C_out, kernel_size, stride=1, affine=True, **kwargs):\n    super().__init__(locals())\n    self.kernel_size = kernel_size\n    pad = 0 if stride == 1 and kernel_size == 1 else 1\n    self.op = nn.Sequential(\n        nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=pad, bias=False),\n        nn.BatchNorm2d(C_out, affine=affine),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.ConvBnReLU","title":"ConvBnReLU","text":"<pre><code>ConvBnReLU(\n    C_in,\n    C_out,\n    kernel_size,\n    stride=1,\n    affine=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of 2d convolution, followed by 2d batch normalization and ReLU activation.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, C_in, C_out, kernel_size, stride=1, affine=True, **kwargs):\n    super().__init__(locals())\n    self.kernel_size = kernel_size\n    pad = 0 if stride == 1 and kernel_size == 1 else 1\n    self.op = nn.Sequential(\n        nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=pad, bias=False),\n        nn.BatchNorm2d(C_out, affine=affine),\n        nn.ReLU(inplace=False),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.DilConv","title":"DilConv","text":"<pre><code>DilConv(\n    C_in,\n    C_out,\n    kernel_size,\n    stride,\n    padding,\n    dilation,\n    affine=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of a dilated separable convolution as used in the DARTS paper.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(\n    self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True, **kwargs\n):\n    super().__init__(locals())\n\n    C_in = int(C_in)\n    C_out = int(C_out)\n    kernel_size = int(kernel_size)\n    stride = int(stride)\n    padding = int(padding)\n    dilation = int(dilation)\n    affine = bool(affine)\n\n    self.kernel_size = kernel_size\n    self.op = nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(\n            C_in,\n            C_in,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=C_in,\n            bias=False,\n        ),\n        nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n        nn.BatchNorm2d(C_out, affine=affine),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Identity","title":"Identity","text":"<pre><code>Identity(**kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>An implementation of the Identity operation.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(locals())\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.MaxPool1x1","title":"MaxPool1x1","text":"<pre><code>MaxPool1x1(\n    kernel_size, stride, C_in, C_out, affine=True, **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of MaxPool with an optional 1x1 convolution in case stride &gt; 1. The 1x1 convolution is required to increase the number of channels.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, kernel_size, stride, C_in, C_out, affine=True, **kwargs):\n    super().__init__(locals())\n\n    kernel_size = int(kernel_size)\n    stride = int(stride)\n    C_in = int(C_in)\n    C_out = int(C_out)\n    affine = bool(affine)\n\n    self.stride = stride\n    self.maxpool = nn.MaxPool2d(kernel_size, stride=stride, padding=1)\n    if stride &gt; 1:\n        assert C_in is not None and C_out is not None\n        self.conv = nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.SepConv","title":"SepConv","text":"<pre><code>SepConv(\n    C_in,\n    C_out,\n    kernel_size,\n    stride,\n    padding,\n    affine=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of Separable convolution operation as in the DARTS paper, i.e. 2 sepconv directly after another.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True, **kwargs):\n    super().__init__(locals())\n\n    C_in = int(C_in)\n    C_out = int(C_out)\n    kernel_size = int(kernel_size)\n    stride = int(stride)\n    padding = int(padding)\n    affine = bool(affine)\n\n    self.kernel_size = kernel_size\n    self.op = nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(\n            C_in,\n            C_in,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=C_in,\n            bias=False,\n        ),\n        nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),\n        nn.BatchNorm2d(C_in, affine=affine),\n        nn.ReLU(inplace=False),\n        nn.Conv2d(\n            C_in,\n            C_in,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=padding,\n            groups=C_in,\n            bias=False,\n        ),\n        nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n        nn.BatchNorm2d(C_out, affine=affine),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Sequential","title":"Sequential","text":"<pre><code>Sequential(*args, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of <code>torch.nn.Sequential</code> to be used as op on edges.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(locals())\n    self.primitives = args\n    self.op = nn.Sequential(*args)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Stem","title":"Stem","text":"<pre><code>Stem(C_out, C_in=3, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>This is used as an initial layer directly after the image input.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, C_out, C_in=3, **kwargs):\n    super().__init__(locals())\n\n    C_out = int(C_out)\n\n    self.seq = nn.Sequential(\n        nn.Conv2d(C_in, C_out, 3, padding=1, bias=False), nn.BatchNorm2d(C_out)\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Zero","title":"Zero","text":"<pre><code>Zero(stride, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of the zero operation. It removes the connection by multiplying its input with zero.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, stride, **kwargs):\n    \"\"\"\n    When setting stride &gt; 1 then it is assumed that the\n    channels must be doubled.\n    \"\"\"\n    super().__init__(locals())\n    self.stride = int(stride)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Zero1x1","title":"Zero1x1","text":"<pre><code>Zero1x1(stride, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of the zero operation. It removes the connection by multiplying its input with zero.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, stride, **kwargs):\n    \"\"\"\n    When setting stride &gt; 1 then it is assumed that the\n    channels must be doubled.\n    \"\"\"\n    super().__init__(locals())\n    self.stride = int(stride)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/topologies/","title":"Topologies","text":""},{"location":"api/neps/search_spaces/architecture/topologies/#neps.search_spaces.architecture.topologies","title":"neps.search_spaces.architecture.topologies","text":""},{"location":"api/neps/search_spaces/architecture/cfg_variants/cfg_resolution/","title":"Cfg resolution","text":""},{"location":"api/neps/search_spaces/architecture/cfg_variants/cfg_resolution/#neps.search_spaces.architecture.cfg_variants.cfg_resolution","title":"neps.search_spaces.architecture.cfg_variants.cfg_resolution","text":""},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/","title":"Constrained cfg","text":""},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/#neps.search_spaces.architecture.cfg_variants.constrained_cfg","title":"neps.search_spaces.architecture.cfg_variants.constrained_cfg","text":""},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/#neps.search_spaces.architecture.cfg_variants.constrained_cfg.ConstrainedGrammar","title":"ConstrainedGrammar","text":"<pre><code>ConstrainedGrammar(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Grammar</code></p> Source code in <code>neps/search_spaces/architecture/cfg_variants/constrained_cfg.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.constraints = None\n    self.none_operation = None\n    self.constraint_is_class: bool = False\n\n    self._prior: dict = None\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/#neps.search_spaces.architecture.cfg_variants.constrained_cfg.ConstrainedGrammar.compute_space_size","title":"compute_space_size  <code>property</code>","text":"<pre><code>compute_space_size: int\n</code></pre> <p>Computes the size of the space described by the grammar.</p> PARAMETER DESCRIPTION <code>primitive_nonterminal</code> <p>The primitive nonterminal of the grammar. Defaults to \"OPS\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>int</code> <p>size of space described by grammar.</p> <p> TYPE: <code>int</code> </p>"},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/#neps.search_spaces.architecture.cfg_variants.constrained_cfg.ConstrainedGrammar.rand_subtree","title":"rand_subtree","text":"<pre><code>rand_subtree(tree: str) -&gt; Tuple[str, int]\n</code></pre> <p>Helper function to choose a random subtree in a given parse tree. Runs a single pass through the tree (stored as string) to look for the location of swappable nonterminal symbols.</p> PARAMETER DESCRIPTION <code>tree</code> <p>parse tree.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Tuple[str, int]</code> <p>Tuple[str, int]: return the parent node of the subtree and its index.</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>def rand_subtree(self, tree: str) -&gt; Tuple[str, int]:\n    \"\"\"Helper function to choose a random subtree in a given parse tree.\n    Runs a single pass through the tree (stored as string) to look for\n    the location of swappable nonterminal symbols.\n\n    Args:\n        tree (str): parse tree.\n\n    Returns:\n        Tuple[str, int]: return the parent node of the subtree and its index.\n    \"\"\"\n    split_tree = tree.split(\" \")\n    swappable_indices = [\n        i\n        for i in range(0, len(split_tree))\n        if split_tree[i][1:] in self.swappable_nonterminals\n    ]\n    r = np.random.randint(1, len(swappable_indices))\n    chosen_non_terminal = split_tree[swappable_indices[r]][1:]\n    chosen_non_terminal_index = swappable_indices[r]\n    return chosen_non_terminal, chosen_non_terminal_index\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/#neps.search_spaces.architecture.cfg_variants.constrained_cfg.ConstrainedGrammar.remove_subtree","title":"remove_subtree  <code>staticmethod</code>","text":"<pre><code>remove_subtree(\n    tree: str, index: int\n) -&gt; Tuple[str, str, str]\n</code></pre> <p>Helper functioon to remove a subtree from a parse tree given its index. E.g. '(S (S (T 2)) (ADD +) (T 1))' becomes '(S (S (T 2)) ', '(T 1))'  after removing (ADD +)</p> PARAMETER DESCRIPTION <code>tree</code> <p>parse tree</p> <p> TYPE: <code>str</code> </p> <code>index</code> <p>index of the subtree root node</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tuple[str, str, str]</code> <p>Tuple[str, str, str]: part before the subtree, subtree, part past subtree</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>@staticmethod\ndef remove_subtree(tree: str, index: int) -&gt; Tuple[str, str, str]:\n    \"\"\"Helper functioon to remove a subtree from a parse tree\n    given its index.\n    E.g. '(S (S (T 2)) (ADD +) (T 1))'\n    becomes '(S (S (T 2)) ', '(T 1))'  after removing (ADD +)\n\n    Args:\n        tree (str): parse tree\n        index (int): index of the subtree root node\n\n    Returns:\n        Tuple[str, str, str]: part before the subtree, subtree, part past subtree\n    \"\"\"\n    split_tree = tree.split(\" \")\n    pre_subtree = \" \".join(split_tree[:index]) + \" \"\n    #  get chars to the right of split\n    right = \" \".join(split_tree[index + 1 :])\n    # remove chosen subtree\n    # single pass to find the bracket matching the start of the split\n    counter, current_index = 1, 0\n    for char in right:\n        if char == \"(\":\n            counter += 1\n        elif char == \")\":\n            counter -= 1\n        if counter == 0:\n            break\n        current_index += 1\n    post_subtree = right[current_index + 1 :]\n    removed = \"\".join(split_tree[index]) + \" \" + right[: current_index + 1]\n    return (pre_subtree, removed, post_subtree)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/","title":"Categorical","text":""},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical","title":"neps.search_spaces.hyperparameters.categorical","text":"<p>Categorical hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter","title":"CategoricalParameter","text":"<pre><code>CategoricalParameter(\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>ParameterWithPrior[CategoricalTypes, CategoricalTypes]</code>, <code>MutatableParameter</code></p> <p>A list of unordered choices for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters that can take on a discrete set of unordered values. For example, the <code>optimizer</code> hyperparameter in a neural network search space can be a <code>CategoricalParameter</code> with choices like <code>[\"adam\", \"sgd\", \"rmsprop\"]</code>.</p> <pre><code>import neps\n\noptimizer_choice = neps.CategoricalParameter(\n    [\"adam\", \"sgd\", \"rmsprop\"],\n    default=\"adam\"\n)\n</code></pre> <p>Please see the <code>Parameter</code>, <code>ParameterWithPrior</code>, <code>MutatableParameter</code> classes for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>choices</code> <p>choices for the hyperparameter.</p> <p> TYPE: <code>Iterable[float | int | str]</code> </p> <code>default</code> <p>default value for the hyperparameter, must be in <code>choices=</code> if provided.</p> <p> TYPE: <code>float | int | str | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/categorical.py</code> <pre><code>def __init__(\n    self,\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `CategoricalParameter`.\n\n    Args:\n        choices: choices for the hyperparameter.\n        default: default value for the hyperparameter, must be in `choices=`\n            if provided.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    choices = list(choices)\n    if len(choices) &lt;= 1:\n        raise ValueError(\"Categorical choices must have more than one value.\")\n\n    super().__init__(value=None, is_fidelity=False, default=default)\n\n    for choice in choices:\n        if not isinstance(choice, (float, int, str)):\n            raise TypeError(\n                f'Choice \"{choice}\" is not of a valid type (float, int, str)'\n            )\n\n    if not all_unique(choices):\n        raise ValueError(f\"Choices must be unique but got duplicates.\\n{choices}\")\n\n    if default is not None and default not in choices:\n        raise ValueError(\n            f\"Default value {default} is not in the provided choices {choices}\"\n        )\n\n    self.choices = list(choices)\n\n    # NOTE(eddiebergman): If there's ever a very large categorical,\n    # then it would be beneficial to have a lookup table for indices as\n    # currently we do a list.index() operation which is O(n).\n    # However for small sized categoricals this is likely faster than\n    # a lookup table.\n    # For now we can just cache the index of the value and default.\n    self._value_index: int | None = None\n\n    self.default_confidence_choice = default_confidence\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n    self.has_prior = self.default is not None\n    self._default_index: int | None = (\n        self.choices.index(default) if default is not None else None\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.set_default_confidence_score","title":"set_default_confidence_score","text":"<pre><code>set_default_confidence_score(\n    default_confidence: str,\n) -&gt; None\n</code></pre> <p>Set the default confidence score for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default_confidence</code> <p>the choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the confidence score is not a valid choice.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def set_default_confidence_score(self, default_confidence: str) -&gt; None:\n    \"\"\"Set the default confidence score for the hyperparameter.\n\n    Args:\n        default_confidence: the choice of how confident any algorithm should\n            be in the default value being a good value.\n\n    Raises:\n        ValueError: if the confidence score is not a valid choice.\n    \"\"\"\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        cls_name = self.__class__.__name__\n        raise ValueError(\n            f\"Invalid default confidence score: {default_confidence}\"\n            f\" for {cls_name}. Expected one of:\"\n            f\" {list(self.DEFAULT_CONFIDENCE_SCORES.keys())}\"\n        )\n\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/","title":"Constant","text":""},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant","title":"neps.search_spaces.hyperparameters.constant","text":"<p>Constant hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter","title":"ConstantParameter","text":"<pre><code>ConstantParameter(value: T)\n</code></pre> <p>               Bases: <code>Parameter[T, T]</code></p> <p>A constant value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with values that should not change during optimization. For example, the <code>batch_size</code> hyperparameter in a neural network search space can be a <code>ConstantParameter</code> with a value of <code>32</code>.</p> <pre><code>import neps\n\nbatch_size = neps.ConstantParameter(32)\n</code></pre> <p>Note</p> <p>As the name suggests, the value of a <code>ConstantParameter</code> only have one value and so its <code>.default</code> and <code>.value</code> should always be the same.</p> <p>This also implies that the <code>.default</code> can never be <code>None</code>.</p> <p>Please use <code>.set_constant_value()</code> if you need to change the value of the constant parameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>T</code> </p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>def __init__(self, value: T):\n    \"\"\"Create a new `ConstantParameter`.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n    super().__init__(value=value, default=value, is_fidelity=False)  # type: ignore\n    self._value: T = value  # type: ignore\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: T\n</code></pre> <p>Get the value of the constant parameter.</p>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.sample","title":"sample","text":"<pre><code>sample() -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Will set the <code>.value</code> to the sampled value.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Will set the [`.value`][neps.search_spaces.Parameter.value] to the\n    sampled value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value()\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.set_constant_value","title":"set_constant_value","text":"<pre><code>set_constant_value(value: T) -&gt; None\n</code></pre> <p>Set the value of the constant parameter.</p> <p>Note</p> <p>This method is used to set the <code>.value</code> including the <code>.default</code> It is used internally and should not be used by the user.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>def set_constant_value(self, value: T) -&gt; None:\n    \"\"\"Set the value of the constant parameter.\n\n    !!! note\n\n        This method is used to set the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        including the [`.default`][neps.search_spaces.parameter.Parameter.default]\n        It is used internally and should not be used by the user.\n    \"\"\"\n    self._value = value\n    self.default = value\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.set_default","title":"set_default","text":"<pre><code>set_default(default: T | None) -&gt; None\n</code></pre> <p>Set the default of the constant parameter.</p> <p>Note</p> <p>This method is a no-op but will raise a <code>ValueError</code> if the default is different from the current default.</p> <p>Please see <code>.set_constant_value()</code> which can be used to set both the <code>.value</code> and the <code>.default</code> at once</p> PARAMETER DESCRIPTION <code>default</code> <p>value to set the default to.</p> <p> TYPE: <code>T | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the default is different from the current default.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>@override\ndef set_default(self, default: T | None) -&gt; None:\n    \"\"\"Set the default of the constant parameter.\n\n    !!! note\n\n        This method is a no-op but will raise a `ValueError` if the default\n        is different from the current default.\n\n        Please see\n        [`.set_constant_value()`][neps.search_spaces.hyperparameters.constant.ConstantParameter.set_constant_value]\n        which can be used to set both the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        and the [`.default`][neps.search_spaces.parameter.Parameter.default] at once\n\n    Args:\n        default: value to set the default to.\n\n    Raises:\n        ValueError: if the default is different from the current default.\n    \"\"\"\n    if default != self.default:\n        raise ValueError(\n            f\"Constant does not allow changing the default value. \"\n            f\"Tried to set default to {default}, but it is already {self.default}\"\n        )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.set_value","title":"set_value","text":"<pre><code>set_value(value: T | None) -&gt; None\n</code></pre> <p>Set the value of the constant parameter.</p> <p>Note</p> <p>This method is a no-op but will raise a <code>ValueError</code> if the value is different from the current value.</p> <p>Please see <code>.set_constant_value()</code> which can be used to set both the <code>.value</code> and the <code>.default</code> at once</p> PARAMETER DESCRIPTION <code>value</code> <p>value to set the parameter to.</p> <p> TYPE: <code>T | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the value is different from the current value.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>@override\ndef set_value(self, value: T | None) -&gt; None:\n    \"\"\"Set the value of the constant parameter.\n\n    !!! note\n\n        This method is a no-op but will raise a `ValueError` if the value\n        is different from the current value.\n\n        Please see\n        [`.set_constant_value()`][neps.search_spaces.hyperparameters.constant.ConstantParameter.set_constant_value]\n        which can be used to set both the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        and the [`.default`][neps.search_spaces.parameter.Parameter.default] at once\n\n    Args:\n        value: value to set the parameter to.\n\n    Raises:\n        ValueError: if the value is different from the current value.\n    \"\"\"\n    if value != self._value:\n        raise ValueError(\n            f\"Constant does not allow chaning the set value. \"\n            f\"Tried to set value to {value}, but it is already {self.value}\"\n        )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/","title":"Float","text":""},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float","title":"neps.search_spaces.hyperparameters.float","text":"<p>Float hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter","title":"FloatParameter","text":"<pre><code>FloatParameter(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>NumericalParameter[float]</code></p> <p>A float value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous float values, optionally specifying if it exists on a log scale. For example, <code>l2_norm</code> could be a value in <code>(0.1)</code>, while the <code>learning_rate</code> hyperparameter in a neural network search space can be a <code>FloatParameter</code> with a range of <code>(0.0001, 0.1)</code> but on a log scale.</p> <pre><code>import neps\n\nl2_norm = neps.FloatParameter(0, 1)\nlearning_rate = neps.FloatParameter(1e-4, 1e-1, log=True)\n</code></pre> <p>Please see the <code>NumericalParameter</code> class for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsidering prior based optimization..</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/float.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `FloatParameter`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsidering prior based optimization..\n    \"\"\"\n    super().__init__(\n        lower=float(lower),\n        upper=float(upper),\n        log=log,\n        default=float(default) if default is not None else None,\n        default_confidence=default_confidence,\n        is_fidelity=is_fidelity,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.grid","title":"grid","text":"<pre><code>grid(\n    *, size: int, include_endpoint: bool = True\n) -&gt; list[T]\n</code></pre> <p>Generate a grid of values for the numerical hyperparameter.</p> <p>Duplicates</p> <p>The grid may contain duplicates if the hyperparameter is an integer, for example if the lower bound is <code>0</code> and the upper bound is <code>10</code>, but <code>size=20</code>.</p> PARAMETER DESCRIPTION <code>size</code> <p>The number of values to generate.</p> <p> TYPE: <code>int</code> </p> <code>include_endpoint</code> <p>Whether to include the upper bound in the grid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list[T]</code> <p>A list of values for the numerical hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def grid(self, *, size: int, include_endpoint: bool = True) -&gt; list[T]:\n    \"\"\"Generate a grid of values for the numerical hyperparameter.\n\n    !!! note \"Duplicates\"\n\n        The grid may contain duplicates if the hyperparameter is an integer,\n        for example if the lower bound is `0` and the upper bound is `10`, but\n        `size=20`.\n\n    Args:\n        size: The number of values to generate.\n        include_endpoint: Whether to include the upper bound in the grid.\n\n    Returns:\n        A list of values for the numerical hyperparameter.\n    \"\"\"\n    return [\n        self.normalized_to_value(x)\n        for x in np.linspace(0, 1, num=size, endpoint=include_endpoint)\n    ]\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.set_default_confidence_score","title":"set_default_confidence_score","text":"<pre><code>set_default_confidence_score(\n    default_confidence: str,\n) -&gt; None\n</code></pre> <p>Set the default confidence score for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default_confidence</code> <p>the choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the confidence score is not a valid choice.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def set_default_confidence_score(self, default_confidence: str) -&gt; None:\n    \"\"\"Set the default confidence score for the hyperparameter.\n\n    Args:\n        default_confidence: the choice of how confident any algorithm should\n            be in the default value being a good value.\n\n    Raises:\n        ValueError: if the confidence score is not a valid choice.\n    \"\"\"\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        cls_name = self.__class__.__name__\n        raise ValueError(\n            f\"Invalid default confidence score: {default_confidence}\"\n            f\" for {cls_name}. Expected one of:\"\n            f\" {list(self.DEFAULT_CONFIDENCE_SCORES.keys())}\"\n        )\n\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.to_float","title":"to_float","text":"<pre><code>to_float() -&gt; FloatParameter\n</code></pre> <p>Convert the numerical hyperparameter to a float hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_float(self) -&gt; FloatParameter:\n    \"\"\"Convert the numerical hyperparameter to a float hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import FloatParameter\n\n    float_hp = FloatParameter(\n        lower=float(self.lower),\n        upper=float(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=float(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    float_hp.set_value(float(self.value) if self.value is not None else None)\n    return float_hp\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.to_integer","title":"to_integer","text":"<pre><code>to_integer() -&gt; IntegerParameter\n</code></pre> <p>Convert the numerical hyperparameter to an integer hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_integer(self) -&gt; IntegerParameter:\n    \"\"\"Convert the numerical hyperparameter to an integer hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import IntegerParameter\n\n    as_int = lambda x: int(np.rint(x))\n\n    int_hp = IntegerParameter(\n        lower=as_int(self.lower),\n        upper=as_int(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=as_int(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    int_hp.set_value(as_int(self.value) if self.value is not None else None)\n    return int_hp\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/","title":"Integer","text":""},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer","title":"neps.search_spaces.hyperparameters.integer","text":"<p>Float hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter","title":"IntegerParameter","text":"<pre><code>IntegerParameter(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>NumericalParameter[int]</code></p> <p>An integer value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous integer values, optionally specifying f it exists on a log scale. For example, <code>batch_size</code> could be a value in <code>(32, 128)</code>, while the <code>num_layers</code> hyperparameter in a neural network search space can be a <code>IntegerParameter</code> with a range of <code>(1, 1000)</code> but on a log scale.</p> <pre><code>import neps\n\nbatch_size = neps.IntegerParameter(32, 128)\nnum_layers = neps.IntegerParameter(1, 1000, log=True)\n</code></pre> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/integer.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `IntegerParameter`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    lower = int(np.rint(lower))\n    upper = int(np.rint(upper))\n    _size = upper - lower + 1\n    if _size &lt;= 1:\n        raise ValueError(\n            f\"IntegerParameter: expected at least 2 possible values in the range,\"\n            f\" got upper={upper}, lower={lower}.\"\n        )\n\n    super().__init__(\n        lower=int(np.rint(lower)),\n        upper=int(np.rint(upper)),\n        log=log,\n        is_fidelity=is_fidelity,\n        default=int(np.rint(default)) if default is not None else None,\n        default_confidence=default_confidence,\n    )\n\n    # We subtract/add 0.499999 from lower/upper bounds respectively, such that\n    # sampling in the float space gives equal probability for all integer values,\n    # i.e. [x - 0.499999, x + 0.499999]\n    self.float_hp = FloatParameter(\n        lower=self.lower - 0.499999,\n        upper=self.upper + 0.499999,\n        log=self.log,\n        is_fidelity=is_fidelity,\n        default=default,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter.grid","title":"grid","text":"<pre><code>grid(\n    *, size: int, include_endpoint: bool = True\n) -&gt; list[T]\n</code></pre> <p>Generate a grid of values for the numerical hyperparameter.</p> <p>Duplicates</p> <p>The grid may contain duplicates if the hyperparameter is an integer, for example if the lower bound is <code>0</code> and the upper bound is <code>10</code>, but <code>size=20</code>.</p> PARAMETER DESCRIPTION <code>size</code> <p>The number of values to generate.</p> <p> TYPE: <code>int</code> </p> <code>include_endpoint</code> <p>Whether to include the upper bound in the grid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list[T]</code> <p>A list of values for the numerical hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def grid(self, *, size: int, include_endpoint: bool = True) -&gt; list[T]:\n    \"\"\"Generate a grid of values for the numerical hyperparameter.\n\n    !!! note \"Duplicates\"\n\n        The grid may contain duplicates if the hyperparameter is an integer,\n        for example if the lower bound is `0` and the upper bound is `10`, but\n        `size=20`.\n\n    Args:\n        size: The number of values to generate.\n        include_endpoint: Whether to include the upper bound in the grid.\n\n    Returns:\n        A list of values for the numerical hyperparameter.\n    \"\"\"\n    return [\n        self.normalized_to_value(x)\n        for x in np.linspace(0, 1, num=size, endpoint=include_endpoint)\n    ]\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter.to_float","title":"to_float","text":"<pre><code>to_float() -&gt; FloatParameter\n</code></pre> <p>Convert the numerical hyperparameter to a float hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_float(self) -&gt; FloatParameter:\n    \"\"\"Convert the numerical hyperparameter to a float hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import FloatParameter\n\n    float_hp = FloatParameter(\n        lower=float(self.lower),\n        upper=float(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=float(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    float_hp.set_value(float(self.value) if self.value is not None else None)\n    return float_hp\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter.to_integer","title":"to_integer","text":"<pre><code>to_integer() -&gt; IntegerParameter\n</code></pre> <p>Convert the numerical hyperparameter to an integer hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_integer(self) -&gt; IntegerParameter:\n    \"\"\"Convert the numerical hyperparameter to an integer hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import IntegerParameter\n\n    as_int = lambda x: int(np.rint(x))\n\n    int_hp = IntegerParameter(\n        lower=as_int(self.lower),\n        upper=as_int(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=as_int(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    int_hp.set_value(as_int(self.value) if self.value is not None else None)\n    return int_hp\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/","title":"Numerical","text":""},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical","title":"neps.search_spaces.hyperparameters.numerical","text":"<p>The <code>NumericalParameter</code> is a <code>Parameter</code> that represents a numerical range.</p> <p>The two primary numerical hyperparameters are:</p> <ul> <li><code>FloatParameter</code> for continuous     float values.</li> <li><code>IntegerParameter</code> for discrete     integer values.</li> </ul> <p>The <code>NumericalParameter</code> is a base class for both of these hyperparameters, and includes methods from both <code>ParameterWithPrior</code>, allowing you to set a confidence along with a <code>.default</code> that can be used with certain algorithms, as well as <code>MutatableParameter</code>, which allows for <code>mutate()</code> and <code>crossover()</code> operations.</p>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter","title":"NumericalParameter","text":"<pre><code>NumericalParameter(\n    lower: T,\n    upper: T,\n    *,\n    log: bool = False,\n    default: T | None,\n    is_fidelity: bool,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>ParameterWithPrior[T, T]</code>, <code>MutatableParameter</code></p> <p>A numerical hyperparameter is bounded by a lower and upper value.</p> ATTRIBUTE DESCRIPTION <code>lower</code> <p>The lower bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>upper</code> <p>The upper bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>log</code> <p>Whether the hyperparameter is in log space.</p> <p> TYPE: <code>bool</code> </p> <code>log_value</code> <p>The log value of the hyperparameter, if <code>log=True</code>.</p> <p> TYPE: <code>float | None</code> </p> <code>log_bounds</code> <p>The log bounds of the hyperparameter, if <code>log=True</code>.</p> <p> TYPE: <code>tuple[float, float] | None</code> </p> <code>log_default</code> <p>The log default value of the hyperparameter, if <code>log=True</code> and a <code>default</code> is set.</p> <p> TYPE: <code>float | None</code> </p> <code>default_confidence_choice</code> <p>The default confidence choice.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> </p> <code>default_confidence_score</code> <p>The default confidence score.</p> <p> TYPE: <code>float</code> </p> <code>has_prior</code> <p>Whether the hyperparameter has a prior.</p> <p> TYPE: <code>bool</code> </p> PARAMETER DESCRIPTION <code>lower</code> <p>The lower bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>upper</code> <p>The upper bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>log</code> <p>Whether the hyperparameter is in log space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>The default value of the hyperparameter.</p> <p> TYPE: <code>T | None</code> </p> <code>is_fidelity</code> <p>Whether the hyperparameter is a fidelity parameter.</p> <p> TYPE: <code>bool</code> </p> <code>default_confidence</code> <p>The default confidence choice.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def __init__(\n    self,\n    lower: T,\n    upper: T,\n    *,\n    log: bool = False,\n    default: T | None,\n    is_fidelity: bool,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Initialize the numerical hyperparameter.\n\n    Args:\n        lower: The lower bound of the numerical hyperparameter.\n        upper: The upper bound of the numerical hyperparameter.\n        log: Whether the hyperparameter is in log space.\n        default: The default value of the hyperparameter.\n        is_fidelity: Whether the hyperparameter is a fidelity parameter.\n        default_confidence: The default confidence choice.\n    \"\"\"\n    super().__init__(value=None, default=default, is_fidelity=is_fidelity)  # type: ignore\n    _cls_name = self.__class__.__name__\n    if lower &gt;= upper:\n        raise ValueError(\n            f\"{_cls_name} parameter: bounds error (lower &gt;= upper). Actual values: \"\n            f\"lower={lower}, upper={upper}\"\n        )\n\n    if log and (lower &lt;= 0 or upper &lt;= 0):\n        raise ValueError(\n            f\"{_cls_name} parameter: bounds error (log scale cant have bounds &lt;= 0).\"\n            f\" Actual values: lower={lower}, upper={upper}\"\n        )\n\n    if default is not None and not lower &lt;= default &lt;= upper:\n        raise ValueError(\n            f\"Float parameter: default bounds error. Expected lower &lt;= default\"\n            f\" &lt;= upper, but got lower={lower}, default={default},\"\n            f\" upper={upper}\"\n        )\n\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        raise ValueError(\n            f\"{_cls_name} parameter: default confidence score error. Expected one of \"\n            f\"{list(self.DEFAULT_CONFIDENCE_SCORES.keys())}, but got \"\n            f\"{default_confidence}\"\n        )\n\n    # Validate 'log' and 'is_fidelity' types to prevent configuration errors\n    # from the YAML input\n    for param, value in {\"log\": log, \"is_fidelity\": is_fidelity}.items():\n        if not isinstance(value, bool):\n            raise TypeError(\n                f\"Expected '{param}' to be a boolean, but got type: \"\n                f\"{type(value).__name__}\"\n            )\n\n    self.lower: T = lower\n    self.upper: T = upper\n    self.log: bool = log\n    self.log_value: float | None = None\n    self.log_bounds: tuple[float, float] | None = None\n    self.log_default: float | None = None\n    if self.log:\n        self.log_bounds = (float(np.log(lower)), float(np.log(upper)))\n        self.log_default = (\n            float(np.log(self.default)) if self.default is not None else None\n        )\n\n    self.default_confidence_choice: Literal[\"low\", \"medium\", \"high\"] = (\n        default_confidence\n    )\n\n    self.default_confidence_score: float = self.DEFAULT_CONFIDENCE_SCORES[\n        default_confidence\n    ]\n    self.has_prior: bool = self.default is not None\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.clone","title":"clone  <code>abstractmethod</code>","text":"<pre><code>clone() -&gt; Self\n</code></pre> <p>Create a copy of the <code>Parameter</code>.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef clone(self) -&gt; Self:\n    \"\"\"Create a copy of the `Parameter`.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.grid","title":"grid","text":"<pre><code>grid(\n    *, size: int, include_endpoint: bool = True\n) -&gt; list[T]\n</code></pre> <p>Generate a grid of values for the numerical hyperparameter.</p> <p>Duplicates</p> <p>The grid may contain duplicates if the hyperparameter is an integer, for example if the lower bound is <code>0</code> and the upper bound is <code>10</code>, but <code>size=20</code>.</p> PARAMETER DESCRIPTION <code>size</code> <p>The number of values to generate.</p> <p> TYPE: <code>int</code> </p> <code>include_endpoint</code> <p>Whether to include the upper bound in the grid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list[T]</code> <p>A list of values for the numerical hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def grid(self, *, size: int, include_endpoint: bool = True) -&gt; list[T]:\n    \"\"\"Generate a grid of values for the numerical hyperparameter.\n\n    !!! note \"Duplicates\"\n\n        The grid may contain duplicates if the hyperparameter is an integer,\n        for example if the lower bound is `0` and the upper bound is `10`, but\n        `size=20`.\n\n    Args:\n        size: The number of values to generate.\n        include_endpoint: Whether to include the upper bound in the grid.\n\n    Returns:\n        A list of values for the numerical hyperparameter.\n    \"\"\"\n    return [\n        self.normalized_to_value(x)\n        for x in np.linspace(0, 1, num=size, endpoint=include_endpoint)\n    ]\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.normalized_to_value","title":"normalized_to_value  <code>abstractmethod</code>","text":"<pre><code>normalized_to_value(normalized_value: float) -&gt; ValueT\n</code></pre> <p>Convert a normalized value back to value in the defined hyperparameter range.</p> PARAMETER DESCRIPTION <code>normalized_value</code> <p>normalized value to convert.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef normalized_to_value(self, normalized_value: float) -&gt; ValueT:\n    \"\"\"Convert a normalized value back to value in the defined hyperparameter range.\n\n    Args:\n        normalized_value: normalized value to convert.\n\n    Returns:\n        The value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.sample_value","title":"sample_value  <code>abstractmethod</code>","text":"<pre><code>sample_value(*, user_priors: bool = False) -&gt; ValueT\n</code></pre> <p>Sample a new value.</p> <p>Similar to <code>Parameter.sample_value()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef sample_value(self, *, user_priors: bool = False) -&gt; ValueT:\n    \"\"\"Sample a new value.\n\n    Similar to\n    [`Parameter.sample_value()`][neps.search_spaces.Parameter.sample_value],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        The sampled value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.set_default","title":"set_default  <code>abstractmethod</code>","text":"<pre><code>set_default(default: ValueT | None) -&gt; None\n</code></pre> <p>Set the default value for the hyperparameter.</p> <p>The <code>default=</code> is used as a prior and used to inform algorithms about a decent default value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_default(self, default: ValueT | None) -&gt; None:\n    \"\"\"Set the default value for the hyperparameter.\n\n    The `default=` is used as a prior and used to inform\n    algorithms about a decent default value for the hyperparameter.\n\n    Args:\n        default: default value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.set_default_confidence_score","title":"set_default_confidence_score","text":"<pre><code>set_default_confidence_score(\n    default_confidence: str,\n) -&gt; None\n</code></pre> <p>Set the default confidence score for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default_confidence</code> <p>the choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the confidence score is not a valid choice.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def set_default_confidence_score(self, default_confidence: str) -&gt; None:\n    \"\"\"Set the default confidence score for the hyperparameter.\n\n    Args:\n        default_confidence: the choice of how confident any algorithm should\n            be in the default value being a good value.\n\n    Raises:\n        ValueError: if the confidence score is not a valid choice.\n    \"\"\"\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        cls_name = self.__class__.__name__\n        raise ValueError(\n            f\"Invalid default confidence score: {default_confidence}\"\n            f\" for {cls_name}. Expected one of:\"\n            f\" {list(self.DEFAULT_CONFIDENCE_SCORES.keys())}\"\n        )\n\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.set_value","title":"set_value  <code>abstractmethod</code>","text":"<pre><code>set_value(value: ValueT | None) -&gt; None\n</code></pre> <p>Set the value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_value(self, value: ValueT | None) -&gt; None:\n    \"\"\"Set the value for the hyperparameter.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.to_float","title":"to_float","text":"<pre><code>to_float() -&gt; FloatParameter\n</code></pre> <p>Convert the numerical hyperparameter to a float hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_float(self) -&gt; FloatParameter:\n    \"\"\"Convert the numerical hyperparameter to a float hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import FloatParameter\n\n    float_hp = FloatParameter(\n        lower=float(self.lower),\n        upper=float(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=float(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    float_hp.set_value(float(self.value) if self.value is not None else None)\n    return float_hp\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.to_integer","title":"to_integer","text":"<pre><code>to_integer() -&gt; IntegerParameter\n</code></pre> <p>Convert the numerical hyperparameter to an integer hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_integer(self) -&gt; IntegerParameter:\n    \"\"\"Convert the numerical hyperparameter to an integer hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import IntegerParameter\n\n    as_int = lambda x: int(np.rint(x))\n\n    int_hp = IntegerParameter(\n        lower=as_int(self.lower),\n        upper=as_int(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=as_int(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    int_hp.set_value(as_int(self.value) if self.value is not None else None)\n    return int_hp\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.value_to_normalized","title":"value_to_normalized  <code>abstractmethod</code>","text":"<pre><code>value_to_normalized(value: ValueT) -&gt; float\n</code></pre> <p>Convert a value to a normalized value.</p> <p>Normalization is different per hyperparameter type, but roughly refers to numeric values.</p> <ul> <li><code>(0, 1)</code> scaling in the case of     a <code>NumericalParameter</code>,</li> <li><code>{0.0, 1.0}</code> for a <code>ConstantParameter</code>,</li> <li><code>[0, 1, ..., n]</code> for a     <code>Categorical</code>.</li> </ul> PARAMETER DESCRIPTION <code>value</code> <p>value to convert.</p> <p> TYPE: <code>ValueT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The normalized value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef value_to_normalized(self, value: ValueT) -&gt; float:\n    \"\"\"Convert a value to a normalized value.\n\n    Normalization is different per hyperparameter type,\n    but roughly refers to numeric values.\n\n    * `(0, 1)` scaling in the case of\n        a [`NumericalParameter`][neps.search_spaces.NumericalParameter],\n    * `{0.0, 1.0}` for a [`ConstantParameter`][neps.search_spaces.ConstantParameter],\n    * `[0, 1, ..., n]` for a\n        [`Categorical`][neps.search_spaces.CategoricalParameter].\n\n    Args:\n        value: value to convert.\n\n    Returns:\n        The normalized value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/status/status/","title":"Status","text":""},{"location":"api/neps/status/status/#neps.status.status","title":"neps.status.status","text":"<p>Functions to get the status of a run and save the status to CSV files.</p>"},{"location":"api/neps/status/status/#neps.status.status.get_run_summary_csv","title":"get_run_summary_csv","text":"<pre><code>get_run_summary_csv(root_directory: str | Path) -&gt; None\n</code></pre> <p>Create CSV files summarizing the run data.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory of the NePS run.</p> <p> TYPE: <code>str | Path</code> </p> Source code in <code>neps/status/status.py</code> <pre><code>def get_run_summary_csv(root_directory: str | Path) -&gt; None:\n    \"\"\"Create CSV files summarizing the run data.\n\n    Args:\n        root_directory: The root directory of the NePS run.\n    \"\"\"\n    post_run_csv(root_directory=root_directory)\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.get_summary_dict","title":"get_summary_dict","text":"<pre><code>get_summary_dict(\n    root_directory: str | Path, *, add_details: bool = False\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a dict that summarizes a run.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory given to neps.run.</p> <p> TYPE: <code>str | Path</code> </p> <code>add_details</code> <p>If true, add detailed dicts for previous_results, pending_configs, and pending_configs_free.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>summary_dict</code> <p>Information summarizing a run</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>neps/status/status.py</code> <pre><code>def get_summary_dict(\n    root_directory: str | Path,\n    *,\n    add_details: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Create a dict that summarizes a run.\n\n    Args:\n        root_directory: The root directory given to neps.run.\n        add_details: If true, add detailed dicts for previous_results, pending_configs,\n            and pending_configs_free.\n\n    Returns:\n        summary_dict: Information summarizing a run\n    \"\"\"\n    root_directory = Path(root_directory)\n\n    # NOTE: We don't lock the shared state since we are just reading and don't need to\n    # make decisions based on the state\n    shared_state = SharedState(root_directory)\n    shared_state.update_from_disk()\n\n    trials_by_state = shared_state.trials_by_state()\n\n    evaluated: dict[ConfigID, _ConfigResultForStats] = {}\n\n    for trial in chain(\n        trials_by_state[Trial.State.SUCCESS],\n        trials_by_state[Trial.State.ERROR],\n    ):\n        assert trial.report is not None\n        _result_for_stats = _ConfigResultForStats(\n            trial.id,\n            trial.config,\n            \"error\" if isinstance(trial.report, ErrorReport) else trial.report.results,\n            trial.metadata,\n        )\n        evaluated[trial.id] = _result_for_stats\n\n    in_progress = {\n        trial.id: trial.config for trial in trials_by_state[Trial.State.IN_PROGRESS]\n    }\n    pending = {trial.id: trial.config for trial in trials_by_state[Trial.State.PENDING]}\n\n    summary: dict[str, Any] = {}\n\n    if add_details:\n        summary[\"previous_results\"] = evaluated\n        summary[\"pending_configs\"] = {**in_progress, **pending}\n        summary[\"pending_configs_free\"] = pending\n\n    summary[\"num_evaluated_configs\"] = len(evaluated)\n    summary[\"num_pending_configs\"] = len(in_progress) + len(pending)\n    summary[\"num_pending_configs_with_worker\"] = len(in_progress)\n\n    summary[\"best_loss\"] = float(\"inf\")\n    summary[\"best_config_id\"] = None\n    summary[\"best_config_metadata\"] = None\n    summary[\"best_config\"] = None\n    summary[\"num_error\"] = 0\n    for evaluation in evaluated.values():\n        if evaluation.result == \"error\":\n            summary[\"num_error\"] += 1\n        loss = evaluation.loss\n        if isinstance(loss, float) and loss &lt; summary[\"best_loss\"]:\n            summary[\"best_loss\"] = loss\n            summary[\"best_config\"] = evaluation.config\n            summary[\"best_config_id\"] = evaluation.id\n            summary[\"best_config_metadata\"] = evaluation.metadata\n\n    return summary\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.post_run_csv","title":"post_run_csv","text":"<pre><code>post_run_csv(root_directory: str | Path) -&gt; None\n</code></pre> <p>Create CSV files summarizing the run data.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory of the NePS run.</p> <p> TYPE: <code>str | Path</code> </p> Source code in <code>neps/status/status.py</code> <pre><code>def post_run_csv(root_directory: str | Path) -&gt; None:\n    \"\"\"Create CSV files summarizing the run data.\n\n    Args:\n        root_directory: The root directory of the NePS run.\n    \"\"\"\n    csv_config_data, csv_rundata, csv_locker = _initiate_summary_csv(root_directory)\n\n    df_config_data, df_run_data = _get_dataframes_from_summary(\n        root_directory,\n        include_metadatas=True,\n        include_results=True,\n        include_configs=True,\n    )\n\n    _save_data_to_csv(\n        csv_config_data,\n        csv_rundata,\n        csv_locker,\n        df_config_data,\n        df_run_data,\n    )\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.status","title":"status","text":"<pre><code>status(\n    root_directory: str | Path,\n    *,\n    best_losses: bool = False,\n    best_configs: bool = False,\n    all_configs: bool = False,\n    print_summary: bool = True\n) -&gt; tuple[\n    dict[str, _ConfigResultForStats], dict[str, SearchSpace]\n]\n</code></pre> <p>Print status information of a neps run and return results.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory given to neps.run.</p> <p> TYPE: <code>str | Path</code> </p> <code>best_losses</code> <p>If true, show the trajectory of the best loss across evaluations</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>best_configs</code> <p>If true, show the trajectory of the best configs and their losses across evaluations</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>all_configs</code> <p>If true, show all configs and their losses</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>print_summary</code> <p>If true, print a summary of the current run state</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>previous_results</code> <p>Already evaluated configurations and results. pending_configs: Configs that have been sampled, but have not finished evaluating</p> <p> TYPE: <code>tuple[dict[str, _ConfigResultForStats], dict[str, SearchSpace]]</code> </p> Source code in <code>neps/status/status.py</code> <pre><code>def status(\n    root_directory: str | Path,\n    *,\n    best_losses: bool = False,\n    best_configs: bool = False,\n    all_configs: bool = False,\n    print_summary: bool = True,\n) -&gt; tuple[dict[str, _ConfigResultForStats], dict[str, SearchSpace]]:\n    \"\"\"Print status information of a neps run and return results.\n\n    Args:\n        root_directory: The root directory given to neps.run.\n        best_losses: If true, show the trajectory of the best loss across evaluations\n        best_configs: If true, show the trajectory of the best configs and their losses\n            across evaluations\n        all_configs: If true, show all configs and their losses\n        print_summary: If true, print a summary of the current run state\n\n    Returns:\n        previous_results: Already evaluated configurations and results.\n        pending_configs: Configs that have been sampled, but have not finished evaluating\n    \"\"\"\n    root_directory = Path(root_directory)\n    summary = get_summary_dict(root_directory, add_details=True)\n\n    if print_summary:\n        print(f\"#Evaluated configs: {summary['num_evaluated_configs']}\")\n        print(f\"#Pending configs: {summary['num_pending_configs']}\")\n        print(\n            f\"#Pending configs with worker: {summary['num_pending_configs_with_worker']}\",\n        )\n\n        print(f\"#Crashed configs: {summary['num_error']}\")\n\n        if len(summary[\"previous_results\"]) == 0:\n            return summary[\"previous_results\"], summary[\"pending_configs\"]\n\n        print()\n        print(f\"Best loss: {summary['best_loss']}\")\n        print(f\"Best config id: {summary['best_config_id']}\")\n        print(f\"Best config: {summary['best_config']}\")\n\n        if best_losses:\n            print()\n            print(\"Best loss across evaluations:\")\n            best_loss_trajectory = root_directory / \"best_loss_trajectory.txt\"\n            print(best_loss_trajectory.read_text(encoding=\"utf-8\"))\n\n        if best_configs:\n            print()\n            print(\"Best configs and their losses across evaluations:\")\n            print(79 * \"-\")\n            best_loss_config = root_directory / \"best_loss_with_config_trajectory.txt\"\n            print(best_loss_config.read_text(encoding=\"utf-8\"))\n\n        if all_configs:\n            print()\n            print(\"All evaluated configs and their losses:\")\n            print(79 * \"-\")\n            all_loss_config = root_directory / \"all_losses_and_configs.txt\"\n            print(all_loss_config.read_text(encoding=\"utf-8\"))\n\n    return summary[\"previous_results\"], summary[\"pending_configs\"]\n</code></pre>"},{"location":"api/neps/utils/common/","title":"Common","text":""},{"location":"api/neps/utils/common/#neps.utils.common","title":"neps.utils.common","text":"<p>Common utility functions used across the library.</p>"},{"location":"api/neps/utils/common/#neps.utils.common.MissingDependencyError","title":"MissingDependencyError","text":"<pre><code>MissingDependencyError(\n    dep: str, cause: Exception, *args: Any\n)\n</code></pre> <p>               Bases: <code>ImportError</code></p> <p>Raise when a dependency is missing for an optional feature.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def __init__(self, dep: str, cause: Exception, *args: Any):\n    \"\"\"Initialize the error with the missing dependency and the original error.\"\"\"\n    super().__init__(dep, cause, *args)\n    self.dep = dep\n    self.__cause__ = cause  # This is what `raise a from b` does\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.filter_instances","title":"filter_instances","text":"<pre><code>filter_instances(\n    itr: Iterable[Any], *types: type\n) -&gt; list[Any]\n</code></pre> <p>Filter instances of a collection by the given types.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def filter_instances(itr: Iterable[Any], *types: type) -&gt; list[Any]:\n    \"\"\"Filter instances of a collection by the given types.\"\"\"\n    return [el for el in itr if isinstance(el, types)]\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.get_initial_directory","title":"get_initial_directory","text":"<pre><code>get_initial_directory(\n    pipeline_directory: Path | str | None = None,\n) -&gt; Path\n</code></pre> <p>Find the initial directory based on its existence and the presence of the \"previous_config.id\" file.</p> PARAMETER DESCRIPTION <code>pipeline_directory</code> <p>The current config directory.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>The initial directory.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def get_initial_directory(pipeline_directory: Path | str | None = None) -&gt; Path:\n    \"\"\"Find the initial directory based on its existence and the presence of\n    the \"previous_config.id\" file.\n\n    Args:\n        pipeline_directory: The current config directory.\n\n    Returns:\n        The initial directory.\n    \"\"\"\n    if pipeline_directory is not None:\n        pipeline_directory = Path(pipeline_directory)\n    else:\n        trial = get_in_progress_trial()\n        if trial is None:\n            raise ValueError(\n                \"No current trial was found to get the initial directory! This should not\"\n                \" happen. Please report this issue and in the meantime you may provide\"\n                \" a directory manually.\"\n            )\n        pipeline_directory = trial.pipeline_dir\n\n    # TODO(eddiebergman): Can we just make this a method of the Trial class somehow?\n    # This relies on the fact it's always called \"previous_config.id\" which could subtly\n    # break, if it were to be updated.\n\n    # Recursively find the initial directory\n    current_pipeline_directory = pipeline_directory\n    while True:\n        previous_pipeline_directory_id = current_pipeline_directory / \"previous_config.id\"\n        if not previous_pipeline_directory_id.exists():\n            # Initial directory found\n            return pipeline_directory\n\n        optim_result_dir = pipeline_directory.parent\n        with previous_pipeline_directory_id.open(\"r\") as config_id_file:\n            config_id = config_id_file.read()\n\n        current_pipeline_directory = optim_result_dir / f\"config_{config_id}\"\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.get_searcher_data","title":"get_searcher_data","text":"<pre><code>get_searcher_data(\n    searcher: str | Path,\n    *,\n    loading_custom_searcher: bool = False\n) -&gt; tuple[dict[str, Any], str]\n</code></pre> <p>Returns the data from the YAML file associated with the specified searcher.</p> PARAMETER DESCRIPTION <code>searcher</code> <p>The name of the searcher.</p> <p> TYPE: <code>str | Path</code> </p> <code>loading_custom_searcher</code> <p>Flag if searcher contains a custom yaml</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[dict[str, Any], str]</code> <p>The content of the YAML file and searcher name.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def get_searcher_data(\n    searcher: str | Path, *, loading_custom_searcher: bool = False\n) -&gt; tuple[dict[str, Any], str]:\n    \"\"\"Returns the data from the YAML file associated with the specified searcher.\n\n    Args:\n        searcher: The name of the searcher.\n        loading_custom_searcher: Flag if searcher contains a custom yaml\n\n    Returns:\n        The content of the YAML file and searcher name.\n    \"\"\"\n    if loading_custom_searcher:\n        user_yaml_path = Path(searcher).with_suffix(\".yaml\")\n\n        if not user_yaml_path.exists():\n            raise FileNotFoundError(\n                \"Failed to get info for searcher from user-defined YAML file. \"\n                f\"File '{searcher}.yaml' does not exist at '{user_yaml_path}'\"\n            )\n\n        with user_yaml_path.open(\"r\") as file:\n            data = yaml.safe_load(file)\n\n        file_name = user_yaml_path.stem\n        searcher = data.pop(\"name\", file_name)\n\n    else:\n        # TODO(eddiebergman): This is a bad idea as it relies on folder structure to be\n        # correct, we should either have a dedicated resource folder or at least have\n        # this defined as a constant somewhere, incase we access elsewhere.\n        # Seems like we could just include this as a method on `SearcherConfigs` class.\n        # TODO(eddiebergman): Need to make sure that these yaml files are actually\n        # included in a source dist when published to PyPI.\n\n        # This is pointing to yaml file directory elsewhere in the source code.\n        resource_path = (\n            Path(__file__).parent.parent.absolute()\n            / \"optimizers\"\n            / \"default_searchers\"\n            / searcher\n        ).with_suffix(\".yaml\")\n\n        from neps.optimizers.info import SearcherConfigs\n\n        searchers = SearcherConfigs.get_searchers()\n\n        if not resource_path.exists():\n            raise FileNotFoundError(\n                f\"Searcher '{searcher}' not in:\\n{', '.join(searchers)}\"\n            )\n\n        with resource_path.open() as file:\n            data = yaml.safe_load(file)\n\n    return data, searcher  # type: ignore\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.get_value","title":"get_value","text":"<pre><code>get_value(obj: Any) -&gt; Any\n</code></pre> <p>Honestly, don't know why you would use this. Please try not to.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def get_value(obj: Any) -&gt; Any:\n    \"\"\"Honestly, don't know why you would use this. Please try not to.\"\"\"\n    if obj is None:\n        return None\n    if isinstance(obj, (str, int, float, bool)):\n        return obj\n    if isinstance(obj, dict):\n        return {key: get_value(value) for key, value in obj.items()}\n    if isinstance(obj, list):\n        return [get_value(item) for item in obj]\n\n    return obj.__name__\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.has_instance","title":"has_instance","text":"<pre><code>has_instance(itr: Iterable[Any], *types: type) -&gt; bool\n</code></pre> <p>Check if any instance in the collection is of the given types.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def has_instance(itr: Iterable[Any], *types: type) -&gt; bool:\n    \"\"\"Check if any instance in the collection is of the given types.\"\"\"\n    return any(isinstance(el, types) for el in itr)\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.instance_from_map","title":"instance_from_map","text":"<pre><code>instance_from_map(\n    mapping: dict[str, Any],\n    request: str | list | tuple | type,\n    name: str = \"mapping\",\n    *,\n    allow_any: bool = True,\n    as_class: bool = False,\n    kwargs: dict | None = None\n) -&gt; Any\n</code></pre> <p>Get an instance of an class from a mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Mapping from string keys to classes or instances</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>request</code> <p>A key from the mapping. If allow_any is True, could also be an object or a class, to use a custom object.</p> <p> TYPE: <code>str | list | tuple | type</code> </p> <code>name</code> <p>Name of the mapping used in error messages</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mapping'</code> </p> <code>allow_any</code> <p>If set to True, allows using custom classes/objects.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_class</code> <p>If the class should be returned without beeing instanciated</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Arguments used for the new instance, if created. Its purpose is to serve at default arguments if the user doesn't built the object.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the request is invalid (not a string if allow_any is False), or invalid key.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def instance_from_map(  # noqa: C901, PLR0912\n    mapping: dict[str, Any],\n    request: str | list | tuple | type,\n    name: str = \"mapping\",\n    *,\n    allow_any: bool = True,\n    as_class: bool = False,\n    kwargs: dict | None = None,\n) -&gt; Any:\n    \"\"\"Get an instance of an class from a mapping.\n\n    Arguments:\n        mapping: Mapping from string keys to classes or instances\n        request: A key from the mapping. If allow_any is True, could also be an\n            object or a class, to use a custom object.\n        name: Name of the mapping used in error messages\n        allow_any: If set to True, allows using custom classes/objects.\n        as_class: If the class should be returned without beeing instanciated\n        kwargs: Arguments used for the new instance, if created. Its purpose is\n            to serve at default arguments if the user doesn't built the object.\n\n    Raises:\n        ValueError: if the request is invalid (not a string if allow_any is False),\n            or invalid key.\n    \"\"\"\n    # Split arguments of the form (request, kwargs)\n    args_dict = kwargs or {}\n    if isinstance(request, Sequence) and not isinstance(request, str):\n        if len(request) != 2:\n            raise ValueError(\n                \"When building an instance and specifying arguments, \"\n                \"you should give a pair (class, arguments)\"\n            )\n        request, req_args_dict = request\n\n        if not isinstance(req_args_dict, Mapping):\n            raise ValueError(\"The arguments should be given as a dictionary\")\n\n        args_dict = {**args_dict, **req_args_dict}\n\n    # Then, get the class/instance from the request\n    if isinstance(request, str):\n        if request not in mapping:\n            raise ValueError(f\"{request} doesn't exists for {name}\")\n\n        instance = mapping[request]\n    elif allow_any:\n        instance = request\n    else:\n        raise ValueError(f\"Object {request} invalid key for {name}\")\n\n    if isinstance(instance, MissingDependencyError):\n        raise instance\n\n    # Check if the request is a class if it is mandatory\n    if (args_dict or as_class) and not is_partial_class(instance):\n        raise ValueError(\n            f\"{instance} is not a class and can't be used with additional arguments\"\n        )\n\n    # Give the arguments to the class\n    if args_dict:\n        instance = partial(instance, **args_dict)\n\n    if as_class:\n        return instance\n\n    if is_partial_class(instance):\n        try:\n            instance = instance()\n        except TypeError as e:\n            raise TypeError(f\"{e} when calling {instance} with {args_dict}\") from e\n\n    return instance\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.is_partial_class","title":"is_partial_class","text":"<pre><code>is_partial_class(obj: Any) -&gt; bool\n</code></pre> <p>Check if the object is a (partial) class, or an instance.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def is_partial_class(obj: Any) -&gt; bool:\n    \"\"\"Check if the object is a (partial) class, or an instance.\"\"\"\n    if isinstance(obj, partial):\n        obj = obj.func\n    return inspect.isclass(obj)\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    model: Module | None = None,\n    optimizer: Optimizer | None = None,\n) -&gt; dict | None\n</code></pre> <p>Load a checkpoint and return the model state_dict and checkpoint values.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Directory where the checkpoint is located.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_name</code> <p>The name of the checkpoint file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'checkpoint'</code> </p> <code>model</code> <p>The PyTorch model to load.</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to load.</p> <p> TYPE: <code>Optimizer | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict | None</code> <p>A dictionary containing the checkpoint values, or None if the checkpoint file does not exist hence no checkpointing was previously done.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def load_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    model: torch.nn.Module | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n) -&gt; dict | None:\n    \"\"\"Load a checkpoint and return the model state_dict and checkpoint values.\n\n    Args:\n        directory: Directory where the checkpoint is located.\n        checkpoint_name: The name of the checkpoint file.\n        model: The PyTorch model to load.\n        optimizer: The optimizer to load.\n\n    Returns:\n        A dictionary containing the checkpoint values, or None if the checkpoint file\n        does not exist hence no checkpointing was previously done.\n    \"\"\"\n    if directory is None:\n        trial = get_in_progress_trial()\n\n        if trial is None:\n            return None\n\n        directory = trial.disk.previous_pipeline_dir\n        if directory is None:\n            return None\n\n    directory = Path(directory)\n    checkpoint_path = (directory / checkpoint_name).with_suffix(\".pth\")\n\n    if not checkpoint_path.exists():\n        return None\n\n    checkpoint = torch.load(checkpoint_path)\n\n    if model is not None and \"model_state_dict\" in checkpoint:\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n    return checkpoint  # type: ignore\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.load_lightning_checkpoint","title":"load_lightning_checkpoint","text":"<pre><code>load_lightning_checkpoint(\n    checkpoint_dir: Path | str,\n    previous_pipeline_directory: Path | str | None = None,\n) -&gt; tuple[Path, dict] | tuple[None, None]\n</code></pre> <p>Load the latest checkpoint file from the specified directory.</p> <p>This function searches for possible checkpoint files in the <code>checkpoint_dir</code> and loads the latest one if found. It returns a tuple with the checkpoint path and the loaded checkpoint data.</p> PARAMETER DESCRIPTION <code>previous_pipeline_directory</code> <p>The previous pipeline directory.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_dir</code> <p>The directory where checkpoint files are stored.</p> <p> TYPE: <code>Path | str</code> </p> RETURNS DESCRIPTION <code>tuple[Path, dict] | tuple[None, None]</code> <p>A tuple containing the checkpoint path (str) and the loaded checkpoint data (dict) or (None, None) if no checkpoint files are found in the directory.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def load_lightning_checkpoint(\n    checkpoint_dir: Path | str,\n    previous_pipeline_directory: Path | str | None = None,\n) -&gt; tuple[Path, dict] | tuple[None, None]:\n    \"\"\"Load the latest checkpoint file from the specified directory.\n\n    This function searches for possible checkpoint files in the `checkpoint_dir` and loads\n    the latest one if found. It returns a tuple with the checkpoint path and the loaded\n    checkpoint data.\n\n    Args:\n        previous_pipeline_directory: The previous pipeline directory.\n        checkpoint_dir: The directory where checkpoint files are stored.\n\n    Returns:\n        A tuple containing the checkpoint path (str) and the loaded checkpoint data (dict)\n        or (None, None) if no checkpoint files are found in the directory.\n    \"\"\"\n    if previous_pipeline_directory is None:\n        trial = get_in_progress_trial()\n        if trial is not None:\n            previous_pipeline_directory = trial.disk.previous_pipeline_dir\n\n        if previous_pipeline_directory is None:\n            return None, None\n\n    # Search for possible checkpoints to continue training\n    ckpt_files = list(Path(checkpoint_dir).glob(\"*.ckpt\"))\n\n    if len(ckpt_files) == 0:\n        raise FileNotFoundError(\n            \"No checkpoint files were located in the checkpoint directory\"\n        )\n\n    if len(ckpt_files) &gt; 1:\n        raise ValueError(\n            \"The number of checkpoint files is more than expected (1) \"\n            \"which makes if difficult to find the correct file.\"\n            \" Please save other checkpoint files in a different directory.\"\n        )\n\n    assert len(ckpt_files) == 1\n    checkpoint_path = ckpt_files[0]\n    checkpoint = torch.load(checkpoint_path)\n    return checkpoint_path, checkpoint\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    values_to_save: dict | None = None,\n    model: Module | None = None,\n    optimizer: Optimizer | None = None,\n) -&gt; None\n</code></pre> <p>Save a checkpoint including model state_dict and optimizer state_dict to a file.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Directory where the checkpoint will be saved.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> <code>values_to_save</code> <p>Additional values to save in the checkpoint.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The PyTorch model to save.</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to save.</p> <p> TYPE: <code>Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_name</code> <p>The name of the checkpoint file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'checkpoint'</code> </p> Source code in <code>neps/utils/common.py</code> <pre><code>def save_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    values_to_save: dict | None = None,\n    model: torch.nn.Module | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n) -&gt; None:\n    \"\"\"Save a checkpoint including model state_dict and optimizer state_dict to a file.\n\n    Args:\n        directory: Directory where the checkpoint will be saved.\n        values_to_save: Additional values to save in the checkpoint.\n        model: The PyTorch model to save.\n        optimizer: The optimizer to save.\n        checkpoint_name: The name of the checkpoint file.\n    \"\"\"\n    if directory is None:\n        in_progress_trial = get_in_progress_trial()\n\n        if in_progress_trial is None:\n            raise ValueError(\n                \"No current trial was found to save the checkpoint! This should not\"\n                \" happen. Please report this issue and in the meantime you may provide a\"\n                \" directory manually.\"\n            )\n        directory = in_progress_trial.pipeline_dir\n\n    directory = Path(directory)\n    checkpoint_path = (directory / checkpoint_name).with_suffix(\".pth\")\n\n    saved_dict = {}\n\n    if model is not None:\n        saved_dict[\"model_state_dict\"] = model.state_dict()\n    if optimizer is not None:\n        saved_dict[\"optimizer_state_dict\"] = optimizer.state_dict()\n\n    if values_to_save is not None:\n        saved_dict.update(values_to_save)\n\n    torch.save(saved_dict, checkpoint_path)\n</code></pre>"},{"location":"api/neps/utils/data_loading/","title":"Data loading","text":""},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading","title":"neps.utils.data_loading","text":"<p>Utility functions for loading data from disk.</p>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.BestLossesDict","title":"BestLossesDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>Summary of the best losses over multiple seeds.</p>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.get_id_from_path","title":"get_id_from_path","text":"<pre><code>get_id_from_path(path: str | Path | None) -&gt; int | None\n</code></pre> <p>Extracts the id from the given path.</p> <p>The id is the last part of the path, which is a multiple digit number.</p> Note <p>I think this refers to task ids and not config ids!!!</p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def get_id_from_path(path: str | Path | None) -&gt; int | None:\n    \"\"\"Extracts the id from the given path.\n\n    The id is the last part of the path, which is a multiple digit number.\n\n    Note:\n        I think this refers to task ids and not config ids!!!\n    \"\"\"\n    if path is None:\n        return None\n    numbers = re.findall(r\"\\d+\", str(path))\n    if len(numbers) == 0:\n        return None\n\n    return int(numbers[-1])\n</code></pre>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.is_valid_dev_path","title":"is_valid_dev_path","text":"<pre><code>is_valid_dev_path(path: str | Path | None) -&gt; bool\n</code></pre> <p>Checks if the given path is a valid path to development stages.</p> <p>It follows the pattern task_00000/dev_00000, where 00000 is replaced by the task and development stage ids.</p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def is_valid_dev_path(path: str | Path | None) -&gt; bool:\n    \"\"\"Checks if the given path is a valid path to development stages.\n\n    It follows the pattern task_00000/dev_00000, where 00000 is replaced by the\n    task and development stage ids.\n    \"\"\"\n    if path is None:\n        return False\n\n    # TODO: Test for \\ and | in the path, not only any non-alphanumerical character.\n    #  Currently, false positives are possible.\n    #  This regex expression does not work: \".*task_\\d+[\\/\\\\]dev_\\d+\"\n    pattern = re.compile(r\".*task_\\d+\\Wdev_\\d+\")\n    return pattern.fullmatch(str(path)) is not None and Path(path).is_dir()\n</code></pre>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.is_valid_seed_path","title":"is_valid_seed_path","text":"<pre><code>is_valid_seed_path(path: str | Path | None) -&gt; bool\n</code></pre> <p>Checks if the given path is a valid path to a seed.</p> <p>It follows the pattern seed_00000, where 00000 is replaced by the seed.</p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def is_valid_seed_path(path: str | Path | None) -&gt; bool:\n    \"\"\"Checks if the given path is a valid path to a seed.\n\n    It follows the pattern seed_00000, where 00000 is replaced by the seed.\n    \"\"\"\n    if path is None:\n        return False\n    path = Path(path)\n\n    if not path.is_dir():\n        return False\n\n    return path.name.startswith(\"seed\")\n</code></pre>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.is_valid_task_path","title":"is_valid_task_path","text":"<pre><code>is_valid_task_path(path: str | Path | None) -&gt; bool\n</code></pre> <p>Checks if the given path is a valid task path.</p> <p>It follows the pattern task_00000, where 00000 is replaced by the task id.</p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def is_valid_task_path(path: str | Path | None) -&gt; bool:\n    \"\"\"Checks if the given path is a valid task path.\n\n    It follows the pattern task_00000, where 00000 is replaced by the task id.\n    \"\"\"\n    if path is None:\n        return False\n\n    return (\n        _VALID_TASK_PATH_PATTERN.fullmatch(str(path)) is not None and Path(path).is_dir()\n    )\n</code></pre>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.read_tasks_and_dev_stages_from_disk","title":"read_tasks_and_dev_stages_from_disk","text":"<pre><code>read_tasks_and_dev_stages_from_disk(\n    paths: list[str | Path],\n) -&gt; dict[int, dict[int, dict[str, _ConfigResultForStats]]]\n</code></pre> <p>Reads the given tasks and dev stages from the disk.</p> PARAMETER DESCRIPTION <code>paths</code> <p>List of paths to the previous runs.</p> <p> TYPE: <code>list[str | Path]</code> </p> RETURNS DESCRIPTION <code>dict[int, dict[int, dict[str, _ConfigResultForStats]]]</code> <p>dict[task_id, dict[dev_stage, dict[config_id, ConfigResult]].</p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def read_tasks_and_dev_stages_from_disk(\n    paths: list[str | Path],\n) -&gt; dict[int, dict[int, dict[str, _ConfigResultForStats]]]:\n    \"\"\"Reads the given tasks and dev stages from the disk.\n\n    Args:\n        paths: List of paths to the previous runs.\n\n    Returns:\n        dict[task_id, dict[dev_stage, dict[config_id, ConfigResult]].\n    \"\"\"\n    path_iter = chain.from_iterable(Path(path).iterdir() for path in paths)\n\n    results: dict[int, dict[int, dict[str, _ConfigResultForStats]]] = {}\n\n    for task_dir_path in path_iter:\n        if not is_valid_task_path(task_dir_path):\n            continue\n\n        task_id = get_id_from_path(task_dir_path)\n        if task_id is None:\n            continue\n\n        results[task_id] = {}\n\n        for dev_dir_path in task_dir_path.iterdir():\n            if not is_valid_dev_path(dev_dir_path):\n                continue\n\n            dev_id = get_id_from_path(dev_dir_path)\n            if dev_id is None:\n                continue\n\n            state = SharedState(Path(dev_dir_path))\n            state.update_from_disk()\n            trials_by_state = state.trials_by_state()\n\n            evaluated: dict[ConfigID, _ConfigResultForStats] = {}\n\n            for trial in chain(\n                trials_by_state[Trial.State.SUCCESS],\n                trials_by_state[Trial.State.ERROR],\n            ):\n                assert trial.report is not None\n                _result_for_stats = _ConfigResultForStats(\n                    trial.id,\n                    trial.config,\n                    \"error\"\n                    if isinstance(trial.report, ErrorReport)\n                    else trial.report.results,\n                    trial.metadata,\n                )\n                evaluated[trial.id] = _result_for_stats\n\n            results[task_id][dev_id] = evaluated\n\n    return results\n</code></pre>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.read_user_prior_results_from_disk","title":"read_user_prior_results_from_disk","text":"<pre><code>read_user_prior_results_from_disk(\n    path: str | Path,\n) -&gt; dict[str, dict[str, _ConfigResultForStats]]\n</code></pre> <p>Reads the user prior results from the disk.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to the user prior results.</p> <p> TYPE: <code>str | Path</code> </p> RETURNS DESCRIPTION <code>dict[str, dict[str, _ConfigResultForStats]]</code> <p>dict[prior_dir_name, dict[config_id, ConfigResult]].</p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def read_user_prior_results_from_disk(\n    path: str | Path,\n) -&gt; dict[str, dict[str, _ConfigResultForStats]]:\n    \"\"\"Reads the user prior results from the disk.\n\n    Args:\n        path: Path to the user prior results.\n\n    Returns:\n        dict[prior_dir_name, dict[config_id, ConfigResult]].\n    \"\"\"\n    path = Path(path)\n    if not path.is_dir():\n        raise ValueError(f\"Path '{path}' is not a directory.\")\n\n    results = {}\n    for prior_dir in path.iterdir():\n        if not prior_dir.is_dir():\n            continue\n\n        state = SharedState(prior_dir)\n        with state.sync(lock=False):\n            evaluated: dict[ConfigID, _ConfigResultForStats] = {}\n            trials_by_state = state.trials_by_state()\n\n            for trial in chain(\n                trials_by_state[Trial.State.SUCCESS],\n                trials_by_state[Trial.State.ERROR],\n            ):\n                assert trial.report is not None\n                _result_for_stats = _ConfigResultForStats(\n                    trial.id,\n                    trial.config,\n                    \"error\"\n                    if isinstance(trial.report, ErrorReport)\n                    else trial.report.results,\n                    trial.metadata,\n                )\n                evaluated[trial.id] = _result_for_stats\n\n            results[prior_dir.name] = evaluated\n\n    return results\n</code></pre>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.summarize_results","title":"summarize_results","text":"<pre><code>summarize_results(\n    working_dir: str | Path,\n    final_task_id: int | None = None,\n    final_dev_id: int | None = None,\n    sub_dir: str = \"\",\n    *,\n    write_to_file: bool = True\n) -&gt; BestLossesDict\n</code></pre> <p>Summarizes the results of the given working directory.</p> <p>This includes runs over multiple seeds. The results are saved in the working directory.</p> PARAMETER DESCRIPTION <code>working_dir</code> <p>path to the working directory that contains directories for all seeds</p> <p> TYPE: <code>str | Path</code> </p> <code>final_task_id</code> <p>id of the tasks whose results should be summarized. If None, all tasks are summarized.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>final_dev_id</code> <p>if of the development stage whose results should be summarized. If None, all development stages are summarized.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>sub_dir</code> <p>subdirectory to look into for specific seeds. * If subdir is provided: <code>working_dir/something/&lt;subdir&gt;</code> * Otherwise: <code>working_dir/something</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>write_to_file</code> <p>if True, the results are written to a file in the working directory, using the latest taks and dev stage ids. <code>summary_task_&lt;task_id&gt;_dev_&lt;dev_id&gt;.yaml</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def summarize_results(  # noqa: C901\n    working_dir: str | Path,\n    final_task_id: int | None = None,\n    final_dev_id: int | None = None,\n    sub_dir: str = \"\",\n    *,\n    write_to_file: bool = True,\n) -&gt; BestLossesDict:\n    \"\"\"Summarizes the results of the given working directory.\n\n    This includes runs over multiple seeds.\n    The results are saved in the working directory.\n\n    Args:\n        working_dir: path to the working directory that contains directories for all seeds\n        final_task_id: id of the tasks whose results should be summarized.\n            If None, all tasks are summarized.\n        final_dev_id: if of the development stage whose results should be summarized.\n            If None, all development stages are summarized.\n        sub_dir: subdirectory to look into for specific seeds.\n            * If subdir is provided: `working_dir/something/&lt;subdir&gt;`\n            * Otherwise: `working_dir/something`\n        write_to_file: if True, the results are written to a file in the working\n            directory, using the latest taks and dev stage ids.\n            `summary_task_&lt;task_id&gt;_dev_&lt;dev_id&gt;.yaml`\n    \"\"\"\n    working_dir = Path(working_dir)\n\n    best_losses = []\n    for seed_dir in working_dir.iterdir():\n        if not is_valid_seed_path(seed_dir):\n            continue\n\n        if sub_dir:\n            seed_dir = seed_dir / sub_dir  # noqa: PLW2901\n\n        final_results: dict[ConfigID, _ConfigResultForStats]\n        if final_task_id is not None and final_dev_id is not None:\n            results = read_tasks_and_dev_stages_from_disk([seed_dir])\n\n            # TODO(unknown): only use IDs if provided\n            final_results = results[final_task_id][final_dev_id]\n        else:\n            state = SharedState(Path(seed_dir))\n            with state.sync(lock=False):\n                trials_by_state = state.trials_by_state()\n\n                final_results = {}\n                for trial in chain(\n                    trials_by_state[Trial.State.SUCCESS],\n                    trials_by_state[Trial.State.ERROR],\n                ):\n                    assert trial.report is not None\n                    _result_for_stats = _ConfigResultForStats(\n                        trial.id,\n                        trial.config,\n                        \"error\"\n                        if isinstance(trial.report, ErrorReport)\n                        else trial.report.results,\n                        trial.metadata,\n                    )\n                    final_results[trial.id] = _result_for_stats\n\n        # This part is copied from neps.status()\n        best_loss: float = float(\"inf\")\n        num_error = 0\n        for _, evaluation in final_results.items():\n            if evaluation.result == \"error\":\n                num_error += 1\n            loss = _get_loss(evaluation.result, ignore_errors=True)\n            if isinstance(loss, float) and loss &lt; best_loss:\n                best_loss = loss\n\n        best_losses.append(best_loss)\n\n    if len(best_losses) == 0:\n        raise ValueError(f\"No results found in directort {working_dir}.\")\n\n    best_losses_dict = BestLossesDict(\n        best_loss_mean=float(np.mean(best_losses)),\n        best_loss_std=float(np.std(best_losses)),\n        best_loss_std_err=float(np.std(best_losses) / np.sqrt(np.size(best_losses))),\n        best_loss_min=float(np.min(best_losses)),\n        best_loss_max=float(np.max(best_losses)),\n        best_loss_median=float(np.median(best_losses)),\n        best_loss_quantile_25=float(np.quantile(best_losses, 0.25)),\n        best_loss_quantile_75=float(np.quantile(best_losses, 0.75)),\n    )\n\n    if write_to_file:\n        task_id_str = str(final_task_id).zfill(5)\n        dev_id_str = str(final_dev_id).zfill(5)\n        file_path = working_dir / (\"summary_task_\" + task_id_str + \"_dev_\" + dev_id_str)\n\n        with file_path.with_suffix(\".yaml\").open(\"w\") as f:\n            yaml.dump(best_losses_dict, f, default_flow_style=False)\n\n        with file_path.with_suffix(\".json\").open(\"w\") as f:\n            json.dump(best_losses_dict, f)\n\n    return best_losses_dict\n</code></pre>"},{"location":"api/neps/utils/data_loading/#neps.utils.data_loading.summarize_results_all_tasks_all_devs","title":"summarize_results_all_tasks_all_devs","text":"<pre><code>summarize_results_all_tasks_all_devs(\n    path: str | Path,\n    sub_dir: str = \"\",\n    file_name: str = \"summary\",\n    user_prior_dir: str | Path | None = None,\n) -&gt; Any\n</code></pre> <p>Summarizes the results of all tasks and all development stages.</p> <p>This includes runs overrmultiple seeds. The results are saved in the working directory.</p> Source code in <code>neps/utils/data_loading.py</code> <pre><code>def summarize_results_all_tasks_all_devs(\n    path: str | Path,\n    sub_dir: str = \"\",\n    file_name: str = \"summary\",\n    user_prior_dir: str | Path | None = None,\n) -&gt; Any:\n    \"\"\"Summarizes the results of all tasks and all development stages.\n\n    This includes runs overrmultiple seeds. The results are saved in\n    the working directory.\n    \"\"\"\n    # go into the first seed directory and read the tasks and dev stages\n    path = Path(path)\n    os.scandir(path)\n\n    # TODO(eddiebergman): Please see issue #80\n    for seed_dir in path.iterdir():\n        if not is_valid_seed_path(seed_dir):\n            continue\n\n        seed_dir_path = seed_dir / sub_dir if sub_dir else seed_dir\n        results = read_tasks_and_dev_stages_from_disk([seed_dir_path])\n        break\n    else:\n        raise ValueError(f\"No results found in directory {path}.\")\n\n    summary = {}\n    for task_id, task in results.items():\n        for dev_id, _ in task.items():\n            summary[(task_id, dev_id)] = summarize_results(\n                path,\n                final_task_id=task_id,\n                final_dev_id=dev_id,\n                sub_dir=sub_dir,\n                write_to_file=False,\n            )\n\n    summary_user_prior = {}\n    # TODO(eddiebergman): Please see issue #80, figure out what user_prior_dir is\n    if user_prior_dir is not None:\n        user_prior_dir = Path(user_prior_dir)\n\n        if sub_dir:\n            previously_inferred_path = os.path.join(sub_dir, str(user_prior_dir))  # noqa: PTH118\n            raise NotImplementedError(\n                \"Sorry, don't know what should have been done here but we now explicitly\"\n                \"raise instead of silently summarizing what would be a non-existant path\"\n                f\"before. Previously inferred path was: {previously_inferred_path}\"\n            )\n\n        user_prior_results = read_user_prior_results_from_disk(user_prior_dir)\n        for prior_name, _ in user_prior_results.items():\n            summary_user_prior[prior_name] = summarize_results(\n                working_dir=path,\n                sub_dir=str(user_prior_dir / prior_name),\n                write_to_file=False,\n            )\n\n    with (path / file_name).with_suffix(\".jsonl\").open(\"w\") as f:\n        # write jsonl file with one line per task and dev stage\n        for (task_id, dev_id), metrics in summary.items():\n            f.write(\n                json.dumps(\n                    {\"IDs\": {\"task_id\": task_id, \"dev_id\": dev_id}, \"metrics\": metrics}\n                )\n            )\n            f.write(\"\\n\")\n        for prior_name, metrics in summary_user_prior.items():\n            f.write(json.dumps({\"IDs\": {\"prior_name\": prior_name}, \"metrics\": metrics}))\n            f.write(\"\\n\")\n</code></pre>"},{"location":"api/neps/utils/files/","title":"Files","text":""},{"location":"api/neps/utils/files/#neps.utils.files","title":"neps.utils.files","text":"<p>Utilities for file operations.</p>"},{"location":"api/neps/utils/files/#neps.utils.files.deserialize","title":"deserialize","text":"<pre><code>deserialize(path: Path | str) -&gt; dict[str, Any]\n</code></pre> <p>Deserialize data from a yaml file.</p> Source code in <code>neps/utils/files.py</code> <pre><code>def deserialize(path: Path | str) -&gt; dict[str, Any]:\n    \"\"\"Deserialize data from a yaml file.\"\"\"\n    with Path(path).open(\"r\") as file_stream:\n        return yaml.full_load(file_stream)  # type: ignore\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.empty_file","title":"empty_file","text":"<pre><code>empty_file(file_path: Path) -&gt; bool\n</code></pre> <p>Check if a file does not exist, or if it does, if it is empty.</p> Source code in <code>neps/utils/files.py</code> <pre><code>def empty_file(file_path: Path) -&gt; bool:\n    \"\"\"Check if a file does not exist, or if it does, if it is empty.\"\"\"\n    return not file_path.exists() or file_path.stat().st_size &lt;= 0\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.serialize","title":"serialize","text":"<pre><code>serialize(\n    data: Any, path: Path | str, *, sort_keys: bool = True\n) -&gt; None\n</code></pre> <p>Serialize data to a yaml file.</p> Source code in <code>neps/utils/files.py</code> <pre><code>def serialize(data: Any, path: Path | str, *, sort_keys: bool = True) -&gt; None:\n    \"\"\"Serialize data to a yaml file.\"\"\"\n    data = _serializable_format(data)\n    path = Path(path)\n    with path.open(\"w\") as file_stream:\n        try:\n            return yaml.safe_dump(data, file_stream, sort_keys=sort_keys)\n        except yaml.representer.RepresenterError as e:\n            raise TypeError(\n                \"Could not serialize to yaml! The object \"\n                f\"{e.args[1]} of type {type(e.args[1])} is not.\"\n            ) from e\n</code></pre>"},{"location":"api/neps/utils/run_args/","title":"Run args","text":""},{"location":"api/neps/utils/run_args/#neps.utils.run_args","title":"neps.utils.run_args","text":"<p>This module provides utility functions for handling yaml content of run_args. It includes functions for loading and processing configurations.</p>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.check_double_reference","title":"check_double_reference","text":"<pre><code>check_double_reference(\n    func: Callable,\n    func_arguments: dict,\n    yaml_arguments: dict,\n) -&gt; None\n</code></pre> <p>Checks if no argument is defined both via function arguments and YAML.</p> PARAMETER DESCRIPTION <code>func</code> <p>The function to check arguments against.</p> <p> TYPE: <code>Callable</code> </p> <code>func_arguments</code> <p>A dictionary containing the provided arguments to the</p> <p> TYPE: <code>Dict</code> </p> <code>yaml_arguments</code> <p>A dictionary containing the arguments provided via a YAML</p> <p> TYPE: <code>Dict</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If any provided argument is defined both via function arguments and</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def check_double_reference(\n    func: Callable, func_arguments: dict, yaml_arguments: dict\n) -&gt; None:\n    \"\"\"Checks if no argument is defined both via function arguments and YAML.\n\n    Args:\n        func (Callable): The function to check arguments against.\n        func_arguments (Dict): A dictionary containing the provided arguments to the\n        function and their values.\n        yaml_arguments (Dict): A dictionary containing the arguments provided via a YAML\n        file.\n\n    Raises:\n        ValueError: If any provided argument is defined both via function arguments and\n        the YAML file.\n    \"\"\"\n    sig = inspect.signature(func)\n\n    for name, param in sig.parameters.items():\n        if param.default != func_arguments[name]:\n            if name == RUN_ARGS:\n                # Ignoring run_args argument\n                continue\n            if name in yaml_arguments:\n                raise ValueError(\n                    f\"Conflict for argument '{name}': Argument is defined both via \"\n                    f\"function arguments and YAML, which is not allowed.\"\n                )\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.check_essential_arguments","title":"check_essential_arguments","text":"<pre><code>check_essential_arguments(\n    run_pipeline: Callable | None,\n    root_directory: str | None,\n    pipeline_space: dict | None,\n    max_cost_total: int | None,\n    max_evaluation_total: int | None,\n    searcher: BaseOptimizer | None,\n    run_args: str | None,\n) -&gt; None\n</code></pre> <p>Validates essential NePS configuration arguments.</p> <p>Ensures 'run_pipeline', 'root_directory', 'pipeline_space', and either 'max_cost_total' or 'max_evaluation_total' are provided for NePS execution. Raises ValueError with missing argument details. Additionally, checks 'searcher' is a BaseOptimizer if 'pipeline_space' is absent.</p> PARAMETER DESCRIPTION <code>run_pipeline</code> <p>Function for the pipeline execution.</p> <p> TYPE: <code>Callable | None</code> </p> <code>root_directory</code> <p>Directory path for data storage.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_space</code> <p>search space for this run.</p> <p> TYPE: <code>dict | None</code> </p> <code>max_cost_total</code> <p>Max allowed total cost for experiments.</p> <p> TYPE: <code>int | None</code> </p> <code>max_evaluation_total</code> <p>Max allowed evaluations.</p> <p> TYPE: <code>int | None</code> </p> <code>searcher</code> <p>Optimizer for the configuration space.</p> <p> TYPE: <code>BaseOptimizer | None</code> </p> <code>run_args</code> <p>A YAML file containing the configuration settings.</p> <p> TYPE: <code>str | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Missing or invalid essential arguments.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def check_essential_arguments(\n    run_pipeline: Callable | None,\n    root_directory: str | None,\n    pipeline_space: dict | None,\n    max_cost_total: int | None,\n    max_evaluation_total: int | None,\n    searcher: BaseOptimizer | None,\n    run_args: str | None,\n) -&gt; None:\n    \"\"\"Validates essential NePS configuration arguments.\n\n    Ensures 'run_pipeline', 'root_directory', 'pipeline_space', and either\n    'max_cost_total' or 'max_evaluation_total' are provided for NePS execution.\n    Raises ValueError with missing argument details. Additionally, checks 'searcher'\n    is a BaseOptimizer if 'pipeline_space' is absent.\n\n    Args:\n        run_pipeline: Function for the pipeline execution.\n        root_directory (str): Directory path for data storage.\n        pipeline_space: search space for this run.\n        max_cost_total: Max allowed total cost for experiments.\n        max_evaluation_total: Max allowed evaluations.\n        searcher: Optimizer for the configuration space.\n        run_args: A YAML file containing the configuration settings.\n\n    Raises:\n        ValueError: Missing or invalid essential arguments.\n    \"\"\"\n    if not run_pipeline:\n        raise ValueError(\"'run_pipeline' is required but was not provided.\")\n    if not root_directory:\n        raise ValueError(\"'root_directory' is required but was not provided.\")\n    if not pipeline_space and (run_args or not isinstance(searcher, BaseOptimizer)):\n        # handling special case for searcher instance, in which user doesn't have to\n        # provide the search_space because it's the argument of the searcher.\n        raise ValueError(\"'pipeline_space' is required but was not provided.\")\n\n    if not max_evaluation_total and not max_cost_total:\n        raise ValueError(\n            \"'max_evaluation_total' or 'max_cost_total' is required but \"\n            \"both were not provided.\"\n        )\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.check_run_args","title":"check_run_args","text":"<pre><code>check_run_args(settings: dict) -&gt; None\n</code></pre> <p>Validates the types of NePS configuration settings.</p> <p>Checks that each setting's value type matches its expected type. Raises TypeError for type mismatches.</p> PARAMETER DESCRIPTION <code>settings</code> <p>NePS configuration settings.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>For mismatched setting value types.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def check_run_args(settings: dict) -&gt; None:\n    \"\"\"Validates the types of NePS configuration settings.\n\n    Checks that each setting's value type matches its expected type. Raises\n    TypeError for type mismatches.\n\n    Args:\n        settings (dict): NePS configuration settings.\n\n    Raises:\n        TypeError: For mismatched setting value types.\n    \"\"\"\n    # Mapping parameter names to their allowed types\n    # [task_id, development_stage_id, pre_load_hooks] require special handling of type,\n    # that's why they are not listed\n    expected_types = {\n        RUN_PIPELINE: Callable,\n        ROOT_DIRECTORY: str,\n        # TODO: Support CS.ConfigurationSpace for pipeline_space\n        PIPELINE_SPACE: (str, dict),\n        OVERWRITE_WORKING_DIRECTORY: bool,\n        POST_RUN_SUMMARY: bool,\n        MAX_EVALUATIONS_TOTAL: int,\n        MAX_COST_TOTAL: (int, float),\n        MAX_EVALUATIONS_PER_RUN: int,\n        CONTINUE_UNTIL_MAX_EVALUATION_COMPLETED: bool,\n        LOSS_VALUE_ON_ERROR: float,\n        COST_VALUE_ON_ERROR: float,\n        IGNORE_ERROR: bool,\n        SEARCHER_KWARGS: dict,\n    }\n    for param, value in settings.items():\n        if param in (DEVELOPMENT_STAGE_ID, TASK_ID):\n            # this argument can be Any\n            continue\n        elif param == PRE_LOAD_HOOKS:  # noqa: RET507\n            # check if all items in pre_load_hooks are callable objects\n            if not all(callable(item) for item in value):\n                raise TypeError(\"All items in 'pre_load_hooks' must be callable.\")\n        elif param == SEARCHER:\n            if not (isinstance(value, (str, dict)) or issubclass(value, BaseOptimizer)):\n                raise TypeError(\n                    \"Parameter 'searcher' must be a string or a class that is a subclass \"\n                    \"of BaseOptimizer.\"\n                )\n        else:\n            try:\n                expected_type = expected_types[param]\n            except KeyError as e:\n                raise KeyError(f\"{param} is not a valid argument of neps\") from e\n            if not isinstance(value, expected_type):  # type: ignore\n                raise TypeError(\n                    f\"Parameter '{param}' expects a value of type {expected_type}, got \"\n                    f\"{type(value)} instead.\"\n                )\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.config_loader","title":"config_loader","text":"<pre><code>config_loader(path: str) -&gt; dict\n</code></pre> <p>Loads a YAML file and returns the contents under the 'run_args' key.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to the YAML file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Content of the yaml (dict)</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the file at 'path' does not exist.</p> <code>ValueError</code> <p>If the file is not a valid YAML.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def config_loader(path: str) -&gt; dict:\n    \"\"\"Loads a YAML file and returns the contents under the 'run_args' key.\n\n    Args:\n        path (str): Path to the YAML file.\n\n    Returns:\n        Content of the yaml (dict)\n\n    Raises:\n        FileNotFoundError: If the file at 'path' does not exist.\n        ValueError: If the file is not a valid YAML.\n    \"\"\"\n    try:\n        with open(path) as file:  # noqa: PTH123\n            config = yaml.safe_load(file)\n    except FileNotFoundError as e:\n        raise FileNotFoundError(\n            f\"The specified file was not found: '{path}'.\"\n            f\" Please make sure that the path is correct and \"\n            f\"try again.\"\n        ) from e\n    except yaml.YAMLError as e:\n        raise ValueError(f\"The file at {path} is not a valid YAML file.\") from e\n\n    return config\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.extract_leaf_keys","title":"extract_leaf_keys","text":"<pre><code>extract_leaf_keys(\n    d: dict, special_keys: dict | None = None\n) -&gt; tuple[dict, dict]\n</code></pre> <p>Recursive function to extract leaf keys and their values from a nested dictionary. Special keys (e.g.'run_pipeline') are also extracted if present and their corresponding values (dict) at any level in the nested structure.</p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary to extract values from.</p> <p> TYPE: <code>dict</code> </p> <code>special_keys</code> <p>A dictionary to store values of special keys.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[dict, dict]</code> <p>A tuple containing the leaf keys dictionary and the dictionary for special keys.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def extract_leaf_keys(d: dict, special_keys: dict | None = None) -&gt; tuple[dict, dict]:\n    \"\"\"Recursive function to extract leaf keys and their values from a nested dictionary.\n    Special keys (e.g.'run_pipeline') are also extracted if present\n    and their corresponding values (dict) at any level in the nested structure.\n\n    Args:\n        d (dict): The dictionary to extract values from.\n        special_keys (dict|None): A dictionary to store values of special keys.\n\n    Returns:\n        A tuple containing the leaf keys dictionary and the dictionary for\n        special keys.\n    \"\"\"\n    if special_keys is None:\n        special_keys = {\n            RUN_PIPELINE: None,\n            PRE_LOAD_HOOKS: None,\n            SEARCHER: None,\n            PIPELINE_SPACE: None,\n        }\n\n    leaf_keys = {}\n    for k, v in d.items():\n        if k in special_keys and v != \"None\":\n            special_keys[k] = v\n        elif isinstance(v, dict):\n            # Recursively call to explore nested dictionaries\n            nested_leaf_keys, _ = extract_leaf_keys(v, special_keys)\n            leaf_keys.update(nested_leaf_keys)\n        elif v is not None and v != \"None\":\n            leaf_keys[k] = v\n    return leaf_keys, special_keys\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.get_run_args_from_yaml","title":"get_run_args_from_yaml","text":"<pre><code>get_run_args_from_yaml(path: str) -&gt; dict\n</code></pre> <p>Load and validate NEPS run arguments from a specified YAML configuration file provided via run_args.</p> <p>This function reads a YAML file, extracts the arguments required by NePS, validates these arguments, and then returns them in a dictionary. It checks for the presence and validity of expected parameters, and distinctively handles more complex configurations, specifically those that are dictionaries(e.g. pipeline_space) or objects(e.g. run_pipeline) requiring loading.</p> PARAMETER DESCRIPTION <code>path</code> <p>The file path to the YAML configuration file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary of validated run arguments.</p> RAISES DESCRIPTION <code>KeyError</code> <p>If any parameter name is invalid.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def get_run_args_from_yaml(path: str) -&gt; dict:\n    \"\"\"Load and validate NEPS run arguments from a specified YAML configuration file\n    provided via run_args.\n\n    This function reads a YAML file, extracts the arguments required by NePS,\n    validates these arguments, and then returns them in a dictionary. It checks for the\n    presence and validity of expected parameters, and distinctively handles more complex\n    configurations, specifically those that are dictionaries(e.g. pipeline_space) or\n    objects(e.g. run_pipeline) requiring loading.\n\n    Args:\n        path (str): The file path to the YAML configuration file.\n\n    Returns:\n        A dictionary of validated run arguments.\n\n    Raises:\n        KeyError: If any parameter name is invalid.\n    \"\"\"\n    # Load the YAML configuration file\n    config = config_loader(path)\n\n    # Initialize an empty dictionary to hold the extracted settings\n    settings = {}\n\n    # List allowed NePS run arguments with simple types (e.g., string, int). Parameters\n    # like 'run_pipeline', 'preload_hooks', 'pipeline_space',\n    # and 'searcher' are excluded due to needing specialized processing.\n    expected_parameters = [\n        ROOT_DIRECTORY,\n        MAX_EVALUATIONS_TOTAL,\n        MAX_COST_TOTAL,\n        OVERWRITE_WORKING_DIRECTORY,\n        POST_RUN_SUMMARY,\n        DEVELOPMENT_STAGE_ID,\n        TASK_ID,\n        MAX_EVALUATIONS_PER_RUN,\n        CONTINUE_UNTIL_MAX_EVALUATION_COMPLETED,\n        LOSS_VALUE_ON_ERROR,\n        COST_VALUE_ON_ERROR,\n        IGNORE_ERROR,\n    ]\n\n    # Flatten the YAML file's structure to separate flat parameters (flat_config) and\n    # those needing special handling (special_configs).\n    flat_config, special_configs = extract_leaf_keys(config)\n\n    # Check if flatten dict (flat_config) just contains the expected parameters\n    for parameter, value in flat_config.items():\n        if parameter in expected_parameters:\n            settings[parameter] = value\n        else:\n            raise KeyError(\n                f\"Parameter '{parameter}' is not an argument of neps.run() \"\n                f\"provided via run_args.\"\n                f\"See here all valid arguments:\"\n                f\" {', '.join(expected_parameters)}, \"\n                f\"'run_pipeline', 'preload_hooks', 'pipeline_space'\"\n            )\n\n    # Process complex configurations (e.g., 'pipeline_space', 'searcher') and integrate\n    # them into 'settings'.\n    handle_special_argument_cases(settings, special_configs)\n\n    # check if all provided arguments have legal types\n    check_run_args(settings)\n\n    logger.debug(\n        f\"The 'run_args' arguments: {settings} are now extracted and type-tested from \"\n        f\"referenced YAML.\"\n    )\n\n    return settings\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.handle_special_argument_cases","title":"handle_special_argument_cases","text":"<pre><code>handle_special_argument_cases(\n    settings: dict, special_configs: dict\n) -&gt; None\n</code></pre> <p>Process and integrate special configuration cases into the 'settings' dictionary.</p> <p>This function updates 'settings' with values from 'special_configs'. It handles specific keys that require more complex processing, such as 'pipeline_space' and 'searcher', which may need to load a function/dict from paths. It also manages nested configurations like 'pre_load_hooks' which need individual processing or function loading.</p> PARAMETER DESCRIPTION <code>settings</code> <p>The dictionary to be updated with processed configurations.</p> <p> TYPE: <code>dict</code> </p> <code>special_configs</code> <p>A dictionary containing configuration keys and values                   that require special processing.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def handle_special_argument_cases(settings: dict, special_configs: dict) -&gt; None:\n    \"\"\"Process and integrate special configuration cases into the 'settings' dictionary.\n\n    This function updates 'settings' with values from 'special_configs'. It handles\n    specific keys that require more complex processing, such as 'pipeline_space' and\n    'searcher', which may need to load a function/dict from paths. It also manages nested\n    configurations like 'pre_load_hooks' which need individual processing or function\n    loading.\n\n    Args:\n        settings (dict): The dictionary to be updated with processed configurations.\n        special_configs (dict): A dictionary containing configuration keys and values\n                              that require special processing.\n\n    \"\"\"\n    # process special configs\n    process_run_pipeline(RUN_PIPELINE, special_configs, settings)\n    process_pipeline_space(PIPELINE_SPACE, special_configs, settings)\n    process_searcher(SEARCHER, special_configs, settings)\n\n    if special_configs[PRE_LOAD_HOOKS] is not None:\n        # Loads the pre_load_hooks functions and add them in a list to settings.\n        settings[PRE_LOAD_HOOKS] = load_hooks_from_config(special_configs[PRE_LOAD_HOOKS])\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.load_and_return_object","title":"load_and_return_object","text":"<pre><code>load_and_return_object(\n    module_path: str, object_name: str, key: str\n) -&gt; object\n</code></pre> <p>Dynamically loads an object from a given module file path.</p> <p>This function attempts to dynamically import an object by its name from a specified module path. If the initial import fails, it retries with a '.py' extension appended to the path.</p> PARAMETER DESCRIPTION <code>module_path</code> <p>File system path to the Python module.</p> <p> TYPE: <code>str</code> </p> <code>object_name</code> <p>Name of the object to import from the module.</p> <p> TYPE: <code>str</code> </p> <code>key</code> <p>Identifier for the argument causing the error, for enhanced error</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>object</code> <p>The imported object from the module.</p> <p> TYPE: <code>object</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If the module or object cannot be found, with a message detailing</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def load_and_return_object(module_path: str, object_name: str, key: str) -&gt; object:\n    \"\"\"Dynamically loads an object from a given module file path.\n\n    This function attempts to dynamically import an object by its name from a specified\n    module path. If the initial import fails, it retries with a '.py' extension appended\n    to the path.\n\n    Args:\n        module_path (str): File system path to the Python module.\n        object_name (str): Name of the object to import from the module.\n        key (str): Identifier for the argument causing the error, for enhanced error\n        feedback.\n\n    Returns:\n        object: The imported object from the module.\n\n    Raises:\n        ImportError: If the module or object cannot be found, with a message detailing\n        the issue.\n    \"\"\"\n\n    def import_object(path: str) -&gt; object | None:\n        try:\n            # Convert file system path to module path, removing '.py' if present.\n            module_name = (\n                path[:-3].replace(\"/\", \".\")\n                if path.endswith(\".py\")\n                else path.replace(\"/\", \".\")\n            )\n\n            # Dynamically import the module.\n            spec = importlib.util.spec_from_file_location(module_name, path)\n            if spec is None or spec.loader is None:\n                return None  # Failed to load module spec.\n            module = importlib.util.module_from_spec(spec)\n            sys.modules[module_name] = module\n            spec.loader.exec_module(module)\n\n            # Retrieve the object.\n            imported_object = getattr(module, object_name, None)\n            if imported_object is None:\n                return None  # Object not found in module.\n            return imported_object\n        except FileNotFoundError:\n            return None  # File not found.\n\n    # Attempt to import the object using the provided path.\n    imported_object = import_object(module_path)\n    if imported_object is None:\n        # If the object could not be imported, attempt again by appending '.py',\n        # if not already present.\n        if not module_path.endswith(\".py\"):\n            module_path += \".py\"\n            imported_object = import_object(module_path)\n\n        if imported_object is None:\n            raise ImportError(\n                f\"Failed to import '{object_name}' for argument '{key}'. \"\n                f\"Module path '{module_path}' not found or object does not \"\n                f\"exist.\"\n            )\n\n    return imported_object\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.load_hooks_from_config","title":"load_hooks_from_config","text":"<pre><code>load_hooks_from_config(pre_load_hooks_dict: dict) -&gt; list\n</code></pre> <p>Loads hook functions from a dictionary of configurations.</p> PARAMETER DESCRIPTION <code>pre_load_hooks_dict</code> <p>Dictionary with hook names as keys and paths as values</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>List</code> <p>List of loaded hook functions.</p> <p> TYPE: <code>list</code> </p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def load_hooks_from_config(pre_load_hooks_dict: dict) -&gt; list:\n    \"\"\"Loads hook functions from a dictionary of configurations.\n\n    Args:\n        pre_load_hooks_dict (Dict): Dictionary with hook names as keys and paths as values\n\n    Returns:\n        List: List of loaded hook functions.\n    \"\"\"\n    loaded_hooks = []\n    for name, path in pre_load_hooks_dict.items():\n        hook_func = load_and_return_object(path, name, PRE_LOAD_HOOKS)\n        loaded_hooks.append(hook_func)\n    return loaded_hooks\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.process_pipeline_space","title":"process_pipeline_space","text":"<pre><code>process_pipeline_space(\n    key: str, special_configs: dict, settings: dict\n) -&gt; None\n</code></pre> <p>Process or load the pipeline space configuration.</p> <p>This function checks if the given key exists in the <code>special_configs</code> dictionary. If it exists, it processes the associated value, which can be either a dictionary or a string. Based on the keys of the dictionary it decides if the pipeline_space have to be loaded or needs to be converted into a neps search_space structure. The processed pipeline space is then stored in the <code>settings</code> dictionary under the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to check in the <code>special_configs</code> dictionary.</p> <p> TYPE: <code>str</code> </p> <code>special_configs</code> <p>The dictionary containing special configuration values.</p> <p> TYPE: <code>dict</code> </p> <code>settings</code> <p>The dictionary where the processed pipeline space will be stored.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the value associated with the key is neither a string nor a</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def process_pipeline_space(key: str, special_configs: dict, settings: dict) -&gt; None:\n    \"\"\"Process or load the pipeline space configuration.\n\n    This function checks if the given key exists in the `special_configs` dictionary.\n    If it exists, it processes the associated value, which can be either a dictionary\n    or a string. Based on the keys of the dictionary it decides if the pipeline_space\n    have to be loaded or needs to be converted into a neps search_space structure.\n    The processed pipeline space is then stored in the `settings`\n    dictionary under the given key.\n\n    Args:\n        key (str): The key to check in the `special_configs` dictionary.\n        special_configs (dict): The dictionary containing special configuration values.\n        settings (dict): The dictionary where the processed pipeline space will be stored.\n\n    Raises:\n        TypeError: If the value associated with the key is neither a string nor a\n        dictionary.\n    \"\"\"\n    if special_configs.get(key) is not None:\n        pipeline_space = special_configs[key]\n        # Define the type of processed_pipeline_space to accommodate both situations\n        if isinstance(pipeline_space, dict):\n            # determine if dict contains path_loading or the actual search space\n            expected_keys = {\"path\", \"name\"}\n            actual_keys = set(pipeline_space.keys())\n            if expected_keys != actual_keys:\n                # pipeline_space directly defined in run_args yaml\n                processed_pipeline_space = pipeline_space_from_yaml(pipeline_space)\n            else:\n                # pipeline_space stored in a python dict, not using a yaml\n                processed_pipeline_space = load_and_return_object(\n                    pipeline_space[\"path\"], pipeline_space[\"name\"], key\n                )  # type: ignore\n        elif isinstance(pipeline_space, str):\n            # load yaml from path\n            processed_pipeline_space = pipeline_space_from_yaml(pipeline_space)\n        else:\n            raise TypeError(\n                f\"Value for {key} must be a string or a dictionary, \"\n                f\"but got {type(pipeline_space).__name__}.\"\n            )\n        settings[key] = processed_pipeline_space\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.process_run_pipeline","title":"process_run_pipeline","text":"<pre><code>process_run_pipeline(\n    key: str, special_configs: dict, settings: dict\n) -&gt; None\n</code></pre> <p>Processes the run pipeline configuration and updates the settings dictionary.</p> PARAMETER DESCRIPTION <code>key</code> <p>Key to look up in special_configs.</p> <p> TYPE: <code>str</code> </p> <code>special_configs</code> <p>Dictionary of special configurations.</p> <p> TYPE: <code>dict</code> </p> <code>settings</code> <p>Dictionary to update with the processed function.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>KeyError</code> <p>If required keys ('path' and 'name') are missing in the config.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def process_run_pipeline(key: str, special_configs: dict, settings: dict) -&gt; None:\n    \"\"\"Processes the run pipeline configuration and updates the settings dictionary.\n\n    Args:\n        key (str): Key to look up in special_configs.\n        special_configs (dict): Dictionary of special configurations.\n        settings (dict): Dictionary to update with the processed function.\n\n    Raises:\n        KeyError: If required keys ('path' and 'name') are missing in the config.\n    \"\"\"\n    if special_configs.get(key) is not None:\n        config = special_configs[key]\n        try:\n            func = load_and_return_object(config[\"path\"], config[\"name\"], key)\n            settings[key] = func\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing key for argument {key}: {e}. Expect 'path' \"\n                f\"and 'name' as keys when loading '{key}' \"\n                f\"from 'run_args'\"\n            ) from e\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.process_searcher","title":"process_searcher","text":"<pre><code>process_searcher(\n    key: str, special_configs: dict, settings: dict\n) -&gt; None\n</code></pre> <p>Processes the searcher configuration and updates the settings dictionary.</p> <p>Checks if the key exists in special_configs. If found, it processes the value based on its type. Updates settings with the processed searcher.</p> PARAMETER DESCRIPTION <code>key</code> <p>Key to look up in special_configs.</p> <p> TYPE: <code>str</code> </p> <code>special_configs</code> <p>Dictionary of special configurations.</p> <p> TYPE: <code>dict</code> </p> <code>settings</code> <p>Dictionary to update with the processed searcher.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the value for the key is neither a string, Path, nor a dictionary.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def process_searcher(key: str, special_configs: dict, settings: dict) -&gt; None:\n    \"\"\"Processes the searcher configuration and updates the settings dictionary.\n\n    Checks if the key exists in special_configs. If found, it processes the\n    value based on its type. Updates settings with the processed searcher.\n\n    Args:\n        key (str): Key to look up in special_configs.\n        special_configs (dict): Dictionary of special configurations.\n        settings (dict): Dictionary to update with the processed searcher.\n\n    Raises:\n        TypeError: If the value for the key is neither a string, Path, nor a dictionary.\n    \"\"\"\n    if special_configs.get(key) is not None:\n        searcher = special_configs[key]\n        if isinstance(searcher, dict):\n            # determine if dict contains path_loading or the actual searcher config\n            expected_keys = {\"path\", \"name\"}\n            actual_keys = set(searcher.keys())\n            if expected_keys.issubset(actual_keys):\n                path = searcher.pop(\"path\")\n                name = searcher.pop(\"name\")\n                settings[SEARCHER_KWARGS] = searcher\n                searcher = load_and_return_object(path, name, key)\n\n        elif isinstance(searcher, (str, Path)):\n            pass\n        else:\n            raise TypeError(\n                f\"Value for {key} must be a string or a dictionary, \"\n                f\"but got {type(searcher).__name__}.\"\n            )\n        settings[key] = searcher\n</code></pre>"},{"location":"api/neps/utils/types/","title":"Types","text":""},{"location":"api/neps/utils/types/#neps.utils.types","title":"neps.utils.types","text":"<p>Primitive types to be used in NePS or consumers of NePS.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.AttrDict","title":"AttrDict","text":"<pre><code>AttrDict(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>dict</code></p> <p>Dictionary that allows access to keys as attributes.</p> Source code in <code>neps/utils/types.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    \"\"\"Initialize like a dict.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.__dict__ = self\n</code></pre>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult","title":"ConfigResult  <code>dataclass</code>","text":"<pre><code>ConfigResult(\n    id: str,\n    config: SearchSpace,\n    result: ResultDict | ERROR,\n    metadata: dict,\n)\n</code></pre> <p>Primary class through which optimizers recieve results.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: SearchSpace\n</code></pre> <p>Configuration that was evaluated.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>Unique identifier for the configuration.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: dict\n</code></pre> <p>Any additional data to store with this config and result.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.result","title":"result  <code>instance-attribute</code>","text":"<pre><code>result: ResultDict | ERROR\n</code></pre> <p>Some dictionary of results.</p>"},{"location":"dev_docs/contributing/","title":"Introduction","text":""},{"location":"dev_docs/contributing/#getting-help","title":"Getting Help","text":"<p>Please use our github and raise an issue at: automl/neps</p>"},{"location":"dev_docs/contributing/#development-workflow","title":"Development Workflow","text":"<p>We use one main branch <code>master</code> and feature branches for development. We use pull requests to merge feature branches into <code>master</code>. Versions released to PyPI are tagged with a version number.</p> <p>Automatic checks are run on every pull request and on every commit to <code>master</code>.</p>"},{"location":"dev_docs/contributing/#installation","title":"Installation","text":"<p>There are three required steps and one optional:</p> <ol> <li>Optional: Install miniconda and create an environment</li> <li>Install poetry</li> <li>Install the neps package using poetry</li> <li>Activate pre-commit for the repository</li> </ol> <p>For instructions see below.</p>"},{"location":"dev_docs/contributing/#1-optional-install-miniconda-and-create-a-virtual-environment","title":"1. Optional: Install miniconda and create a virtual environment","text":"<p>To manage python versions install e.g., miniconda with</p> <pre><code>wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O install_miniconda.sh\nbash install_miniconda.sh -b -p $HOME/.conda  # Change to place of preference\nrm install_miniconda.sh\n</code></pre> <p>Consider running <code>~/.conda/bin/conda init</code> or <code>~/.conda/bin/conda init zsh</code> .</p> <p>Then finally create the environment and activate it</p> <pre><code>conda create -n neps python=3.10\nconda activate neps\n</code></pre>"},{"location":"dev_docs/contributing/#2-install-poetry","title":"2. Install poetry","text":"<p>First, install poetry, e.g., via</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n# or directly into your virtual env using `pip install poetry`\n</code></pre> <p>Then consider appending</p> <pre><code>export PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre> <p>to your <code>.zshrc</code> / <code>.bashrc</code> or alternatively simply running the export manually.</p>"},{"location":"dev_docs/contributing/#3-install-the-neps-package-using-poetry","title":"3. Install the neps Package Using poetry","text":"<p>Clone the repository, e.g.,</p> <pre><code>git clone https://github.com/automl/neps.git\ncd neps\n</code></pre> <p>Then, inside the main directory of neps run</p> <pre><code>poetry install\n</code></pre> <p>This will installthe neps package but also additional dev dependencies.</p>"},{"location":"dev_docs/contributing/#4-activate-pre-commit-for-the-repository","title":"4. Activate pre-commit for the repository","text":"<p>With the python environment used to install the neps package run in the main directory of neps</p> <pre><code>pre-commit install\n</code></pre> <p>This install a set of hooks that will run basic linting and type checking before every comment. If you ever need to unsinstall the hooks, you can do so with <code>pre-commit uninstall</code>. These mostly consist of <code>ruff</code> for formatting and linting and <code>mypy</code> for type checking.</p> <p>We highly recommend you install at least <code>ruff</code> either on command line, or in the editor of your choice, e.g. VSCode, PyCharm.</p>"},{"location":"dev_docs/contributing/#checks-and-tests","title":"Checks and Tests","text":"<p>We have setup checks and tests at several points in the development flow:</p> <ul> <li>At every commit we automatically run a suite of pre-commit hooks that perform static code analysis, autoformating, and sanity checks. This is setup during our installation process.</li> <li>At every commit / push locally running a minimal suite of integration tests is encouraged. The tests correspond directly to examples in neps_examples and only check for crash-causing errors.</li> <li>At every push all integration tests and regression tests are run automatically using github actions.</li> </ul>"},{"location":"dev_docs/contributing/#linting-ruff","title":"Linting (Ruff)","text":"<p>For linting we use <code>ruff</code> for checking code quality. You can install it locally and use it as so:</p> <pre><code>pip install ruff\nruff check --fix neps  # the --fix flag will try to fix issues it can automatically\n</code></pre> <p>This will also be run using <code>pre-commit</code> hooks.</p> <p>To ignore a rule for a specific line, you can add a comment with <code>ruff: disable</code> at the end of the line, e.g.</p> <pre><code>for x, y in zip(a, b):  # noqa: &lt;ERRCODE&gt;\n    pass\n</code></pre> <p>The configuration of <code>ruff</code> is in the <code>pyproject.toml</code> file and we refer you to the documentation if you require any changes to be made.</p> <p>There you can find the documentation for all of the rules employed.</p>"},{"location":"dev_docs/contributing/#type-checking-mypy","title":"Type Checking (Mypy)","text":"<p>For type checking we use <code>mypy</code>. You can install it locally and use it as so:</p> <pre><code>pip install mypy\nmypy neps\n</code></pre> <p>Types are helpful for making your code more understandable by your editor and tools, allowing them to warn you of potential issues, as well as allow for safer refactoring. Copilot also works better with types.</p> <p>To ignore some error you can use <code># type: ignore</code> at the end of the line, e.g.</p> <pre><code>code = \"foo\"  # type: ignore\n</code></pre> <p>A common place to ignore types is when dealing with numpy arrays, tensors and pandas, where the type checker can not be sure of the return type.</p> <pre><code>df.mean()  # Is this another dataframe, a series or a single number?\n</code></pre> <p>In the worse case, please just use <code>Any</code> and move on with your life, the type checker is meant to help you catch bugs, not hinder you. However it will take some experience to know whe it's trying to tell you something useful vs. something it just can not infer properly. A good rule of thumb is that you're only dealing with simple native types from python or types defined from NePS, there is probably a good reason for a mypy error.</p> <p>If you have issues regarding typing, please feel free to reach out for help <code>@eddiebergman</code>.</p>"},{"location":"dev_docs/contributing/#examples-and-integration-tests","title":"Examples and Integration Tests","text":"<p>We use examples in neps_examples as integration tests, which we run from the main directory via</p> <pre><code>pytest\n</code></pre> <p>If tests fail for you on the master, please raise an issue on github, preferabbly with some informationon the error, traceback and the environment in which you are running, i.e. python version, OS, etc.</p>"},{"location":"dev_docs/contributing/#regression-tests","title":"Regression Tests","text":"<p>Regression tests are run on each push to the repository to assure the performance of the optimizers don't degrade.</p> <p>Currently, regression runs are recorded on JAHS-Bench-201 data for 2 tasks: <code>cifar10</code> and <code>fashion_mnist</code> and only for optimizers: <code>random_search</code>, <code>bayesian_optimization</code>, <code>mf_bayesian_optimization</code>, <code>regularized_evolution</code>. This information is stored in the <code>tests/regression_runner.py</code> as two lists: <code>TASKS</code>, <code>OPTIMIZERS</code>. The recorded results are stored as a json dictionary in the <code>tests/losses.json</code> file.</p>"},{"location":"dev_docs/contributing/#adding-new-optimizer-algorithms","title":"Adding new optimizer algorithms","text":"<p>Once a new algorithm is added to NEPS library, we need to first record the performance of the algorithm for 100 optimization runs.</p> <ul> <li> <p>If the algorithm expects standard loss function (pipeline) and accepts fidelity hyperparameters in pipeline space, then recording results only requires adding the optimizer name into <code>OPTIMIZERS</code> list in <code>tests/regression_runner.py</code> and running <code>tests/regression_runner.py</code></p> </li> <li> <p>In case your algorithm requires custom pipeline and/or pipeline space you can modify the <code>runner.run_pipeline</code> and <code>runner.pipeline_space</code> attributes of the <code>RegressionRunner</code> after initialization (around line <code>#322</code> in <code>tests/regression_runner.py</code>)</p> </li> </ul> <p>You can verify the optimizer is recorded by rerunning the <code>regression_runner.py</code>. Now regression test will be run on your new optimizer as well on every push.</p>"},{"location":"dev_docs/contributing/#regression-test-metrics","title":"Regression test metrics","text":"<p>For each regression test the algorithm is run 10 times to sample its performance, then they are statistically compared to the 100 recorded runs. We use these 3 boolean metrics to define the performance of the algorithm on any task:</p> <ol> <li>Kolmogorov-Smirnov test for goodness of fit - <code>pvalue</code> &gt;= 10%</li> <li>Absolute median distance - bounded within 92.5% confidence range of the expected median distance</li> <li>Median improvement - Median improvement over the recorded median</li> </ol> <p>Test metrics are run for each <code>(optimizer, task)</code> combination separately and then collected. The collected metrics are then further combined into 2 metrics</p> <ol> <li>Task pass - either both <code>Kolmogorov-Smirnov test</code> and <code>Absolute median distance</code> test passes or just <code>Median improvement</code></li> <li>Test aggregate - Sum_over_tasks(<code>Kolmogorov-Smirnov test</code> + <code>Absolute median distance</code> + 2 * <code>Median improvement</code>)</li> </ol> <p>Finally, a test for an optimizer only passes when at least for one of the tasks <code>Task pass</code> is true, and <code>Test aggregate</code> is higher than 1 + <code>number of tasks</code></p>"},{"location":"dev_docs/contributing/#on-regression-test-failures","title":"On regression test failures","text":"<p>Regression tests are stochastic by nature, so they might fail occasionally even the algorithm performance didn't degrade. In the case of regression test failure, try running it again first, if the problem still persists, then you can contact Danny Stoll or Samir. You can also run tests locally by running:</p> <pre><code>poetry run pytest -m regression_all\n</code></pre>"},{"location":"dev_docs/contributing/#disabling-and-skipping-checks-etc","title":"Disabling and Skipping Checks etc.","text":""},{"location":"dev_docs/contributing/#pre-commit-how-to-not-run-hooks","title":"Pre-commit: How to not run hooks?","text":"<p>To commit without running <code>pre-commit</code> use <code>git commit --no-verify -m &lt;COMMIT MESSAGE&gt;</code>.</p>"},{"location":"dev_docs/contributing/#mypy-how-to-ignore-warnings","title":"Mypy: How to ignore warnings?","text":"<p>There are two options:</p> <ul> <li>Disable the warning locally:</li> </ul> <pre><code>code = \"foo\"  # type: ignore\n</code></pre>"},{"location":"dev_docs/contributing/#managing-dependencies","title":"Managing Dependencies","text":"<p>To manage dependencies and for package distribution we use poetry (replaces pip).</p>"},{"location":"dev_docs/contributing/#add-dependencies","title":"Add dependencies","text":"<p>To install a dependency use</p> <pre><code>poetry add dependency\n</code></pre> <p>and commit the updated <code>pyproject.toml</code> to git.</p> <p>For more advanced dependency management see examples in <code>pyproject.toml</code> or have a look at the poetry documentation.</p>"},{"location":"dev_docs/contributing/#install-dependencies-added-by-others","title":"Install dependencies added by others","text":"<p>When other contributors added dependencies to <code>pyproject.toml</code>, you can install them via</p> <pre><code>poetry lock\npoetry install\n</code></pre>"},{"location":"dev_docs/contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs, more specifically Material for MkDocs for documentation. To support documentation for multiple versions, we use the plugin mike.</p> <p>Source files for the documentation are under <code>/docs</code> and configuration at  mkdocs.yml.</p> <p>To build and view the documentation run</p> <pre><code>mike deploy 0.5.1 latest\nmike serve\n</code></pre> <p>and open the URL shown by the <code>mike serve</code> command.</p> <p>To publish the documentation run</p> <pre><code>mike deploy 0.5.1 latest -p\n</code></pre>"},{"location":"dev_docs/contributing/#releasing-a-new-version","title":"Releasing a New Version","text":"<p>There are four steps to releasing a new version of neps:</p> <ol> <li>Understand Semantic Versioning</li> <li>Update the Package Version</li> <li>Commit and Push With a Version Tag</li> <li>Update Documentation</li> <li>Publish on PyPI</li> </ol>"},{"location":"dev_docs/contributing/#0-understand-semantic-versioning","title":"0. Understand Semantic Versioning","text":"<p>We follow the semantic versioning scheme.</p>"},{"location":"dev_docs/contributing/#1-update-the-package-version-and-citationcff","title":"1. Update the Package Version and CITATION.cff","text":"<pre><code>poetry version v0.9.0\n</code></pre> <p>and manually change the version specified in <code>CITATION.cff</code>.</p>"},{"location":"dev_docs/contributing/#2-commit-with-a-version-tag","title":"2. Commit with a Version Tag","text":"<p>First commit and test</p> <pre><code>git add pyproject.toml\ngit commit -m \"Bump version from v0.8.4 to v0.9.0\"\npytest\n</code></pre> <p>Then tag and push</p> <pre><code>git tag v0.9.0\ngit push --tags\ngit push\n</code></pre>"},{"location":"dev_docs/contributing/#3-update-documentation","title":"3. Update Documentation","text":"<p>First check if the documentation has any issues via</p> <pre><code>mike deploy 0.9.0 latest -u\nmike serve\n</code></pre> <p>and then looking at it.</p> <p>Afterwards, publish it via</p> <pre><code>mike deploy 0.9.0 latest -up\n</code></pre>"},{"location":"dev_docs/contributing/#4-publish-on-pypi","title":"4. Publish on PyPI","text":"<p>To publish to PyPI:</p> <ol> <li>Get publishing rights, e.g., asking Danny or Maciej or Neeratyoy.</li> <li>Be careful, once on PyPI we can not change things.</li> <li>Run</li> </ol> <pre><code>poetry publish --build\n</code></pre> <p>This will ask for your PyPI credentials.</p>"},{"location":"dev_docs/roadmap/","title":"Roadmap","text":""},{"location":"dev_docs/roadmap/#next-up","title":"Next up","text":""},{"location":"dev_docs/roadmap/#features","title":"Features","text":"<ul> <li>Improve handling of multi-fidelity for large scale (slurm script modification)</li> <li>Evaluate and maybe improve ease-of-use of NePS and DDP etc.</li> <li>Optimize dependencies</li> <li>Improved examples</li> </ul>"},{"location":"dev_docs/roadmap/#fixes","title":"Fixes","text":"<ul> <li>Acq search mutation for HPs potentially only mutates 1 parameter</li> <li><code>ignore_errors</code> should work seamlessly with all optimizers</li> </ul>"},{"location":"dev_docs/roadmap/#refactoring","title":"Refactoring","text":"<ul> <li>Rename: run_pipeline = evaluate_pipeline | evaluate_pipeline_error | compute_pipeline_error | train_and_evaluate</li> <li>Rename: loss = validation_error | error | pipeline_error</li> <li>Rename: XParameter = XSpace or just X?</li> <li>Rename: default-x to prior-x</li> <li>Rename: Use max_cost_total everywhere instead of budget</li> </ul>"},{"location":"dev_docs/roadmap/#documentation","title":"Documentation","text":"<ul> <li>Keep citations doc up to date</li> </ul>"},{"location":"dev_docs/roadmap/#tests","title":"Tests","text":"<ul> <li>Regression tests to run on each push</li> </ul>"},{"location":"dev_docs/roadmap/#before-100-version","title":"Before 1.0.0 version","text":""},{"location":"dev_docs/roadmap/#features_1","title":"Features","text":"<ul> <li>Generate pdf plot after each evaluation</li> <li>Finegrained control over user prior</li> <li>Print search space upon run</li> <li>Utility to generate code for best architecture</li> <li>Core algorithmic feature set (research)</li> </ul>"},{"location":"dev_docs/roadmap/#fixes_1","title":"Fixes","text":"<ul> <li>Contact pypi.org/project/neps/ to free up <code>pip install neps</code></li> </ul>"},{"location":"dev_docs/roadmap/#refactoring_1","title":"Refactoring","text":"<ul> <li>Improve neps.optimizers:<ul> <li>Maintained vs unmaintained optimizers</li> <li>Remove unnecessary / broken optimizers</li> <li>Merge GP and hierarchical GP</li> </ul> </li> <li>Break up search space and config aspect</li> </ul>"},{"location":"dev_docs/roadmap/#documentation_1","title":"Documentation","text":"<ul> <li>NAS documentation</li> </ul>"},{"location":"dev_docs/roadmap/#after-100","title":"After 1.0.0","text":""},{"location":"dev_docs/roadmap/#features_2","title":"Features","text":"<ul> <li>Utility neps.clean to manage existing run results</li> <li>Collect data optionally via phone-home to webserver</li> </ul>"},{"location":"dev_docs/roadmap/#documentation_2","title":"Documentation","text":"<ul> <li>Keep a changelog</li> </ul>"},{"location":"examples/","title":"Overview","text":"<ol> <li> <p>Basic usage examples demonstrate fundamental usage. Learn how to perform Hyperparameter Optimization (HPO), Neural Architecture Search (NAS), and Joint Architecture and Hyperparameter Search (JAHS). Understand how to analyze runs on a basic level, emphasizing that no neural network training is involved at this stage; the search is performed on functions to introduce NePS.</p> </li> <li> <p>Efficiency examples showcase how to enhance efficiency in NePS. Learn about expert priors, multi-fidelity, and parallelization to streamline your pipeline and optimize search processes.</p> </li> <li> <p>Convenience examples show tensorboard compatibility and its integration, explore the compatibility with PyTorch Lightning, and understand file management within the run pipeline function used in NePS.</p> </li> <li> <p>Experimental examples tailored for NePS contributors. These examples provide insights and practices for experimental scenarios.</p> </li> <li> <p>Templates to find a basic fill-in template to kickstart your hyperparameter search with NePS. Use this template as a foundation for your projects, saving time and ensuring a structured starting point.</p> </li> <li> <p>YAML usage examples to define NePS configurations and search spaces with YAML files, streamlining the setup and execution of experiments.</p> </li> </ol>"},{"location":"examples/basic_usage/analyse/","title":"Analyse","text":"<pre><code>\"\"\" How to generate a summary (neps.status) and visualizations (neps.plot) of a run.\n\nBefore running this example analysis, run the hyperparameters example with:\n\n    python -m neps_examples.basic_usage.hyperparameters\n\"\"\"\nimport neps\n\n# 1. At all times, NePS maintains several files in the root directory that are human\n# read-able and can be useful\n\n# 2. Printing a summary and reading in results.\n# Alternatively use `python -m neps.status results/hyperparameters_example`\nresults, pending_configs = neps.status(\"results/hyperparameters_example\")\nconfig_id = \"1\"\nprint(results[config_id].config)\nprint(results[config_id].result)\nprint(results[config_id].metadata)\n\n# 3. Get the summary as a dictionary\nsummary = neps.get_summary_dict(\"results/hyperparameters_example\")\nprint(summary)\n\n# 4. Generating plots to the root directory (results/hyperparameters_example)\n# Alternatively use `python -m neps.plot results/hyperparameters_example`\nneps.plot(\"results/hyperparameters_example\")\n</code></pre>"},{"location":"examples/basic_usage/architecture/","title":"Architecture","text":"<pre><code>from __future__ import annotations\n\nimport logging\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\nfrom neps.search_spaces.architecture.primitives import AbstractPrimitive\n\n\nclass DownSampleBlock(AbstractPrimitive):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__(locals())\n        self.conv_a = ReLUConvBN(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1\n        )\n        self.conv_b = ReLUConvBN(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.downsample = nn.Sequential(\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False\n            ),\n        )\n\n    def forward(self, inputs):\n        basicblock = self.conv_a(inputs)\n        basicblock = self.conv_b(basicblock)\n        residual = self.downsample(inputs)\n        return residual + basicblock\n\n\nclass ReLUConvBN(AbstractPrimitive):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super().__init__(locals())\n\n        self.kernel_size = kernel_size\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels, affine=True, track_running_stats=True),\n        )\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass AvgPool(AbstractPrimitive):\n    def __init__(self, **kwargs):\n        super().__init__(kwargs)\n        self.op = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n    def forward(self, x):\n        return self.op(x)\n\n\nprimitives = {\n    \"Sequential15\": topos.get_sequential_n_edge(15),\n    \"DenseCell\": topos.get_dense_n_node_dag(4),\n    \"down\": {\"op\": DownSampleBlock},\n    \"avg_pool\": {\"op\": AvgPool},\n    \"id\": {\"op\": ops.Identity},\n    \"conv3x3\": {\"op\": ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ReLUConvBN, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0},\n}\n\n\nstructure = {\n    \"S\": [\"Sequential15(C, C, C, C, C, down, C, C, C, C, C, down, C, C, C, C, C)\"],\n    \"C\": [\"DenseCell(OPS, OPS, OPS, OPS, OPS, OPS)\"],\n    \"OPS\": [\"id\", \"conv3x3\", \"conv1x1\", \"avg_pool\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 16 if predecessor_values is None else predecessor_values[\"out_channels\"]\n    out_channels = in_channels * 2 if op_name == \"DownSampleBlock\" else in_channels\n    return dict(in_channels=in_channels, out_channels=out_channels)\n\n\ndef run_pipeline(architecture):\n    in_channels = 3\n    base_channels = 16\n    n_classes = 10\n    out_channels_factor = 4\n\n    # E.g., in shape = (N, 3, 32, 32) =&gt; out shape = (N, 10)\n    model = architecture.to_pytorch()\n    model = nn.Sequential(\n        nn.Conv2d(in_channels, base_channels, 3, padding=1, bias=False),\n        nn.BatchNorm2d(base_channels),\n        model,\n        nn.BatchNorm2d(base_channels * out_channels_factor),\n        nn.ReLU(inplace=True),\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(base_channels * out_channels_factor, n_classes),\n    )\n    return 1\n\n\npipeline_space = dict(\n    architecture=neps.ArchitectureParameter(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n    )\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/architecture\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/basic_usage/architecture_and_hyperparameters/","title":"Architecture and hyperparameters","text":"<pre><code>import logging\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\nfrom neps.search_spaces.architecture.primitives import AbstractPrimitive\n\n\nclass DownSampleBlock(AbstractPrimitive):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__(locals())\n        self.conv_a = ReLUConvBN(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1\n        )\n        self.conv_b = ReLUConvBN(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.downsample = nn.Sequential(\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False\n            ),\n        )\n\n    def forward(self, inputs):\n        basicblock = self.conv_a(inputs)\n        basicblock = self.conv_b(basicblock)\n        residual = self.downsample(inputs)\n        return residual + basicblock\n\n\nclass ReLUConvBN(AbstractPrimitive):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super().__init__(locals())\n\n        self.kernel_size = kernel_size\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels, affine=True, track_running_stats=True),\n        )\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass AvgPool(AbstractPrimitive):\n    def __init__(self, **kwargs):\n        super().__init__(kwargs)\n        self.op = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n    def forward(self, x):\n        return self.op(x)\n\n\nprimitives = {\n    \"Sequential15\": topos.get_sequential_n_edge(15),\n    \"DenseCell\": topos.get_dense_n_node_dag(4),\n    \"down\": {\"op\": DownSampleBlock},\n    \"avg_pool\": {\"op\": AvgPool},\n    \"id\": {\"op\": ops.Identity},\n    \"conv3x3\": {\"op\": ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ReLUConvBN, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0},\n}\n\n\nstructure = {\n    \"S\": [\"Sequential15(C, C, C, C, C, down, C, C, C, C, C, down, C, C, C, C, C)\"],\n    \"C\": [\"DenseCell(OPS, OPS, OPS, OPS, OPS, OPS)\"],\n    \"OPS\": [\"id\", \"conv3x3\", \"conv1x1\", \"avg_pool\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 16 if predecessor_values is None else predecessor_values[\"out_channels\"]\n    out_channels = in_channels * 2 if op_name == \"DownSampleBlock\" else in_channels\n    return dict(in_channels=in_channels, out_channels=out_channels)\n\n\ndef run_pipeline(**config):\n    optimizer = config[\"optimizer\"]\n    learning_rate = config[\"learning_rate\"]\n    model = config[\"architecture\"].to_pytorch()\n\n    target_params = 1531258\n    number_of_params = sum(p.numel() for p in model.parameters())\n    validation_error = abs(target_params - number_of_params) / target_params\n\n    target_lr = 10e-3\n    validation_error += abs(target_lr - learning_rate) / target_lr\n    validation_error += int(optimizer == \"sgd\")\n\n    return validation_error\n\n\npipeline_space = dict(\n    architecture=neps.ArchitectureParameter(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n    ),\n    optimizer=neps.CategoricalParameter(choices=[\"sgd\", \"adam\"]),\n    learning_rate=neps.FloatParameter(lower=10e-7, upper=10e-3, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hyperparameters_architecture_example\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/basic_usage/hpo_usage_example/","title":"Hpo usage example","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\ndef run_pipeline(\n    float_name1,\n    float_name2,\n    categorical_name1,\n    categorical_name2,\n    integer_name1,\n    integer_name2,\n):\n    # neps optimize to find values that maximizes sum, for demonstration only\n    loss = -float(\n        np.sum(\n            [float_name1, float_name2, categorical_name1, integer_name1, integer_name2]\n        )\n    )\n    if categorical_name2 == \"a\":\n        loss += 1\n\n    return loss\n\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=\"search_space_example.yaml\",\n    root_directory=\"results/hyperparameters_example\",\n    post_run_summary=True,\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/basic_usage/hyperparameters/","title":"Hyperparameters","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(float1, float2, categorical, integer1, integer2):\n    loss = -float(np.sum([float1, float2, int(categorical), integer1, integer2]))\n    time.sleep(0.7)  # For demonstration purposes\n    return loss\n\n\npipeline_space = dict(\n    float1=neps.FloatParameter(lower=0, upper=1),\n    float2=neps.FloatParameter(lower=-10, upper=10),\n    categorical=neps.CategoricalParameter(choices=[0, 1]),\n    integer1=neps.IntegerParameter(lower=0, upper=1),\n    integer2=neps.IntegerParameter(lower=1, upper=1000, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hyperparameters_example\",\n    post_run_summary=True,\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/convenience/logging_additional_info/","title":"Logging additional info","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(float1, float2, categorical, integer1, integer2):\n    start = time.time()\n    loss = -float(np.sum([float1, float2, int(categorical), integer1, integer2]))\n    end = time.time()\n    return {\n        \"loss\": loss,\n        \"info_dict\": {  # Optionally include additional information as an info_dict\n            \"train_time\": end - start,\n        },\n    }\n\n\npipeline_space = dict(\n    float1=neps.FloatParameter(lower=0, upper=1),\n    float2=neps.FloatParameter(lower=-10, upper=10),\n    categorical=neps.CategoricalParameter(choices=[0, 1]),\n    integer1=neps.IntegerParameter(lower=0, upper=1),\n    integer2=neps.IntegerParameter(lower=1, upper=1000, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/logging_additional_info\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/convenience/neps_tblogger_tutorial/","title":"Neps tblogger tutorial","text":"<pre><code>\"\"\"\nNePS tblogger With TensorBoard\n==============================\n\n1- Introduction\n---------------\nWelcome to the NePS tblogger with TensorBoard tutorial. This guide will walk you\nthrough the process of using the NePS tblogger class to monitor performance\ndata for different hyperparameter configurations during optimization.\n\nAssuming you have experience with NePS, this tutorial aims to showcase the power\nof visualization using tblogger. To go directly to that part, check lines 244-264\nor search for 'Start Tensorboard Logging'.\n\n2- Learning Objectives\n----------------------\nBy completing this tutorial, you will:\n\n- Understand the role of NePS tblogger in HPO and NAS.\n- Learn to define search spaces within NePS for different model configurations.\n- Build a comprehensive run pipeline to train and evaluate models.\n- Utilize TensorBoard to visualize and compare performance metrics of different\n  model configurations.\n\n3- Setup\n--------\nBefore we begin, ensure you have the necessary dependencies installed. To install\nthe 'NePS' package, use the following command:\n\n```bash\npip install neural-pipeline-search\n```\n\nAdditionally, note that 'NePS' does not include 'torchvision' as a dependency.\nYou can install it with this command:\n\n```bash\npip install torchvision\n```\n\nMake sure to download the torchvision version that fits with your pytorch\nversion. More info on this link:\n\nhttps://pypi.org/project/torchvision/\n\nThese dependencies ensure you have everything you need for this tutorial.\n\n\"\"\"\n\nimport logging\nimport random\nimport time\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.transforms import transforms\n\nimport neps\nfrom neps.plot.tensorboard_eval import tblogger\n\n\"\"\"\nSteps for a successful training pipeline:\n\n#1 Define the seeds for reproducibility.\n#2 Prepare the input data.\n#3 Design the model.\n#4 Design the pipeline search spaces.\n#5 Design the run pipeline function.\n#6 Use neps.run the run the entire search using your specified searcher.\n\nEach step will be covered in detail thourghout the code\n\n\"\"\"\n\n#############################################################\n# Definig the seeds for reproducibility\n\n\ndef set_seed(seed=123):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\n#############################################################\n# Prepare the input data. For this tutorial we use the MNIST dataset.\n\n\ndef MNIST(\n    batch_size: int = 256,\n    n_train_size: float = 0.9,\n    data_reduction_factor: float = 0.5,\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    # Download MNIST training and test datasets if not already downloaded.\n    train_dataset = torchvision.datasets.MNIST(\n        root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\n    )\n    test_dataset = torchvision.datasets.MNIST(\n        root=\"./data\", train=False, transform=transforms.ToTensor(), download=True\n    )\n\n    # Determine the size of the reduced training dataset for faster training\n    # and calculate the size of the training subset from the reduced dataset\n    reduced_dataset_train = int(data_reduction_factor * len(train_dataset))\n    train_size = int(n_train_size * reduced_dataset_train)\n\n    # Create a random sampler for the training and validation data\n    train_sampler = SubsetRandomSampler(range(train_size))\n    valid_sampler = SubsetRandomSampler(range(train_size, reduced_dataset_train))\n\n    # Create DataLoaders for training, validation, and test datasets.\n    train_dataloader = DataLoader(\n        dataset=train_dataset, batch_size=batch_size, shuffle=False, sampler=train_sampler\n    )\n    val_dataloader = DataLoader(\n        dataset=train_dataset, batch_size=batch_size, shuffle=False, sampler=valid_sampler\n    )\n    test_dataloader = DataLoader(\n        dataset=test_dataset, batch_size=batch_size, shuffle=False\n    )\n\n    return train_dataloader, val_dataloader, test_dataloader\n\n\n#############################################################\n# Design small MLP model to be able to represent the input data.\n\n\nclass MLP(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.linear1 = nn.Linear(in_features=784, out_features=196)\n        self.linear2 = nn.Linear(in_features=196, out_features=98)\n        self.linear3 = nn.Linear(in_features=98, out_features=10)\n\n    def forward(self, x: torch.Tensor):\n        # Flattening the grayscaled image from 1x28x28 (CxWxH) to 784.\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.linear1(x))\n        x = F.relu(self.linear2(x))\n        x = self.linear3(x)\n\n        return x\n\n\n#############################################################\n# Define the training step. Return the validation error and\n# misclassified images.\n\n\ndef loss_ev(model: nn.Module, data_loader: DataLoader) -&gt; float:\n    # Set the model in evaluation mode (no gradient computation).\n    model.eval()\n\n    correct = 0\n    total = 0\n\n    # Disable gradient computation for efficiency.\n    with torch.no_grad():\n        for x, y in data_loader:\n            output = model(x)\n\n            # Get the predicted class for each input.\n            _, predicted = torch.max(output.data, 1)\n\n            # Update the correct and total counts.\n            correct += (predicted == y).sum().item()\n            total += y.size(0)\n\n    # Calculate the accuracy and return the error rate.\n    accuracy = correct / total\n    error_rate = 1 - accuracy\n    return error_rate\n\n\ndef training(\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    criterion: nn.Module,\n    train_loader: DataLoader,\n    validation_loader: DataLoader,\n) -&gt; Tuple[float, torch.Tensor]:\n    \"\"\"\n    Function that trains the model for one epoch and evaluates the model\n    on the validation set.\n\n    Args:\n        model (nn.Module): Model to be trained.\n        optimizer (torch.optim.Optimizer): Optimizer used to train the weights.\n        criterion (nn.Module) : Loss function to use.\n        train_loader (DataLoader): DataLoader containing the training data.\n        validation_loader (DataLoader): DataLoader containing the validation data.\n\n    Returns:\n    Tuple[float, torch.Tensor]: A tuple containing the validation error (float)\n                                and a tensor of misclassified images.\n    \"\"\"\n    incorrect_images = []\n    model.train()\n\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n\n        predicted_labels = torch.argmax(output, dim=1)\n        incorrect_mask = predicted_labels != y\n        incorrect_images.append(x[incorrect_mask])\n\n    # Calculate validation loss using the loss_ev function.\n    validation_loss = loss_ev(model, validation_loader)\n\n    # Return the misclassified image by during model training.\n    if len(incorrect_images) &gt; 0:\n        incorrect_images = torch.cat(incorrect_images, dim=0)\n\n    return (validation_loss, incorrect_images)\n\n\n#############################################################\n# Design the pipeline search spaces.\n\n\ndef pipeline_space() -&gt; dict:\n    pipeline = dict(\n        lr=neps.FloatParameter(lower=1e-5, upper=1e-1, log=True),\n        optim=neps.CategoricalParameter(choices=[\"Adam\", \"SGD\"]),\n        weight_decay=neps.FloatParameter(lower=1e-4, upper=1e-1, log=True),\n    )\n\n    return pipeline\n\n\n#############################################################\n# Implement the pipeline run search.\n\n\ndef run_pipeline(lr, optim, weight_decay):\n    # Create the network model.\n    model = MLP()\n\n    if optim == \"Adam\":\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optim == \"SGD\":\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(\n            \"Optimizer choices are defined differently in the pipeline_space\"\n        )\n\n    max_epochs = 2  # Epochs to train the model, can be parameterized as fidelity\n\n    # Load the MNIST dataset for training, validation, and testing.\n    train_loader, validation_loader, test_loader = MNIST(\n        batch_size=96, n_train_size=0.6, data_reduction_factor=0.75\n    )\n\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.75)\n    criterion = nn.CrossEntropyLoss()\n\n    for i in range(max_epochs):\n        loss, miss_img = training(\n            optimizer=optimizer,\n            model=model,\n            criterion=criterion,\n            train_loader=train_loader,\n            validation_loader=validation_loader,\n        )\n\n        # Gathering the gradient mean in each layer\n        mean_gradient = []\n        for layer in model.children():\n            layer_gradients = [param.grad for param in layer.parameters()]\n            if layer_gradients:\n                mean_gradient.append(\n                    torch.mean(torch.cat([grad.view(-1) for grad in layer_gradients]))\n                )\n\n        ###################### Start Tensorboard Logging ######################\n\n        # The following tblogge` will result in:\n\n        # 1. Loss curves of each configuration at each epoch.\n        # 2. Decay curve of the learning rate at each epoch.\n        # 3. Wrongly classified images by the model.\n        # 4. First two layer gradients passed as scalar configs.\n\n        tblogger.log(\n            loss=loss,\n            current_epoch=i,\n            write_summary_incumbent=False,  # Set to `True` for a live incumbent trajectory.\n            writer_config_scalar=True,  # Set to `True` for a live loss trajectory for each config.\n            writer_config_hparam=True,  # Set to `True` for live parallel coordinate, scatter plot matrix, and table view.\n            # Appending extra data\n            extra_data={\n                \"lr_decay\": tblogger.scalar_logging(value=scheduler.get_last_lr()[0]),\n                \"miss_img\": tblogger.image_logging(image=miss_img, counter=2, seed=2),\n                \"layer_gradient1\": tblogger.scalar_logging(value=mean_gradient[0]),\n                \"layer_gradient2\": tblogger.scalar_logging(value=mean_gradient[1]),\n            },\n        )\n\n        ###################### End Tensorboard Logging ######################\n\n        scheduler.step()\n\n        print(f\"  Epoch {i + 1} / {max_epochs} Val Error: {loss} \")\n\n    # Calculate training and test accuracy.\n    train_accuracy = loss_ev(model, train_loader)\n    test_accuracy = loss_ev(model, test_loader)\n\n    # Return a dictionary with relevant metrics and information.\n    return {\n        \"loss\": loss,\n        \"info_dict\": {\n            \"train_accuracy\": train_accuracy,\n            \"test_accuracy\": test_accuracy,\n            \"cost\": max_epochs,\n        },\n    }\n\n\n#############################################################\n# Running neps with BO as the searcher.\n\nif __name__ == \"__main__\":\n    \"\"\"\n    When running this code without any arguments, it will by default\n    run bayesian optimization with 3 evaluations total.\n\n    ```bash\n    python neps_tblogger_tutorial.py\n    ```\n    \"\"\"\n    start_time = time.time()\n\n    set_seed(112)\n    logging.basicConfig(level=logging.INFO)\n\n    # To check the status of tblogger:\n    # tblogger.get_status()\n\n    run_args = dict(\n        run_pipeline=run_pipeline,\n        pipeline_space=pipeline_space(),\n        root_directory=\"results/neps_tblogger_example\",\n        searcher=\"random_search\",\n    )\n\n    neps.run(\n        **run_args,\n        max_evaluations_total=2,\n    )\n\n    \"\"\"\n    To check live plots during this search, please open a new terminal\n    and make sure to be at the same level directory of your project and\n    run the following command on the file created by neps root_directory.\n\n    ```bash:\n    tensorboard --logdir output\n    ```\n\n    To be able to check the visualization of tensorboard make sure to\n    follow the local link provided.\n\n    http://localhost:6006/\n\n    Double-check the directory path you've provided; if you're not seeing\n    any visualizations and have followed the tutorial closely, there\n    might be an error in the directory specification. Remember that\n    TensorBoard runs in the command line without checking if the directory\n    actually exists.\n    \"\"\"\n\n    # Disables tblogger for the continued run\n    tblogger.disable()\n\n    neps.run(\n        **run_args,\n        max_evaluations_total=3,  # continues the previous run for 1 more evaluation\n    )\n\n    \"\"\"\n    This second run of one more configuration will not add to the tensorboard logs.\n    \"\"\"\n\n    end_time = time.time()  # Record the end time\n    execution_time = end_time - start_time\n    logging.info(f\"Execution time: {execution_time} seconds\\n\")\n</code></pre>"},{"location":"examples/convenience/neps_x_lightning/","title":"Neps x lightning","text":"<pre><code>\"\"\"\nExploring NePS Compatibility with PyTorch Lightning\n=======================================================\n\n1. Introduction:\n----------------\nWelcome to this tutorial on utilizing NePS-generated files and directories\nin conjunction with PyTorch Lightning.\n\n2. Setup:\n---------\nEnsure you have the necessary dependencies installed. You can install the 'NePS'\npackage by executing the following command:\n\n```bash\npip install neural-pipeline-search\n```\n\nAdditionally, note that 'NePS' does not include 'torchvision' as a dependency.\nYou can install it with this command:\n\n```bash\npip install torchvision==0.14\n```\n\nMake sure to download the torchvision version that is compatible with your\npytorch version. More info on this link:\n\nhttps://pypi.org/project/torchvision/\n\nAdditionally, you will need to install the PyTorch Lightning package. This can\nbe achieved with the following command:\n\n```bash\npip install lightning\n```\n\nThese dependencies ensure you have everything you need for this tutorial.\n\"\"\"\nimport argparse\nimport glob\nimport logging\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport lightning as L\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import TensorBoardLogger\nfrom torch.utils.data import SubsetRandomSampler\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import transforms\n\nimport neps\nfrom neps.utils.common import get_initial_directory, load_lightning_checkpoint\n\n#############################################################\n# Definig the seeds for reproducibility\n\n\ndef set_seed(seed=123):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\n#############################################################\n# Define the lightning model\n\n\nclass LitMNIST(L.LightningModule):\n    def __init__(\n        self,\n        configuration: dict,\n        n_train: int = 8192,\n        n_valid: int = 1024,\n    ):\n        super().__init__()\n\n        # Initialize the model's hyperparameters with the configuration\n        self.save_hyperparameters(configuration)\n\n        self.n_train = n_train\n        self.n_valid = n_valid\n\n        # Define data transformation and loss function\n        self.transform = transforms.ToTensor()\n        self.criterion = nn.NLLLoss()\n\n        # Define the model's architecture\n        self.linear1 = nn.Linear(in_features=784, out_features=392)\n        self.linear2 = nn.Linear(in_features=392, out_features=196)\n        self.linear3 = nn.Linear(in_features=196, out_features=10)\n\n        # Define PyTorch Lightning metrics for training, validation, and testing\n        metric = Accuracy(task=\"multiclass\", num_classes=10)\n        self.train_accuracy = metric.clone()\n        self.val_accuracy = metric.clone()\n        self.test_accuracy = metric.clone()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Forward pass function\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.linear1(x))\n        x = F.relu(self.linear2(x))\n        x = self.linear3(x)\n\n        return F.log_softmax(x, dim=1)\n\n    def common_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Perform a forward pass and compute loss, predictions, and get the ground\n        truth labels for a batch of data.\n        \"\"\"\n        x, y = batch\n        logits = self.forward(x)\n        loss = self.criterion(logits, y)\n        preds = torch.argmax(logits, dim=1)\n\n        return loss, preds, y\n\n    def training_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; float:\n        loss, preds, y = self.common_step(batch, batch_idx)\n        self.train_accuracy.update(preds, y)\n\n        self.log_dict(\n            {\"train_loss\": loss, \"train_acc\": self.val_accuracy.compute()},\n            on_epoch=True,\n            on_step=False,\n            prog_bar=True,\n        )\n\n        return loss\n\n    def validation_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        loss, preds, y = self.common_step(batch, batch_idx)\n        self.val_accuracy.update(preds, y)\n\n        self.log_dict(\n            {\"val_loss\": loss, \"val_acc\": self.val_accuracy.compute()},\n            on_epoch=True,\n            on_step=False,\n            prog_bar=True,\n        )\n\n    def test_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        _, preds, y = self.common_step(batch, batch_idx)\n        self.test_accuracy.update(preds, y)\n\n        self.log(name=\"test_acc\", value=self.test_accuracy.compute())\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        # Configure and return the optimizer based on the configuration\n        if self.hparams.optimizer == \"Adam\":\n            optimizer = torch.optim.Adam(\n                self.parameters(),\n                lr=self.hparams.lr,\n                weight_decay=self.hparams.weight_decay,\n            )\n        elif self.hparams.optimizer == \"SGD\":\n            optimizer = torch.optim.SGD(\n                self.parameters(),\n                lr=self.hparams.lr,\n                weight_decay=self.hparams.weight_decay,\n            )\n        else:\n            raise ValueError(\n                \"The optimizer choices is not one of the available optimizers\"\n            )\n        return optimizer\n\n    def on_train_end(self):\n        # Get the metric at the end of the training and log it with respect to\n        # it's hyperparameters\n        val_acc_metric = {\n            \"val_accuracy\": self.val_accuracy.compute(),\n        }\n\n        # Log hyperparameters\n        self.logger.log_hyperparams(self.hparams, metrics=val_acc_metric)\n\n    def prepare_data(self) -&gt; None:\n        # Downloading the dataste if not already downloaded\n        MNIST(self.hparams.data_dir, train=True, download=True)\n        MNIST(self.hparams.data_dir, train=False, download=True)\n\n    def setup(self, stage: str = None) -&gt; None:\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            self.mnist_full = MNIST(\n                self.hparams.data_dir, train=True, transform=self.transform\n            )\n\n            # Create random subsets of the training dataset for validation.\n            self.train_sampler = SubsetRandomSampler(range(self.n_train))\n            self.val_sampler = SubsetRandomSampler(\n                range(self.n_train, self.n_train + self.n_valid)\n            )\n\n        # Assign test dataset for use in dataloader\n        if stage == \"test\" or stage is None:\n            self.mnist_test = MNIST(\n                self.hparams.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self) -&gt; DataLoader:\n        return DataLoader(\n            self.mnist_full,\n            batch_size=self.hparams.batch_size,\n            sampler=self.train_sampler,\n            num_workers=16,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        return DataLoader(\n            self.mnist_full,\n            batch_size=self.hparams.batch_size,\n            sampler=self.val_sampler,\n            num_workers=16,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        return DataLoader(\n            self.mnist_test,\n            batch_size=self.hparams.batch_size,\n            num_workers=16,\n        )\n\n\n#############################################################\n# Define search space\n\n\ndef search_space() -&gt; dict:\n    # Define a dictionary to represent the hyperparameter search space\n    space = dict(\n        data_dir=neps.ConstantParameter(\"./data\"),\n        batch_size=neps.ConstantParameter(64),\n        lr=neps.FloatParameter(lower=1e-5, upper=1e-2, log=True, default=1e-3),\n        weight_decay=neps.FloatParameter(\n            lower=1e-5, upper=1e-3, log=True, default=5e-4\n        ),\n        optimizer=neps.CategoricalParameter(choices=[\"Adam\", \"SGD\"], default=\"Adam\"),\n        epochs=neps.IntegerParameter(lower=1, upper=9, log=False, is_fidelity=True),\n    )\n    return space\n\n\n#############################################################\n# Define the run pipeline function\n\n\ndef run_pipeline(pipeline_directory, previous_pipeline_directory, **config) -&gt; dict:\n    # Initialize the first directory to store the event and checkpoints files\n    init_dir = get_initial_directory(pipeline_directory)\n    checkpoint_dir = init_dir / \"checkpoints\"\n\n    # Initialize the model and checkpoint dir\n    model = LitMNIST(config)\n\n    # Create the TensorBoard logger for logging\n    logger = TensorBoardLogger(\n        save_dir=init_dir, name=\"data\", version=\"logs\", default_hp_metric=False\n    )\n\n    # Add checkpoints at the end of training\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=checkpoint_dir,\n        filename=\"{epoch}-{val_loss:.2f}\",\n    )\n\n    # Use this function to load the previous checkpoint if it exists\n    checkpoint_path, checkpoint = load_lightning_checkpoint(\n        previous_pipeline_directory=previous_pipeline_directory,\n        checkpoint_dir=checkpoint_dir,\n    )\n\n    if checkpoint is None:\n        previously_spent_epochs = 0\n    else:\n        previously_spent_epochs = checkpoint[\"epoch\"]\n\n    # Create a PyTorch Lightning Trainer\n    epochs = config[\"epochs\"]\n\n    trainer = L.Trainer(\n        logger=logger,\n        max_epochs=epochs,\n        callbacks=[checkpoint_callback],\n    )\n\n    # Train the model and retrieve training/validation metrics\n    if checkpoint_path:\n        trainer.fit(model, ckpt_path=checkpoint_path)\n    else:\n        trainer.fit(model)\n\n    train_accuracy = trainer.logged_metrics.get(\"train_acc\", None)\n    val_loss = trainer.logged_metrics.get(\"val_loss\", None)\n    val_accuracy = trainer.logged_metrics.get(\"val_acc\", None)\n\n    # Test the model and retrieve test metrics\n    trainer.test(model)\n\n    test_accuracy = trainer.logged_metrics.get(\"test_acc\", None)\n\n    return {\n        \"loss\": val_loss,\n        \"cost\": epochs - previously_spent_epochs,\n        \"info_dict\": {\n            \"train_accuracy\": train_accuracy,\n            \"val_accuracy\": val_accuracy,\n            \"test_accuracy\": test_accuracy,\n        },\n    }\n\n\nif __name__ == \"__main__\":\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--max_evaluations_total\",\n        type=int,\n        default=15,\n        help=\"Number of different configurations to train\",\n    )\n    args = parser.parse_args()\n\n    # Initialize the logger and record start time\n    start_time = time.time()\n    set_seed(112)\n    logging.basicConfig(level=logging.INFO)\n\n    # Run NePS with specified parameters\n    neps.run(\n        run_pipeline=run_pipeline,\n        pipeline_space=search_space(),\n        root_directory=\"results/hyperband\",\n        max_evaluations_total=args.max_evaluations_total,\n        searcher=\"hyperband\",\n    )\n\n    # Record the end time and calculate execution time\n    end_time = time.time()\n    execution_time = end_time - start_time\n\n    # Log the execution time\n    logging.info(f\"Execution time: {execution_time} seconds\")\n</code></pre>"},{"location":"examples/convenience/running_on_slurm_scripts/","title":"Running on slurm scripts","text":"<pre><code>\"\"\" Example that shows HPO with NePS based on a slurm script.\n\"\"\"\n\nimport logging\nimport os\nimport time\nfrom pathlib import Path\n\nimport neps\n\n\ndef _ask_to_submit_slurm_script(pipeline_directory: Path, script: str):\n    script_path = pipeline_directory / \"submit.sh\"\n    logging.info(f\"Submitting the script {script_path} (see below): \\n\\n{script}\")\n\n    # You may want to remove the below check and not ask before submitting every time\n    if input(\"Ok to submit? [Y|n] -- \").lower() in {\"y\", \"\"}:\n        script_path.write_text(script)\n        os.system(f\"sbatch {script_path}\")\n    else:\n        raise ValueError(\"We generated a slurm script that should not be submitted.\")\n\n\ndef _get_validation_error(pipeline_directory: Path):\n    validation_error_file = pipeline_directory / \"validation_error_from_slurm_job.txt\"\n    if validation_error_file.exists():\n        return float(validation_error_file.read_text())\n    return None\n\n\ndef run_pipeline_via_slurm(\n    pipeline_directory: Path, optimizer: str, learning_rate: float\n):\n    script = f\"\"\"#!/bin/bash\n#SBATCH --time 0-00:05\n#SBATCH --job-name test\n#SBATCH --partition cpu-cascadelake\n#SBATCH --error \"{pipeline_directory}/%N_%A_%x_%a.oe\"\n#SBATCH --output \"{pipeline_directory}/%N_%A_%x_%a.oe\"\n# Plugin your python script here\npython -c \"print('Learning rate {learning_rate} and optimizer {optimizer}')\"\n# At the end of training and validation create this file\necho -10 &gt; {pipeline_directory}/validation_error_from_slurm_job.txt\n\"\"\"\n\n    # Now we submit and wait until the job has created validation_error_from_slurm_job.txt\n    _ask_to_submit_slurm_script(pipeline_directory, script)\n    while validation_error := _get_validation_error(pipeline_directory) is None:\n        logging.info(\"Waiting until the job has finished.\")\n        time.sleep(60)  # Adjust to something reasonable\n    return validation_error\n\n\npipeline_space = dict(\n    optimizer=neps.CategoricalParameter(choices=[\"sgd\", \"adam\"]),\n    learning_rate=neps.FloatParameter(lower=10e-7, upper=10e-3, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline_via_slurm,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/slurm_script_example\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/convenience/working_directory_per_pipeline/","title":"Working directory per pipeline","text":"<pre><code>import logging\nfrom pathlib import Path\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(pipeline_directory: Path, float1, categorical, integer1):\n    # When adding pipeline_directory to run_pipeline, neps detects its presence and\n    # passes a directory unique for each pipeline configuration. You can then use this\n    # pipeline_directory to create / save files pertaining to a specific pipeline, e.g.:\n    weight_file = pipeline_directory / \"weight_file.txt\"\n    weight_file.write_text(\"0\")\n\n    loss = -float(np.sum([float1, int(categorical), integer1]))\n    return loss\n\n\npipeline_space = dict(\n    float1=neps.FloatParameter(lower=0, upper=1),\n    categorical=neps.CategoricalParameter(choices=[0, 1]),\n    integer1=neps.IntegerParameter(lower=0, upper=1),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/working_directory_per_pipeline\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/declarative_usage/","title":"Declarative Usage in NePS for Neural Network Optimization","text":"<p>This folder contains examples and templates for optimizing neural networks using NePS, configured via YAML files. These configurations allow easy adjustments to experiment parameters and search spaces, enabling fine-tuning of your models without modifying Python code.</p>"},{"location":"examples/declarative_usage/#hpo_examplepy","title":"<code>hpo_example.py</code>","text":"<p>This Python script demonstrates how to integrate NePS with a neural network training pipeline for hyperparameter optimization. It utilizes a YAML configuration file to set up and run the experiments.</p> <pre><code>import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport neps\n\n\"\"\"\nThis script demonstrates the integration of a simple neural network training pipeline\nwith NePS for hyperparameter optimization, focusing on the MNIST dataset.\n\n- SimpleNN Class: A PyTorch neural network model that constructs a feedforward\n  architecture based on input size, number of layers, and neurons per layer.\n\n- Training Pipeline: A function that takes hyperparameters (number of layers, neurons,\n  epochs, learning rate, optimizer type) to train and validate the SimpleNN model on\n  the MNIST dataset. Supports Adam and SGD optimizers.\n\n- NEPS Integration: Shows how to automate hyperparameter tuning using NEPS. Configuration\n  settings are specified in a YAML file ('run_args.yaml'), which is passed to the NePS\n  optimization process via the `run_args` parameter.\n\nUsage:\n1. Define model architecture and training logic in `SimpleNN` and `training_pipeline`.\n2. Configure hyperparameters and optimization settings in 'config.yaml'.\n3. Launch optimization with NePS by calling `neps.run`, specifying the training pipeline,\nand configuration file(config.yaml).\n\"\"\"\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, num_layers, num_neurons):\n        super().__init__()\n        layers = [nn.Flatten()]\n\n        for _ in range(num_layers):\n            layers.append(nn.Linear(input_size, num_neurons))\n            layers.append(nn.ReLU())\n            input_size = num_neurons  # Set input size for the next layer\n\n        layers.append(nn.Linear(num_neurons, 10))  # Output layer for 10 classes\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndef training_pipeline(num_layers, num_neurons, epochs, learning_rate, optimizer):\n    \"\"\"\n    Trains and validates a simple neural network on the MNIST dataset.\n\n    Args:\n        num_layers (int): Number of hidden layers in the network.\n        num_neurons (int): Number of neurons in each hidden layer.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate for the optimizer.\n        optimizer (str): Name of the optimizer to use ('adam' or 'sgd').\n\n    Returns:\n        float: The average loss over the validation set after training.\n\n    Raises:\n        KeyError: If the specified optimizer is not supported.\n    \"\"\"\n    # Transformations applied on each image\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                (0.1307,), (0.3081,)\n            ),  # Mean and Std Deviation for MNIST\n        ]\n    )\n\n    # Loading MNIST dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=True, download=True, transform=transform\n    )\n    train_set, val_set = torch.utils.data.random_split(dataset, [55000, 5000])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=1000, shuffle=False)\n\n    model = SimpleNN(28 * 28, num_layers, num_neurons)\n    criterion = nn.CrossEntropyLoss()\n\n    # Select optimizer\n    if optimizer == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    else:\n        raise KeyError(f\"optimizer {optimizer} is not available\")\n\n    # Training loop\n\n    for epoch in range(epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            output = model(data)\n            val_loss += criterion(output, target).item()\n\n    val_loss /= len(val_loader.dataset)\n    return val_loss\n\n\nif __name__ == \"__main__\":\n    # Configure logging to display important messages from NePS.\n    logging.basicConfig(level=logging.INFO)\n\n    # Run optimization using neps.run(...). Arguments can be provided directly to neps.run\n    # or defined in a configuration file (e.g., \"config.yaml\") passed through\n    # the run_args parameter.\n    neps.run(training_pipeline, run_args=\"config.yaml\")\n</code></pre>"},{"location":"examples/declarative_usage/#configyaml","title":"<code>config.yaml</code>","text":"<p>This YAML file defines the NePS arguments for the experiment. By editing this file, users can customize their experiments without modifying the Python script.</p> <pre><code>experiment:\n  root_directory: \"results/example_run\"\n  max_evaluations_total: 20\n  overwrite_working_directory: true\n  post_run_summary: true\n  development_stage_id: \"beta\"\n\npipeline_space:\n  epochs: 5\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true\n  num_layers:\n    lower: 1\n    upper: 5\n  optimizer:\n    choices: [\"adam\", \"sgd\"]\n  num_neurons:\n    lower: 64\n    upper: 128\n\nsearcher:\n  strategy: \"bayesian_optimization\"\n  initial_design_size: 5\n  surrogate_model: gp\n</code></pre>"},{"location":"examples/declarative_usage/#quick-start-guide","title":"Quick Start Guide","text":"<ol> <li>Review the YAML File: Examine <code>config.yaml</code> to understand the available configurations and how they are structured.</li> <li>Run the Example Script: Execute hpo_example.py, by providing <code>config.yaml</code> via the run_args agrument to NePS.    This will initiate a hyperparameter optimization task based on your YAML configurations.</li> <li>Modify YAML File: Experiment with adjusting the parameters in the YAML file to see how changes affect your    search experiments. This is a great way to learn about the impact of different configurations on your results.</li> </ol> <p>By following these steps and utilizing the provided YAML files, you'll be able to efficiently set up, run, and modify your NePS experiments. Enjoy the flexibility and simplicity that comes with managing your experiment configurations in YAML!</p>"},{"location":"examples/declarative_usage/hpo_example/","title":"Hpo example","text":"<pre><code>import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport neps\n\n\"\"\"\nThis script demonstrates the integration of a simple neural network training pipeline\nwith NePS for hyperparameter optimization, focusing on the MNIST dataset.\n\n- SimpleNN Class: A PyTorch neural network model that constructs a feedforward\n  architecture based on input size, number of layers, and neurons per layer.\n\n- Training Pipeline: A function that takes hyperparameters (number of layers, neurons,\n  epochs, learning rate, optimizer type) to train and validate the SimpleNN model on\n  the MNIST dataset. Supports Adam and SGD optimizers.\n\n- NEPS Integration: Shows how to automate hyperparameter tuning using NEPS. Configuration\n  settings are specified in a YAML file ('run_args.yaml'), which is passed to the NePS\n  optimization process via the `run_args` parameter.\n\nUsage:\n1. Define model architecture and training logic in `SimpleNN` and `training_pipeline`.\n2. Configure hyperparameters and optimization settings in 'config.yaml'.\n3. Launch optimization with NePS by calling `neps.run`, specifying the training pipeline,\nand configuration file(config.yaml).\n\"\"\"\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, num_layers, num_neurons):\n        super().__init__()\n        layers = [nn.Flatten()]\n\n        for _ in range(num_layers):\n            layers.append(nn.Linear(input_size, num_neurons))\n            layers.append(nn.ReLU())\n            input_size = num_neurons  # Set input size for the next layer\n\n        layers.append(nn.Linear(num_neurons, 10))  # Output layer for 10 classes\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndef training_pipeline(num_layers, num_neurons, epochs, learning_rate, optimizer):\n    \"\"\"\n    Trains and validates a simple neural network on the MNIST dataset.\n\n    Args:\n        num_layers (int): Number of hidden layers in the network.\n        num_neurons (int): Number of neurons in each hidden layer.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate for the optimizer.\n        optimizer (str): Name of the optimizer to use ('adam' or 'sgd').\n\n    Returns:\n        float: The average loss over the validation set after training.\n\n    Raises:\n        KeyError: If the specified optimizer is not supported.\n    \"\"\"\n    # Transformations applied on each image\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                (0.1307,), (0.3081,)\n            ),  # Mean and Std Deviation for MNIST\n        ]\n    )\n\n    # Loading MNIST dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=True, download=True, transform=transform\n    )\n    train_set, val_set = torch.utils.data.random_split(dataset, [55000, 5000])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=1000, shuffle=False)\n\n    model = SimpleNN(28 * 28, num_layers, num_neurons)\n    criterion = nn.CrossEntropyLoss()\n\n    # Select optimizer\n    if optimizer == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    else:\n        raise KeyError(f\"optimizer {optimizer} is not available\")\n\n    # Training loop\n\n    for epoch in range(epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            output = model(data)\n            val_loss += criterion(output, target).item()\n\n    val_loss /= len(val_loader.dataset)\n    return val_loss\n\n\nif __name__ == \"__main__\":\n    # Configure logging to display important messages from NePS.\n    logging.basicConfig(level=logging.INFO)\n\n    # Run optimization using neps.run(...). Arguments can be provided directly to neps.run\n    # or defined in a configuration file (e.g., \"config.yaml\") passed through\n    # the run_args parameter.\n    neps.run(training_pipeline, run_args=\"config.yaml\")\n</code></pre>"},{"location":"examples/efficiency/","title":"Parallelization","text":"<p>In order to run neps in parallel on multiple processes or multiple machines, simply call <code>neps.run</code> multiple times. All calls to <code>neps.run</code> need to use the same <code>root_directory</code> on the same filesystem to synchronize between the <code>neps.run</code>'s.</p> <p>For example, start the HPO example in two shells from the same directory as below.</p> <p>In shell 1:</p> <pre><code>python -m neps_examples.basic_usage.hyperparameters\n</code></pre> <p>In shell 2:</p> <pre><code>python -m neps_examples.basic_usage.hyperparameters\n</code></pre>"},{"location":"examples/efficiency/expert_priors_for_hyperparameters/","title":"Expert priors for hyperparameters","text":"<pre><code>import logging\nimport time\n\nimport neps\n\n\ndef run_pipeline(some_float, some_integer, some_cat):\n    start = time.time()\n    if some_cat != \"a\":\n        y = some_float + some_integer\n    else:\n        y = -some_float - some_integer\n    end = time.time()\n    return {\n        \"loss\": y,\n        \"info_dict\": {\n            \"test_score\": y,\n            \"train_time\": end - start,\n        },\n    }\n\n\n# neps uses the default values and a confidence in this default value to construct a prior\n# that speeds up the search\npipeline_space = dict(\n    some_float=neps.FloatParameter(\n        lower=1, upper=1000, log=True, default=900, default_confidence=\"medium\"\n    ),\n    some_integer=neps.IntegerParameter(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    some_cat=neps.CategoricalParameter(\n        choices=[\"a\", \"b\", \"c\"], default=\"a\", default_confidence=\"high\"\n    ),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/user_priors_example\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/efficiency/multi_fidelity/","title":"Multi fidelity","text":"<pre><code>import logging\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nimport neps\n\n\nclass TheModelClass(nn.Module):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef get_model_and_optimizer(learning_rate):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n    model = TheModelClass()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    return model, optimizer\n\n\n# Important: Include the \"pipeline_directory\" and \"previous_pipeline_directory\" arguments\n# in your run_pipeline function. This grants access to NePS's folder system and is\n# critical for leveraging efficient multi-fidelity optimization strategies.\n\n\ndef run_pipeline(pipeline_directory, previous_pipeline_directory, learning_rate, epoch):\n    model, optimizer = get_model_and_optimizer(learning_rate)\n    checkpoint_name = \"checkpoint.pth\"\n\n    if previous_pipeline_directory is not None:\n        # Read in state of the model after the previous fidelity rung\n        checkpoint = torch.load(previous_pipeline_directory / checkpoint_name)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        epochs_previously_spent = checkpoint[\"epoch\"]\n    else:\n        epochs_previously_spent = 0\n\n    # Train model here ...\n\n    # Save model to disk\n    torch.save(\n        {\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        },\n        pipeline_directory / checkpoint_name,\n    )\n\n    loss = np.log(learning_rate / epoch)  # Replace with actual error\n    epochs_spent_in_this_call = epoch - epochs_previously_spent  # Optional for stopping\n    return dict(loss=loss, cost=epochs_spent_in_this_call)\n\n\npipeline_space = dict(\n    learning_rate=neps.FloatParameter(lower=1e-4, upper=1e0, log=True),\n    epoch=neps.IntegerParameter(lower=1, upper=10, is_fidelity=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/multi_fidelity_example\",\n    # Optional: Do not start another evaluation after &lt;=100 epochs, corresponds to cost\n    # field above.\n    max_cost_total=100,\n)\n</code></pre>"},{"location":"examples/efficiency/multi_fidelity_and_expert_priors/","title":"Multi fidelity and expert priors","text":"<pre><code>import logging\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(float1, float2, integer1, fidelity):\n    loss = -float(np.sum([float1, float2, integer1])) / fidelity\n    return loss\n\n\npipeline_space = dict(\n    float1=neps.FloatParameter(\n        lower=1, upper=1000, log=False, default=600, default_confidence=\"medium\"\n    ),\n    float2=neps.FloatParameter(\n        lower=-10, upper=10, default=0, default_confidence=\"medium\"\n    ),\n    integer1=neps.IntegerParameter(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    fidelity=neps.IntegerParameter(lower=1, upper=10, is_fidelity=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/multifidelity_priors\",\n    max_evaluations_total=25,  # For an alternate stopping method see multi_fidelity.py\n)\n</code></pre>"},{"location":"examples/experimental/cost_aware/","title":"Cost aware","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(\n    pipeline_directory, float1, float2, categorical, integer1, integer2\n):\n    start = time.time()\n    y = -float(np.sum([float1, float2, int(categorical), integer1, integer2]))\n    end = time.time()\n    return {\n        \"loss\": y,\n        \"cost\": (end - start) + float1,\n    }\n\n\npipeline_space = dict(\n    float1=neps.FloatParameter(lower=0, upper=1, log=False),\n    float2=neps.FloatParameter(\n        lower=0, upper=10, log=False, default=10, default_confidence=\"medium\"\n    ),\n    categorical=neps.CategoricalParameter(choices=[0, 1]),\n    integer1=neps.IntegerParameter(lower=0, upper=1, log=False),\n    integer2=neps.IntegerParameter(lower=0, upper=1, log=False),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/cost_aware_example\",\n    searcher=\"cost_cooling\",\n    max_evaluations_total=12,  # TODO(Jan): remove\n    initial_design_size=5,\n    budget=100,\n)\nprevious_results, pending_configs = neps.status(\"results/cost_aware_example\")\n</code></pre>"},{"location":"examples/experimental/expert_priors_for_architecture_and_hyperparameters/","title":"Expert priors for architecture and hyperparameters","text":"<pre><code>import logging\nimport time\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\n\nprimitives = {\n    \"id\": ops.Identity(),\n    \"conv3x3\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 1},\n    \"avg_pool\": {\"op\": ops.AvgPool1x1, \"kernel_size\": 3, \"stride\": 1},\n    \"downsample\": {\"op\": ops.ResNetBasicblock, \"stride\": 2},\n    \"residual\": topos.Residual,\n    \"diamond\": topos.Diamond,\n    \"linear\": topos.get_sequential_n_edge(2),\n    \"diamond_mid\": topos.DiamondMid,\n}\n\nstructure = {\n    \"S\": [\n        \"diamond D2 D2 D1 D1\",\n        \"diamond D1 D2 D2 D1\",\n        \"diamond D1 D1 D2 D2\",\n        \"linear D2 D1\",\n        \"linear D1 D2\",\n        \"diamond_mid D1 D2 D1 D2 D1\",\n        \"diamond_mid D2 D2 Cell D1 D1\",\n    ],\n    \"D2\": [\n        \"diamond D1 D1 D1 D1\",\n        \"linear D1 D1\",\n        \"diamond_mid D1 D1 Cell D1 D1\",\n    ],\n    \"D1\": [\n        \"diamond D1Helper D1Helper Cell Cell\",\n        \"diamond Cell Cell D1Helper D1Helper\",\n        \"diamond D1Helper Cell Cell D1Helper\",\n        \"linear D1Helper Cell\",\n        \"linear Cell D1Helper\",\n        \"diamond_mid D1Helper D1Helper Cell Cell Cell\",\n        \"diamond_mid Cell D1Helper D1Helper D1Helper Cell\",\n    ],\n    \"D1Helper\": [\"linear Cell downsample\"],\n    \"Cell\": [\n        \"residual OPS OPS OPS\",\n        \"diamond OPS OPS OPS OPS\",\n        \"linear OPS OPS\",\n        \"diamond_mid OPS OPS OPS OPS OPS\",\n    ],\n    \"OPS\": [\"conv3x3\", \"conv1x1\", \"avg_pool\", \"id\"],\n}\n\nprior_distr = {\n    \"S\": [1 / 7 for _ in range(7)],\n    \"D2\": [1 / 3 for _ in range(3)],\n    \"D1\": [1 / 7 for _ in range(7)],\n    \"D1Helper\": [1],\n    \"Cell\": [1 / 4 for _ in range(4)],\n    \"OPS\": [1 / 4 for _ in range(4)],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 64 if predecessor_values is None else predecessor_values[\"C_out\"]\n    out_channels = in_channels * 2 if op_name == \"ResNetBasicblock\" else in_channels\n    return dict(C_in=in_channels, C_out=out_channels)\n\n\ndef run_pipeline(some_architecture, some_float, some_integer, some_cat):\n    start = time.time()\n\n    in_channels = 3\n    n_classes = 20\n    base_channels = 64\n    out_channels = 512\n\n    model = some_architecture.to_pytorch()\n    model = nn.Sequential(\n        ops.Stem(base_channels, C_in=in_channels),\n        model,\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(out_channels, n_classes),\n    )\n\n    number_of_params = sum(p.numel() for p in model.parameters())\n    y = abs(1.5e7 - number_of_params)\n\n    if some_cat != \"a\":\n        y *= some_float + some_integer\n    else:\n        y *= -some_float - some_integer\n\n    end = time.time()\n\n    return {\n        \"loss\": y,\n        \"info_dict\": {\n            \"test_score\": y,\n            \"train_time\": end - start,\n        },\n    }\n\n\npipeline_space = dict(\n    some_architecture=neps.FunctionParameter(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n        name=\"pibo\",\n        prior=prior_distr,\n    ),\n    some_float=neps.FloatParameter(\n        lower=1, upper=1000, log=True, default=900, default_confidence=\"medium\"\n    ),\n    some_integer=neps.IntegerParameter(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    some_cat=neps.CategoricalParameter(\n        choices=[\"a\", \"b\", \"c\"], default=\"a\", default_confidence=\"high\"\n    ),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/user_priors_with_graphs\",\n    max_evaluations_total=15,\n    log_prior_weighted=True,\n)\n</code></pre>"},{"location":"examples/experimental/fault_tolerance/","title":"Fault tolerance","text":"<pre><code>\"\"\" To test the fault tolerance, run this script multiple times.\n\"\"\"\n\nimport logging\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nimport neps\n\n\nclass TheModelClass(nn.Module):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef get_model_and_optimizer(learning_rate):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n    model = TheModelClass()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    return model, optimizer\n\n\ndef run_pipeline(pipeline_directory, learning_rate):\n    model, optimizer = get_model_and_optimizer(learning_rate)\n    checkpoint_path = pipeline_directory / \"checkpoint.pth\"\n\n    # Check if there is a previous state of the model training that crashed\n    if checkpoint_path.exists():\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        epoch_already_trained = checkpoint[\"epoch\"]\n        print(f\"Read in model trained for {epoch_already_trained} epochs\")\n    else:\n        epoch_already_trained = 0\n\n    for epoch in range(epoch_already_trained, 101):\n        epoch += 1\n\n        # Train model here ....\n\n        # Repeatedly save your progress\n        if epoch % 10 == 0:\n            torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": optimizer.state_dict(),\n                },\n                checkpoint_path,\n            )\n\n        # Here we simulate a crash! E.g., due to job runtime limits\n        if epoch == 50 and learning_rate &lt; 0.2:\n            print(\"Oh no! A simulated crash!\")\n            exit()\n\n    return learning_rate  # Replace with actual error\n\n\npipeline_space = dict(\n    learning_rate=neps.FloatParameter(lower=0, upper=1),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/fault_tolerance_example\",\n    max_evaluations_total=15,\n)\nprevious_results, pending_configs = neps.status(\"results/fault_tolerance_example\")\n</code></pre>"},{"location":"examples/experimental/hierarchical_architecture/","title":"Hierarchical architecture","text":"<pre><code>from __future__ import annotations\n\nimport logging\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\n\nprimitives = {\n    \"id\": ops.Identity(),\n    \"conv3x3\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 1},\n    \"avg_pool\": {\"op\": ops.AvgPool1x1, \"kernel_size\": 3, \"stride\": 1},\n    \"downsample\": {\"op\": ops.ResNetBasicblock, \"stride\": 2},\n    \"residual\": topos.Residual,\n    \"diamond\": topos.Diamond,\n    \"linear\": topos.get_sequential_n_edge(2),\n    \"diamond_mid\": topos.DiamondMid,\n}\n\nstructure = {\n    \"S\": [\n        \"diamond D2 D2 D1 D1\",\n        \"diamond D1 D2 D2 D1\",\n        \"diamond D1 D1 D2 D2\",\n        \"linear D2 D1\",\n        \"linear D1 D2\",\n        \"diamond_mid D1 D2 D1 D2 D1\",\n        \"diamond_mid D2 D2 Cell D1 D1\",\n    ],\n    \"D2\": [\n        \"diamond D1 D1 D1 D1\",\n        \"linear D1 D1\",\n        \"diamond_mid D1 D1 Cell D1 D1\",\n    ],\n    \"D1\": [\n        \"diamond D1Helper D1Helper Cell Cell\",\n        \"diamond Cell Cell D1Helper D1Helper\",\n        \"diamond D1Helper Cell Cell D1Helper\",\n        \"linear D1Helper Cell\",\n        \"linear Cell D1Helper\",\n        \"diamond_mid D1Helper D1Helper Cell Cell Cell\",\n        \"diamond_mid Cell D1Helper D1Helper D1Helper Cell\",\n    ],\n    \"D1Helper\": [\"linear Cell downsample\"],\n    \"Cell\": [\n        \"residual OPS OPS OPS\",\n        \"diamond OPS OPS OPS OPS\",\n        \"linear OPS OPS\",\n        \"diamond_mid OPS OPS OPS OPS OPS\",\n    ],\n    \"OPS\": [\"conv3x3\", \"conv1x1\", \"avg_pool\", \"id\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 64 if predecessor_values is None else predecessor_values[\"C_out\"]\n    out_channels = in_channels * 2 if op_name == \"ResNetBasicblock\" else in_channels\n    return dict(C_in=in_channels, C_out=out_channels)\n\n\ndef run_pipeline(architecture):\n    in_channels = 3\n    n_classes = 20\n    base_channels = 64\n    out_channels = 512\n\n    model = architecture.to_pytorch()\n    model = nn.Sequential(\n        ops.Stem(base_channels, C_in=in_channels),\n        model,\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(out_channels, n_classes),\n    )\n\n    number_of_params = sum(p.numel() for p in model.parameters())\n    validation_error = abs(1.5e7 - number_of_params)\n\n    return validation_error\n\n\npipeline_space = dict(\n    architecture=neps.FunctionParameter(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n        name=\"makrograph\",\n    )\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hierarchical_architecture_example\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/experimental/hierarchical_architecture_hierarchical_GP/","title":"hierarchical architecture hierarchical GP","text":"<pre><code>from __future__ import annotations\n\nimport logging\nimport time\n\nfrom torch import nn\n\nimport neps\nfrom neps.optimizers.bayesian_optimization.kernels import GraphKernelMapping\nfrom neps.optimizers.bayesian_optimization.models.gp_hierarchy import (\n    ComprehensiveGPHierarchy,\n)\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\n\nprimitives = {\n    \"id\": ops.Identity(),\n    \"conv3x3\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 1},\n    \"avg_pool\": {\"op\": ops.AvgPool1x1, \"kernel_size\": 3, \"stride\": 1},\n    \"downsample\": {\"op\": ops.ResNetBasicblock, \"stride\": 2},\n    \"residual\": topos.Residual,\n    \"diamond\": topos.Diamond,\n    \"linear\": topos.get_sequential_n_edge(2),\n    \"diamond_mid\": topos.DiamondMid,\n}\n\nstructure = {\n    \"S\": [\n        \"diamond D2 D2 D1 D1\",\n        \"diamond D1 D2 D2 D1\",\n        \"diamond D1 D1 D2 D2\",\n        \"linear D2 D1\",\n        \"linear D1 D2\",\n        \"diamond_mid D1 D2 D1 D2 D1\",\n        \"diamond_mid D2 D2 Cell D1 D1\",\n    ],\n    \"D2\": [\n        \"diamond D1 D1 D1 D1\",\n        \"linear D1 D1\",\n        \"diamond_mid D1 D1 Cell D1 D1\",\n    ],\n    \"D1\": [\n        \"diamond D1Helper D1Helper Cell Cell\",\n        \"diamond Cell Cell D1Helper D1Helper\",\n        \"diamond D1Helper Cell Cell D1Helper\",\n        \"linear D1Helper Cell\",\n        \"linear Cell D1Helper\",\n        \"diamond_mid D1Helper D1Helper Cell Cell Cell\",\n        \"diamond_mid Cell D1Helper D1Helper D1Helper Cell\",\n    ],\n    \"D1Helper\": [\"linear Cell downsample\"],\n    \"Cell\": [\n        \"residual OPS OPS OPS\",\n        \"diamond OPS OPS OPS OPS\",\n        \"linear OPS OPS\",\n        \"diamond_mid OPS OPS OPS OPS OPS\",\n    ],\n    \"OPS\": [\"conv3x3\", \"conv1x1\", \"avg_pool\", \"id\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 64 if predecessor_values is None else predecessor_values[\"C_out\"]\n    out_channels = in_channels * 2 if op_name == \"ResNetBasicblock\" else in_channels\n    return dict(C_in=in_channels, C_out=out_channels)\n\n\ndef run_pipeline(architecture):\n    start = time.time()\n\n    in_channels = 3\n    n_classes = 20\n    base_channels = 64\n    out_channels = 512\n\n    model = architecture.to_pytorch()\n    model = nn.Sequential(\n        ops.Stem(base_channels, C_in=in_channels),\n        model,\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(out_channels, n_classes),\n    )\n\n    number_of_params = sum(p.numel() for p in model.parameters())\n    y = abs(1.5e7 - number_of_params) / 1.5e7\n\n    end = time.time()\n\n    return {\n        \"loss\": y,\n        \"info_dict\": {\n            \"test_score\": y,\n            \"train_time\": end - start,\n        },\n    }\n\n\npipeline_space = dict(\n    architecture=neps.FunctionParameter(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n        name=\"makrograph\",\n        return_graph_per_hierarchy=True,\n    )\n)\n\nearly_hierarchies_considered = \"0_1_2_3\"\nhierarchy_considered = [int(hl) for hl in early_hierarchies_considered.split(\"_\")]\ngraph_kernels = [\"wl\"] * (len(hierarchy_considered) + 1)\nwl_h = [2, 1] + [2] * (len(hierarchy_considered) - 1)\ngraph_kernels = [\n    GraphKernelMapping[kernel](\n        h=wl_h[j],\n        oa=False,\n        se_kernel=None,\n    )\n    for j, kernel in enumerate(graph_kernels)\n]\nsurrogate_model = ComprehensiveGPHierarchy\nsurrogate_model_args = {\n    \"graph_kernels\": graph_kernels,\n    \"hp_kernels\": [],\n    \"verbose\": False,\n    \"hierarchy_consider\": hierarchy_considered,\n    \"d_graph_features\": 0,\n    \"vectorial_features\": None,\n}\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hierarchical_architecture_example_new\",\n    max_evaluations_total=15,\n    searcher=\"bayesian_optimization\",\n    surrogate_model=surrogate_model,\n    surrogate_model_args=surrogate_model_args,\n)\n\nprevious_results, pending_configs = neps.status(\n    \"results/hierarchical_architecture_example_new\"\n)\n</code></pre>"},{"location":"examples/experimental/user_priors_from_arbitrary_densities/","title":"User priors from arbitrary densities","text":"<pre><code>import neps\n\ndef run_pipeline(some_float, some_integer, some_cat):\n    if some_cat != \"a\":\n        y = some_float + some_integer\n    else:\n        y = -some_float - some_integer\n    return y\n\n# ========================================================================================\n# Current API\n# User prior is given as a default value and a confidence level specified in the parameter itself\npipeline_space = dict(\n    some_float=neps.FloatParameter(\n        lower=1, upper=1000, log=True, default=900, default_confidence=\"medium\"\n    ),\n    some_integer=neps.IntegerParameter(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    some_cat=neps.CategoricalParameter(\n        choices=[\"a\", \"b\", \"c\"], default=\"a\", default_confidence=\"high\"\n    )\n)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results\",\n    max_evaluations_total=15,\n)\n\n# ========================================================================================\n# New API, variant 01\n# User prior is passed to neps.run and not specified in the pipeline_space\n# The prior is given as one of the following:\n# 1) A (non-factorized) density function that returns the likelihood of a given parameter configuration\n# 2) A dicttionary of marginal densities for each parameter. Then the factorized density is used.\n# 3) A dictionary of default values and confidence levels for each parameter. Then a gaussian prior is used.\n\npipeline_space = dict(\n    some_float=neps.FloatParameter(lower=1, upper=1000, log=True),\n    some_integer=neps.IntegerParameter(lower=0, upper=50),\n    some_cat=neps.CategoricalParameter(choices=[\"a\", \"b\", \"c\"])\n)\n\n# 1) A (non-factorized) density function that returns the likelihood of a given parameter configuration\ndef prior_01(some_float, some_integer, some_cat):\n    # some exponential distribution\n    if some_cat != \"a\":\n        return np.exp(-(some_float + some_integer - 1))\n    else:\n        return np.exp(-(-some_float - some_integer + 1050))\n\n# 2) A dictionary of marginal densities for each parameter. Then the factorized density is used.\nprior_02 = dict(\n    some_float=lambda x: 1/400 if 800 &lt; x &lt; 1000 else 1/1600, # prior on interval [800, 1000]\n    some_integer=lambda k: 30**k/np.math.factorial(k) * np.exp(-k), # poisson prior on integers k=30\n    some_cat=lambda x: 1/2*(x==\"b\") + 1/3*(x==\"c\") + 1/6*(x==\"a\")\n)\n\n# 3) A dictionary of default values and confidence levels for each parameter. Then a gaussian prior is used.\nprior_03 = dict(\n    some_float=dict(default=900, default_confidence=\"medium\"),\n    some_integer=dict(default=35, default_confidence=\"low\"),\n    some_cat=dict(default=\"a\", default_confidence=\"high\")\n)\n\n# Combination of 2) and 3)\nprior_04 = dict(\n    some_float=dict(default=900, default_confidence=\"medium\"),\n    some_integer=lambda k: 30**k/np.math.factorial(k) * np.exp(-k), # poisson prior on integers k=30\n    some_cat=dict(default=\"a\", default_confidence=\"high\")\n)\n\n# Pass the prior to neps.run\n\nneps.run(\n    prior=prior_01, # or prior_02 or prior_03 or prior_04\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results\",\n    max_evaluations_total=15,\n)\n\n# ========================================================================================\n# New API, variant 02\n# User prior is specfied in the pipeline_space and not directly passed to neps.run\n# Same possibiities for priors as in variant 01\n\n# 1) A (non-factorized) density function that returns the likelihood of a given parameter configuration\ndef prior_01(some_float, some_integer, some_cat):\n    # some exponential distribution\n    if some_cat != \"a\":\n        return np.exp(-(some_float + some_integer - 1))\n    else:\n        return np.exp(-(-some_float - some_integer + 1050))\n\npipeline_space_01 = dict(\n    some_float=neps.FloatParameter(lower=1, upper=1000, log=True),\n    some_integer=neps.IntegerParameter(lower=0, upper=50),\n    some_cat=neps.CategoricalParameter(choices=[\"a\", \"b\", \"c\"]),\n    _prior=prior_01\n)\n\n# 2) A dictionary of marginal densities for each parameter. Then the factorized density is used.\npipeline_space_02 = dict(\n    some_float=neps.FloatParameter(\n        lower=1, upper=1000, log=True,\n        prior_fun=lambda x: 1/400 if 800 &lt; x &lt; 1000 else 1/1600\n    ),\n    some_integer=neps.IntegerParameter(lower=0, upper=50,\n        prior_fun=lambda k: 30**k/np.math.factorial(k) * np.exp(-k)\n),\n    some_cat=neps.CategoricalParameter(choices=[\"a\", \"b\", \"c\"],\n        prior_fun=lambda x: 1/2*(x==\"b\") + 1/3*(x==\"c\") + 1/6*(x==\"a\")\n    )\n)\n\n# 3) A dictionary of default values and confidence levels for each parameter. Then a gaussian prior is used.\n# Same as in the current API\npipeline_space_03 = dict(\n    some_float=neps.FloatParameter(\n        lower=1, upper=1000, log=True, default=900, default_confidence=\"medium\"\n    ),\n    some_integer=neps.IntegerParameter(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    some_cat=neps.CategoricalParameter(\n        choices=[\"a\", \"b\", \"c\"], default=\"a\", default_confidence=\"high\"\n    )\n)\n\n# Combination of 2) and 3)\npipeline_space_04 = dict(\n    some_float=neps.FloatParameter(\n        lower=1, upper=1000, log=True, default=900, default_confidence=\"medium\",\n    ),\n    some_integer=neps.IntegerParameter(\n        lower=0, upper=50,\n        prior_fun=lambda k: 30**k/np.math.factorial(k) * np.exp(-k)\n    ),\n    some_cat=neps.CategoricalParameter(\n        choices=[\"a\", \"b\", \"c\"], default=\"a\", default_confidence=\"high\")\n)\n\n# Pass the pipeline_space to neps.run\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space_01, # or pipeline_space_02 or pipeline_space_03 or pipeline_space_04\n    root_directory=\"results\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/template/basic_template/","title":"Basic template","text":"<pre><code>\"\"\"\nNOTE!!! This code is not meant to be executed.\nIt is only to serve as a template to help interface NePS with an existing ML/DL pipeline.\n\nThe following script is designed as a template for using NePS.\nIt describes the crucial components that a user needs to provide in order to interface\na NePS optimizer.\n\nThe 2 crucial components are:\n* The search space, called the `pipeline_space` in NePS\n  * This defines the set of hyperparameters that the optimizer will search over\n  * This declaration also allows injecting priors in the form of defaults per hyperparameter\n* The `run_pipeline` function\n  * This function is called by the optimizer and is responsible for running the pipeline\n  * The function should at the minimum expect the hyperparameters as keyword arguments\n  * The function should return the loss of the pipeline as a float\n    * If the return value is a dictionary, it should have a key called \"loss\" with the loss as a float\n\n\nOverall, running an optimizer from NePS involves 4 clear steps:\n1. Importing neccessary packages including neps.\n2. Designing the search space as a dictionary.\n3. Creating the run_pipeline and returning the loss and other wanted metrics.\n4. Using neps run with the optimizer of choice.\n\"\"\"\n\nimport logging\n\nimport neps\n\n\nlogger = logging.getLogger(\"neps_template.run\")\n\n\ndef pipeline_space() -&gt; dict:\n    # Create the search space based on NEPS parameters and return the dictionary.\n    # Example:\n    space = dict(\n        lr=neps.FloatParameter(\n            lower=1e-5,\n            upper=1e-2,\n            log=True,      # If True, the search space is sampled in log space\n            default=1e-3,  # a non-None value here acts as the mode of the prior distribution\n        ),\n    )\n    return space\n\n\ndef run_pipeline(**config) -&gt; dict | float:\n    # Run pipeline should include the following steps:\n\n    # 1. Defining the model.\n    # 1.1 Load any checkpoint if necessary\n    # 2. Each optimization variable should get its values from the pipeline space.\n    #   Example:\n    #   learning_rate = config[\"lr\"]\n    # 3. The training loop\n    # 3.1 Save any checkpoint if necessary\n    # 4. Returning the loss, which can be either as a single float or as part of\n    #   an info dictionary containing other metrics.\n\n    # Can use global logger to log any information\n    logger.info(f\"Running pipeline with config: {config}\")\n\n    return dict or float\n\n\nif __name__ == \"__main__\":\n    # 1. Creating the logger\n\n\n    # 2. Passing the correct arguments to the neps.run function\n    # For more information on the searcher, please take a look at this link:\n    # https://github.com/automl/neps/tree/master/neps/optimizers/README.md\n\n    neps.run(\n        run_pipeline=run_pipeline,        # User TODO (defined above)\n        pipeline_space=pipeline_space(),  # User TODO (defined above)\n        root_directory=\"results\",\n        max_evaluations_total=10,\n    )\n</code></pre>"},{"location":"examples/template/lightning_template/","title":"Lightning template","text":"<pre><code>\"\"\" Boilerplate code to optimize a simple PyTorch Lightning model.\n\nNOTE!!! This code is not meant to be executed.\nIt is only to serve as a template to help interface NePS with an existing ML/DL pipeline.\n\n\nThe following script describes the crucial components that a user needs to provide\nin order to interface with Lightning.\n\nThe 3 crucial components are:\n* The search space, called the `pipeline_space` in NePS\n  * This defines the set of hyperparameters that the optimizer will search over\n  * This declaration also allows injecting priors in the form of defaults per hyperparameter\n* The `lightning module`\n  * This defines the training, validation, and testing of the model\n  * This distributes the hyperparameters\n  * This can be used to create the Dataloaders for training, validation, and testing\n* The `run_pipeline` function\n  * This function is called by the optimizer and is responsible for running the pipeline\n  * The function should at the minimum expect the hyperparameters as keyword arguments\n  * The function should return the loss of the pipeline as a float\n    * If the return value is a dictionary, it should have a key called \"loss\" with the loss as a float\n\nOverall, running an optimizer from NePS with Lightning involves 5 clear steps:\n1. Importing neccessary packages including NePS and Lightning.\n2. Designing the search space as a dictionary.\n3. Creating the LightningModule with the required parameters\n4. Creating the run_pipeline and returning the loss and other wanted metrics.\n5. Using neps run with the optimizer of choice.\n\nFor a more detailed guide, please refer to:\nhttps://github.com/automl/neps/blob/master/neps_examples/convenience/neps_x_lightning.py\n\"\"\"\nimport logging\n\nimport lightning as L\nimport torch\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\nimport neps\nfrom neps.utils.common import get_initial_directory, load_lightning_checkpoint\n\nlogger = logging.getLogger(\"neps_template.run\")\n\n\ndef pipeline_space() -&gt; dict:\n    # Create the search space based on NEPS parameters and return the dictionary.\n    # IMPORTANT:\n    space = dict(\n        lr=neps.FloatParameter(\n            lower=1e-5,\n            upper=1e-2,\n            log=True,  # If True, the search space is sampled in log space\n            default=1e-3,  # a non-None value here acts as the mode of the prior distribution\n        ),\n        optimizer=neps.CategoricalParameter(choices=[\"Adam\", \"SGD\"], default=\"Adam\"),\n        epochs=neps.IntegerParameter(\n            lower=1,\n            upper=9,\n            is_fidelity=True,  # IMPORTANT to set this to True for the fidelity parameter\n        ),\n    )\n    return space\n\n\nclass LitModel(L.LightningModule):\n    def __init__(self, configuration: dict):\n        super().__init__()\n\n        self.save_hyperparameters(configuration)\n\n        # You can now define your criterion, data transforms, model layers, and\n        # metrics obtained during training\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Forward pass function\n        pass\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        # Training step function\n        # Training metric of choice\n        pass\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        # Validation step function\n        # Validation metric of choice\n        pass\n\n    def test_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        # Test step function\n        # Test metric of choice\n        pass\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        # Define the optimizer base on the configuration\n        if self.hparams.optimizer == \"Adam\":\n            optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n        elif self.hparams.optimizer == \"SGD\":\n            optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams.lr)\n        else:\n            raise ValueError(f\"{self.hparams.optimizer} is not a valid optimizer\")\n        return optimizer\n\n    # Here one can now configure the dataloaders for the model\n    # Further details can be found here:\n    # https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n    # https://github.com/automl/neps/blob/master/neps_examples/convenience/neps_x_lightning.py\n\n\ndef run_pipeline(\n    pipeline_directory,  # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the config's immediate lower fidelity\n    **config,  # The hyperparameters to be used in the pipeline\n) -&gt; dict | float:\n    # Start by getting the initial directory which will be used to store tensorboard\n    # event files and checkpoint files\n    init_dir = get_initial_directory(pipeline_directory)\n    checkpoint_dir = init_dir / \"checkpoints\"\n    tensorboard_dir = init_dir / \"tensorboard\"\n\n    # Create the model\n    model = LitModel(config)\n\n    # Create the TensorBoard logger and the checkpoint callback\n    logger = TensorBoardLogger(\n        save_dir=tensorboard_dir, name=\"data\", version=\"logs\", default_hp_metric=False\n    )\n    checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir)\n\n    # Checking for any checkpoint files and checkpoint data, returns None if\n    # no checkpoint files exist.\n    checkpoint_path, checkpoint_data = load_lightning_checkpoint(\n        previous_pipeline_directory=previous_pipeline_directory,\n        checkpoint_dir=checkpoint_dir,\n    )\n\n    # Create a PyTorch Lightning Trainer\n    epochs = config[\"epochs\"]\n\n    trainer = L.Trainer(\n        logger=logger,\n        max_epochs=epochs,\n        callbacks=[checkpoint_callback],\n    )\n\n    # Train, test, and get their corresponding metrics\n    if checkpoint_path:\n        trainer.fit(model, ckpt_path=checkpoint_path)\n    else:\n        trainer.fit(model)\n    val_loss = trainer.logged_metrics.get(\"val_loss\", None)\n\n    trainer.test(model)\n    test_loss = trainer.logged_metrics.get(\"test_loss\", None)\n\n    # Return a dictionary with the results, or a single float value (loss)\n    return {\n        \"loss\": val_loss,\n        \"info_dict\": {\n            \"test_loss\": test_loss,\n        },\n    }\n\n\n# end of run_pipeline\n\nif __name__ == \"__main__\":\n    neps.run(\n        run_pipeline=run_pipeline,  # User TODO (defined above)\n        pipeline_space=pipeline_space(),  # User TODO (defined above)\n        root_directory=\"results\",\n        max_evaluations_total=25,  # total number of times `run_pipeline` is called\n        searcher=\"priorband\",  # \"priorband_bo\" for longer budgets, and set `initial_design_size``\n    )\n</code></pre>"},{"location":"examples/template/priorband_template/","title":"Priorband template","text":"<pre><code>\"\"\" Boilerplate code to optimize a simple PyTorch model using PriorBand.\n\nNOTE!!! This code is not meant to be executed.\nIt is only to serve as a template to help interface NePS with an existing ML/DL pipeline.\n\n\nThe following script is designed as a template for using `PriorBand` from NePS.\nIt describes the crucial components that a user needs to provide in order to interface PriorBand.\n\nThe 2 crucial components are:\n* The search space, called the `pipeline_space` in NePS\n  * This defines the set of hyperparameters that the optimizer will search over\n  * This declaration also allows injecting priors in the form of defaults per hyperparameter\n* The `run_pipeline` function\n  * This function is called by the optimizer and is responsible for running the pipeline\n  * The function should at the minimum expect the hyperparameters as keyword arguments\n  * The function should return the loss of the pipeline as a float\n    * If the return value is a dictionary, it should have a key called \"loss\" with the loss as a float\n\n\nOverall, running an optimizer from NePS involves 4 clear steps:\n1. Importing neccessary packages including neps.\n2. Designing the search space as a dictionary.\n3. Creating the run_pipeline and returning the loss and other wanted metrics.\n4. Using neps run with the optimizer of choice.\n\"\"\"\n\n\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport neps\nfrom neps.utils.common import load_checkpoint, save_checkpoint\n\nlogger = logging.getLogger(\"neps_template.run\")\n\n\ndef pipeline_space() -&gt; dict:\n    # Create the search space based on NEPS parameters and return the dictionary.\n    # IMPORTANT:\n    space = dict(\n        lr=neps.FloatParameter(\n            lower=1e-5,\n            upper=1e-2,\n            log=True,  # If True, the search space is sampled in log space\n            default=1e-3,  # a non-None value here acts as the mode of the prior distribution\n        ),\n        wd=neps.FloatParameter(\n            lower=0,\n            upper=1e-1,\n            log=True,\n            default=1e-3,\n        ),\n        epoch=neps.IntegerParameter(\n            lower=1,\n            upper=10,\n            is_fidelity=True,  # IMPORTANT to set this to True for the fidelity parameter\n        ),\n    )\n    return space\n\n\ndef run_pipeline(\n    pipeline_directory,  # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the config's immediate lower fidelity\n    **config,  # The hyperparameters to be used in the pipeline\n) -&gt; dict | float:\n    # Defining the model\n    #  Can define outside the function or import from a file, package, etc.\n    class my_model(nn.Module):\n        def __init__(self) -&gt; None:\n            super().__init__()\n            self.linear1 = nn.Linear(in_features=224, out_features=512)\n            self.linear2 = nn.Linear(in_features=512, out_features=10)\n\n        def forward(self, x):\n            x = F.relu(self.linear1(x))\n            x = self.linear2(x)\n            return x\n\n    # Instantiates the model\n    model = my_model()\n\n    # IMPORTANT: Extracting hyperparameters from passed config\n    learning_rate = config[\"lr\"]\n    weight_decay = config[\"wd\"]\n\n    # Initializing the optimizer\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n    )\n\n    ## Checkpointing\n    # loading the checkpoint if it exists\n    previous_state = load_checkpoint(  # predefined function from neps\n        directory=previous_pipeline_directory,\n        model=model,  # relies on pass-by-reference\n        optimizer=optimizer,  # relies on pass-by-reference\n    )\n    # adjusting run budget based on checkpoint\n    if previous_state is not None:\n        epoch_already_trained = previous_state[\"epochs\"]\n        # + Anything else saved in the checkpoint.\n    else:\n        epoch_already_trained = 0\n        # + Anything else with default value.\n\n    # Extracting target epochs from config\n    max_epochs = config.fidelity.value if config.has_fidelity else None\n    if max_epochs is None:\n        raise ValueError(\"The fidelity parameter is not defined in the config.\")\n\n    # User TODO:\n    #  Load relevant data for training and validation\n\n    # Actual model training\n    for epoch in range(epoch_already_trained, max_epochs):\n        # Training loop\n        ...\n        # Validation loop\n        ...\n        logger.info(f\"Epoch: {epoch}, Loss: {...}, Val. acc.: {...}\")\n\n    # Save the checkpoint data in the current directory\n    save_checkpoint(\n        directory=pipeline_directory,\n        values_to_save={\"epochs\": max_epochs},\n        model=model,\n        optimizer=optimizer,\n    )\n\n    # Return a dictionary with the results, or a single float value (loss)\n    return {\n        \"loss\": ...,\n        \"info_dict\": {\n            \"train_accuracy\": ...,\n            \"test_accuracy\": ...,\n        },\n    }\n\n\n# end of run_pipeline\n\n\nif __name__ == \"__main__\":\n    neps.run(\n        run_pipeline=run_pipeline,  # User TODO (defined above)\n        pipeline_space=pipeline_space(),  # User TODO (defined above)\n        root_directory=\"results\",\n        max_evaluations_total=25,  # total number of times `run_pipeline` is called\n        searcher=\"priorband\",  # \"priorband_bo\" for longer budgets, and set `initial_design_size``\n    )\n</code></pre>"},{"location":"reference/analyse/","title":"Analysing Runs","text":"<p>NePS has some convenient utilities to help you to understand the results after you've run your runs. All of the results and state are stored and communicated on disk, which you can access using the <code>python -m neps.status ROOT_DIRECTORY</code> command or integrate live logging directly into your training loop and visualize the results using TensorBoard.</p> <p>To get a quick overview of the results, you can use the <code>python -m neps.plot ROOT_DIRECTORY</code> command.</p>"},{"location":"reference/analyse/#status","title":"Status","text":"<p>To show status information about a neural pipeline search run, use</p> <pre><code>python -m neps.status ROOT_DIRECTORY\n</code></pre> <p>If you need more status information than is printed per default (e.g., the best config over time), please have a look at</p> <pre><code>python -m neps.status --help\n</code></pre> <p>Using <code>watch</code></p> <p>To show the status repeatedly, on unix systems you can use</p> <pre><code>watch --interval 30 python -m neps.status ROOT_DIRECTORY\n</code></pre>"},{"location":"reference/analyse/#cli-commands","title":"CLI commands","text":"<p>To generate plots to the root directory, run</p> <pre><code>python -m neps.plot ROOT_DIRECTORY\n</code></pre> <p>Currently, this creates one plot that shows the best error value across the number of evaluations.</p>"},{"location":"reference/analyse/#whats-on-disk","title":"What's on disk?","text":"<p>In the root directory, NePS maintains several files at all times that are human readable and can be useful If you pass the <code>post_run_summary=</code> argument to <code>neps.run()</code>, NePS will also generate a summary CSV file for you.</p> <code>neps.run(..., post_run_summary=True)</code><code>neps.run(..., post_run_summary=False)</code> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 result.yaml\n\u251c\u2500\u2500 summary_csv\n\u2502  \u251c\u2500\u2500 config_data.csv\n\u2502  \u2514\u2500\u2500 run_status.csv\n\u251c\u2500\u2500 all_losses_and_configs.txt\n\u251c\u2500\u2500 best_loss_trajectory.txt\n\u2514\u2500\u2500 best_loss_with_config_trajectory.txt\n</code></pre> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 result.yaml\n\u251c\u2500\u2500 all_losses_and_configs.txt\n\u251c\u2500\u2500 best_loss_trajectory.txt\n\u2514\u2500\u2500 best_loss_with_config_trajectory.txt\n</code></pre> <p>The <code>config_data.csv</code> contains all configuration details in CSV format, ordered by ascending <code>loss</code>. Details include configuration hyperparameters, any returned result from the <code>run_pipeline</code> function, and metadata information.</p> <p>The <code>run_status.csv</code> provides general run details, such as the number of sampled configs, best configs, number of failed configs, best loss, etc.</p>"},{"location":"reference/analyse/#tensorboard-integration","title":"TensorBoard Integration","text":"<p>TensorBoard serves as a valuable tool for visualizing machine learning experiments, offering the ability to observe losses and metrics throughout the model training process. In NePS, we use this to show metrics of configurations during training in addition to comparisons to different hyperparameters used in the search for better diagnosis of the model.</p>"},{"location":"reference/analyse/#logging-things","title":"Logging Things","text":"<p>The <code>tblogger.log()</code> function is invoked within the model's training loop to facilitate logging of key metrics.</p> <p>We also provide some utility functions to make it easier to log things like:</p> <ul> <li>Scalars through <code>tblogger.scalar_logging()</code></li> <li>Images through <code>tblogger.image_logging()</code></li> </ul> <p>You can provide these through the <code>extra_data=</code> argument in the <code>tblogger.log()</code> function.</p> <p>For an example usage of all these features please refer to the example!</p> <pre><code>tblogger.log(\n    loss=loss,\n    current_epoch=i,\n    write_summary_incumbent=False,  # Set to `True` for a live incumbent trajectory.\n    writer_config_scalar=True,  # Set to `True` for a live loss trajectory for each config.\n    writer_config_hparam=True,  # Set to `True` for live parallel coordinate, scatter plot matrix, and table view.\n\n    # Name the dictionary keys as the names of the values\n    # you want to log and pass one of the following functions\n    # as the values for a successful logging process.\n    extra_data={\n        \"lr_decay\": tblogger.scalar_logging(value=scheduler.get_last_lr()[0]),\n        \"miss_img\": tblogger.image_logging(image=miss_img, counter=2, seed=2),\n        \"layer_gradient1\": tblogger.scalar_logging(value=mean_gradient[0]),\n        \"layer_gradient2\": tblogger.scalar_logging(value=mean_gradient[1]),\n    },\n)\n</code></pre> <p>Tip</p> <p>The logger function is primarily designed for use within the <code>run_pipeline</code> function during the training of the neural network.</p> Quick Reference <code>tblogger.log()</code><code>tblogger.scalar_logging()</code><code>tblogger.image_logging()</code>"},{"location":"reference/analyse/#neps.plot.tensorboard_eval.tblogger.log","title":"neps.plot.tensorboard_eval.tblogger.log  <code>staticmethod</code>","text":"<pre><code>log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None\n) -&gt; None\n</code></pre> <p>Log experiment data to the logger, including scalar values, hyperparameters, and images.</p> PARAMETER DESCRIPTION <code>loss</code> <p>Current loss value.</p> <p> TYPE: <code>float</code> </p> <code>current_epoch</code> <p>Current epoch of the experiment (used as the global step).</p> <p> TYPE: <code>int</code> </p> <code>writer_config_scalar</code> <p>Displaying the loss or accuracy curve on tensorboard (default: True)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>writer_config_hparam</code> <p>Write hyperparameters logging of the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>write_summary_incumbent</code> <p>Set to <code>True</code> for a live incumbent trajectory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>extra_data</code> <p>Additional experiment data for logging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None,\n) -&gt; None:\n    \"\"\"Log experiment data to the logger, including scalar values,\n    hyperparameters, and images.\n\n    Args:\n        loss: Current loss value.\n        current_epoch: Current epoch of the experiment (used as the global step).\n        writer_config_scalar: Displaying the loss or accuracy\n            curve on tensorboard (default: True)\n        writer_config_hparam: Write hyperparameters logging of the configs.\n        write_summary_incumbent: Set to `True` for a live incumbent trajectory.\n        extra_data: Additional experiment data for logging.\n    \"\"\"\n    if tblogger.disable_logging:\n        tblogger.logger_bool = False\n        return\n\n    tblogger.logger_bool = True\n\n    tblogger.current_epoch = current_epoch\n    tblogger.loss = loss\n    tblogger.write_incumbent = write_summary_incumbent\n\n    tblogger._initiate_internal_configurations()\n\n    if writer_config_scalar:\n        tblogger._write_scalar_config(tag=\"Loss\", value=loss)\n\n    if writer_config_hparam:\n        tblogger._write_hparam_config()\n\n    if extra_data is not None:\n        for key in extra_data:\n            if extra_data[key][0] == \"scalar\":\n                tblogger._write_scalar_config(tag=str(key), value=extra_data[key][1])\n\n            elif extra_data[key][0] == \"image\":\n                tblogger._write_image_config(\n                    tag=str(key),\n                    image=extra_data[key][1],\n                    counter=extra_data[key][2],\n                    resize_images=extra_data[key][3],\n                    random_images=extra_data[key][4],\n                    num_images=extra_data[key][5],\n                    seed=extra_data[key][6],\n                )\n</code></pre>"},{"location":"reference/analyse/#neps.plot.tensorboard_eval.tblogger.scalar_logging","title":"neps.plot.tensorboard_eval.tblogger.scalar_logging  <code>staticmethod</code>","text":"<pre><code>scalar_logging(value: float) -&gt; tuple[str, float]\n</code></pre> <p>Prepare a scalar value for logging.</p> PARAMETER DESCRIPTION <code>value</code> <p>The scalar value to be logged.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tuple</code> <p>A tuple containing the logging mode and the value for logging.     The tuple format is (logging_mode, value).</p> <p> TYPE: <code>tuple[str, float]</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef scalar_logging(value: float) -&gt; tuple[str, float]:\n    \"\"\"Prepare a scalar value for logging.\n\n    Args:\n        value (float): The scalar value to be logged.\n\n    Returns:\n        Tuple: A tuple containing the logging mode and the value for logging.\n            The tuple format is (logging_mode, value).\n    \"\"\"\n    logging_mode = \"scalar\"\n    return (logging_mode, value)\n</code></pre>"},{"location":"reference/analyse/#neps.plot.tensorboard_eval.tblogger.image_logging","title":"neps.plot.tensorboard_eval.tblogger.image_logging  <code>staticmethod</code>","text":"<pre><code>image_logging(\n    image: Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | RandomState | None = None\n) -&gt; tuple[\n    str,\n    Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | RandomState | None,\n]\n</code></pre> <p>Prepare an image tensor for logging.</p> PARAMETER DESCRIPTION <code>image</code> <p>Image tensor to be logged.</p> <p> TYPE: <code>Tensor</code> </p> <code>counter</code> <p>Counter value associated with the images.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>resize_images</code> <p>List of integers for image sizes after resizing.</p> <p> TYPE: <code>list[None | int] | None</code> DEFAULT: <code>None</code> </p> <code>random_images</code> <p>Images are randomly selected if True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_images</code> <p>Number of images to log.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>seed</code> <p>Seed value or RandomState instance to control randomness.</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[str, Tensor, int, list[None | int] | None, bool, int, int | RandomState | None]</code> <p>A tuple containing the logging mode and all the necessary parameters for image logging. Tuple: (logging_mode, img_tensor, counter, resize_images,                 random_images, num_images, seed).</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef image_logging(\n    image: torch.Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | np.random.RandomState | None = None,\n) -&gt; tuple[\n    str,\n    torch.Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | np.random.RandomState | None,\n]:\n    \"\"\"Prepare an image tensor for logging.\n\n    Args:\n        image: Image tensor to be logged.\n        counter: Counter value associated with the images.\n        resize_images: List of integers for image sizes after resizing.\n        random_images: Images are randomly selected if True.\n        num_images: Number of images to log.\n        seed: Seed value or RandomState instance to control randomness.\n\n    Returns:\n        A tuple containing the logging mode and all the necessary parameters for\n        image logging.\n        Tuple: (logging_mode, img_tensor, counter, resize_images,\n                        random_images, num_images, seed).\n    \"\"\"\n    logging_mode = \"image\"\n    return (\n        logging_mode,\n        image,\n        counter,\n        resize_images,\n        random_images,\n        num_images,\n        seed,\n    )\n</code></pre>"},{"location":"reference/analyse/#visualizing-results","title":"Visualizing Results","text":"<p>The following command will open a local host for TensorBoard visualizations, allowing you to view them either in real-time or after the run is complete.</p> <pre><code>tensorboard --logdir path/to/root_directory\n</code></pre> <p>This image shows visualizations related to scalar values logged during training. Scalars typically include metrics such as loss, incumbent trajectory, a summary of losses for all configurations, and any additional data provided via the <code>extra_data</code> argument in the <code>tblogger.log</code> function.</p> <p></p> <p>This image represents visualizations related to logged images during training. It could include snapshots of input data, model predictions, or any other image-related information. In our case, we use images to depict instances of incorrect predictions made by the model.</p> <p></p> <p>The following images showcase visualizations related to hyperparameter logging in TensorBoard. These plots include three different views, providing insights into the relationship between different hyperparameters and their impact on the model.</p> <p>In the table view, you can explore hyperparameter configurations across five different trials. The table displays various hyperparameter values alongside corresponding evaluation metrics.</p> <p></p> <p>The parallel coordinate plot offers a holistic perspective on hyperparameter configurations. By presenting multiple hyperparameters simultaneously, this view allows you to observe the interactions between variables, providing insights into their combined influence on the model.</p> <p></p> <p>The scatter plot matrix view provides an in-depth analysis of pairwise relationships between different hyperparameters. By visualizing correlations and patterns, this view aids in identifying key interactions that may influence the model's performance.</p> <p></p>"},{"location":"reference/declarative_usage/","title":"Declarative Usage","text":""},{"location":"reference/declarative_usage/#introduction","title":"Introduction","text":""},{"location":"reference/declarative_usage/#configuring-with-yaml","title":"Configuring with YAML","text":"<p>Configure your experiments using a YAML file, which serves as a central reference for setting up your project. This approach simplifies sharing, reproducing and modifying configurations.</p> <p>Note</p> <p>You can partially define arguments in the YAML file and partially provide the arguments directly to <code>neps.run</code>. However, double referencing is not allowed. You cannot define the same argument in both places.</p>"},{"location":"reference/declarative_usage/#simple-yaml-example","title":"Simple YAML Example","text":"<p>Below is a straightforward YAML configuration example for NePS covering the required arguments.</p> config.yamlrun_neps.py <pre><code># Basic NePS Configuration Example\npipeline_space:\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true  # Log scale for learning rate\n  epochs:\n    lower: 5\n    upper: 20\n    is_fidelity: true\n  optimizer:\n    choices: [adam, sgd, adamw]\n  batch_size: 64\n\nroot_directory: path/to/results       # Directory for result storage\nmax_evaluations_total: 20             # Budget\n</code></pre> <pre><code>import neps\n\ndef run_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n\nif __name__ == \"__main__\":\n    neps.run(run_pipeline, run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#including-run_pipeline-in-run_args-for-external-referencing","title":"Including <code>run_pipeline</code> in <code>run_args</code> for External Referencing","text":"<p>In addition to setting experimental parameters via YAML, this configuration example also specifies the pipeline function and its location, enabling more flexible project structures.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code># Simple NePS configuration including run_pipeline\nrun_pipeline:\n  path: path/to/your/run_pipeline.py  # Path to the function file\n  name: example_pipeline              # Function name within the file\n\npipeline_space:\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true  # Log scale for learning rate\n  epochs:\n    lower: 5\n    upper: 20\n    is_fidelity: true\n  optimizer:\n    choices: [adam, sgd, adamw]\n  batch_size: 64\n\nroot_directory: path/to/results       # Directory for result storage\nmax_evaluations_total: 20             # Budget\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\n# No need to define run_pipeline here. NePS loads it directly from the specified path.\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#comprehensive-yaml-configuration-template","title":"Comprehensive YAML Configuration Template","text":"<p>This example showcases a more comprehensive YAML configuration, which includes not only the essential parameters but also advanced settings for more complex setups.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code># Full Configuration Template for NePS\nrun_pipeline:\n  path: path/to/your/run_pipeline.py  # Path to the function file\n  name: example_pipeline              # Function name within the file\n\npipeline_space:\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true\n  epochs:\n    lower: 5\n    upper: 20\n    is_fidelity: true\n  optimizer:\n    choices: [adam, sgd, adamw]\n  batch_size: 64\n\nroot_directory: path/to/results       # Directory for result storage\nmax_evaluations_total: 20             # Budget\nmax_cost_total:\n\n# Debug and Monitoring\noverwrite_working_directory: true\npost_run_summary: false\ndevelopment_stage_id:\ntask_id:\n\n# Parallelization Setup\nmax_evaluations_per_run:\ncontinue_until_max_evaluation_completed: false\n\n# Error Handling\nloss_value_on_error:\ncost_value_on_error:\nignore_errors:\n\n# Customization Options\nsearcher: hyperband       # Internal key to select a NePS optimizer.\n\n# Hooks\npre_load_hooks:\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\n# Executes the configuration specified in your YAML file\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre> <p>The <code>searcher</code> key used in the YAML configuration corresponds to the same keys used for selecting an optimizer directly through <code>neps.run</code>. For a detailed list of integrated optimizers, see here</p> <p>Note on undefined keys in <code>run_args</code> (config.yaml)</p> <p>Not all configurations are explicitly defined in this template. Any undefined key in the YAML file is mapped to the internal default settings of NePS. This ensures that your experiments can run even if certain parameters are omitted.</p>"},{"location":"reference/declarative_usage/#different-use-cases","title":"Different Use Cases","text":""},{"location":"reference/declarative_usage/#customizing-neps-optimizer","title":"Customizing NePS optimizer","text":"<p>Customize an internal NePS optimizer by specifying its parameters directly under the key <code>searcher</code> in the <code>config.yaml</code> file.</p> <p>Note</p> <p>For <code>searcher_kwargs</code> of <code>neps.run</code>, the optimizer arguments passed via the YAML file and those passed directly via <code>neps.run</code> will be merged. In this special case, if the same argument is referenced in both places, <code>searcher_kwargs</code> will be prioritized and set for this argument.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Customizing NePS Searcher\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      optimizer:\n        choices: [adam, sgd, adamw]\n      epochs: 50\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n    searcher:\n      strategy: bayesian_optimization     # key for neps searcher\n      name: \"my_bayesian\"                 # optional; changing the searcher_name for better recognition\n      # Specific arguments depending on the searcher\n      initial_design_size: 7\n      surrogate_model: gp\n      acquisition: EI\n      acquisition_sampler: random\n      random_interleave_prob: 0.1\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre> <p>For detailed information about the available optimizers and their parameters, please visit the optimizer page</p>"},{"location":"reference/declarative_usage/#testing-multiple-optimizer-configurations","title":"Testing Multiple Optimizer Configurations","text":"<p>Simplify experiments with multiple optimizer settings by outsourcing the optimizer configuration.</p> config.yamlsearcher_setup.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Optimizer settings from YAML configuration\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      optimizer:\n        choices: [adam, sgd, adamw]\n      epochs: 50\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n\n    searcher: path/to/your/searcher_setup.yaml\n</code></pre> <pre><code>strategy: bayesian_optimization\n# Specific arguments depending on the searcher\ninitial_design_size: 7\nsurrogate_model: gp\nacquisition: EI\nlog_prior_weighted: false\nacquisition_sampler: random\nrandom_interleave_prob: 0.1\ndisable_priors: false\nprior_confidence: high\nsample_default_first: false\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#handling-large-search-spaces","title":"Handling Large Search Spaces","text":"<p>Manage large search spaces by outsourcing the pipeline space configuration in a separate YAML file or for keeping track of your experiments.</p> config.yamlpipeline_space.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Pipeline space settings from separate YAML\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space: path/to/your/pipeline_space.yaml\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n</code></pre> <pre><code># Pipeline_space including priors and fidelity\nlearning_rate:\n  lower: 1e-5\n  upper: 1e-1\n  log: true  # Log scale for learning rate\n  default: 1e-2\n  default_confidence: \"medium\"\nepochs:\n  lower: 5\n  upper: 20\n  is_fidelity: true\ndropout_rate:\n  lower: 0.1\n  upper: 0.5\n  default: 0.2\n  default_confidence: \"high\"\noptimizer:\n  choices: [adam, sgd, adamw]\n  default: adam\n  # if default confidence is not defined it gets its default 'low'\nbatch_size: 64\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size, dropout_rate):\n    model = initialize_model(dropout_rate)\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#using-architecture-search-spaces","title":"Using Architecture Search Spaces","text":"<p>Since the option for defining the search space via YAML is limited to HPO, grammar-based search spaces or architecture search spaces must be loaded via a dictionary, which is then referenced in the <code>config.yaml</code>.</p> config.yamlsearch_space.pyrun_pipeline.pyrun_neps.py <pre><code>    # Loading pipeline space from a python dict\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      path: path/to/your/search_space.py  # Path to the dict file\n      name: pipeline_space                # Name of the dict instance\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n</code></pre> <pre><code>from __future__ import annotations\nfrom torch import nn\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\nfrom neps.search_spaces.architecture.primitives import AbstractPrimitive\n\n\nclass DownSampleBlock(AbstractPrimitive):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__(locals())\n        self.conv_a = ReLUConvBN(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1\n        )\n        self.conv_b = ReLUConvBN(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.downsample = nn.Sequential(\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False\n            ),\n        )\n\n    def forward(self, inputs):\n        basicblock = self.conv_a(inputs)\n        basicblock = self.conv_b(basicblock)\n        residual = self.downsample(inputs)\n        return residual + basicblock\n\n\nclass ReLUConvBN(AbstractPrimitive):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super().__init__(locals())\n\n        self.kernel_size = kernel_size\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels, affine=True, track_running_stats=True),\n        )\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass AvgPool(AbstractPrimitive):\n    def __init__(self, **kwargs):\n        super().__init__(kwargs)\n        self.op = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n    def forward(self, x):\n        return self.op(x)\n\n\nprimitives = {\n    \"Sequential15\": topos.get_sequential_n_edge(15),\n    \"DenseCell\": topos.get_dense_n_node_dag(4),\n    \"down\": {\"op\": DownSampleBlock},\n    \"avg_pool\": {\"op\": AvgPool},\n    \"id\": {\"op\": ops.Identity},\n    \"conv3x3\": {\"op\": ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ReLUConvBN, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0},\n}\n\n\nstructure = {\n    \"S\": [\"Sequential15(C, C, C, C, C, down, C, C, C, C, C, down, C, C, C, C, C)\"],\n    \"C\": [\"DenseCell(OPS, OPS, OPS, OPS, OPS, OPS)\"],\n    \"OPS\": [\"id\", \"conv3x3\", \"conv1x1\", \"avg_pool\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 16 if predecessor_values is None else predecessor_values[\"out_channels\"]\n    out_channels = in_channels * 2 if op_name == \"DownSampleBlock\" else in_channels\n    return dict(in_channels=in_channels, out_channels=out_channels)\n\n\npipeline_space = dict(\n    architecture=neps.ArchitectureParameter(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n    ),\n    optimizer=neps.CategoricalParameter(choices=[\"sgd\", \"adam\"]),\n    learning_rate=neps.FloatParameter(lower=10e-7, upper=10e-3, log=True),\n)\n</code></pre> <pre><code>from torch import nn\n\n\ndef example_pipeline(architecture, optimizer, learning_rate):\n    in_channels = 3\n    base_channels = 16\n    n_classes = 10\n    out_channels_factor = 4\n\n    # E.g., in shape = (N, 3, 32, 32) =&gt; out shape = (N, 10)\n    model = architecture.to_pytorch()\n    model = nn.Sequential(\n        nn.Conv2d(in_channels, base_channels, 3, padding=1, bias=False),\n        nn.BatchNorm2d(base_channels),\n        model,\n        nn.BatchNorm2d(base_channels * out_channels_factor),\n        nn.ReLU(inplace=True),\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(base_channels * out_channels_factor, n_classes),\n    )\n    training_loss = train_model(model, optimizer, learning_rate)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#integrating-custom-optimizers","title":"Integrating Custom Optimizers","text":"<p>For people who want to write their own optimizer class as a subclass of the base optimizer, you can load your own custom optimizer class and define its arguments in <code>config.yaml</code>.</p> <p>Note: You can still overwrite arguments via searcher_kwargs of <code>neps.run</code> like for the internal searchers.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Loading Optimizer Class\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      optimizer:\n        choices: [adam, sgd, adamw]\n      epochs: 50\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n    searcher:\n      path: path/to/your/searcher.py      # Path to the class\n      name: CustomOptimizer               # class name within the file\n      # Specific arguments depending on your searcher\n      initial_design_size: 7\n      surrogate_model: gp\n      acquisition: EI\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#adding-custom-hooks-to-your-configuration","title":"Adding Custom Hooks to Your Configuration","text":"<p>Define hooks in your YAML configuration to extend the functionality of your experiment.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Hooks\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      epochs:\n        lower: 5\n        upper: 20\n        is_fidelity: true\n      optimizer:\n        choices: [adam, sgd, adamw]\n      batch_size: 64\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n\n    pre_load_hooks:\n        hook1: path/to/your/hooks.py # (function_name: Path to the function's file)\n        hook2: path/to/your/hooks.py # Different function name 'hook2' from the same file source\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/neps_run/","title":"Configuring and Running Optimizations","text":"<p>The <code>neps.run()</code> function is the core interface for running Hyperparameter and/or architecture search using optimizers in NePS.</p> <p>This document breaks down the core arguments that allow users to control the optimization process in NePS. Please see the documentation of <code>neps.run()</code> for a full list.</p>"},{"location":"reference/neps_run/#required-arguments","title":"Required Arguments","text":"<p>To operate, NePS requires at minimum the following three arguments <code>neps.run(run_pipeline=..., pipeline_space=..., root_directory=...)</code>:</p> <pre><code>import neps\n\ndef run(learning_rate: float, epochs: int) -&gt; float:\n    # Your code here\n\n    return loss\n\nneps.run(\n    run_pipeline=run, # (1)!\n    pipeline_space={, # (2)!\n        \"learning_rate\": neps.Float(1e-3, 1e-1, log=True),\n        \"epochs\": neps.Integer(10, 100)\n    },\n    root_directory=\"path/to/result_dir\" # (3)!\n)\n</code></pre> <ol> <li>The objective function, targeted by NePS for minimization, by evaluation various configurations.     It requires these configurations as input and should return either a dictionary or a sole loss value as the output.     For correct setup instructions, refer to the run pipeline page</li> <li>This defines the search space for the configurations from which the optimizer samples.     It accepts either a dictionary with the configuration names as keys, a path to a YAML configuration file, or a <code>configSpace.ConfigurationSpace</code> object.     For comprehensive information and examples, please refer to the detailed guide available here</li> <li>The directory path where the information about the optimization and its progress gets stored.     This is also used to synchronize multiple calls to <code>neps.run()</code> for parallelization.</li> </ol> <p>To learn more about the <code>run_pipeline</code> function and the <code>pipeline_space</code> configuration, please refer to the run pipeline and pipeline space pages.</p>"},{"location":"reference/neps_run/#budget-how-long-to-run","title":"Budget, how long to run?","text":"<p>To define a budget, provide <code>max_evaluations_total=</code> to <code>neps.run()</code>, to specify the total number of evaluations to conduct before halting the optimization process, or <code>max_cost_total=</code> to specify a cost threshold for your own custom cost metric, such as time, energy, or monetary.</p> <pre><code>```python\ndef run(learning_rate: float, epochs: int) -&gt; float:\n    start = time.time()\n\n    # Your code here\n    end = time.time()\n    duration = end - start\n    return {\"loss\": loss, \"cost\": duration}\n\nneps.run(\n    max_evaluations_total=10, # (1)!\n    max_cost_total=1000, # (2)!\n)\n</code></pre> <ol> <li>Specifies the total number of evaluations to conduct before halting the optimization process.</li> <li>Prevents the initiation of new evaluations once this cost threshold is surpassed.     This can be any kind of cost metric you like, such as time, energy, or monetary, as long as you can calculate it.     This requires adding a cost value to the output of the <code>run_pipeline</code> function, for example, return <code>{'loss': loss, 'cost': cost}</code>.     For more details, please refer here</li> </ol>"},{"location":"reference/neps_run/#getting-some-feedback-logging","title":"Getting some feedback, logging","text":"<p>Most of NePS will not print anything to the console. To view the progress of workers, you can do so by enabling logging through logging.basicConfig.</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nneps.run(...)\n</code></pre> <p>Please refer to Python's logging documentation for more information on how to customize the logging output.</p>"},{"location":"reference/neps_run/#continuing-runs","title":"Continuing Runs","text":"<p>To continue a run, all you need to do is provide the same <code>root_directory=</code> to <code>neps.run()</code> as before, with an increased <code>max_evaluations_total=</code> or <code>max_cost_total=</code>.</p> <pre><code>def run(learning_rate: float, epochs: int) -&gt; float:\n    start = time.time()\n\n    # Your code here\n    end = time.time()\n    duration = end - start\n    return {\"loss\": loss, \"cost\": duration}\n\nneps.run(\n    # Increase the total number of trials from 10 as set previously to 50\n    max_evaluations_total=50,\n)\n</code></pre> <p>If the run previously stopped due to reaching a budget and you specify the same budget, the worker will immediatly stop as it will remember the amount of budget it used previously.</p>"},{"location":"reference/neps_run/#overwriting-a-run","title":"Overwriting a Run","text":"<p>To overwrite a run, simply provide the same <code>root_directory=</code> to <code>neps.run()</code> as before, with the <code>overwrite_working_directory=True</code> argument.</p> <pre><code>neps.run(\n    ...,\n    root_directory=\"path/to/previous_result_dir\",\n    overwrite_working_directory=True,\n)\n</code></pre> <p>Warning</p> <p>This will delete the folder specified by <code>root_directory=</code> and all its contents.</p>"},{"location":"reference/neps_run/#getting-the-results","title":"Getting the results","text":"<p>The results of the optimization process are stored in the <code>root_directory=</code> provided to <code>neps.run()</code>. To obtain a summary of the optimization process, you can enable the <code>post_run_summary=True</code> argument in <code>neps.run()</code>, while will generate a summary csv after the run has finished.</p> Result Directorypython <p>The root directory after utilizing this argument will look like the following:</p> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 result.yaml\n\u251c\u2500\u2500 summary_csv         # Only if post_run_summary=True\n\u2502  \u251c\u2500\u2500 config_data.csv\n\u2502  \u2514\u2500\u2500 run_status.csv\n\u251c\u2500\u2500 all_losses_and_configs.txt\n\u251c\u2500\u2500 best_loss_trajectory.txt\n\u2514\u2500\u2500 best_loss_with_config_trajectory.txt\n</code></pre> <pre><code>neps.run(..., post_run_summary=True)\n</code></pre> <p>To capture the results of the optimization process, you can use tensorbaord logging with various utilities to integrate closer to NePS. For more information, please refer to the analyses page page.</p>"},{"location":"reference/neps_run/#parallelization","title":"Parallelization","text":"<p>NePS utilizes the file-system and locks as a means of communication for implementing parallelization and resuming runs. As a result, you can start multiple <code>neps.run()</code> from different processes however you like and they will synchronize, as long as they share the same <code>root_directory=</code>. Any new workers that come online will automatically pick up work and work together to until the budget is exhausted.</p> Worker scriptShell <pre><code># worker.py\nneps.run(\n    run_pipeline=...,\n    pipeline_space=...,\n    root_directory=\"some/path\",\n    max_evaluations_total=100,\n    max_evaluations_per_run=10, # (1)!\n    continue_until_max_evaluation_completed=True, # (2)!\n    overwrite_working_directory=False, #!!!\n)\n</code></pre> <ol> <li>Limits the number of evaluations for this specific call of <code>neps.run()</code>.</li> <li>Evaluations in-progress count towards max_evaluations_total, halting new ones when this limit is reached.     Setting this to <code>True</code> enables continuous sampling of new evaluations until the total of completed ones meets max_evaluations_total, optimizing resource use in time-sensitive scenarios.</li> </ol> <p>Warning</p> <p>Ensure <code>overwrite_working_directory=False</code> to prevent newly spawned workers from deleting the shared directory!</p> <pre><code># Start 3 workers\npython worker.py &amp;\npython worker.py &amp;\npython worker.py &amp;\n</code></pre>"},{"location":"reference/neps_run/#yaml-configuration","title":"YAML Configuration","text":"<p>You have the option to configure all arguments using a YAML file through <code>neps.run(run_args=...)</code>. For more on yaml usage, please visit the dedicated page on usage of YAML with NePS.</p> <p>Parameters not explicitly defined within this file will receive their default values.</p> Yaml ConfigurationPython <pre><code># path/to/your/config.yaml\nrun_pipeline:\n  path: \"path/to/your/run_pipeline.py\" # File path of the run_pipeline function\n  name: \"name_of_your_run_pipeline\" # Function name\npipeline_space: \"path/to/your/search_space.yaml\" # Path of the search space yaml file\nroot_directory: \"neps_results\"  # Output directory for results\nmax_evaluations_total: 100\npost_run_summary: # Defaults applied if left empty\nsearcher:\n  strategy: \"bayesian_optimization\"\n  initial_design_size: 5\n  surrogate_model: \"gp\"\n</code></pre> <pre><code>neps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/neps_run/#handling-errors","title":"Handling Errors","text":"<p>Things go wrong during optimization runs and it's important to consider what to do in these cases. By default, NePS will halt the optimization process when an error but you can choose to <code>ignore_errors=</code>, providing a <code>loss_value_on_error=</code> and <code>cost_value_on_error=</code> to control what values should be reported to the optimization process.</p> <pre><code>def run(learning_rate: float, epochs: int) -&gt; float:\n    if whoops_my_gpu_died():\n        raise RuntimeError(\"Oh no! GPU died!\")\n\n    ...\n    return loss\n\nneps.run(\n    loss_value_on_error=100, # (1)!\n    cost_value_on_error=10, # (2)!\n    ignore_errors=True, # (3)!\n)\n</code></pre> <ol> <li>If an error occurs, the loss value for that configuration will be set to 100.</li> <li>If an error occurs, the cost value for that configuration will be set to 100.</li> <li>Continue the optimization process even if an error occurs, otherwise throwing an exception and halting the process.</li> </ol> <p>Note</p> <p>Any runs that error will still count towards the total <code>max_evaluations_total</code> or <code>max_evaluations_per_run</code>.</p>"},{"location":"reference/neps_run/#selecting-an-optimizer","title":"Selecting an Optimizer","text":"<p>By default NePS intelligently selects the most appropriate search strategy based on your defined configurations in <code>pipeline_space=</code>, one of the arguments to <code>neps.run()</code>.</p> <p>The characteristics of your search space, as represented in the <code>pipeline_space=</code>, play a crucial role in determining which optimizer NePS will choose. This automatic selection process ensures that the strategy aligns with the specific requirements and nuances of your search space, thereby optimizing the effectiveness of the hyperparameter and/or architecture optimization.</p> <p>You can also manually select a specific or custom optimizer that better matches your specific needs. For more information about the available searchers and how to customize your own, refer here.</p>"},{"location":"reference/neps_run/#managing-experiments","title":"Managing Experiments","text":"<p>While tuning pipelines, it is common to run multiple experiments, perhaps varying the search space, the metric, the model or any other factors of your development. We provide two extra arguments to help manage directories for these, <code>development_stage_id=</code> and <code>task_id=</code>.</p> <pre><code>def run1(learning_rate: float, epochs: int) -&gt; float:\n    # Only tuning learning rate\n\n    return\n\ndef run2(learning_rate: float, l2: float, epochs: int) -&gt; float:\n    # Tuning learning rate and l2 regularization\n\n    return\n\nneps.run(\n    ...,\n    task_id=\"l2_regularization\", # (1)!\n    development_stage_id=\"003\", # (2)!\n)\n</code></pre> <ol> <li>An identifier used when working with multiple development stages.     Instead of creating new root directories, use this identifier to save the results of an optimization run in a separate dev_id folder within the root_directory.</li> <li>An identifier used when the optimization process involves multiple tasks.     This functions similarly to <code>development_stage_id=</code>, but it creates a folder named after the <code>task_id=</code>, providing an organized way to separate results for different tasks within the <code>root_directory=</code>.</li> </ol>"},{"location":"reference/neps_run/#others","title":"Others","text":"<ul> <li><code>pre_load_hooks=</code>: A list of hook functions to be called before loading results.</li> </ul>"},{"location":"reference/optimizers/","title":"Optimizer Configuration","text":"<p>Before running the optimizer for your AutoML tasks, you have several configuration options to tailor the optimization process to your specific needs. These options allow you to customize the optimizer's behavior according to your preferences and requirements.</p>"},{"location":"reference/optimizers/#1-automatic-optimizer-selection","title":"1. Automatic Optimizer Selection","text":"<p>If you prefer not to specify a particular optimizer for your AutoML task, you can simply pass <code>\"default\"</code> or <code>None</code> for the neps searcher. NePS will automatically choose the best optimizer based on the characteristics of your search space. This provides a hassle-free way to get started quickly.</p> <p>The optimizer selection is based on the following characteristics of your <code>pipeline_space</code>:</p> <ul> <li>If it has fidelity: <code>hyperband</code></li> <li>If it has both fidelity and a prior: <code>priorband</code></li> <li>If it has a prior: <code>pibo</code></li> <li>If it has neither: <code>bayesian_optimization</code></li> </ul> <p>For example, running the following format, without specifying a searcher will choose an optimizer depending on the <code>pipeline_space</code> passed. <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    # no searcher specified\n)\n</code></pre></p>"},{"location":"reference/optimizers/#2-choosing-one-of-neps-optimizers","title":"2. Choosing one of NePS Optimizers","text":"<p>We have also prepared some optimizers with specific hyperparameters that we believe can generalize well to most AutoML tasks and use cases. For more details on the available default optimizers and the algorithms that can be called, please refer to the next section on SearcherConfigs.</p> <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    # searcher specified, along with an argument\n    searcher=\"bayesian_optimization\",\n    initial_design_size=5,\n)\n</code></pre> <p>For more optimizers, please refer here .</p>"},{"location":"reference/optimizers/#3-custom-optimizer-configuration-via-yaml","title":"3. Custom Optimizer Configuration via YAML","text":"<p>For users who want more control over the optimizer's hyperparameters, you can create your own YAML configuration file. In this file, you can specify the hyperparameters for your preferred optimizer. To use this custom configuration, provide the path to your YAML file using the <code>searcher</code> parameter when running the optimizer. The library will then load your custom settings and use them for optimization.</p> <p>Here's the format of a custom YAML (<code>custom_bo.yaml</code>) configuration using <code>Bayesian Optimization</code> as an example:</p> <pre><code>strategy: bayesian_optimization\nname: my_custom_bo  # optional; otherwise, your searcher will be named after your YAML file, here 'custom_bo'.\n# Specific arguments depending on the searcher\ninitial_design_size: 7\nsurrogate_model: gp\nacquisition: EI\nlog_prior_weighted: false\nacquisition_sampler: random\nrandom_interleave_prob: 0.1\ndisable_priors: false\nprior_confidence: high\nsample_default_first: false\n</code></pre> <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    searcher=\"path/to/custom_bo.yaml\",\n)\n</code></pre>"},{"location":"reference/optimizers/#4-hyperparameter-overrides","title":"4. Hyperparameter Overrides","text":"<p>If you want to make on-the-fly adjustments to the optimizer's hyperparameters without modifying the YAML configuration file, you can do so by passing keyword arguments (kwargs) to the neps.run function itself. This enables you to fine-tune specific hyperparameters without the need for YAML file updates. Any hyperparameter values provided as kwargs will take precedence over those specified in the YAML configuration.</p> <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    searcher=\"path/to/custom_bo.yaml\",\n    initial_design_size=5,        # overrides value in custom_bo.yaml\n    random_interleave_prob=0.25  # overrides value in custom_bo.yaml\n)\n</code></pre>"},{"location":"reference/optimizers/#note-for-contributors","title":"Note for Contributors","text":"<p>When designing a new optimizer, it's essential to create a YAML configuration file in the <code>default_searcher</code> folder under <code>neps.src.optimizers</code>. This YAML file should contain the default configuration settings that you believe should be used when the user chooses the  searcher.</p> <p>Even when many hyperparameters might be set to their default values as specified in the code, it is still considered good practice to include them in the YAML file. This is because the <code>SearcherConfigs</code> method relies on the arguments from the YAML file to display the optimizer's configuration to the user.</p>"},{"location":"reference/optimizers/#searcher-configurations","title":"Searcher Configurations","text":"<p>The <code>SearcherConfigs</code> class provides a set of useful functions to manage and retrieve default configuration details for NePS optimizers. These functions can help you understand and interact with the available searchers and their associated algorithms and configurations.</p>"},{"location":"reference/optimizers/#importing-searcherconfigs","title":"Importing <code>SearcherConfigs</code>","text":"<p>Before you can use the <code>SearcherConfigs</code> class to manage and retrieve default configuration details for NePS optimizers, make sure to import it into your Python script. You can do this with the following import statement:</p> <pre><code>from neps.optimizers.info import SearcherConfigs\n</code></pre> <p>Once you have imported the class, you can proceed to use its functions to explore the available searchers, algorithms, and configuration details.</p>"},{"location":"reference/optimizers/#list-available-searchers","title":"List Available Searchers","text":"<p>To list all the available searchers that can be used in NePS runs, you can use the <code>get_searchers</code> function. It provides you with a list of searcher names:</p> <pre><code>searchers = SearcherConfigs.get_searchers()\nprint(\"Available searchers:\", searchers)\n</code></pre>"},{"location":"reference/optimizers/#list-available-searching-algorithms","title":"List Available Searching Algorithms","text":"<p>The <code>get_available_algorithms</code> function helps you discover the searching algorithms available within the NePS searchers:</p> <pre><code>algorithms = SearcherConfigs.get_available_algorithms()\nprint(\"Available searching algorithms:\", algorithms)\n</code></pre>"},{"location":"reference/optimizers/#find-searchers-using-a-specific-algorithm","title":"Find Searchers Using a Specific Algorithm","text":"<p>If you want to identify which NePS searchers are using a specific searching algorithm (e.g., Bayesian Optimization, Hyperband, PriorBand...), you can use the <code>get_searcher_from_algorithm</code> function. It returns a list of searchers utilizing the specified algorithm:</p> <pre><code>algorithm = \"bayesian_optimization\"  # Replace with the desired algorithm\nsearchers = SearcherConfigs.get_searcher_from_algorithm(algorithm)\nprint(f\"Searchers using {algorithm}:\", searchers)\n</code></pre>"},{"location":"reference/optimizers/#retrieve-searcher-configuration-details","title":"Retrieve Searcher Configuration Details","text":"<p>To access the configuration details of a specific searcher, you can use the <code>get_searcher_kwargs</code> function. Provide the name of the searcher you are interested in, and it will return the searcher's configuration:</p> <pre><code>searcher_name = \"pibo\"  # Replace with the desired NePS searcher name\nsearcher_kwargs = SearcherConfigs.get_searcher_kwargs(searcher_name)\nprint(f\"Configuration of {searcher_name}:\", searcher_kwargs)\n</code></pre> <p>These functions empower you to explore and manage the available NePS searchers and their configurations effectively.</p>"},{"location":"reference/pipeline_space/","title":"Initializing the Pipeline Space","text":"<p>In NePS, we need to define a <code>pipeline_space</code>. This space can be structured through various approaches, including a Python dictionary, a YAML file, or ConfigSpace. Each of these methods allows you to specify a set of parameter types, ranging from Float and Categorical to specialized architecture parameters. Whether you choose a dictionary, YAML file, or ConfigSpace, your selected method serves as a container or framework within which these parameters are defined and organized. This section not only guides you through the process of setting up your <code>pipeline_space</code> using these methods but also provides detailed instructions and examples on how to effectively incorporate various parameter types, ensuring that NePS can utilize them in the optimization process.</p>"},{"location":"reference/pipeline_space/#parameters","title":"Parameters","text":"<p>NePS currently features 4 primary hyperparameter types:</p> <ul> <li><code>CategoricalParameter</code></li> <li><code>FloatParameter</code></li> <li><code>IntegerParameter</code></li> <li><code>ConstantParameter</code></li> </ul> <p>Using these types, you can define the parameters that NePS will optimize during the search process. The most basic way to pass these parameters is through a Python dictionary, where each key-value pair represents a parameter name and its respective type. For example, the following Python dictionary defines a <code>pipeline_space</code> with four parameters for optimizing a deep learning model:</p> <pre><code>pipeline_space = {\n    \"learning_rate\": neps.FloatParameter(0.00001, 0.1, log=True),\n    \"num_epochs\": neps.IntegerParameter(3, 30, is_fidelity=True),\n    \"optimizer\": neps.CategoricalParameter([\"adam\", \"sgd\", \"rmsprop\"]),\n    \"dropout_rate\": neps.ConstantParameter(0.5),\n}\n\nneps.run(.., pipeline_space=pipeline_space)\n</code></pre> Quick Parameter Reference <code>CategoricalParameter</code><code>FloatParameter</code><code>IntegerParameter</code><code>ConstantParameter</code>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter","title":"neps.search_spaces.hyperparameters.categorical.CategoricalParameter","text":"<pre><code>CategoricalParameter(\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>ParameterWithPrior[CategoricalTypes, CategoricalTypes]</code>, <code>MutatableParameter</code></p> <p>A list of unordered choices for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters that can take on a discrete set of unordered values. For example, the <code>optimizer</code> hyperparameter in a neural network search space can be a <code>CategoricalParameter</code> with choices like <code>[\"adam\", \"sgd\", \"rmsprop\"]</code>.</p> <pre><code>import neps\n\noptimizer_choice = neps.CategoricalParameter(\n    [\"adam\", \"sgd\", \"rmsprop\"],\n    default=\"adam\"\n)\n</code></pre> <p>Please see the <code>Parameter</code>, <code>ParameterWithPrior</code>, <code>MutatableParameter</code> classes for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>choices</code> <p>choices for the hyperparameter.</p> <p> TYPE: <code>Iterable[float | int | str]</code> </p> <code>default</code> <p>default value for the hyperparameter, must be in <code>choices=</code> if provided.</p> <p> TYPE: <code>float | int | str | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/categorical.py</code> <pre><code>def __init__(\n    self,\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `CategoricalParameter`.\n\n    Args:\n        choices: choices for the hyperparameter.\n        default: default value for the hyperparameter, must be in `choices=`\n            if provided.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    choices = list(choices)\n    if len(choices) &lt;= 1:\n        raise ValueError(\"Categorical choices must have more than one value.\")\n\n    super().__init__(value=None, is_fidelity=False, default=default)\n\n    for choice in choices:\n        if not isinstance(choice, (float, int, str)):\n            raise TypeError(\n                f'Choice \"{choice}\" is not of a valid type (float, int, str)'\n            )\n\n    if not all_unique(choices):\n        raise ValueError(f\"Choices must be unique but got duplicates.\\n{choices}\")\n\n    if default is not None and default not in choices:\n        raise ValueError(\n            f\"Default value {default} is not in the provided choices {choices}\"\n        )\n\n    self.choices = list(choices)\n\n    # NOTE(eddiebergman): If there's ever a very large categorical,\n    # then it would be beneficial to have a lookup table for indices as\n    # currently we do a list.index() operation which is O(n).\n    # However for small sized categoricals this is likely faster than\n    # a lookup table.\n    # For now we can just cache the index of the value and default.\n    self._value_index: int | None = None\n\n    self.default_confidence_choice = default_confidence\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n    self.has_prior = self.default is not None\n    self._default_index: int | None = (\n        self.choices.index(default) if default is not None else None\n    )\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.set_default_confidence_score","title":"set_default_confidence_score","text":"<pre><code>set_default_confidence_score(\n    default_confidence: str,\n) -&gt; None\n</code></pre> <p>Set the default confidence score for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default_confidence</code> <p>the choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the confidence score is not a valid choice.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def set_default_confidence_score(self, default_confidence: str) -&gt; None:\n    \"\"\"Set the default confidence score for the hyperparameter.\n\n    Args:\n        default_confidence: the choice of how confident any algorithm should\n            be in the default value being a good value.\n\n    Raises:\n        ValueError: if the confidence score is not a valid choice.\n    \"\"\"\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        cls_name = self.__class__.__name__\n        raise ValueError(\n            f\"Invalid default confidence score: {default_confidence}\"\n            f\" for {cls_name}. Expected one of:\"\n            f\" {list(self.DEFAULT_CONFIDENCE_SCORES.keys())}\"\n        )\n\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter","title":"neps.search_spaces.hyperparameters.float.FloatParameter","text":"<pre><code>FloatParameter(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>NumericalParameter[float]</code></p> <p>A float value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous float values, optionally specifying if it exists on a log scale. For example, <code>l2_norm</code> could be a value in <code>(0.1)</code>, while the <code>learning_rate</code> hyperparameter in a neural network search space can be a <code>FloatParameter</code> with a range of <code>(0.0001, 0.1)</code> but on a log scale.</p> <pre><code>import neps\n\nl2_norm = neps.FloatParameter(0, 1)\nlearning_rate = neps.FloatParameter(1e-4, 1e-1, log=True)\n</code></pre> <p>Please see the <code>NumericalParameter</code> class for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsidering prior based optimization..</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/float.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `FloatParameter`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsidering prior based optimization..\n    \"\"\"\n    super().__init__(\n        lower=float(lower),\n        upper=float(upper),\n        log=log,\n        default=float(default) if default is not None else None,\n        default_confidence=default_confidence,\n        is_fidelity=is_fidelity,\n    )\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter.grid","title":"grid","text":"<pre><code>grid(\n    *, size: int, include_endpoint: bool = True\n) -&gt; list[T]\n</code></pre> <p>Generate a grid of values for the numerical hyperparameter.</p> <p>Duplicates</p> <p>The grid may contain duplicates if the hyperparameter is an integer, for example if the lower bound is <code>0</code> and the upper bound is <code>10</code>, but <code>size=20</code>.</p> PARAMETER DESCRIPTION <code>size</code> <p>The number of values to generate.</p> <p> TYPE: <code>int</code> </p> <code>include_endpoint</code> <p>Whether to include the upper bound in the grid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list[T]</code> <p>A list of values for the numerical hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def grid(self, *, size: int, include_endpoint: bool = True) -&gt; list[T]:\n    \"\"\"Generate a grid of values for the numerical hyperparameter.\n\n    !!! note \"Duplicates\"\n\n        The grid may contain duplicates if the hyperparameter is an integer,\n        for example if the lower bound is `0` and the upper bound is `10`, but\n        `size=20`.\n\n    Args:\n        size: The number of values to generate.\n        include_endpoint: Whether to include the upper bound in the grid.\n\n    Returns:\n        A list of values for the numerical hyperparameter.\n    \"\"\"\n    return [\n        self.normalized_to_value(x)\n        for x in np.linspace(0, 1, num=size, endpoint=include_endpoint)\n    ]\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter.set_default_confidence_score","title":"set_default_confidence_score","text":"<pre><code>set_default_confidence_score(\n    default_confidence: str,\n) -&gt; None\n</code></pre> <p>Set the default confidence score for the hyperparameter.</p> PARAMETER DESCRIPTION <code>default_confidence</code> <p>the choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the confidence score is not a valid choice.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def set_default_confidence_score(self, default_confidence: str) -&gt; None:\n    \"\"\"Set the default confidence score for the hyperparameter.\n\n    Args:\n        default_confidence: the choice of how confident any algorithm should\n            be in the default value being a good value.\n\n    Raises:\n        ValueError: if the confidence score is not a valid choice.\n    \"\"\"\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        cls_name = self.__class__.__name__\n        raise ValueError(\n            f\"Invalid default confidence score: {default_confidence}\"\n            f\" for {cls_name}. Expected one of:\"\n            f\" {list(self.DEFAULT_CONFIDENCE_SCORES.keys())}\"\n        )\n\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter.to_float","title":"to_float","text":"<pre><code>to_float() -&gt; FloatParameter\n</code></pre> <p>Convert the numerical hyperparameter to a float hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_float(self) -&gt; FloatParameter:\n    \"\"\"Convert the numerical hyperparameter to a float hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import FloatParameter\n\n    float_hp = FloatParameter(\n        lower=float(self.lower),\n        upper=float(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=float(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    float_hp.set_value(float(self.value) if self.value is not None else None)\n    return float_hp\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.FloatParameter.to_integer","title":"to_integer","text":"<pre><code>to_integer() -&gt; IntegerParameter\n</code></pre> <p>Convert the numerical hyperparameter to an integer hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_integer(self) -&gt; IntegerParameter:\n    \"\"\"Convert the numerical hyperparameter to an integer hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import IntegerParameter\n\n    as_int = lambda x: int(np.rint(x))\n\n    int_hp = IntegerParameter(\n        lower=as_int(self.lower),\n        upper=as_int(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=as_int(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    int_hp.set_value(as_int(self.value) if self.value is not None else None)\n    return int_hp\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.IntegerParameter","title":"neps.search_spaces.hyperparameters.integer.IntegerParameter","text":"<pre><code>IntegerParameter(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>NumericalParameter[int]</code></p> <p>An integer value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous integer values, optionally specifying f it exists on a log scale. For example, <code>batch_size</code> could be a value in <code>(32, 128)</code>, while the <code>num_layers</code> hyperparameter in a neural network search space can be a <code>IntegerParameter</code> with a range of <code>(1, 1000)</code> but on a log scale.</p> <pre><code>import neps\n\nbatch_size = neps.IntegerParameter(32, 128)\nnum_layers = neps.IntegerParameter(1, 1000, log=True)\n</code></pre> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/integer.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `IntegerParameter`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    lower = int(np.rint(lower))\n    upper = int(np.rint(upper))\n    _size = upper - lower + 1\n    if _size &lt;= 1:\n        raise ValueError(\n            f\"IntegerParameter: expected at least 2 possible values in the range,\"\n            f\" got upper={upper}, lower={lower}.\"\n        )\n\n    super().__init__(\n        lower=int(np.rint(lower)),\n        upper=int(np.rint(upper)),\n        log=log,\n        is_fidelity=is_fidelity,\n        default=int(np.rint(default)) if default is not None else None,\n        default_confidence=default_confidence,\n    )\n\n    # We subtract/add 0.499999 from lower/upper bounds respectively, such that\n    # sampling in the float space gives equal probability for all integer values,\n    # i.e. [x - 0.499999, x + 0.499999]\n    self.float_hp = FloatParameter(\n        lower=self.lower - 0.499999,\n        upper=self.upper + 0.499999,\n        log=self.log,\n        is_fidelity=is_fidelity,\n        default=default,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.IntegerParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.IntegerParameter.grid","title":"grid","text":"<pre><code>grid(\n    *, size: int, include_endpoint: bool = True\n) -&gt; list[T]\n</code></pre> <p>Generate a grid of values for the numerical hyperparameter.</p> <p>Duplicates</p> <p>The grid may contain duplicates if the hyperparameter is an integer, for example if the lower bound is <code>0</code> and the upper bound is <code>10</code>, but <code>size=20</code>.</p> PARAMETER DESCRIPTION <code>size</code> <p>The number of values to generate.</p> <p> TYPE: <code>int</code> </p> <code>include_endpoint</code> <p>Whether to include the upper bound in the grid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list[T]</code> <p>A list of values for the numerical hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def grid(self, *, size: int, include_endpoint: bool = True) -&gt; list[T]:\n    \"\"\"Generate a grid of values for the numerical hyperparameter.\n\n    !!! note \"Duplicates\"\n\n        The grid may contain duplicates if the hyperparameter is an integer,\n        for example if the lower bound is `0` and the upper bound is `10`, but\n        `size=20`.\n\n    Args:\n        size: The number of values to generate.\n        include_endpoint: Whether to include the upper bound in the grid.\n\n    Returns:\n        A list of values for the numerical hyperparameter.\n    \"\"\"\n    return [\n        self.normalized_to_value(x)\n        for x in np.linspace(0, 1, num=size, endpoint=include_endpoint)\n    ]\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.IntegerParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.IntegerParameter.to_float","title":"to_float","text":"<pre><code>to_float() -&gt; FloatParameter\n</code></pre> <p>Convert the numerical hyperparameter to a float hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_float(self) -&gt; FloatParameter:\n    \"\"\"Convert the numerical hyperparameter to a float hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import FloatParameter\n\n    float_hp = FloatParameter(\n        lower=float(self.lower),\n        upper=float(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=float(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    float_hp.set_value(float(self.value) if self.value is not None else None)\n    return float_hp\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.IntegerParameter.to_integer","title":"to_integer","text":"<pre><code>to_integer() -&gt; IntegerParameter\n</code></pre> <p>Convert the numerical hyperparameter to an integer hyperparameter.</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def to_integer(self) -&gt; IntegerParameter:\n    \"\"\"Convert the numerical hyperparameter to an integer hyperparameter.\"\"\"\n    from neps.search_spaces.hyperparameters.integer import IntegerParameter\n\n    as_int = lambda x: int(np.rint(x))\n\n    int_hp = IntegerParameter(\n        lower=as_int(self.lower),\n        upper=as_int(self.upper),\n        is_fidelity=self.is_fidelity,\n        default=as_int(self.default) if self.default is not None else None,\n        default_confidence=self.default_confidence_choice,  # type: ignore\n    )\n    int_hp.set_value(as_int(self.value) if self.value is not None else None)\n    return int_hp\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.ConstantParameter","title":"neps.search_spaces.hyperparameters.constant.ConstantParameter","text":"<pre><code>ConstantParameter(value: T)\n</code></pre> <p>               Bases: <code>Parameter[T, T]</code></p> <p>A constant value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with values that should not change during optimization. For example, the <code>batch_size</code> hyperparameter in a neural network search space can be a <code>ConstantParameter</code> with a value of <code>32</code>.</p> <pre><code>import neps\n\nbatch_size = neps.ConstantParameter(32)\n</code></pre> <p>Note</p> <p>As the name suggests, the value of a <code>ConstantParameter</code> only have one value and so its <code>.default</code> and <code>.value</code> should always be the same.</p> <p>This also implies that the <code>.default</code> can never be <code>None</code>.</p> <p>Please use <code>.set_constant_value()</code> if you need to change the value of the constant parameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>T</code> </p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>def __init__(self, value: T):\n    \"\"\"Create a new `ConstantParameter`.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n    super().__init__(value=value, default=value, is_fidelity=False)  # type: ignore\n    self._value: T = value  # type: ignore\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.ConstantParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: T\n</code></pre> <p>Get the value of the constant parameter.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.ConstantParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: SerializedT) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>SerializedT</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: SerializedT) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    deserialized_value = self.deserialize_value(value)\n    self.set_value(deserialized_value)\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.ConstantParameter.sample","title":"sample","text":"<pre><code>sample() -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Will set the <code>.value</code> to the sampled value.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Will set the [`.value`][neps.search_spaces.Parameter.value] to the\n    sampled value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value()\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.ConstantParameter.set_constant_value","title":"set_constant_value","text":"<pre><code>set_constant_value(value: T) -&gt; None\n</code></pre> <p>Set the value of the constant parameter.</p> <p>Note</p> <p>This method is used to set the <code>.value</code> including the <code>.default</code> It is used internally and should not be used by the user.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>def set_constant_value(self, value: T) -&gt; None:\n    \"\"\"Set the value of the constant parameter.\n\n    !!! note\n\n        This method is used to set the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        including the [`.default`][neps.search_spaces.parameter.Parameter.default]\n        It is used internally and should not be used by the user.\n    \"\"\"\n    self._value = value\n    self.default = value\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.ConstantParameter.set_default","title":"set_default","text":"<pre><code>set_default(default: T | None) -&gt; None\n</code></pre> <p>Set the default of the constant parameter.</p> <p>Note</p> <p>This method is a no-op but will raise a <code>ValueError</code> if the default is different from the current default.</p> <p>Please see <code>.set_constant_value()</code> which can be used to set both the <code>.value</code> and the <code>.default</code> at once</p> PARAMETER DESCRIPTION <code>default</code> <p>value to set the default to.</p> <p> TYPE: <code>T | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the default is different from the current default.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>@override\ndef set_default(self, default: T | None) -&gt; None:\n    \"\"\"Set the default of the constant parameter.\n\n    !!! note\n\n        This method is a no-op but will raise a `ValueError` if the default\n        is different from the current default.\n\n        Please see\n        [`.set_constant_value()`][neps.search_spaces.hyperparameters.constant.ConstantParameter.set_constant_value]\n        which can be used to set both the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        and the [`.default`][neps.search_spaces.parameter.Parameter.default] at once\n\n    Args:\n        default: value to set the default to.\n\n    Raises:\n        ValueError: if the default is different from the current default.\n    \"\"\"\n    if default != self.default:\n        raise ValueError(\n            f\"Constant does not allow changing the default value. \"\n            f\"Tried to set default to {default}, but it is already {self.default}\"\n        )\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.ConstantParameter.set_value","title":"set_value","text":"<pre><code>set_value(value: T | None) -&gt; None\n</code></pre> <p>Set the value of the constant parameter.</p> <p>Note</p> <p>This method is a no-op but will raise a <code>ValueError</code> if the value is different from the current value.</p> <p>Please see <code>.set_constant_value()</code> which can be used to set both the <code>.value</code> and the <code>.default</code> at once</p> PARAMETER DESCRIPTION <code>value</code> <p>value to set the parameter to.</p> <p> TYPE: <code>T | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the value is different from the current value.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>@override\ndef set_value(self, value: T | None) -&gt; None:\n    \"\"\"Set the value of the constant parameter.\n\n    !!! note\n\n        This method is a no-op but will raise a `ValueError` if the value\n        is different from the current value.\n\n        Please see\n        [`.set_constant_value()`][neps.search_spaces.hyperparameters.constant.ConstantParameter.set_constant_value]\n        which can be used to set both the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        and the [`.default`][neps.search_spaces.parameter.Parameter.default] at once\n\n    Args:\n        value: value to set the parameter to.\n\n    Raises:\n        ValueError: if the value is different from the current value.\n    \"\"\"\n    if value != self._value:\n        raise ValueError(\n            f\"Constant does not allow chaning the set value. \"\n            f\"Tried to set value to {value}, but it is already {self.value}\"\n        )\n</code></pre>"},{"location":"reference/pipeline_space/#using-your-knowledge-providing-a-prior","title":"Using your knowledge, providing a Prior","text":"<p>When optimizing, you can provide your own knowledge using the parameters <code>default=</code>. By indicating a <code>default=</code> we take this to be your user prior, your knowledge about where a good value for this parameter lies.</p> <p>You can also specify a <code>default_confidence=</code> to indicate how strongly you want NePS, to focus on these, one of either <code>\"low\"</code>, <code>\"medium\"</code>, or <code>\"high\"</code>.</p> <p>Currently the two major algorithms that exploit this in NePS are <code>PriorBand</code> (prior-based <code>HyperBand</code>) and <code>PiBO</code>, a version of Bayesian Optimization which uses Priors.</p> <pre><code>import neps\n\nneps.run(\n    ...,\n    pipeline_space={\n        \"learning_rate\": neps.FloatParameter(1e-4, 1e-1, log=True, default=1e-2, default_confidence=\"medium\"),\n        \"num_epochs\": neps.IntegerParameter(3, 30, is_fidelity=True),\n        \"optimizer\": neps.CategoricalParameter([\"adam\", \"sgd\", \"rmsprop\"], default=\"adam\", default_confidence=\"low\"),\n        \"dropout_rate\": neps.ConstantParameter(0.5),\n    }\n)\n</code></pre> <p>Must set <code>default=</code> for all parameters, if any</p> <p>If you specify <code>default=</code> for one parameter, you must do so for all your variables. This will be improved in future versions.</p> <p>Interaction with <code>is_fidelity</code></p> <p>If you specify <code>is_fidelity=True</code> for one parameter, the <code>default=</code> and <code>default_confidence=</code> are ignored. This will be dissallowed in future versions.</p>"},{"location":"reference/pipeline_space/#defining-a-pipeline-space-using-yaml","title":"Defining a pipeline space using YAML","text":"<p>Create a YAML file (e.g., <code>./pipeline_space.yaml</code>) with the parameter definitions following this structure.</p> <code>./pipeline_space.yaml</code><code>run.py</code> <pre><code>learning_rate:\n  type: float\n  lower: 2e-3\n  upper: 0.1\n  log: true\n\nnum_epochs:\n  type: int\n  lower: 3\n  upper: 30\n  is_fidelity: true\n\noptimizer:\n  type: categorical\n  choices: [\"adam\", \"sgd\", \"rmsprop\"]\n\ndropout_rate: 0.5\n</code></pre> <pre><code>neps.run(.., pipeline_space=\"./pipeline_space.yaml\")\n</code></pre> <p>When defining the <code>pipeline_space</code> using a YAML file, if the <code>type</code> argument is not specified, the NePS will automatically infer the data type based on the value provided.</p> <ul> <li>If <code>lower</code> and <code>upper</code> are provided, then if they are both integers, the type will be inferred as <code>int</code>,     otherwise as <code>float</code>. You can provide scientific notation for floating-point numbers as well.</li> <li>If <code>choices</code> are provided, the type will be inferred as <code>categorical</code>.</li> <li>If just a numeric or string is provided, the type will be inferred as <code>constant</code>.</li> </ul> <p>If none of these hold, an error will be raised.</p>"},{"location":"reference/pipeline_space/#using-configspace","title":"Using ConfigSpace","text":"<p>For users familiar with the <code>ConfigSpace</code> library, can also define the <code>pipeline_space</code> through <code>ConfigurationSpace()</code></p> <pre><code>from configspace import ConfigurationSpace, Float\n\nconfigspace = ConfigurationSpace(\n    {\n        \"learning_rate\": Float(\"learning_rate\", bounds=(1e-4, 1e-1), log=True)\n        \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"],\n        \"dropout_rate\": 0.5,\n    }\n)\n</code></pre> <p>Warning</p> <p>Parameters you wish to use as a fidelity are not support through ConfigSpace at this time.</p> <p>For additional information on ConfigSpace and its features, please visit the following link.</p>"},{"location":"reference/pipeline_space/#supported-architecture-parameter-types","title":"Supported Architecture parameter Types","text":"<p>A comprehensive documentation for the Architecture parameter is not available at this point.</p> <p>If you are interested in exploring architecture parameters, you can find detailed examples and usage in the following resources:</p> <ul> <li>Basic Usage Examples - Basic usage     examples that can help you understand the fundamentals of Architecture parameters.</li> <li>Experimental Examples - For more advanced     and experimental use cases, including Hierarchical parameters, check out this collection of examples.</li> </ul> <p>Warning</p> <p>The configuration of <code>pipeline_space</code> from a YAML file does not currently support architecture parameter types.</p>"},{"location":"reference/run_pipeline/","title":"The run function","text":""},{"location":"reference/run_pipeline/#introduction","title":"Introduction","text":"<p>The <code>run_pipeline=</code> function is crucial for NePS. It encapsulates the objective function to be minimized, which could range from a regular equation to a full training and evaluation pipeline for a neural network.</p> <p>This function receives the configuration to be utilized from the parameters defined in the search space. Consequently, it executes the same set of instructions or equations based on the provided configuration to minimize the objective function.</p> <p>We will show some basic usages and some functionalites this function would require for successful implementation.</p>"},{"location":"reference/run_pipeline/#types-of-returns","title":"Types of Returns","text":""},{"location":"reference/run_pipeline/#1-single-value","title":"1. Single Value","text":"<p>Assuming the <code>pipeline_space=</code> was already created (have a look at pipeline space for more details). A <code>run_pipeline=</code> function with an objective of minimizing the loss will resemble the following:</p> <pre><code>def run_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n\n    return loss\n</code></pre>"},{"location":"reference/run_pipeline/#2-dictionary","title":"2. Dictionary","text":"<p>In this section, we will outline the special variables that are expected to be returned when the <code>run_pipeline=</code> function returns a dictionary.</p>"},{"location":"reference/run_pipeline/#loss","title":"Loss","text":"<p>One crucial return variable is the <code>loss</code>. This metric serves as a fundamental indicator for the optimizer. One option is to return a dictionary with the <code>loss</code> as a key, along with other user-chosen metrics.</p> <p>Note</p> <p>Loss can be any value that is to be minimized by the objective function.</p> <pre><code>def run_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n    reverse_loss = -loss\n\n    return {\n        \"loss\": loss,\n        \"info_dict\": {\n            \"reverse_loss\": reverse_loss\n            ...\n        }\n    }\n</code></pre>"},{"location":"reference/run_pipeline/#cost","title":"Cost","text":"<p>Along with the return of the <code>loss</code>, the <code>run_pipeline=</code> function would optionally need to return a <code>cost</code> in certain cases. Specifically when the <code>max_cost_total</code> parameter is being utilized in the <code>neps.run</code> function.</p> <p>Note</p> <p><code>max_cost_total</code> sums the cost from all returned configuration results and checks whether the maximum allowed cost has been reached (if so, the search will come to an end).</p> <pre><code>import neps\nimport logging\n\n\ndef run_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n    cost = 2\n\n    return {\n        \"loss\": loss,\n        \"cost\": cost,\n    }\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    neps.run(\n        run_pipeline=run_pipeline,\n        pipeline_space=pipeline_space, # Assuming the pipeline space is defined\n        root_directory=\"results/bo\",\n        max_cost_total=10,\n        searcher=\"bayesian_optimization\",\n    )\n</code></pre> <p>Each evaluation carries a cost of 2. Hence in this example, the Bayesian optimization search is set to perform 5 evaluations.</p>"},{"location":"reference/run_pipeline/#arguments-for-convenience","title":"Arguments for Convenience","text":"<p>NePS also provides the <code>pipeline_directory</code> and the <code>previous_pipeline_directory</code> as arguments in the <code>run_pipeline=</code> function for user convenience.</p> <p>Regard an example to be run with a multi-fidelity searcher, some checkpointing would be advantageos such that one does not have to train the configuration from scratch when the configuration qualifies to higher fidelity brackets.</p> <pre><code>def run_pipeline(\n    pipeline_directory,           # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the immediate lower fidelity config\n    **config,                     # The hyperparameters to be used in the pipeline\n):\n    # Assume element3 is our fidelity element\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    # Load any saved checkpoints\n    checkpoint_name = \"checkpoint.pth\"\n    start_element_3 = 0\n\n    if previous_pipeline_directory is not None:\n        # Read in state of the model after the previous fidelity rung\n        checkpoint = torch.load(previous_pipeline_directory / checkpoint_name)\n        prev_element_3 = checkpoint[\"element_3\"]\n    else:\n        prev_element_3 = 0\n\n    start_element_3 += prev_element_3\n\n    loss = 0\n    for i in range(start_element_3, element_3):\n        loss += element_1 - element_2\n\n    torch.save(\n        {\n            \"element_3\": element_3,\n        },\n        pipeline_directory / checkpoint_name,\n    )\n\n    return loss\n</code></pre> <p>This could allow the proper navigation to the trained models and further train them on higher fidelities without repeating the entire training process.</p>"}]}