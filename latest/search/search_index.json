{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neural Pipeline Search (NePS)","text":"<p>Welcome to NePS, a powerful and flexible Python library for hyperparameter optimization (HPO) and neural architecture search (NAS) with its primary goal: make HPO and NAS usable for deep learners in practice.</p> <p>NePS houses recently published and also well-established algorithms that can all be run massively parallel on distributed setups, with tools to analyze runs, restart runs, etc., all tailored to the needs of deep learning experts.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>In addition to the features offered by traditional HPO and NAS libraries, NePS, e.g., stands out with:</p> <ol> <li>Hyperparameter Optimization (HPO) with Prior Knowledge and Cheap Proxies:  NePS excels in efficiently tuning hyperparameters using algorithms that enable users to make use of their prior knowledge within the search space. This is leveraged by the insights presented in:<ul> <li>PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning</li> <li>\u03c0BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization </li> </ul> </li> <li>Neural Architecture Search (NAS) with General Search Spaces:      NePS is equipped to handle context-free grammar search spaces, providing advanced capabilities for designing and optimizing architectures. this is leveraged by the insights presented in:<ul> <li>Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars </li> </ul> </li> <li>Easy Parallelization and Design Tailored to DL:       NePS simplifies the process of parallelizing optimization tasks both on individual computers and in distributed      computing environments. As NePS is made for deep learners, all technical choices are made with DL in mind and common      DL tools such as Tensorboard are embraced.</li> </ol> <p>Tip</p> <p>Check out:</p> <ul> <li>Reference documentation for a quick overview.</li> <li>API for a more detailed reference.</li> <li>Examples for copy-pastable code to get started.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the latest release from PyPI run</p> <pre><code>pip install neural-pipeline-search\n</code></pre> <p>To get the latest version from Github run</p> <pre><code>pip install git+https://github.com/automl/neps.git\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Using <code>neps</code> always follows the same pattern:</p> <ol> <li>Define a <code>run_pipeline</code> function capable of evaluating different architectural and/or hyperparameter configurations    for your problem.</li> <li>Define a search space named <code>pipeline_space</code> of those Parameters e.g. via a dictionary</li> <li>Call <code>neps.run</code> to optimize <code>run_pipeline</code> over <code>pipeline_space</code></li> </ol> <p>In code, the usage pattern can look like this:</p> <pre><code>import neps\nimport logging\n\n\n# 1. Define a function that accepts hyperparameters and computes the validation error\ndef run_pipeline(\n        hyperparameter_a: float, hyperparameter_b: int, architecture_parameter: str\n) -&gt; dict:\n    # Create your model\n    model = MyModel(architecture_parameter)\n\n    # Train and evaluate the model with your training pipeline\n    validation_error = train_and_eval(\n        model, hyperparameter_a, hyperparameter_b\n    )\n    return validation_error\n\n\n# 2. Define a search space of parameters; use the same parameter names as in run_pipeline\npipeline_space = dict(\n    hyperparameter_a=neps.Float(\n        lower=0.001, upper=0.1, log=True  # The search space is sampled in log space\n    ),\n    hyperparameter_b=neps.Integer(lower=1, upper=42),\n    architecture_parameter=neps.Categorical([\"option_a\", \"option_b\"]),\n)\n\n# 3. Run the NePS optimization\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"path/to/save/results\",  # Replace with the actual path.\n    max_evaluations_total=100,\n)\n</code></pre>"},{"location":"#declarative-usage","title":"Declarative Usage","text":"<p>NePS offers a declarative approach to efficiently manage experiments. This method is particularly suitable for conducting and managing a large number of experiments with different settings. Below is the example from Basic Usage: <pre><code>run_pipeline:\n  path: path/to/your/run_pipeline.py  # Path to the function file\n  name: run_pipeline              # Function name within the file\nroot_directory: \"path/to/save/results\"\npipeline_space:\n  hyperparameter_a:\n    lower: 1e-3\n    upper: 1e-1\n    log: True  # Log scale for learning rate\n  hyperparameter_b:\n    lower: 1\n    upper: 42\n  architecture_parameter:\n    choices: [option_a, option_b]\n\nmax_evaluations_total: 100\n</code></pre> <pre><code>neps run --run-args path/to/your/config.yaml\n</code></pre> If you would like to learn more about how to use this, click here.</p>"},{"location":"#examples","title":"Examples","text":"<p>Discover how NePS works through these examples:</p> <ul> <li> <p>Hyperparameter Optimization: Learn the essentials of hyperparameter optimization with NePS.</p> </li> <li> <p>Multi-Fidelity Optimization: Understand how to leverage multi-fidelity optimization for efficient model tuning.</p> </li> <li> <p>Utilizing Expert Priors for Hyperparameters: Learn how to incorporate expert priors for more efficient hyperparameter selection.</p> </li> <li> <p>Architecture Search: Dive into (hierarchical) architecture search in NePS.</p> </li> <li> <p>Additional NePS Examples: Explore more examples, including various use cases and advanced configurations in NePS.</p> </li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Please see the documentation for contributors.</p>"},{"location":"#citations","title":"Citations","text":"<p>For pointers on citing the NePS package and papers refer to our documentation on citations.</p>"},{"location":"citations/","title":"Citations","text":""},{"location":"citations/#citation-of-the-software","title":"Citation of The Software","text":"<p>For citing NePS, please refer to the following:</p>"},{"location":"citations/#apa-style","title":"APA Style","text":"<pre><code>Stoll, D., Mallik, N., Schrodi, S., Janowski, M., Garibov, S., Abou Chakra, T., Rogalla, D., Bergman, E., Hvarfner, C., Binxin, R., Kober, N., Vallaeys, T., &amp; Hutter, F. (2023). Neural Pipeline Search (NePS) (Version 0.11.0) [Computer software]. https://github.com/automl/neps\n</code></pre>"},{"location":"citations/#bibtex-style","title":"BibTex Style","text":"<pre><code>@software{Stoll_Neural_Pipeline_Search_2023,\nauthor = {Stoll, Danny and Mallik, Neeratyoy and Schrodi, Simon and Janowski, Maciej and Garibov, Samir and Abou Chakra, Tarek and Rogalla, Daniel and Bergman, Eddie and Hvarfner, Carl and Binxin, Ru and Kober, Nils and Vallaeys, Th\u00e9ophane and Hutter, Frank},\nmonth = oct,\ntitle = {{Neural Pipeline Search (NePS)}},\nurl = {https://github.com/automl/neps},\nversion = {0.11.0},\nyear = {2023}\n}\n</code></pre>"},{"location":"citations/#citation-of-papers","title":"Citation of Papers","text":""},{"location":"citations/#priorband","title":"PriorBand","text":"<p>If you have used PriorBand as the optimizer, please use the bibtex below:</p> <pre><code>@inproceedings{mallik2023priorband,\ntitle = {PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning},\nauthor = {Neeratyoy Mallik and Eddie Bergman and Carl Hvarfner and Danny Stoll and Maciej Janowski and Marius Lindauer and Luigi Nardi and Frank Hutter},\nyear = {2023},\nbooktitle = {Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)},\nkeywords = {}\n}\n</code></pre>"},{"location":"citations/#hierarchichal-nas-with-context-free-grammars","title":"Hierarchichal NAS with Context-free Grammars","text":"<p>If you have used the context-free grammar search space and the graph kernels implemented in NePS for the paper Hierarchical NAS, please use the bibtex below:</p> <pre><code>@inproceedings{schrodi2023hierarchical,\ntitle = {Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars},\nauthor = {Simon Schrodi and Danny Stoll and Binxin Ru and Rhea Sanjay Sukthanker and Thomas Brox and Frank Hutter},\nyear = {2023},\nbooktitle = {Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)},\nkeywords = {}\n}\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Getting started with NePS involves a straightforward yet powerful process, centering around its three main components. This approach ensures flexibility and efficiency in evaluating different architecture and hyperparameter configurations for your problem.</p> <p>NePS requires Python 3.8 or higher. You can install it via pip or from source.</p> <pre><code>pip install neural-pipeline-search\n</code></pre>"},{"location":"getting_started/#the-3-main-components","title":"The 3 Main Components","text":"<ol> <li> <p>Execute with <code>neps.run()</code>: Optimize your <code>run_pipeline=</code> over the <code>pipeline_space=</code> using this function. For a thorough overview of the arguments and their explanations, check out the detailed documentation.</p> </li> <li> <p>Define a <code>run_pipeline=</code> Function: This function is essential for evaluating different configurations. You'll implement the specific logic for your problem within this function. For detailed instructions on initializing and effectively using <code>run_pipeline=</code>, refer to the guide.</p> </li> <li> <p>Establish a <code>pipeline_space=</code>: Your search space for defining parameters. You can structure this in various formats, including dictionaries, YAML, or ConfigSpace. The guide offers insights into defining and configuring your search space.</p> </li> </ol> <p>By following these steps and utilizing the extensive resources provided in the guides, you can tailor NePS to meet your specific requirements, ensuring a streamlined and effective optimization process.</p>"},{"location":"getting_started/#basic-usage","title":"Basic Usage","text":"<p>In code, the usage pattern can look like this:</p> <pre><code>import neps\nimport logging\n\n\ndef run_pipeline(  # (1)!\n        hyperparameter_a: float,\n        hyperparameter_b: int,\n        architecture_parameter: str,\n) -&gt; dict:\n    # insert here your own model\n    model = MyModel(architecture_parameter)\n\n    # insert here your training/evaluation pipeline\n    validation_error, training_error = train_and_eval(\n        model, hyperparameter_a, hyperparameter_b\n    )\n\n    return {\n        \"loss\": validation_error,  # ! (2)\n        \"info_dict\": {\n            \"training_error\": training_error\n            # + Other metrics\n        },\n    }\n\n\npipeline_space = {  # (3)!\n    \"hyperparameter_b\": neps.Integer(1, 42, is_fidelity=True),  # ! (4)\n    \"hyperparameter_a\": neps.Float(1e-3, 1e-1, log=True)  # ! (5)\n    \"architecture_parameter\": neps.Categorical([\"option_a\", \"option_b\", \"option_c\"]),\n}\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    neps.run(\n        run_pipeline=run_pipeline,\n        pipeline_space=pipeline_space,\n        root_directory=\"path/to/save/results\",  # Replace with the actual path.\n        max_evaluations_total=100,\n        searcher=\"hyperband\"  # Optional specifies the search strategy,\n        # otherwise NePs decides based on your data.\n    )\n</code></pre> <ol> <li>Define a function that accepts hyperparameters and computes the validation error.</li> <li>Return a dictionary with the objective to minimize and any additional information.</li> <li>Define a search space of the parameters of interest; ensure that the names are consistent with those defined in the run_pipeline function.</li> <li>Use <code>is_fidelity=True</code> for a multi-fidelity approach.</li> <li>Use <code>log=True</code> for a log-spaced hyperparameter.</li> </ol> <p>Tip</p> <p>Please visit the full reference for a more comprehensive walkthrough of defining budgets, optimizers, YAML configuration, parallelism, and more.</p>"},{"location":"getting_started/#examples","title":"Examples","text":"<p>Discover the features of NePS through these practical examples:</p> <ul> <li> <p>Hyperparameter Optimization (HPO): Learn the essentials of hyperparameter optimization with NePS.</p> </li> <li> <p>Architecture Search with Primitives: Dive into architecture search using primitives in NePS.</p> </li> <li> <p>Multi-Fidelity Optimization: Understand how to leverage multi-fidelity optimization for efficient model tuning.</p> </li> <li> <p>Utilizing Expert Priors for Hyperparameters: Learn how to incorporate expert priors for more efficient hyperparameter selection.</p> </li> <li> <p>Additional NePS Examples: Explore more examples, including various use cases and advanced configurations in NePS.</p> </li> </ul>"},{"location":"api/","title":"API","text":"<p>Use the tree to navigate the API documentation.</p>"},{"location":"api/neps/api/","title":"Api","text":""},{"location":"api/neps/api/#neps.api","title":"neps.api","text":"<p>API for the neps package.</p>"},{"location":"api/neps/api/#neps.api.run","title":"run","text":"<pre><code>run(\n    run_pipeline: Callable | None = Default(None),\n    root_directory: str | Path | None = Default(None),\n    pipeline_space: (\n        dict[str, Parameter]\n        | str\n        | Path\n        | ConfigurationSpace\n        | None\n    ) = Default(None),\n    run_args: str | Path | None = Default(None),\n    overwrite_working_directory: bool = Default(False),\n    post_run_summary: bool = Default(True),\n    development_stage_id=Default(None),\n    task_id=Default(None),\n    max_evaluations_total: int | None = Default(None),\n    max_evaluations_per_run: int | None = Default(None),\n    continue_until_max_evaluation_completed: bool = Default(\n        False\n    ),\n    max_cost_total: int | float | None = Default(None),\n    ignore_errors: bool = Default(False),\n    loss_value_on_error: None | float = Default(None),\n    cost_value_on_error: None | float = Default(None),\n    pre_load_hooks: Iterable | None = Default(None),\n    searcher: (\n        Literal[\n            \"default\",\n            \"bayesian_optimization\",\n            \"random_search\",\n            \"hyperband\",\n            \"priorband\",\n            \"mobster\",\n            \"asha\",\n        ]\n        | BaseOptimizer\n        | Path\n    ) = Default(\"default\"),\n    **searcher_kwargs\n) -&gt; None\n</code></pre> <p>Run a neural pipeline search.</p> To parallelize <p>To run a neural pipeline search with multiple processes or machines, simply call run(.) multiple times (optionally on different machines). Make sure that root_directory points to the same folder on the same filesystem, otherwise, the multiple calls to run(.) will be independent.</p> PARAMETER DESCRIPTION <code>run_pipeline</code> <p>The objective function to minimize.</p> <p> TYPE: <code>Callable | None</code> DEFAULT: <code>Default(None)</code> </p> <code>pipeline_space</code> <p>The search space to minimize over.</p> <p> TYPE: <code>dict[str, Parameter] | str | Path | ConfigurationSpace | None</code> DEFAULT: <code>Default(None)</code> </p> <code>root_directory</code> <p>The directory to save progress to. This is also used to synchronize multiple calls to run(.) for parallelization.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>Default(None)</code> </p> <code>run_args</code> <p>An option for providing the optimization settings e.g. max_evaluations_total in a YAML file.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>Default(None)</code> </p> <code>overwrite_working_directory</code> <p>If true, delete the working directory at the start of the run. This is, e.g., useful when debugging a run_pipeline function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>Default(False)</code> </p> <code>post_run_summary</code> <p>If True, creates a csv file after each worker is done, holding summary information about the configs and results.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>Default(True)</code> </p> <code>development_stage_id</code> <p>ID for the current development stage. Only needed if you work with multiple development stages.</p> <p> DEFAULT: <code>Default(None)</code> </p> <code>task_id</code> <p>ID for the current task. Only needed if you work with multiple tasks.</p> <p> DEFAULT: <code>Default(None)</code> </p> <code>max_evaluations_total</code> <p>Number of evaluations after which to terminate.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>Default(None)</code> </p> <code>max_evaluations_per_run</code> <p>Number of evaluations the specific call to run(.) should maximally do.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>Default(None)</code> </p> <code>continue_until_max_evaluation_completed</code> <p>If true, only stop after max_evaluations_total have been completed. This is only relevant in the parallel setting.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>Default(False)</code> </p> <code>max_cost_total</code> <p>No new evaluations will start when this cost is exceeded. Requires returning a cost in the run_pipeline function, e.g., <code>return dict(loss=loss, cost=cost)</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>Default(None)</code> </p> <code>ignore_errors</code> <p>Ignore hyperparameter settings that threw an error and do not raise an error. Error configs still count towards max_evaluations_total.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>Default(False)</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>Default(None)</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>Default(None)</code> </p> <code>pre_load_hooks</code> <p>List of functions that will be called before load_results().</p> <p> TYPE: <code>Iterable | None</code> DEFAULT: <code>Default(None)</code> </p> <code>searcher</code> <p>Which optimizer to use. Can be a string identifier, an instance of BaseOptimizer, or a Path to a custom optimizer.</p> <p> TYPE: <code>Literal['default', 'bayesian_optimization', 'random_search', 'hyperband', 'priorband', 'mobster', 'asha'] | BaseOptimizer | Path</code> DEFAULT: <code>Default('default')</code> </p> <code>**searcher_kwargs</code> <p>Will be passed to the searcher. This is usually only needed by neps develolpers.</p> <p> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If deprecated argument working_directory is used.</p> <code>ValueError</code> <p>If root_directory is None.</p> Example <p>import neps</p> <p>def run_pipeline(some_parameter: float):    validation_error = -some_parameter    return validation_error</p> <p>pipeline_space = dict(some_parameter=neps.Float(lower=0, upper=1))</p> <p>logging.basicConfig(level=logging.INFO) neps.run(    run_pipeline=run_pipeline,    pipeline_space=pipeline_space,    root_directory=\"usage_example\",    max_evaluations_total=5, )</p> Source code in <code>neps/api.py</code> <pre><code>def run(\n    run_pipeline: Callable | None = Default(None),\n    root_directory: str | Path | None = Default(None),\n    pipeline_space: (\n        dict[str, Parameter] | str | Path | CS.ConfigurationSpace | None\n    ) = Default(None),\n    run_args: str | Path | None = Default(None),\n    overwrite_working_directory: bool = Default(False),\n    post_run_summary: bool = Default(True),\n    development_stage_id=Default(None),\n    task_id=Default(None),\n    max_evaluations_total: int | None = Default(None),\n    max_evaluations_per_run: int | None = Default(None),\n    continue_until_max_evaluation_completed: bool = Default(False),\n    max_cost_total: int | float | None = Default(None),\n    ignore_errors: bool = Default(False),\n    loss_value_on_error: None | float = Default(None),\n    cost_value_on_error: None | float = Default(None),\n    pre_load_hooks: Iterable | None = Default(None),\n    searcher: (\n        Literal[\n            \"default\",\n            \"bayesian_optimization\",\n            \"random_search\",\n            \"hyperband\",\n            \"priorband\",\n            \"mobster\",\n            \"asha\",\n        ]\n        | BaseOptimizer\n        | Path\n    ) = Default(\"default\"),\n    **searcher_kwargs,\n) -&gt; None:\n    \"\"\"Run a neural pipeline search.\n\n    To parallelize:\n        To run a neural pipeline search with multiple processes or machines,\n        simply call run(.) multiple times (optionally on different machines). Make sure\n        that root_directory points to the same folder on the same filesystem, otherwise,\n        the multiple calls to run(.) will be independent.\n\n    Args:\n        run_pipeline: The objective function to minimize.\n        pipeline_space: The search space to minimize over.\n        root_directory: The directory to save progress to. This is also used to\n            synchronize multiple calls to run(.) for parallelization.\n        run_args: An option for providing the optimization settings e.g.\n            max_evaluations_total in a YAML file.\n        overwrite_working_directory: If true, delete the working directory at the start of\n            the run. This is, e.g., useful when debugging a run_pipeline function.\n        post_run_summary: If True, creates a csv file after each worker is done,\n            holding summary information about the configs and results.\n        development_stage_id: ID for the current development stage. Only needed if\n            you work with multiple development stages.\n        task_id: ID for the current task. Only needed if you work with multiple\n            tasks.\n        max_evaluations_total: Number of evaluations after which to terminate.\n        max_evaluations_per_run: Number of evaluations the specific call to run(.) should\n            maximally do.\n        continue_until_max_evaluation_completed: If true, only stop after\n            max_evaluations_total have been completed. This is only relevant in the\n            parallel setting.\n        max_cost_total: No new evaluations will start when this cost is exceeded. Requires\n            returning a cost in the run_pipeline function, e.g.,\n            `return dict(loss=loss, cost=cost)`.\n        ignore_errors: Ignore hyperparameter settings that threw an error and do not raise\n            an error. Error configs still count towards max_evaluations_total.\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error and will use given loss value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error and will use given cost value instead. default: None\n        pre_load_hooks: List of functions that will be called before load_results().\n        searcher: Which optimizer to use. Can be a string identifier, an\n            instance of BaseOptimizer, or a Path to a custom optimizer.\n        **searcher_kwargs: Will be passed to the searcher. This is usually only needed by\n            neps develolpers.\n\n    Raises:\n        ValueError: If deprecated argument working_directory is used.\n        ValueError: If root_directory is None.\n\n\n    Example:\n        &gt;&gt;&gt; import neps\n\n        &gt;&gt;&gt; def run_pipeline(some_parameter: float):\n        &gt;&gt;&gt;    validation_error = -some_parameter\n        &gt;&gt;&gt;    return validation_error\n\n        &gt;&gt;&gt; pipeline_space = dict(some_parameter=neps.Float(lower=0, upper=1))\n\n        &gt;&gt;&gt; logging.basicConfig(level=logging.INFO)\n        &gt;&gt;&gt; neps.run(\n        &gt;&gt;&gt;    run_pipeline=run_pipeline,\n        &gt;&gt;&gt;    pipeline_space=pipeline_space,\n        &gt;&gt;&gt;    root_directory=\"usage_example\",\n        &gt;&gt;&gt;    max_evaluations_total=5,\n        &gt;&gt;&gt; )\n    \"\"\"\n    if \"working_directory\" in searcher_kwargs:\n        raise ValueError(\n            \"The argument 'working_directory' is deprecated, please use 'root_directory' \"\n            \"instead\"\n        )\n\n    if \"budget\" in searcher_kwargs:\n        warnings.warn(\n            \"The argument: 'budget' is deprecated. In the neps.run call, please, use \"\n            \"'max_cost_total' instead. In future versions using `budget` will fail.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        max_cost_total = searcher_kwargs[\"budget\"]\n        del searcher_kwargs[\"budget\"]\n\n    settings = Settings(locals(), run_args)\n    # TODO: check_essentials,\n\n    # DO NOT use any neps arguments directly; instead, access them via the Settings class.\n    if settings.pre_load_hooks is None:\n        settings.pre_load_hooks = []\n\n    logger.info(f\"Starting neps.run using root directory {settings.root_directory}\")\n\n    # Used to create the yaml holding information about the searcher.\n    # Also important for testing and debugging the api.\n    searcher_info = {\n        \"searcher_name\": \"\",\n        \"searcher_alg\": \"\",\n        \"searcher_selection\": \"\",\n        \"neps_decision_tree\": True,\n        \"searcher_args\": {},\n    }\n\n    # special case if you load your own optimizer via run_args\n    if inspect.isclass(settings.searcher):\n        if issubclass(settings.searcher, BaseOptimizer):\n            search_space = SearchSpace(**settings.pipeline_space)\n            # aligns with the behavior of the internal neps searcher which also overwrites\n            # its arguments by using searcher_kwargs\n            # TODO habe hier searcher kwargs gedroppt, sprich das merging muss davor statt\n            # finden\n            searcher_info[\"searcher_args\"] = settings.searcher_kwargs\n            settings.searcher = settings.searcher(\n                search_space, **settings.searcher_kwargs\n            )\n        else:\n            # Raise an error if searcher is not a subclass of BaseOptimizer\n            raise TypeError(\n                \"The provided searcher must be a class that inherits from BaseOptimizer.\"\n            )\n\n    if isinstance(settings.searcher, BaseOptimizer):\n        searcher_instance = settings.searcher\n        searcher_info[\"searcher_name\"] = \"baseoptimizer\"\n        searcher_info[\"searcher_alg\"] = settings.searcher.whoami()\n        searcher_info[\"searcher_selection\"] = \"user-instantiation\"\n        searcher_info[\"neps_decision_tree\"] = False\n    else:\n        (\n            searcher_instance,\n            searcher_info,\n        ) = _run_args(\n            searcher_info=searcher_info,\n            pipeline_space=settings.pipeline_space,\n            max_cost_total=settings.max_cost_total,\n            ignore_errors=settings.ignore_errors,\n            loss_value_on_error=settings.loss_value_on_error,\n            cost_value_on_error=settings.cost_value_on_error,\n            searcher=settings.searcher,\n            **settings.searcher_kwargs,\n        )\n\n    # Check to verify if the target directory contains history of another optimizer state\n    # This check is performed only when the `searcher` is built during the run\n    if not isinstance(settings.searcher, (BaseOptimizer, str, dict, Path)):\n        raise ValueError(\n            f\"Unrecognized `searcher` of type {type(settings.searcher)}. Not str or \"\n            f\"BaseOptimizer.\"\n        )\n    elif isinstance(settings.searcher, BaseOptimizer):\n        # This check is not strict when a user-defined neps.optimizer is provided\n        logger.warning(\n            \"An instantiated optimizer is provided. The safety checks of NePS will be \"\n            \"skipped. Accurate continuation of runs can no longer be guaranteed!\"\n        )\n\n    if settings.task_id is not None:\n        settings.root_directory = Path(settings.root_directory) / (\n            f\"task_\" f\"{settings.task_id}\"\n        )\n    if settings.development_stage_id is not None:\n        settings.root_directory = (\n            Path(settings.root_directory) / f\"dev_{settings.development_stage_id}\"\n        )\n\n    _launch_runtime(\n        evaluation_fn=settings.run_pipeline,\n        optimizer=searcher_instance,\n        optimizer_info=searcher_info,\n        max_cost_total=settings.max_cost_total,\n        optimization_dir=Path(settings.root_directory),\n        max_evaluations_total=settings.max_evaluations_total,\n        max_evaluations_for_worker=settings.max_evaluations_per_run,\n        continue_until_max_evaluation_completed=settings.continue_until_max_evaluation_completed,\n        loss_value_on_error=settings.loss_value_on_error,\n        cost_value_on_error=settings.cost_value_on_error,\n        ignore_errors=settings.ignore_errors,\n        overwrite_optimization_dir=settings.overwrite_working_directory,\n        pre_load_hooks=settings.pre_load_hooks,\n    )\n\n    if settings.post_run_summary:\n        assert settings.root_directory is not None\n        config_data_path, run_data_path = post_run_csv(settings.root_directory)\n        logger.info(\n            \"The post run summary has been created, which is a csv file with the \"\n            \"output of all data in the run.\"\n            f\"\\nYou can find a csv of all the configuratins at: {config_data_path}.\"\n            f\"\\nYou can find a csv of results at: {run_data_path}.\"\n        )\n    else:\n        logger.info(\n            \"Skipping the creation of the post run summary, which is a csv file with the \"\n            \" output of all data in the run.\"\n            \"\\nSet `post_run_summary=True` to enable it.\"\n        )\n</code></pre>"},{"location":"api/neps/env/","title":"Env","text":""},{"location":"api/neps/env/#neps.env","title":"neps.env","text":"<p>Environment variable parsing for the state.</p>"},{"location":"api/neps/env/#neps.env.get_env","title":"get_env","text":"<pre><code>get_env(\n    key: str, parse: Callable[[str], T], default: V\n) -&gt; T | V\n</code></pre> <p>Get an environment variable or return a default value.</p> Source code in <code>neps/env.py</code> <pre><code>def get_env(key: str, parse: Callable[[str], T], default: V) -&gt; T | V:\n    \"\"\"Get an environment variable or return a default value.\"\"\"\n    if (e := os.environ.get(key)) is not None:\n        value = parse(e)\n        ENV_VARS_USED[key] = (e, value)\n        return value\n\n    ENV_VARS_USED[key] = (default, default)\n    return default\n</code></pre>"},{"location":"api/neps/env/#neps.env.is_nullable","title":"is_nullable","text":"<pre><code>is_nullable(e: str) -&gt; bool\n</code></pre> <p>Check if an environment variable is nullable.</p> Source code in <code>neps/env.py</code> <pre><code>def is_nullable(e: str) -&gt; bool:\n    \"\"\"Check if an environment variable is nullable.\"\"\"\n    return e.lower() in (\"none\", \"n\", \"null\")\n</code></pre>"},{"location":"api/neps/exceptions/","title":"Exceptions","text":""},{"location":"api/neps/exceptions/#neps.exceptions","title":"neps.exceptions","text":"<p>Exceptions for NePS that don't belong in a specific module.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.LockFailedError","title":"LockFailedError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a lock cannot be acquired.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.NePSError","title":"NePSError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all NePS exceptions.</p> <p>This allows an easier way to catch all NePS exceptions if we inherit all exceptions from this class.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.TrialAlreadyExistsError","title":"TrialAlreadyExistsError","text":"<p>               Bases: <code>VersionedResourceAlreadyExistsError</code></p> <p>Raised when a trial already exists in the store.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.TrialNotFoundError","title":"TrialNotFoundError","text":"<p>               Bases: <code>VersionedResourceDoesNotExistsError</code></p> <p>Raised when a trial already exists in the store.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.VersionMismatchError","title":"VersionMismatchError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when the version of a resource does not match the expected version.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.VersionedResourceAlreadyExistsError","title":"VersionedResourceAlreadyExistsError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a version already exists when trying to create a new versioned data.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.VersionedResourceDoesNotExistsError","title":"VersionedResourceDoesNotExistsError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a versioned resource does not exist at a location.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.VersionedResourceRemovedError","title":"VersionedResourceRemovedError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a version already exists when trying to create a new versioned data.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.WorkerFailedToGetPendingTrialsError","title":"WorkerFailedToGetPendingTrialsError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when a worker failed to get pending trials.</p>"},{"location":"api/neps/exceptions/#neps.exceptions.WorkerRaiseError","title":"WorkerRaiseError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised from a worker when an error is raised.</p> <p>Includes additional information on how to recover</p>"},{"location":"api/neps/runtime/","title":"Runtime","text":""},{"location":"api/neps/runtime/#neps.runtime","title":"neps.runtime","text":"<p>TODO.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker","title":"DefaultWorker  <code>dataclass</code>","text":"<pre><code>DefaultWorker(\n    state: NePSState,\n    settings: WorkerSettings,\n    evaluation_fn: Callable[..., float | Mapping[str, Any]],\n    optimizer: BaseOptimizer,\n    worker_id: str,\n    _pre_sample_hooks: (\n        list[Callable[[BaseOptimizer], BaseOptimizer]]\n        | None\n    ) = None,\n    worker_cumulative_eval_count: int = 0,\n    worker_cumulative_eval_cost: float = 0.0,\n    worker_cumulative_evaluation_time_seconds: float = 0.0,\n)\n</code></pre> <p>               Bases: <code>Generic[Loc]</code></p> <p>A default worker for the NePS system.</p> <p>This is the worker that is used by default in the neps.run() loop.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.evaluation_fn","title":"evaluation_fn  <code>instance-attribute</code>","text":"<pre><code>evaluation_fn: Callable[..., float | Mapping[str, Any]]\n</code></pre> <p>The evaluation function to use for the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer: BaseOptimizer\n</code></pre> <p>The optimizer that is in use by the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.settings","title":"settings  <code>instance-attribute</code>","text":"<pre><code>settings: WorkerSettings\n</code></pre> <p>The settings for the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.state","title":"state  <code>instance-attribute</code>","text":"<pre><code>state: NePSState\n</code></pre> <p>The state of the NePS system.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_cumulative_eval_cost","title":"worker_cumulative_eval_cost  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worker_cumulative_eval_cost: float = 0.0\n</code></pre> <p>The cost of the evaluations done by this worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_cumulative_eval_count","title":"worker_cumulative_eval_count  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worker_cumulative_eval_count: int = 0\n</code></pre> <p>The number of evaluations done by this worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_cumulative_evaluation_time_seconds","title":"worker_cumulative_evaluation_time_seconds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worker_cumulative_evaluation_time_seconds: float = 0.0\n</code></pre> <p>The time spent evaluating configurations by this worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.worker_id","title":"worker_id  <code>instance-attribute</code>","text":"<pre><code>worker_id: str\n</code></pre> <p>The id of the worker.</p>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(\n    *,\n    state: NePSState,\n    optimizer: BaseOptimizer,\n    settings: WorkerSettings,\n    evaluation_fn: Callable[..., float | Mapping[str, Any]],\n    _pre_sample_hooks: (\n        list[Callable[[BaseOptimizer], BaseOptimizer]]\n        | None\n    ) = None,\n    worker_id: str | None = None\n) -&gt; DefaultWorker\n</code></pre> <p>Create a new worker.</p> Source code in <code>neps/runtime.py</code> <pre><code>@classmethod\ndef new(\n    cls,\n    *,\n    state: NePSState,\n    optimizer: BaseOptimizer,\n    settings: WorkerSettings,\n    evaluation_fn: Callable[..., float | Mapping[str, Any]],\n    _pre_sample_hooks: list[Callable[[BaseOptimizer], BaseOptimizer]] | None = None,\n    worker_id: str | None = None,\n) -&gt; DefaultWorker:\n    \"\"\"Create a new worker.\"\"\"\n    return DefaultWorker(\n        state=state,\n        optimizer=optimizer,\n        settings=settings,\n        evaluation_fn=evaluation_fn,\n        worker_id=worker_id if worker_id is not None else _default_worker_name(),\n        _pre_sample_hooks=_pre_sample_hooks,\n    )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.DefaultWorker.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the worker.</p> <p>Will keep running until one of the criterion defined by the <code>WorkerSettings</code> is met.</p> Source code in <code>neps/runtime.py</code> <pre><code>def run(self) -&gt; None:  # noqa: C901, PLR0915\n    \"\"\"Run the worker.\n\n    Will keep running until one of the criterion defined by the `WorkerSettings`\n    is met.\n    \"\"\"\n    _set_workers_neps_state(self.state)\n\n    logger.info(\"Launching NePS\")\n\n    _time_monotonic_start = time.monotonic()\n    _error_from_evaluation: Exception | None = None\n\n    _repeated_fail_get_next_trial_count = 0\n    while True:\n        # NOTE: We rely on this function to do logging and raising errors if it should\n        should_stop = self._check_if_should_stop(\n            time_monotonic_start=_time_monotonic_start,\n            error_from_this_worker=_error_from_evaluation,\n        )\n        if should_stop is not False:\n            logger.info(should_stop)\n            break\n\n        try:\n            trial_to_eval = self._get_next_trial_from_state()\n            _repeated_fail_get_next_trial_count = 0\n        except Exception as e:\n            _repeated_fail_get_next_trial_count += 1\n            logger.debug(\n                \"Error while trying to get the next trial to evaluate.\", exc_info=True\n            )\n\n            # NOTE: This is to prevent any infinite loops if we can't get a trial\n            if (\n                _repeated_fail_get_next_trial_count\n                &gt;= N_FAILED_GET_NEXT_PENDING_ATTEMPTS_BEFORE_ERROR\n            ):\n                raise WorkerFailedToGetPendingTrialsError(\n                    \"Worker '%s' failed to get pending trials %d times in a row.\"\n                    \" Bailing!\"\n                ) from e\n\n            continue\n\n        # If we can't set this working to evaluating, then just retry the loop\n        try:\n            trial_to_eval.set_evaluating(\n                time_started=time.time(),\n                worker_id=self.worker_id,\n            )\n            self.state.put_updated_trial(trial_to_eval)\n            n_failed_set_trial_state = 0\n        except VersionMismatchError:\n            n_failed_set_trial_state += 1\n            logger.debug(\n                f\"Another worker has managed to change trial '{trial_to_eval.id}'\"\n                \" to evaluate and put back into state. This is fine and likely means\"\n                \" the other worker is evaluating it.\",\n                exc_info=True,\n            )\n        except Exception:\n            n_failed_set_trial_state += 1\n            logger.error(\n                f\"Error trying to set trial '{trial_to_eval.id}' to evaluating.\",\n                exc_info=True,\n            )\n\n        # NOTE: This is to prevent infinite looping if it somehow keeps getting\n        # the same trial and can't set it to evaluating.\n        if n_failed_set_trial_state != 0:\n            if n_failed_set_trial_state &gt;= N_FAILED_TO_SET_TRIAL_STATE:\n                raise WorkerFailedToGetPendingTrialsError(\n                    \"Worker '%s' failed to set trial to evaluating %d times in a row.\"\n                    \" Bailing!\"\n                )\n            continue\n\n        # We (this worker) has managed to set it to evaluating, now we can evaluate it\n        with _set_global_trial(trial_to_eval):\n            evaluated_trial, report = evaluate_trial(\n                trial=trial_to_eval,\n                evaluation_fn=self.evaluation_fn,\n                default_report_values=self.settings.default_report_values,\n            )\n            evaluation_duration = evaluated_trial.metadata.evaluation_duration\n            assert evaluation_duration is not None\n            self.worker_cumulative_evaluation_time_seconds += evaluation_duration\n\n        self.worker_cumulative_eval_count += 1\n\n        logger.info(\n            \"Worker '%s' evaluated trial: %s as %s.\",\n            self.worker_id,\n            evaluated_trial.id,\n            evaluated_trial.state,\n        )\n\n        if report.cost is not None:\n            self.worker_cumulative_eval_cost += report.cost\n\n        if report.err is not None:\n            logger.error(\n                f\"Error during evaluation of '{evaluated_trial.id}'\"\n                f\" : {evaluated_trial.config}.\"\n            )\n            logger.exception(report.err)\n            _error_from_evaluation = report.err\n\n        self.state.report_trial_evaluation(\n            trial=evaluated_trial,\n            report=report,\n            worker_id=self.worker_id,\n        )\n\n        logger.debug(\"Config %s: %s\", evaluated_trial.id, evaluated_trial.config)\n        logger.debug(\"Loss %s: %s\", evaluated_trial.id, report.loss)\n        logger.debug(\"Cost %s: %s\", evaluated_trial.id, report.loss)\n        logger.debug(\n            \"Learning Curve %s: %s\", evaluated_trial.id, report.learning_curve\n        )\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.get_in_progress_trial","title":"get_in_progress_trial","text":"<pre><code>get_in_progress_trial() -&gt; Trial\n</code></pre> <p>Get the currently running trial in this process.</p> Source code in <code>neps/runtime.py</code> <pre><code>def get_in_progress_trial() -&gt; Trial:\n    \"\"\"Get the currently running trial in this process.\"\"\"\n    if _CURRENTLY_RUNNING_TRIAL_IN_PROCESS is None:\n        raise RuntimeError(\n            \"The worker's NePS state has not been set! This should only be called\"\n            \" from within a `run_pipeline` context. If you are not running a pipeline\"\n            \" and you did not call this function (`get_workers_neps_state`) yourself,\"\n            \" this is a bug and should be reported to NePS.\"\n        )\n    return _CURRENTLY_RUNNING_TRIAL_IN_PROCESS\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.get_workers_neps_state","title":"get_workers_neps_state","text":"<pre><code>get_workers_neps_state() -&gt; NePSState[Path]\n</code></pre> <p>Get the worker's NePS state.</p> Source code in <code>neps/runtime.py</code> <pre><code>def get_workers_neps_state() -&gt; NePSState[Path]:\n    \"\"\"Get the worker's NePS state.\"\"\"\n    if _WORKER_NEPS_STATE is None:\n        raise RuntimeError(\n            \"The worker's NePS state has not been set! This should only be called\"\n            \" from within a `run_pipeline` context. If you are not running a pipeline\"\n            \" and you did not call this function (`get_workers_neps_state`) yourself,\"\n            \" this is a bug and should be reported to NePS.\"\n        )\n    return _WORKER_NEPS_STATE\n</code></pre>"},{"location":"api/neps/runtime/#neps.runtime.register_notify_trial_end","title":"register_notify_trial_end","text":"<pre><code>register_notify_trial_end(\n    key: str, callback: Callable[[Trial], None]\n) -&gt; None\n</code></pre> <p>Register a callback to be called when a trial ends.</p> Source code in <code>neps/runtime.py</code> <pre><code>def register_notify_trial_end(key: str, callback: Callable[[Trial], None]) -&gt; None:\n    \"\"\"Register a callback to be called when a trial ends.\"\"\"\n    _TRIAL_END_CALLBACKS[key] = callback\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/","title":"Base optimizer","text":""},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer","title":"neps.optimizers.base_optimizer","text":""},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer","title":"BaseOptimizer","text":"<pre><code>BaseOptimizer(\n    *,\n    pipeline_space: SearchSpace,\n    patience: int = 50,\n    logger: Logger | None = None,\n    budget: int | float | None = None,\n    loss_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    learning_curve_on_error: (\n        float | list[float] | None\n    ) = None,\n    ignore_errors: bool = False\n)\n</code></pre> <p>Base sampler class. Implements all the low-level work.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    patience: int = 50,\n    logger: logging.Logger | None = None,\n    budget: int | float | None = None,\n    loss_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    learning_curve_on_error: float | list[float] | None = None,\n    ignore_errors: bool = False,\n) -&gt; None:\n    if patience &lt; 1:\n        raise ValueError(\"Patience should be at least 1\")\n\n    self.budget = budget\n    self.pipeline_space = pipeline_space\n    self.patience = patience\n    self.logger = logger or logging.getLogger(\"neps\")\n    self.loss_value_on_error = loss_value_on_error\n    self.cost_value_on_error = cost_value_on_error\n    self.learning_curve_on_error = learning_curve_on_error\n    self.ignore_errors = ignore_errors\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.ask","title":"ask  <code>abstractmethod</code>","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>Sample a new configuration.</p> PARAMETER DESCRIPTION <code>trials</code> <p>All of the trials that are known about.</p> <p> TYPE: <code>Mapping[str, Trial]</code> </p> <code>budget_info</code> <p>information about the budget</p> <p> TYPE: <code>BudgetInfo | None</code> </p> RETURNS DESCRIPTION <code>SampledConfig</code> <p>a sampled configuration dict: state the optimizer would like to keep between calls</p> <p> TYPE: <code>SampledConfig</code> </p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>@abstractmethod\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"Sample a new configuration.\n\n    Args:\n        trials: All of the trials that are known about.\n        budget_info: information about the budget\n\n    Returns:\n        SampledConfig: a sampled configuration\n        dict: state the optimizer would like to keep between calls\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/base_optimizer/#neps.optimizers.base_optimizer.BaseOptimizer.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/info/","title":"Info","text":""},{"location":"api/neps/optimizers/info/#neps.optimizers.info","title":"neps.optimizers.info","text":""},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs","title":"SearcherConfigs","text":"<p>This class provides methods to access default configuration details for NePS optimizers.</p>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_available_algorithms","title":"get_available_algorithms  <code>staticmethod</code>","text":"<pre><code>get_available_algorithms() -&gt; list[str]\n</code></pre> <p>List all available algorithms used by NePS searchers.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of algorithm names.</p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_available_algorithms() -&gt; list[str]:\n    \"\"\"List all available algorithms used by NePS searchers.\n\n    Returns:\n        list[str]: A list of algorithm names.\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n    prev_algorithms = set()\n\n    for file in folder_path.iterdir():\n        if file.suffix == \".yaml\":\n            with file.open(\"r\") as f:\n                searcher_config = yaml.safe_load(f)\n                algorithm = searcher_config.get(\"strategy\")\n                if algorithm:\n                    prev_algorithms.add(algorithm)\n\n    return list(prev_algorithms)\n</code></pre>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_searcher_from_algorithm","title":"get_searcher_from_algorithm  <code>staticmethod</code>","text":"<pre><code>get_searcher_from_algorithm(algorithm: str) -&gt; list[str]\n</code></pre> <p>Get all NePS searchers that use a specific searching algorithm.</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>The name of the algorithm needed for the search.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of searcher names using the specified algorithm.</p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_searcher_from_algorithm(algorithm: str) -&gt; list[str]:\n    \"\"\"Get all NePS searchers that use a specific searching algorithm.\n\n    Args:\n        algorithm (str): The name of the algorithm needed for the search.\n\n    Returns:\n        list[str]: A list of searcher names using the specified algorithm.\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n    searchers = []\n\n    for file in folder_path.iterdir():\n        if file.suffix == \".yaml\":\n            with file.open(\"r\") as f:\n                searcher_config = yaml.safe_load(f)\n                if searcher_config.get(\"strategy\") == algorithm:\n                    searchers.append(file.stem)\n\n    return searchers\n</code></pre>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_searcher_kwargs","title":"get_searcher_kwargs  <code>staticmethod</code>","text":"<pre><code>get_searcher_kwargs(searcher: str) -&gt; str\n</code></pre> <p>Get the kwargs and algorithm setup for a specific searcher.</p> PARAMETER DESCRIPTION <code>searcher</code> <p>The name of the searcher to check the details of.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The raw content of the searcher's configuration</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_searcher_kwargs(searcher: str) -&gt; str:\n    \"\"\"Get the kwargs and algorithm setup for a specific searcher.\n\n    Args:\n        searcher (str): The name of the searcher to check the details of.\n\n    Returns:\n        str: The raw content of the searcher's configuration\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n\n    for file in folder_path.iterdir():\n        if file.suffix == \".yaml\" and file.stem.startswith(searcher):\n            return file.read_text()\n\n    raise FileNotFoundError(\n        f\"Searcher {searcher} not found in default searchers folder.\"\n    )\n</code></pre>"},{"location":"api/neps/optimizers/info/#neps.optimizers.info.SearcherConfigs.get_searchers","title":"get_searchers  <code>staticmethod</code>","text":"<pre><code>get_searchers() -&gt; list[str]\n</code></pre> <p>List all the searcher names that can be used in neps run.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of searcher names.</p> Source code in <code>neps/optimizers/info.py</code> <pre><code>@staticmethod\ndef get_searchers() -&gt; list[str]:\n    \"\"\"List all the searcher names that can be used in neps run.\n\n    Returns:\n        list[str]: A list of searcher names.\n    \"\"\"\n    folder_path = SearcherConfigs._get_searchers_folder_path()\n    searchers = []\n\n    for file in folder_path.iterdir():\n        if file.suffix == \".yaml\":\n            searchers.append(file.stem)\n\n    return searchers\n</code></pre>"},{"location":"api/neps/optimizers/initial_design/","title":"Initial design","text":""},{"location":"api/neps/optimizers/initial_design/#neps.optimizers.initial_design","title":"neps.optimizers.initial_design","text":""},{"location":"api/neps/optimizers/initial_design/#neps.optimizers.initial_design.make_initial_design","title":"make_initial_design","text":"<pre><code>make_initial_design(\n    *,\n    space: SearchSpace,\n    encoder: ConfigEncoder,\n    sampler: Literal[\"sobol\", \"prior\", \"uniform\"] | Sampler,\n    sample_size: int | Literal[\"ndim\"] | None = \"ndim\",\n    sample_default_first: bool = True,\n    sample_fidelity: (\n        Literal[\"min\", \"max\", True]\n        | int\n        | float\n        | dict[str, int | float]\n    ) = True,\n    seed: Generator | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Generate the initial design of the optimization process.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to use.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>encoder</code> <p>The encoder to use for encoding/decoding configurations.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>sampler</code> <p>The sampler to use for the initial design.</p> <p>If set to \"sobol\", a Sobol sequence will be used. If set to \"uniform\", a uniform random sampler will be used. If set to \"prior\", a prior sampler will be used, based on the defaults,     and confidence scores of the hyperparameters. If set to a custom sampler, the sampler will be used directly.</p> <p> TYPE: <code>Literal['sobol', 'prior', 'uniform'] | Sampler</code> </p> <code>sample_size</code> <p>The number of configurations to sample.</p> <p>If \"ndim\", the number of configs will be equal to the number of dimensions. If None, no configurations will be sampled.</p> <p> TYPE: <code>int | Literal['ndim'] | None</code> DEFAULT: <code>'ndim'</code> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sample_fidelity</code> <p>At what fidelity to sample the configurations, including the default.</p> <p>If set to \"min\" or \"max\", the configuration will be sampled at the minimum or maximum fidelity, respectively. If set to an integer or a float, the configuration will be sampled at that fidelity. When specified as a dictionary, the keys should be the names of the fidelity parameters and the values should be the target fidelities. If set to <code>True</code>, the configuration will have its fidelity randomly sampled.</p> <p> TYPE: <code>Literal['min', 'max', True] | int | float | dict[str, int | float]</code> DEFAULT: <code>True</code> </p> <code>seed</code> <p>The seed to use for the random number generation.</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/optimizers/initial_design.py</code> <pre><code>def make_initial_design(  # noqa: PLR0912, C901\n    *,\n    space: SearchSpace,\n    encoder: ConfigEncoder,\n    sampler: Literal[\"sobol\", \"prior\", \"uniform\"] | Sampler,\n    sample_size: int | Literal[\"ndim\"] | None = \"ndim\",\n    sample_default_first: bool = True,\n    sample_fidelity: (\n        Literal[\"min\", \"max\", True] | int | float | dict[str, int | float]\n    ) = True,\n    seed: torch.Generator | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Generate the initial design of the optimization process.\n\n    Args:\n        space: The search space to use.\n        encoder: The encoder to use for encoding/decoding configurations.\n        sampler: The sampler to use for the initial design.\n\n            If set to \"sobol\", a Sobol sequence will be used.\n            If set to \"uniform\", a uniform random sampler will be used.\n            If set to \"prior\", a prior sampler will be used, based on the defaults,\n                and confidence scores of the hyperparameters.\n            If set to a custom sampler, the sampler will be used directly.\n\n        sample_size:\n            The number of configurations to sample.\n\n            If \"ndim\", the number of configs will be equal to the number of dimensions.\n            If None, no configurations will be sampled.\n\n        sample_default_first: Whether to sample the default configuration first.\n        sample_fidelity:\n            At what fidelity to sample the configurations, including the default.\n\n            If set to \"min\" or \"max\", the configuration will be sampled\n            at the minimum or maximum fidelity, respectively. If set to an integer\n            or a float, the configuration will be sampled at that fidelity.\n            When specified as a dictionary, the keys should be the names of the\n            fidelity parameters and the values should be the target fidelities.\n            If set to `True`, the configuration will have its fidelity randomly sampled.\n        seed: The seed to use for the random number generation.\n\n    \"\"\"\n    configs: list[dict[str, Any]] = []\n\n    # First, we establish what fidelity to apply to them.\n    # This block essentially is in charge of creating a fids() function that can\n    # be called to get the fidelities for each sample. Some are constant, some will\n    # sample per config.\n    match sample_fidelity:\n        case \"min\":\n            _fids = {name: fid.lower for name, fid in space.fidelities.items()}\n            fids = lambda: _fids\n        case \"max\":\n            _fids = {name: fid.upper for name, fid in space.fidelities.items()}\n            fids = lambda: _fids\n        case True:\n            fids = lambda: {\n                name: hp.sample_value() for name, hp in space.fidelities.items()\n            }\n        case int() | float():\n            if len(space.fidelities) != 1:\n                raise ValueError(\n                    \"The target fidelity should be specified as a dictionary\"\n                    \" if there are multiple fidelities or no fidelity should\"\n                    \" be specified.\"\n                    \" Current search space has fidelities: \"\n                    f\"{list(space.fidelities.keys())}\"\n                )\n            name = next(iter(space.fidelities.keys()))\n            fids = lambda: {name: sample_fidelity}\n        case Mapping():\n            missing_keys = set(space.fidelities.keys()) - set(sample_fidelity.keys())\n            if any(missing_keys):\n                raise ValueError(\n                    f\"Missing target fidelities for the following fidelities: \"\n                    f\"{missing_keys}\"\n                )\n            fids = lambda: sample_fidelity\n        case _:\n            raise ValueError(\n                \"Invalid value for `sample_default_at_target`. \"\n                \"Expected 'min', 'max', True, int, float, or dict.\"\n            )\n\n    if sample_default_first:\n        # TODO: No way to pass a seed to the sampler\n        default = {\n            name: hp.default if hp.default is not None else hp.sample_value()\n            for name, hp in space.hyperparameters.items()\n        }\n        configs.append({**default, **fids()})\n\n    ndims = len(space.numerical) + len(space.categoricals)\n    if sample_size == \"ndim\":\n        sample_size = ndims\n    elif sample_size is not None and not sample_size &gt; 0:\n        raise ValueError(\n            \"The sample size should be a positive integer if passing an int.\"\n        )\n\n    if sample_size is not None:\n        match sampler:\n            case \"sobol\":\n                sampler = Sampler.sobol(ndim=ndims)\n            case \"uniform\":\n                sampler = Sampler.uniform(ndim=ndims)\n            case \"prior\":\n                sampler = Prior.from_space(space, include_fidelity=False)\n            case _:\n                pass\n\n        encoded_configs = sampler.sample(sample_size * 2, to=encoder.domains, seed=seed)\n        uniq_x = torch.unique(encoded_configs, dim=0)\n        sample_configs = encoder.decode(uniq_x[:sample_size])\n        configs.extend([{**config, **fids()} for config in sample_configs])\n\n    return configs\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer","title":"neps.optimizers.bayesian_optimization.optimizer","text":""},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization","title":"BayesianOptimization","text":"<pre><code>BayesianOptimization(\n    pipeline_space: SearchSpace,\n    *,\n    initial_design_size: int | None = None,\n    use_priors: bool = False,\n    use_cost: bool = False,\n    cost_on_log_scale: bool = True,\n    sample_default_first: bool = False,\n    device: device | None = None,\n    encoder: ConfigEncoder | None = None,\n    seed: int | None = None,\n    budget: Any | None = None,\n    surrogate_model: Any | None = None,\n    loss_value_on_error: Any | None = None,\n    cost_value_on_error: Any | None = None,\n    ignore_errors: Any | None = None\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Implements the basic BO loop.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>initial_design_size</code> <p>Number of samples used before using the surrogate model. If None, it will use the number of parameters in the search space.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>use_priors</code> <p>Whether to use priors set on the hyperparameters during search.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_cost</code> <p>Whether to consider reported \"cost\" from configurations in decision making. If True, the optimizer will weigh potential candidates by how much they cost, incentivising the optimizer to explore cheap, good performing configurations. This amount is modified over time</p> <p>Warning</p> <p>If using <code>cost</code>, cost must be provided in the reports of the trials.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>cost_on_log_scale</code> <p>Whether to use the log of the cost when using cost.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Seed to use for the random number generator of samplers.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to use for the optimization.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> <code>encoder</code> <p>Encoder to use for encoding the configurations. If None, it will will use the default encoder.</p> <p> TYPE: <code>ConfigEncoder | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if initial_design_size &lt; 1</p> <code>ValueError</code> <p>if no kernel is provided</p> Source code in <code>neps/optimizers/bayesian_optimization/optimizer.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    *,\n    initial_design_size: int | None = None,\n    use_priors: bool = False,\n    use_cost: bool = False,\n    cost_on_log_scale: bool = True,\n    sample_default_first: bool = False,\n    device: torch.device | None = None,\n    encoder: ConfigEncoder | None = None,\n    seed: int | None = None,\n    budget: Any | None = None,  # TODO: remove\n    surrogate_model: Any | None = None,  # TODO: remove\n    loss_value_on_error: Any | None = None,  # TODO: remove\n    cost_value_on_error: Any | None = None,  # TODO: remove\n    ignore_errors: Any | None = None,  # TODO: remove\n):\n    \"\"\"Initialise the BO loop.\n\n    Args:\n        pipeline_space: Space in which to search\n        initial_design_size: Number of samples used before using the surrogate model.\n            If None, it will use the number of parameters in the search space.\n        use_priors: Whether to use priors set on the hyperparameters during search.\n        use_cost: Whether to consider reported \"cost\" from configurations in decision\n            making. If True, the optimizer will weigh potential candidates by how much\n            they cost, incentivising the optimizer to explore cheap, good performing\n            configurations. This amount is modified over time\n\n            !!! warning\n\n                If using `cost`, cost must be provided in the reports of the trials.\n\n        cost_on_log_scale: Whether to use the log of the cost when using cost.\n        sample_default_first: Whether to sample the default configuration first.\n        seed: Seed to use for the random number generator of samplers.\n        device: Device to use for the optimization.\n        encoder: Encoder to use for encoding the configurations. If None, it will\n            will use the default encoder.\n\n    Raises:\n        ValueError: if initial_design_size &lt; 1\n        ValueError: if no kernel is provided\n    \"\"\"\n    if seed is not None:\n        raise NotImplementedError(\n            \"Seed is not implemented yet for BayesianOptimization\"\n        )\n    if any(pipeline_space.graphs):\n        raise NotImplementedError(\"Only supports flat search spaces for now!\")\n    if any(pipeline_space.fidelities):\n        raise ValueError(\n            \"Fidelities are not supported for BayesianOptimization.\"\n            \" Please consider setting the fidelity to a constant value.\"\n            f\" Got: {pipeline_space.fidelities}\"\n        )\n\n    super().__init__(pipeline_space=pipeline_space)\n\n    self.encoder = encoder or ConfigEncoder.from_space(\n        space=pipeline_space,\n        include_constants_when_decoding=True,\n    )\n    self.prior = Prior.from_space(pipeline_space) if use_priors is True else None\n    self.use_cost = use_cost\n    self.use_priors = use_priors\n    self.cost_on_log_scale = cost_on_log_scale\n    self.device = device\n    self.sample_default_first = sample_default_first\n    self.n_initial_design = initial_design_size\n    self.init_design: list[dict[str, Any]] | None = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/optimizer/#neps.optimizers.bayesian_optimization.optimizer.BayesianOptimization.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/","title":"Base acquisition","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition","title":"neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition.BaseAcquisition","title":"BaseAcquisition","text":"<pre><code>BaseAcquisition()\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition.py</code> <pre><code>def __init__(self):\n    self.surrogate_model: Any | None = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.base_acquisition.BaseAcquisition.eval","title":"eval  <code>abstractmethod</code>","text":"<pre><code>eval(\n    x: Iterable, *, asscalar: bool = False\n) -&gt; ndarray | Tensor | float\n</code></pre> <p>Evaluate the acquisition function at point x2.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/base_acquisition.py</code> <pre><code>@abstractmethod\ndef eval(\n    self,\n    x: Iterable,\n    *,\n    asscalar: bool = False,\n) -&gt; np.ndarray | torch.Tensor | float:\n    \"\"\"Evaluate the acquisition function at point x2.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/cost_cooling/","title":"Cost cooling","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/cost_cooling/#neps.optimizers.bayesian_optimization.acquisition_functions.cost_cooling","title":"neps.optimizers.bayesian_optimization.acquisition_functions.cost_cooling","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/","title":"Ei","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/#neps.optimizers.bayesian_optimization.acquisition_functions.ei","title":"neps.optimizers.bayesian_optimization.acquisition_functions.ei","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/#neps.optimizers.bayesian_optimization.acquisition_functions.ei.ComprehensiveExpectedImprovement","title":"ComprehensiveExpectedImprovement","text":"<pre><code>ComprehensiveExpectedImprovement(\n    *,\n    augmented_ei: bool = False,\n    xi: float = 0.0,\n    in_fill: str = \"best\",\n    log_ei: bool = False,\n    optimize_on_max_fidelity: bool = True\n)\n</code></pre> <p>               Bases: <code>BaseAcquisition</code></p> <ol> <li> <p>The input x2 is a networkx graph instead of a vectorial input</p> </li> <li> <p>The search space (a collection of x1_graphs) is discrete, so there is no    gradient-based optimisation. Instead, we compute the EI at all candidate points    and empirically select the best position during optimisation</p> </li> </ol> PARAMETER DESCRIPTION <code>augmented_ei</code> <p>Using the Augmented EI heuristic modification to the standard expected improvement algorithm according to Huang (2006).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>xi</code> <p>manual exploration-exploitation trade-off parameter.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>in_fill</code> <p>the criterion to be used for in-fill for the determination of mu_star 'best' means the empirical best observation so far (but could be susceptible to noise), 'posterior' means the best posterior GP mean encountered so far, and is recommended for optimization of more noisy functions. Defaults to \"best\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'best'</code> </p> <code>log_ei</code> <p>log-EI if true otherwise usual EI.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/ei.py</code> <pre><code>def __init__(\n    self,\n    *,\n    augmented_ei: bool = False,\n    xi: float = 0.0,\n    in_fill: str = \"best\",\n    log_ei: bool = False,\n    optimize_on_max_fidelity: bool = True,\n):\n    \"\"\"This is the graph BO version of the expected improvement\n    key differences are:\n\n    1. The input x2 is a networkx graph instead of a vectorial input\n\n    2. The search space (a collection of x1_graphs) is discrete, so there is no\n       gradient-based optimisation. Instead, we compute the EI at all candidate points\n       and empirically select the best position during optimisation\n\n    Args:\n        augmented_ei: Using the Augmented EI heuristic modification to the standard\n            expected improvement algorithm according to Huang (2006).\n        xi: manual exploration-exploitation trade-off parameter.\n        in_fill: the criterion to be used for in-fill for the determination of mu_star\n            'best' means the empirical best observation so far (but could be\n            susceptible to noise), 'posterior' means the best *posterior GP mean*\n            encountered so far, and is recommended for optimization of more noisy\n            functions. Defaults to \"best\".\n        log_ei: log-EI if true otherwise usual EI.\n    \"\"\"\n    super().__init__()\n\n    if in_fill not in [\"best\", \"posterior\"]:\n        raise ValueError(f\"Invalid value for in_fill ({in_fill})\")\n    self.augmented_ei = augmented_ei\n    self.xi = xi\n    self.in_fill = in_fill\n    self.log_ei = log_ei\n    self.incumbent: float | None = None\n    self.optimize_on_max_fidelity = optimize_on_max_fidelity\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ei/#neps.optimizers.bayesian_optimization.acquisition_functions.ei.ComprehensiveExpectedImprovement.eval","title":"eval","text":"<pre><code>eval(\n    x: Sequence[SearchSpace], *, asscalar: bool = False\n) -&gt; ndarray | Tensor | float\n</code></pre> <p>Return the negative expected improvement at the query point x2.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/ei.py</code> <pre><code>def eval(\n    self,\n    x: Sequence[SearchSpace],\n    *,\n    asscalar: bool = False,\n) -&gt; np.ndarray | torch.Tensor | float:\n    \"\"\"Return the negative expected improvement at the query point x2.\"\"\"\n    assert self.incumbent is not None, \"EI function not fitted on model\"\n    assert self.surrogate_model is not None\n\n    space = x[0]\n    if len(space.fidelities) &gt; 0 and self.optimize_on_max_fidelity:\n        assert len(space.fidelities) == 1\n        fid_name, fid = next(iter(space.fidelities.items()))\n        _x = [space.from_dict({**e._values, fid_name: fid.upper}) for e in x]\n    else:\n        _x = list(x)\n\n    mu, cov = self.surrogate_model.predict(_x)\n\n    std = torch.sqrt(torch.diag(cov))\n    mu_star = self.incumbent\n\n    gauss = Normal(torch.zeros(1, device=mu.device), torch.ones(1, device=mu.device))\n    # &gt; u = (mu - mu_star - self.xi) / std\n    # &gt; ei = std * updf + (mu - mu_star - self.xi) * ucdf\n    if self.log_ei:\n        # we expect that f_min is in log-space\n        f_min = mu_star - self.xi\n        v = (f_min - mu) / std\n        ei = torch.exp(f_min) * gauss.cdf(v) - torch.exp(\n            0.5 * torch.diag(cov) + mu\n        ) * gauss.cdf(v - std)\n    else:\n        u = (mu_star - mu - self.xi) / std\n        try:\n            ucdf = gauss.cdf(u)\n        except ValueError as e:\n            print(f\"u: {u}\")  # noqa: T201\n            print(f\"mu_star: {mu_star}\")  # noqa: T201\n            print(f\"mu: {mu}\")  # noqa: T201\n            print(f\"std: {std}\")  # noqa: T201\n            print(f\"diag: {cov.diag()}\")  # noqa: T201\n            raise e\n        updf = torch.exp(gauss.log_prob(u))\n        ei = std * updf + (mu_star - mu - self.xi) * ucdf\n    if self.augmented_ei:\n        sigma_n = self.surrogate_model.likelihood\n        ei *= 1.0 - torch.sqrt(torch.tensor(sigma_n, device=mu.device)) / torch.sqrt(\n            sigma_n + torch.diag(cov)\n        )\n    if isinstance(_x, list) and asscalar:\n        return ei.detach().numpy()\n\n    if asscalar:\n        ei = ei.detach().numpy().item()\n\n    return ei\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/pibo/","title":"Pibo","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/pibo/#neps.optimizers.bayesian_optimization.acquisition_functions.pibo","title":"neps.optimizers.bayesian_optimization.acquisition_functions.pibo","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/pibo/#neps.optimizers.bayesian_optimization.acquisition_functions.pibo--copyright-c-meta-platforms-inc-and-affiliates","title":"Copyright (c) Meta Platforms, Inc. and affiliates.","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/pibo/#neps.optimizers.bayesian_optimization.acquisition_functions.pibo--_1","title":"Pibo","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/pibo/#neps.optimizers.bayesian_optimization.acquisition_functions.pibo--this-source-code-is-licensed-under-the-mit-license-found-in-the","title":"This source code is licensed under the MIT license found in the","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/pibo/#neps.optimizers.bayesian_optimization.acquisition_functions.pibo--license-file-in-the-root-directory-of-this-source-tree","title":"LICENSE file in the root directory of this source tree.","text":"<p>Prior-Guided Acquisition Functions</p> <p>References:</p> <p>.. [Hvarfner2022]     C. Hvarfner, D. Stoll, A. Souza, M. Lindauer, F. Hutter, L. Nardi. PiBO:     Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization.     ICLR 2022.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ucb/","title":"Ucb","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ucb/#neps.optimizers.bayesian_optimization.acquisition_functions.ucb","title":"neps.optimizers.bayesian_optimization.acquisition_functions.ucb","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/ucb/#neps.optimizers.bayesian_optimization.acquisition_functions.ucb.UpperConfidenceBound","title":"UpperConfidenceBound","text":"<pre><code>UpperConfidenceBound(\n    *, beta: float = 1.0, maximize: bool = False\n)\n</code></pre> <p>               Bases: <code>BaseAcquisition</code></p> PARAMETER DESCRIPTION <code>beta</code> <p>Controls the balance between exploration and exploitation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>maximize</code> <p>If True, maximize the given model, else minimize. DEFAULT=False, assumes minimzation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/ucb.py</code> <pre><code>def __init__(self, *, beta: float = 1.0, maximize: bool = False):\n    \"\"\"Upper Confidence Bound (UCB) acquisition function.\n\n    Args:\n        beta: Controls the balance between exploration and exploitation.\n        maximize: If True, maximize the given model, else minimize.\n            DEFAULT=False, assumes minimzation.\n    \"\"\"\n    super().__init__()\n    self.beta = beta  # can be updated as part of the state for dynamism or a schedule\n    self.maximize = maximize\n\n    # to be initialized as part of the state\n    self.surrogate_model = None\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/weighted_acquisition/","title":"Weighted acquisition","text":""},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/weighted_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.weighted_acquisition","title":"neps.optimizers.bayesian_optimization.acquisition_functions.weighted_acquisition","text":"<p>This module provides most of the functionality we require in NePS for now, i.e., we need the ability to apply an arbitrary weight to an acquisition function.</p> <p>I spent some time understanding the meaning of the various dimensions of botorch/gpytorch.</p> <p>The two primary dimensions to consider are:</p> <ul> <li><code>d</code> - The dimensionality of the design space, i.e. how many hyperparameters.</li> <li><code>batch</code> - The number of independent evaluations to make, i.e. how many times to     evaluate the acquisition function.</li> </ul> <p>There are two extra dimensions which are special cases and need to be accounted for.</p> <ul> <li> <p><code>q</code> - Comes from the <code>qXXX</code> variants of acquisition, these will add an extra dimension     <code>q</code> to each <code>batch</code>, where instead of a <code>batch</code> representing a single config to get     the acquisition of, we might instead be getting the acquisition of 5 configs together,     representing the joint utility of evaluating these 5 configs, relative to other sets     of 5 configs. This dimension is reduced away in the final step of the acquisition     when suggesting which set of group of 5 configs to suggest.</p> </li> <li> <p><code>mc_samples</code> - Comes from the <code>SampleReducdingXXX</code> variants of acquisition, will add an     extra dimension <code>mc_samples</code> which represent the amount of Monte Carlo samples used     to estimate the acquisition. These will eventually be reduced away but are present     in the intermediate steps. These variants also seem to have <code>q</code> variants implicitly     and so you are likely to see the <code>q</code> dimension whever you see the <code>mc_samples</code>     dimension, even if it is just <code>q=1</code>.</p> </li> <li> <p><code>m</code> - The number of objectives in the multi-objective case. We will     specifically ignore this for now, however it exists as the last dimension (after <code>d</code>)     and is the first to be reduced away. They are also used in constrainted settings     which we will also ignore for now.</p> </li> </ul> <p>The most expanded tensor shape is the following, with the usual order of reduction being the following below. If you are not using a SamplingReducing variant, you will not see <code>mc_samples</code> and if you are not using a <code>q</code> variant, you will not see <code>q</code>. The simplest case then being <code>acq(tensor: batch x d)</code>.</p> <ul> <li><code>batch x q x d</code>.         reduce(..., d) = Config -&gt; Single number  (!!!Acq applies here!!!)</li> <li><code>batch x q</code>.         expand(mc_samples , ...) = MC Sampling from posterior (I think)</li> <li><code>mc_samples x batch x q</code>.         reduce(..., q) = Joint-Config-Group -&gt; Single number.</li> <li><code>mc_samples x batch</code>         reduce(mc_samples, ...) = MC-samples -&gt; statistical estimate</li> <li><code>batch</code></li> </ul> <p>Finally we get out a batch of values we can argmax over, used to index into either a single configuration or a single index into a joint-group of <code>q</code> configurations.</p> <p>Tip</p> <p>The <code>mc_samples</code> is not of concern to the <code>WeightedAcquisition</code> below, and broadcasting can be used, as a result, the <code>apply_weight</code> function only needs to be able to handle:</p> <ul> <li>(X: batch x q x d, acq_values: batch x q, acq: A) -&gt; batch x q</li> </ul> <p>If utilizing the configurations <code>X</code> for weighting, you effectively will want to reduce the <code>d</code> dimension.</p> <p>As a result of this, acquisition functions need to be able to handle arbitrary dimensions and act accordingly.</p> <p>This module mostly follows the structure of the <code>PriorGuidedAcquisitionFunction</code> which weights the acquisition function by a prior.</p> <ul> <li>botorch.org/api/_modules/botorch/acquisition/prior_guided.html#PriorGuidedAcquisitionFunction</li> </ul> <p>We use this to create a more generic <code>WeightedAcquisition</code> which follows the required structure to make new weightings easier to implement, but also to serve as an educational reference.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/weighted_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.weighted_acquisition.WeightedAcquisition","title":"WeightedAcquisition","text":"<pre><code>WeightedAcquisition(\n    acq: A,\n    apply_weight: Callable[[Tensor, Tensor, A], Tensor],\n)\n</code></pre> <p>               Bases: <code>AcquisitionFunction</code></p> <p>Class for weighting acquisition functions.</p> <p>Please see module docstring for more information.</p> PARAMETER DESCRIPTION <code>acq</code> <p>The base acquisition function.</p> <p> TYPE: <code>A</code> </p> <code>apply_weight</code> <p>A function that takes the acquisition function values, the design points and the acquisition function itself and returns the weighted acquisition function values.</p> <p>Please see the module docstring for more information on the dimensions and how to handle them.</p> <p> TYPE: <code>Callable[[Tensor, Tensor, A], Tensor]</code> </p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/weighted_acquisition.py</code> <pre><code>def __init__(\n    self,\n    acq: A,\n    apply_weight: Callable[[Tensor, Tensor, A], Tensor],\n) -&gt; None:\n    \"\"\"Initialize the weighted acquisition function.\n\n    Args:\n        acq: The base acquisition function.\n        apply_weight: A function that takes the acquisition function values, the\n            design points and the acquisition function itself and returns the\n            weighted acquisition function values.\n\n            Please see the module docstring for more information on the dimensions\n            and how to handle them.\n    \"\"\"\n    super().__init__(model=acq.model)\n    # NOTE: We remove the X_pending from the base acquisition function as we will get\n    # it in our own forward with `@concatenate_pending_points` and pass that forward.\n    # This avoids possible duplicates. Also important to explicitly set it to None\n    # even if it does not exist as otherwise the attribute does not exists -_-\n    if (X_pending := getattr(acq, \"X_pending\", None)) is not None:\n        acq.set_X_pending(None)\n        self.set_X_pending(X_pending)\n    else:\n        acq.set_X_pending(None)\n        self.set_X_pending(None)\n\n    self.apply_weight = apply_weight\n    self.acq = acq\n    self._log = acq._log\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/acquisition_functions/weighted_acquisition/#neps.optimizers.bayesian_optimization.acquisition_functions.weighted_acquisition.WeightedAcquisition.forward","title":"forward","text":"<pre><code>forward(X: Tensor) -&gt; Tensor\n</code></pre> <p>Evaluate a weighted acquisition function on the candidate set X.</p> PARAMETER DESCRIPTION <code>X</code> <p>A tensor of size <code>batch_shape x q x d</code>-dim tensor of <code>q</code> <code>d</code>-dim design points.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor with the <code>d</code> dimension reduced away, representing the weighted acquisition function values at the given design points <code>X</code>.</p> Source code in <code>neps/optimizers/bayesian_optimization/acquisition_functions/weighted_acquisition.py</code> <pre><code>@concatenate_pending_points\n@t_batch_mode_transform()  # type: ignore\ndef forward(self, X: Tensor) -&gt; Tensor:\n    \"\"\"Evaluate a weighted acquisition function on the candidate set X.\n\n    Args:\n        X: A tensor of size `batch_shape x q x d`-dim tensor of `q` `d`-dim\n            design points.\n\n    Returns:\n        A tensor with the `d` dimension reduced away, representing the\n        weighted acquisition function values at the given design points `X`.\n    \"\"\"\n    if isinstance(self.acq, SampleReducingMCAcquisitionFunction):\n        # shape: mc_samples x batch x q-candidates\n        acq_values = self.acq._non_reduced_forward(X)\n        weighted_acq_values = self.apply_weight(acq_values, X, self.acq)\n        q_reduced_acq = self.acq._q_reduction(weighted_acq_values)\n        sample_reduced_acq = self.acq._sample_reduction(q_reduced_acq)\n        return sample_reduced_acq.squeeze(-1)\n\n    # shape: batch x q-candidates\n    acq_values = self.acq(X).unsqueeze(-1)\n    weighted_acq_values = self.apply_weight(acq_values, X, self.acq)\n    return weighted_acq_values.squeeze(-1)\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/ftpfn/","title":"Ftpfn","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/ftpfn/#neps.optimizers.bayesian_optimization.models.ftpfn","title":"neps.optimizers.bayesian_optimization.models.ftpfn","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/ftpfn/#neps.optimizers.bayesian_optimization.models.ftpfn.FTPFNSurrogate","title":"FTPFNSurrogate","text":"<pre><code>FTPFNSurrogate(\n    target_path: Path | None = None,\n    version: str = \"0.0.1\",\n    device: device | None = None,\n)\n</code></pre> <p>Wrapper around the IfBO model.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/ftpfn.py</code> <pre><code>def __init__(\n    self,\n    target_path: Path | None = None,\n    version: str = \"0.0.1\",\n    device: torch.device | None = None,\n):\n    if target_path is None:\n        # TODO: We also probably want to link this to the actual root directory\n        # or some shared directory between runs as relying on the path of the initial\n        # python invocation is likely to lead to issues somewhere.\n        # TODO: ifbo support for windows has issues with decompression\n        # We basically just do the same thing they do but manually\n        target_path = _download_workaround_for_ifbo_issue_10(target_path, version)\n\n    key = (str(target_path), version)\n    ftpfn = _CACHED_FTPFN_MODEL.get(key)\n    if ftpfn is None:\n        ftpfn = FTPFN(target_path=target_path, version=version, device=device)\n        _CACHED_FTPFN_MODEL[key] = ftpfn\n\n    self.ftpfn = ftpfn\n    self.device = self.ftpfn.device\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/ftpfn/#neps.optimizers.bayesian_optimization.models.ftpfn.encode_ftpfn","title":"encode_ftpfn","text":"<pre><code>encode_ftpfn(\n    trials: Mapping[str, Trial],\n    space: SearchSpace,\n    budget_domain: Domain,\n    encoder: ConfigEncoder,\n    *,\n    device: device | None = None,\n    dtype: dtype = FTPFN_DTYPE,\n    error_value: float = 0.0,\n    pending_value: float = nan\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Encode the trials into a format that the FTPFN model can understand.</p> <p>Pending trials</p> <p>For trials which do not have a loss reported yet, they are considered pending. By default this is torch.nan and we recommend fantasizing these values.</p> <p>Error values</p> <p>The FTPFN model requires that all loss values lie in the interval [0, 1]. By default, using the value of <code>error_value=0.0</code>, we encode crashed configurations as having an error value of 0.</p> PARAMETER DESCRIPTION <code>trials</code> <p>The trials to encode</p> <p> TYPE: <code>Mapping[str, Trial]</code> </p> <code>encoder</code> <p>The encoder to use</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>space</code> <p>The search space</p> <p> TYPE: <code>SearchSpace</code> </p> <code>budget_domain</code> <p>The domain to use for the budgets of the FTPFN</p> <p> TYPE: <code>Domain</code> </p> <code>device</code> <p>The device to use</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype to use</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>FTPFN_DTYPE</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor]</code> <p>The encoded trials and their corresponding scores</p> Source code in <code>neps/optimizers/bayesian_optimization/models/ftpfn.py</code> <pre><code>def encode_ftpfn(\n    trials: Mapping[str, Trial],\n    space: SearchSpace,\n    budget_domain: Domain,\n    encoder: ConfigEncoder,\n    *,\n    device: torch.device | None = None,\n    dtype: torch.dtype = FTPFN_DTYPE,\n    error_value: float = 0.0,\n    pending_value: float = torch.nan,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encode the trials into a format that the FTPFN model can understand.\n\n    !!! warning \"Pending trials\"\n\n        For trials which do not have a loss reported yet, they are considered pending.\n        By default this is torch.nan and we recommend fantasizing these values.\n\n    !!! warning \"Error values\"\n\n        The FTPFN model requires that all loss values lie in the interval [0, 1].\n        By default, using the value of `error_value=0.0`, we encode crashed configurations\n        as having an error value of 0.\n\n    Args:\n        trials: The trials to encode\n        encoder: The encoder to use\n        space: The search space\n        budget_domain: The domain to use for the budgets of the FTPFN\n        device: The device to use\n        dtype: The dtype to use\n\n    Returns:\n        The encoded trials and their corresponding **scores**\n    \"\"\"\n    # Select all trials which have something we can actually use for modelling\n    # The absence of a report signifies pending\n    selected = dict(trials.items())\n    assert space.fidelity_name is not None\n    assert space.fidelity is not None\n    assert 0 &lt;= error_value &lt;= 1\n    train_configs = encoder.encode(\n        [t.config for t in selected.values()], device=device, dtype=dtype\n    )\n    ids = torch.tensor(\n        [int(config_id.split(\"_\", maxsplit=1)[0]) for config_id in selected],\n        device=device,\n        dtype=dtype,\n    )\n    # PFN uses `0` id for test configurations\n    ids = ids + 1\n\n    train_fidelities = torch.tensor(\n        [t.config[space.fidelity_name] for t in selected.values()],\n        device=device,\n        dtype=dtype,\n    )\n    train_budgets = budget_domain.cast(\n        train_fidelities, frm=space.fidelity.domain, dtype=dtype\n    )\n\n    # TODO: Document that it's on the user to ensure these are already all bounded\n    # We could possibly include some bounded transform to assert this.\n    minimize_ys = torch.tensor(\n        [\n            pending_value\n            if trial.report is None\n            else (error_value if trial.report.loss is None else trial.report.loss)\n            for trial in trials.values()\n        ],\n        device=device,\n        dtype=dtype,\n    )\n    if minimize_ys.max() &gt; 1 or minimize_ys.min() &lt; 0:\n        raise RuntimeError(\n            \"ifBO requires that all loss values reported lie in the interval [0, 1]\"\n            \" but recieved loss value outside of that range!\"\n            f\"\\n{minimize_ys}\"\n        )\n    maximize_ys = 1 - minimize_ys\n    x_train = torch.cat(\n        [ids.unsqueeze(1), train_budgets.unsqueeze(1), train_configs], dim=1\n    )\n    return x_train, maximize_ys\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/","title":"Gp","text":""},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp","title":"neps.optimizers.bayesian_optimization.models.gp","text":"<p>Gaussian Process models for Bayesian Optimization.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.GPEncodedData","title":"GPEncodedData  <code>dataclass</code>","text":"<pre><code>GPEncodedData(\n    x: Tensor,\n    y: Tensor,\n    cost: Tensor | None = None,\n    x_pending: Tensor | None = None,\n)\n</code></pre> <p>Tensor data of finished configurations.</p>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.default_categorical_kernel","title":"default_categorical_kernel","text":"<pre><code>default_categorical_kernel(\n    N: int, active_dims: tuple[int, ...] | None = None\n) -&gt; ScaleKernel\n</code></pre> <p>Default Categorical kernel for the GP.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def default_categorical_kernel(\n    N: int,\n    active_dims: tuple[int, ...] | None = None,\n) -&gt; ScaleKernel:\n    \"\"\"Default Categorical kernel for the GP.\"\"\"\n    # Following BoTorches implementation of the MixedSingleTaskGP\n    return ScaleKernel(\n        CategoricalKernel(\n            ard_num_dims=N,\n            active_dims=active_dims,\n            lengthscale_constraint=gpytorch.constraints.GreaterThan(1e-6),\n        )\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.fit_and_acquire_from_gp","title":"fit_and_acquire_from_gp","text":"<pre><code>fit_and_acquire_from_gp(\n    *,\n    gp: SingleTaskGP,\n    x_train: Tensor,\n    encoder: ConfigEncoder,\n    acquisition: AcquisitionFunction,\n    prior: Prior | None = None,\n    pibo_exp_term: float | None = None,\n    cost_gp: SingleTaskGP | None = None,\n    costs: Tensor | None = None,\n    cost_percentage_used: float | None = None,\n    costs_on_log_scale: bool = True,\n    seed: int | None = None,\n    n_candidates_required: int | None = None,\n    num_restarts: int = 20,\n    n_initial_start_points: int = 256,\n    maximum_allowed_categorical_combinations: int = 30,\n    acq_options: Mapping[str, Any] | None = None\n) -&gt; Tensor\n</code></pre> <p>Acquire the next configuration to evaluate using a GP.</p> <p>Please see the following for:</p> <ul> <li>Making a GP to pass in:     <code>make_default_single_obj_gp</code></li> <li>Encoding configurations:     <code>encode_trails_for_gp</code></li> </ul> PARAMETER DESCRIPTION <code>gp</code> <p>The GP model to use.</p> <p> TYPE: <code>SingleTaskGP</code> </p> <code>x_train</code> <p>The encoded configurations that have already been evaluated</p> <p> TYPE: <code>Tensor</code> </p> <code>encoder</code> <p>The encoder used for encoding the configurations</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>acquisition</code> <p>The acquisition function to use.</p> <p>A good default is <code>qLogNoisyExpectedImprovement</code> which can handle pending configurations gracefully without fantasization.</p> <p> TYPE: <code>AcquisitionFunction</code> </p> <code>prior</code> <p>The prior to use over configurations. If this is provided, the acquisition function will be further weighted using the piBO acquisition.</p> <p> TYPE: <code>Prior | None</code> DEFAULT: <code>None</code> </p> <code>pibo_exp_term</code> <p>The exponential term for the piBO acquisition. If <code>None</code> is provided, one will be estimated.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>costs</code> <p>The costs of evaluating the configurations. If this is provided, then a secondary GP will be used to estimate the cost of a given configuration and factor into the weighting during the acquisiton of a new configuration.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>cost_percentage_used</code> <p>The percentage of the budget used so far. This is used to determine the strength of the cost cooling. Should be between 0 and 1. Must be provided if costs is provided.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>costs_on_log_scale</code> <p>Whether the costs are on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>encoder</code> <p>The encoder used for encoding the configurations</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>n_candidates_required</code> <p>The number of candidates to return. If left as <code>None</code>, only the best candidate will be returned. Otherwise a list of candidates will be returned.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>num_restarts</code> <p>The number of restarts to use during optimization.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>n_initial_start_points</code> <p>The number of initial start points to use during optimization.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>maximum_allowed_categorical_combinations</code> <p>The maximum number of categorical combinations to allow. If the number of combinations exceeds this, an error will be raised.</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>acq_options</code> <p>Additional options to pass to the botorch <code>optimizer_acqf</code> function.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The encoded next configuration(s) to evaluate. Use the encoder you provided to decode the configuration.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def fit_and_acquire_from_gp(\n    *,\n    gp: SingleTaskGP,\n    x_train: torch.Tensor,\n    encoder: ConfigEncoder,\n    acquisition: AcquisitionFunction,\n    prior: Prior | None = None,\n    pibo_exp_term: float | None = None,\n    cost_gp: SingleTaskGP | None = None,\n    costs: torch.Tensor | None = None,\n    cost_percentage_used: float | None = None,\n    costs_on_log_scale: bool = True,\n    seed: int | None = None,\n    n_candidates_required: int | None = None,\n    num_restarts: int = 20,\n    n_initial_start_points: int = 256,\n    maximum_allowed_categorical_combinations: int = 30,\n    acq_options: Mapping[str, Any] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Acquire the next configuration to evaluate using a GP.\n\n    Please see the following for:\n\n    * Making a GP to pass in:\n        [`make_default_single_obj_gp`][neps.optimizers.bayesian_optimization.models.gp.make_default_single_obj_gp]\n    * Encoding configurations:\n        [`encode_trails_for_gp`][neps.optimizers.bayesian_optimization.models.gp.encode_trails_for_gp]\n\n    Args:\n        gp: The GP model to use.\n        x_train: The encoded configurations that have already been evaluated\n        encoder: The encoder used for encoding the configurations\n        acquisition: The acquisition function to use.\n\n            A good default is `qLogNoisyExpectedImprovement` which can\n            handle pending configurations gracefully without fantasization.\n\n        prior: The prior to use over configurations. If this is provided, the\n            acquisition function will be further weighted using the piBO acquisition.\n        pibo_exp_term: The exponential term for the piBO acquisition. If `None` is\n            provided, one will be estimated.\n        costs: The costs of evaluating the configurations. If this is provided,\n            then a secondary GP will be used to estimate the cost of a given\n            configuration and factor into the weighting during the acquisiton of a new\n            configuration.\n        cost_percentage_used: The percentage of the budget used so far. This is used to\n            determine the strength of the cost cooling. Should be between 0 and 1.\n            Must be provided if costs is provided.\n        costs_on_log_scale: Whether the costs are on a log scale.\n        encoder: The encoder used for encoding the configurations\n        seed: The seed to use.\n        n_candidates_required: The number of candidates to return. If left\n            as `None`, only the best candidate will be returned. Otherwise\n            a list of candidates will be returned.\n        num_restarts: The number of restarts to use during optimization.\n        n_initial_start_points: The number of initial start points to use during\n            optimization.\n        maximum_allowed_categorical_combinations: The maximum number of categorical\n            combinations to allow. If the number of combinations exceeds this, an error\n            will be raised.\n        acq_options: Additional options to pass to the botorch `optimizer_acqf` function.\n\n    Returns:\n        The encoded next configuration(s) to evaluate. Use the encoder you provided\n        to decode the configuration.\n    \"\"\"\n    if seed is not None:\n        raise NotImplementedError(\"Seed is not implemented yet for gps\")\n\n    fit_gpytorch_mll(ExactMarginalLogLikelihood(likelihood=gp.likelihood, model=gp))\n\n    if prior:\n        if pibo_exp_term is None:\n            raise ValueError(\n                \"If providing a prior, you must provide the `pibo_exp_term`.\"\n            )\n\n        acquisition = pibo_acquisition(\n            acquisition,\n            prior=prior,\n            prior_exponent=pibo_exp_term,\n            x_domain=encoder.domains,\n        )\n\n    if costs is not None:\n        if cost_percentage_used is None:\n            raise ValueError(\n                \"If providing costs, you must provide `cost_percentage_used`.\"\n            )\n\n        # We simply ignore missing costs when training the cost GP.\n        missing_costs = torch.isnan(costs)\n        if missing_costs.any():\n            raise ValueError(\n                \"Must have at least some configurations reported with a cost\"\n                \" if using costs with a GP.\"\n            )\n\n        if missing_costs.any():\n            not_missing_mask = ~missing_costs\n            x_train_cost = costs[not_missing_mask]\n            y_train_cost = x_train[not_missing_mask]\n        else:\n            x_train_cost = x_train\n            y_train_cost = costs\n\n        if costs_on_log_scale:\n            transform = ChainedOutcomeTransform(\n                log=Log(),\n                standardize=Standardize(m=1),\n            )\n        else:\n            transform = Standardize(m=1)\n\n        cost_gp = make_default_single_obj_gp(\n            x_train_cost,\n            y_train_cost,\n            encoder=encoder,\n            y_transform=transform,\n        )\n        fit_gpytorch_mll(\n            ExactMarginalLogLikelihood(likelihood=cost_gp.likelihood, model=cost_gp)\n        )\n        acquisition = cost_cooled_acq(\n            acq_fn=acquisition,\n            model=cost_gp,\n            used_budget_percentage=cost_percentage_used,\n        )\n\n    _n = n_candidates_required if n_candidates_required is not None else 1\n\n    candidates, _scores = optimize_acq(\n        acquisition,\n        encoder,\n        n_candidates_required=_n,\n        num_restarts=num_restarts,\n        n_intial_start_points=n_initial_start_points,\n        acq_options=acq_options,\n        maximum_allowed_categorical_combinations=maximum_allowed_categorical_combinations,\n    )\n    return candidates\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.make_default_single_obj_gp","title":"make_default_single_obj_gp","text":"<pre><code>make_default_single_obj_gp(\n    x: Tensor,\n    y: Tensor,\n    encoder: ConfigEncoder,\n    *,\n    y_transform: OutcomeTransform | None = None\n) -&gt; SingleTaskGP\n</code></pre> <p>Default GP for single objective optimization.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def make_default_single_obj_gp(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    encoder: ConfigEncoder,\n    *,\n    y_transform: OutcomeTransform | None = None,\n) -&gt; SingleTaskGP:\n    \"\"\"Default GP for single objective optimization.\"\"\"\n    if y.ndim == 1:\n        y = y.unsqueeze(-1)\n\n    if y_transform is None:\n        y_transform = Standardize(m=1)\n\n    numerics: list[int] = []\n    categoricals: list[int] = []\n    for hp_name, transformer in encoder.transformers.items():\n        if isinstance(transformer, CategoricalToIntegerTransformer):\n            categoricals.append(encoder.index_of[hp_name])\n        else:\n            numerics.append(encoder.index_of[hp_name])\n\n    # Purely vectorial\n    if len(categoricals) == 0:\n        return SingleTaskGP(train_X=x, train_Y=y, outcome_transform=y_transform)\n\n    # Purely categorical\n    if len(numerics) == 0:\n        return SingleTaskGP(\n            train_X=x,\n            train_Y=y,\n            covar_module=default_categorical_kernel(len(categoricals)),\n            outcome_transform=y_transform,\n        )\n\n    # Mixed\n    numeric_kernel = get_covar_module_with_dim_scaled_prior(\n        ard_num_dims=len(numerics),\n        active_dims=tuple(numerics),\n    )\n    cat_kernel = default_categorical_kernel(\n        len(categoricals), active_dims=tuple(categoricals)\n    )\n\n    # WARNING: I previously tried SingleTaskMixedGp which does the following:\n    #\n    # x K((x1, c1), (x2, c2)) =\n    # x     K_cont_1(x1, x2) + K_cat_1(c1, c2) +\n    # x      K_cont_2(x1, x2) * K_cat_2(c1, c2)\n    #\n    # In a toy example with a single binary categorical which acted like F * {0, 1},\n    # the model collapsed to always predicting `0`. Causing all parameters defining F\n    # to essentially be guess at random. This is a lot more stable but likely not as\n    # good...\n    # TODO: Figure out how to improve stability of this.\n    kernel = numeric_kernel + cat_kernel\n\n    return SingleTaskGP(\n        train_X=x, train_Y=y, covar_module=kernel, outcome_transform=y_transform\n    )\n</code></pre>"},{"location":"api/neps/optimizers/bayesian_optimization/models/gp/#neps.optimizers.bayesian_optimization.models.gp.optimize_acq","title":"optimize_acq","text":"<pre><code>optimize_acq(\n    acq_fn: AcquisitionFunction,\n    encoder: ConfigEncoder,\n    *,\n    n_candidates_required: int = 1,\n    num_restarts: int = 20,\n    n_intial_start_points: int | None = None,\n    acq_options: Mapping[str, Any] | None = None,\n    maximum_allowed_categorical_combinations: int = 30\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Optimize the acquisition function.</p> Source code in <code>neps/optimizers/bayesian_optimization/models/gp.py</code> <pre><code>def optimize_acq(\n    acq_fn: AcquisitionFunction,\n    encoder: ConfigEncoder,\n    *,\n    n_candidates_required: int = 1,\n    num_restarts: int = 20,\n    n_intial_start_points: int | None = None,\n    acq_options: Mapping[str, Any] | None = None,\n    maximum_allowed_categorical_combinations: int = 30,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Optimize the acquisition function.\"\"\"\n    acq_options = acq_options or {}\n\n    lower = [domain.lower for domain in encoder.domains]\n    upper = [domain.upper for domain in encoder.domains]\n    bounds = torch.tensor([lower, upper], dtype=torch.float64)\n\n    cat_transformers = {\n        name: t for name, t in encoder.transformers.items() if t.domain.is_categorical\n    }\n    if not any(cat_transformers):\n        # Small heuristic to increase the number of candidates as our dimensionality\n        # increases... we apply a cap.\n        if n_intial_start_points is None:\n            # TODO: Need to investigate how num_restarts is used in botorch to inform\n            # this proxy.\n\n            # Cap out at 4096 when len(bounds) &gt;= 8\n            n_intial_start_points = min(64 * len(bounds) ** 2, 4096)\n\n        return optimize_acqf(\n            acq_function=acq_fn,\n            bounds=bounds,\n            q=n_candidates_required,\n            num_restarts=num_restarts,\n            raw_samples=n_intial_start_points,\n            **acq_options,\n        )\n\n    # We need to generate the product of all possible combinations of categoricals,\n    # first we do a sanity check\n    n_combos = reduce(\n        lambda x, y: x * y,  # type: ignore\n        [t.domain.cardinality for t in cat_transformers.values()],\n        1,\n    )\n    if n_combos &gt; maximum_allowed_categorical_combinations:\n        raise ValueError(\n            \"The number of fixed categorical dimensions is too high. \"\n            \"This will lead to an explosion in the number of possible \"\n            f\"combinations. Got: {n_combos} while the setting for the function\"\n            f\" is: {maximum_allowed_categorical_combinations=}. Consider reducing the \"\n            \"dimensions or consider encoding your categoricals in some other format.\"\n        )\n\n    # Right, now we generate all possible combinations\n    # First, just collect the possible values per cat column\n    # NOTE: Botorchs optim requires them to be as floats\n    cats: dict[int, list[float]] = {\n        encoder.index_of[name]: [\n            float(i)\n            for i in range(transformer.domain.cardinality)  # type: ignore\n        ]\n        for name, transformer in cat_transformers.items()\n    }\n\n    # Second, generate all possible combinations\n    fixed_cats: list[dict[int, float]]\n    if len(cats) == 1:\n        col, choice_indices = next(iter(cats.items()))\n        fixed_cats = [{col: i} for i in choice_indices]\n    else:\n        fixed_cats = [\n            dict(zip(cats.keys(), combo, strict=False))\n            for combo in product(*cats.values())\n        ]\n\n    # TODO: we should deterministically shuffle the fixed_categoricals\n    # as the underlying function does not.\n    return optimize_acqf_mixed(\n        acq_function=acq_fn,\n        bounds=bounds,\n        num_restarts=min(num_restarts // n_combos, 2),\n        raw_samples=n_intial_start_points,\n        q=n_candidates_required,\n        fixed_features_list=fixed_cats,\n        **acq_options,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/grid_search/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/optimizers/grid_search/optimizer/#neps.optimizers.grid_search.optimizer","title":"neps.optimizers.grid_search.optimizer","text":""},{"location":"api/neps/optimizers/multi_fidelity/hyperband/","title":"Hyperband","text":""},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband","title":"neps.optimizers.multi_fidelity.hyperband","text":""},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband","title":"AsynchronousHyperband","text":"<pre><code>AsynchronousHyperband(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | None\n    ) = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>HyperbandBase</code></p> <p>Implements ASHA but as Hyperband.</p> <p>Implements the Promotion variant of ASHA as used in Mobster.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] | None = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    args = {\n        \"pipeline_space\": pipeline_space,\n        \"budget\": budget,\n        \"eta\": eta,\n        \"initial_design_type\": initial_design_type,\n        \"use_priors\": use_priors,\n        \"sampling_policy\": sampling_policy,\n        \"promotion_policy\": promotion_policy,\n        \"loss_value_on_error\": loss_value_on_error,\n        \"cost_value_on_error\": cost_value_on_error,\n        \"ignore_errors\": ignore_errors,\n        \"prior_confidence\": prior_confidence,\n        \"random_interleave_prob\": random_interleave_prob,\n        \"sample_default_first\": sample_default_first,\n        \"sample_default_at_target\": sample_default_at_target,\n    }\n    super().__init__(**args)\n    # overwrite parent class SH brackets with Async SH brackets\n    self.sh_brackets: dict[int, SuccessiveHalvingBase] = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        # key difference from vanilla HB where it runs synchronous SH brackets\n        self.sh_brackets[s] = AsynchronousSuccessiveHalving(**args)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\"\"\"\n    # unlike synchronous SH, the state is not reset at each rung and a configuration\n    # is promoted if the rung has eta configs if it is the top performing\n    # base class allows for retaining the whole optimization state\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    # the rung to sample at\n    bracket_to_run = self._get_bracket_to_run()\n    config, config_id, previous_config_id = self.sh_brackets[\n        bracket_to_run\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperband.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors","title":"AsynchronousHyperbandWithPriors","text":"<pre><code>AsynchronousHyperbandWithPriors(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>AsynchronousHyperband</code></p> <p>Implements ASHA but as Hyperband.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,  # key change to the base Async HB class\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\"\"\"\n    # unlike synchronous SH, the state is not reset at each rung and a configuration\n    # is promoted if the rung has eta configs if it is the top performing\n    # base class allows for retaining the whole optimization state\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    # the rung to sample at\n    bracket_to_run = self._get_bracket_to_run()\n    config, config_id, previous_config_id = self.sh_brackets[\n        bracket_to_run\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.AsynchronousHyperbandWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband","title":"Hyperband","text":"<pre><code>Hyperband(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | None\n    ) = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>HyperbandBase</code></p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] | None = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    args = {\n        \"pipeline_space\": pipeline_space,\n        \"budget\": budget,\n        \"eta\": eta,\n        \"early_stopping_rate\": self.early_stopping_rate,  # HB subsumes this from SH\n        \"initial_design_type\": initial_design_type,\n        \"use_priors\": use_priors,\n        \"sampling_policy\": sampling_policy,\n        \"promotion_policy\": promotion_policy,\n        \"loss_value_on_error\": loss_value_on_error,\n        \"cost_value_on_error\": cost_value_on_error,\n        \"ignore_errors\": ignore_errors,\n        \"prior_confidence\": prior_confidence,\n        \"random_interleave_prob\": random_interleave_prob,\n        \"sample_default_first\": sample_default_first,\n        \"sample_default_at_target\": sample_default_at_target,\n    }\n    super().__init__(**args)\n    # stores the flattened sequence of SH brackets to loop over - the HB heuristic\n    # for (n,r) pairing, i.e., (num. configs, fidelity)\n    self.full_rung_trace = []\n    self.sh_brackets: dict[int, SuccessiveHalvingBase] = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        self.sh_brackets[s] = SuccessiveHalving(**args)\n        # `full_rung_trace` contains the index of SH bracket to run sequentially\n        self.full_rung_trace.extend([s] * len(self.sh_brackets[s].full_rung_trace))\n    # book-keeping variables\n    self.current_sh_bracket: int = 0\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        assert isinstance(sh_bracket, SuccessiveHalving)\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    config, config_id, previous_config_id = self.sh_brackets[\n        self.current_sh_bracket\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.Hyperband.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase","title":"HyperbandBase","text":"<pre><code>HyperbandBase(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | None\n    ) = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalvingBase</code></p> <p>Implements a Hyperband procedure with a sampling and promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] | None = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    args = {\n        \"pipeline_space\": pipeline_space,\n        \"budget\": budget,\n        \"eta\": eta,\n        \"early_stopping_rate\": self.early_stopping_rate,  # HB subsumes this from SH\n        \"initial_design_type\": initial_design_type,\n        \"use_priors\": use_priors,\n        \"sampling_policy\": sampling_policy,\n        \"promotion_policy\": promotion_policy,\n        \"loss_value_on_error\": loss_value_on_error,\n        \"cost_value_on_error\": cost_value_on_error,\n        \"ignore_errors\": ignore_errors,\n        \"prior_confidence\": prior_confidence,\n        \"random_interleave_prob\": random_interleave_prob,\n        \"sample_default_first\": sample_default_first,\n        \"sample_default_at_target\": sample_default_at_target,\n    }\n    super().__init__(**args)\n    # stores the flattened sequence of SH brackets to loop over - the HB heuristic\n    # for (n,r) pairing, i.e., (num. configs, fidelity)\n    self.full_rung_trace = []\n    self.sh_brackets: dict[int, SuccessiveHalvingBase] = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        self.sh_brackets[s] = SuccessiveHalving(**args)\n        # `full_rung_trace` contains the index of SH bracket to run sequentially\n        self.full_rung_trace.extend([s] * len(self.sh_brackets[s].full_rung_trace))\n    # book-keeping variables\n    self.current_sh_bracket: int = 0\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\"\"\"\n    # unlike synchronous SH, the state is not reset at each rung and a configuration\n    # is promoted if the rung has eta configs if it is the top performing\n    # base class allows for retaining the whole optimization state\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.get_config_and_ids","title":"get_config_and_ids  <code>abstractmethod</code>","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>@abstractmethod\ndef get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandBase.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault","title":"HyperbandCustomDefault","text":"<pre><code>HyperbandCustomDefault(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>HyperbandWithPriors</code></p> <p>If prior specified, does 50% times priors and 50% random search like vanilla-HB.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    self.sampling_args = {\n        \"inc\": None,\n        \"weights\": {\n            \"prior\": 0.5,\n            \"inc\": 0,\n            \"random\": 0.5,\n        },\n    }\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_args = self.sampling_args\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        assert isinstance(sh_bracket, SuccessiveHalving)\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    config, config_id, previous_config_id = self.sh_brackets[\n        self.current_sh_bracket\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandCustomDefault.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors","title":"HyperbandWithPriors","text":"<pre><code>HyperbandWithPriors(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>Hyperband</code></p> <p>Implements a Hyperband procedure with a sampling and promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,  # key change to the base HB class\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        assert isinstance(sh_bracket, SuccessiveHalving)\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    config, config_id, previous_config_id = self.sh_brackets[\n        self.current_sh_bracket\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/hyperband/#neps.optimizers.multi_fidelity.hyperband.HyperbandWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/ifbo/","title":"Ifbo","text":""},{"location":"api/neps/optimizers/multi_fidelity/ifbo/#neps.optimizers.multi_fidelity.ifbo","title":"neps.optimizers.multi_fidelity.ifbo","text":""},{"location":"api/neps/optimizers/multi_fidelity/ifbo/#neps.optimizers.multi_fidelity.ifbo.IFBO","title":"IFBO","text":"<pre><code>IFBO(\n    *,\n    pipeline_space: SearchSpace,\n    step_size: int | float = 1,\n    use_priors: bool = False,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n    surrogate_model_args: dict | None = None,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    n_acquisition_new_configs: int = 1000,\n    device: device | None = None,\n    budget: int | float | None = None,\n    loss_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    ignore_errors: bool = False\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Base class for MF-BO algorithms that use DyHPO-like acquisition and budgeting.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>step_size</code> <p>The size of the step to take in the fidelity domain.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>1</code> </p> <code>sampling_policy</code> <p>The type of sampling procedure to use</p> <p> </p> <code>promotion_policy</code> <p>The type of promotion procedure to use</p> <p> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>initial_design_size</code> <p>Number of configs to sample before starting optimization</p> <p>If None, the number of configs will be equal to the number of dimensions.</p> <p> TYPE: <code>int | Literal['ndim']</code> DEFAULT: <code>'ndim'</code> </p> <code>device</code> <p>Device to use for the model</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/optimizers/multi_fidelity/ifbo.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    step_size: int | float = 1,\n    use_priors: bool = False,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n    surrogate_model_args: dict | None = None,\n    initial_design_size: int | Literal[\"ndim\"] = \"ndim\",\n    n_acquisition_new_configs: int = 1_000,\n    device: torch.device | None = None,\n    budget: int | float | None = None,  # TODO: Remove\n    loss_value_on_error: float | None = None,  # TODO: Remove\n    cost_value_on_error: float | None = None,  # TODO: Remove\n    ignore_errors: bool = False,  # TODO: Remove\n):\n    \"\"\"Initialise.\n\n    Args:\n        pipeline_space: Space in which to search\n        step_size: The size of the step to take in the fidelity domain.\n        sampling_policy: The type of sampling procedure to use\n        promotion_policy: The type of promotion procedure to use\n        sample_default_first: Whether to sample the default configuration first\n        initial_design_size: Number of configs to sample before starting optimization\n\n            If None, the number of configs will be equal to the number of dimensions.\n\n        device: Device to use for the model\n    \"\"\"\n    # TODO: I'm not sure how this might effect tables, whose lowest fidelity\n    # might be below to possibly increased lower bound.\n    space, fid_bins = _adjust_pipeline_space_to_match_stepsize(\n        pipeline_space, step_size\n    )\n    assert space.fidelity is not None\n    assert isinstance(space.fidelity_name, str)\n\n    super().__init__(pipeline_space=space)\n    self.step_size = step_size\n    self.use_priors = use_priors\n    self.sample_default_first = sample_default_first\n    self.sample_default_at_target = sample_default_at_target\n    self.device = device\n    self.n_initial_design: int | Literal[\"ndim\"] = initial_design_size\n    self.n_acquisition_new_configs = n_acquisition_new_configs\n    self.surrogate_model_args = surrogate_model_args or {}\n\n    self._min_budget: int | float = space.fidelity.lower\n    self._max_budget: int | float = space.fidelity.upper\n    self._fidelity_name: str = space.fidelity_name\n    self._initial_design: list[dict[str, Any]] | None = None\n\n    self._prior: Prior | None\n    if use_priors:\n        self._prior = Prior.from_space(space, include_fidelity=False)\n    else:\n        self._prior = None\n\n    self._config_encoder: ConfigEncoder = ConfigEncoder.from_space(\n        space=space,\n        include_constants_when_decoding=True,\n        # FTPFN doesn't support categoricals and we were recomended\n        # to just evenly distribute in the unit norm\n        custom_transformers={\n            cat_name: CategoricalToUnitNorm(choices=cat.choices)\n            for cat_name, cat in space.categoricals.items()\n        },\n    )\n\n    # Domain of fidelity values, i.e. what is given in the configs that we\n    # give to the user to evaluate at.\n    self._fid_domain = space.fidelity.domain\n\n    # Domain in which we should pass budgets to ifbo model\n    self._budget_domain = Domain.floating(1 / self._max_budget, 1)\n\n    # Domain from which we assign an index to each budget\n    self._budget_ix_domain = Domain.indices(fid_bins)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/ifbo/#neps.optimizers.multi_fidelity.ifbo.IFBO.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/ifbo/#neps.optimizers.multi_fidelity.ifbo.IFBO.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/","title":"Mf bo","text":""},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo","title":"neps.optimizers.multi_fidelity.mf_bo","text":""},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.MFBOBase","title":"MFBOBase","text":"<p>Designed to work with model-based search on SH-based multi-fidelity algorithms.</p> <p>Requires certain strict assumptions about fidelities and rung maps.</p>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.MFBOBase.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        total_resources = sum(self.rung_map[r] for r in rungs)\n        resources = total_resources / self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/mf_bo/#neps.optimizers.multi_fidelity.mf_bo.MFBOBase.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(\n    rung: int | None = None, **kwargs: Any\n) -&gt; SearchSpace\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int | None = None,\n    **kwargs: Any,\n) -&gt; SearchSpace:\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            raise ValueError(\"Choice of modelling_type not in 'rung', 'joint'\")\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = sample_one_old(\n            self.pipeline_space,\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/","title":"Promotion policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy","title":"neps.optimizers.multi_fidelity.promotion_policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.AsyncPromotionPolicy","title":"AsyncPromotionPolicy","text":"<pre><code>AsyncPromotionPolicy(eta: int, **kwargs: Any)\n</code></pre> <p>               Bases: <code>PromotionPolicy</code></p> <p>Implements an asynchronous promotion from lower to higher fidelity.</p> <p>Promotes whenever a higher fidelity has at least eta configurations.</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def __init__(self, eta: int, **kwargs: Any):\n    super().__init__(eta, **kwargs)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.AsyncPromotionPolicy.retrieve_promotions","title":"retrieve_promotions","text":"<pre><code>retrieve_promotions() -&gt; dict\n</code></pre> <p>Returns the top 1/eta configurations per rung if enough configurations seen.</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def retrieve_promotions(self) -&gt; dict:\n    \"\"\"Returns the top 1/eta configurations per rung if enough configurations seen.\"\"\"\n    assert self.max_rung is not None\n    for rung in range(self.max_rung + 1):\n        if rung == self.max_rung:\n            # cease promotions for the highest rung (configs at max budget)\n            continue\n        # if less than eta configurations seen, no promotions occur as top_k=0\n        top_k = len(self.rung_members_performance[rung]) // self.eta\n        _ordered_idx = np.argsort(self.rung_members_performance[rung])\n        self.rung_promotions[rung] = np.array(self.rung_members[rung])[_ordered_idx][\n            :top_k\n        ].tolist()\n    return self.rung_promotions\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.PromotionPolicy","title":"PromotionPolicy","text":"<pre><code>PromotionPolicy(eta: int)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for implementing a sampling straregy for SH and its subclasses.</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def __init__(self, eta: int):\n    self.rung_members: dict = {}\n    self.rung_members_performance: dict = {}\n    self.rung_promotions: dict = {}\n    self.eta: int = eta\n    self.max_rung: int | None = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.SyncPromotionPolicy","title":"SyncPromotionPolicy","text":"<pre><code>SyncPromotionPolicy(eta: int, **kwargs: Any)\n</code></pre> <p>               Bases: <code>PromotionPolicy</code></p> <p>Implements a synchronous promotion from lower to higher fidelity.</p> <p>Promotes only when all predefined number of config slots are full.</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def __init__(self, eta: int, **kwargs: Any):\n    super().__init__(eta, **kwargs)\n    self.config_map: dict | None = None\n    self.rung_promotions: dict | None = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/promotion_policy/#neps.optimizers.multi_fidelity.promotion_policy.SyncPromotionPolicy.retrieve_promotions","title":"retrieve_promotions","text":"<pre><code>retrieve_promotions() -&gt; dict\n</code></pre> <p>Returns the top 1/eta configurations per rung if enough configurations seen.</p> Source code in <code>neps/optimizers/multi_fidelity/promotion_policy.py</code> <pre><code>def retrieve_promotions(self) -&gt; dict:\n    \"\"\"Returns the top 1/eta configurations per rung if enough configurations seen.\"\"\"\n    assert self.config_map is not None\n\n    self.rung_promotions = {rung: [] for rung in self.config_map}\n    total_rung_evals = 0\n    for rung in sorted(self.config_map.keys(), reverse=True):\n        total_rung_evals += len(self.rung_members[rung])\n        if (\n            total_rung_evals &gt;= self.config_map[rung]\n            and np.isnan(self.rung_members_performance[rung]).sum()\n        ):\n            # if rung is full but incomplete evaluations, pause on promotions, wait\n            return self.rung_promotions\n        if rung == self.max_rung:\n            # cease promotions for the highest rung (configs at max budget)\n            continue\n        if (\n            total_rung_evals &gt;= self.config_map[rung]\n            and np.isnan(self.rung_members_performance[rung]).sum() == 0\n        ):\n            # if rung is full and no incomplete evaluations, find promotions\n            top_k = (self.config_map[rung] // self.eta) - (\n                self.config_map[rung] - len(self.rung_members[rung])\n            )\n            selected_idx = np.argsort(self.rung_members_performance[rung])[:top_k]\n            self.rung_promotions[rung] = self.rung_members[rung][selected_idx]\n    return self.rung_promotions\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/","title":"Sampling policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy","title":"neps.optimizers.multi_fidelity.sampling_policy","text":""},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.EnsemblePolicy","title":"EnsemblePolicy","text":"<pre><code>EnsemblePolicy(\n    pipeline_space: SearchSpace,\n    inc_type: Literal[\n        \"hypersphere\", \"gaussian\", \"crossover\", \"mutation\"\n    ] = \"mutation\",\n)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>Ensemble of sampling policies including sampling randomly, from prior &amp; incumbent.</p> PARAMETER DESCRIPTION <code>SamplingPolicy</code> <p>[description]</p> <p> TYPE: <code>[type]</code> </p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>inc_type</code> <p>str if \"hypersphere\", uniformly samples from around the incumbent within its     distance from the nearest neighbour in history if \"gaussian\", samples from a gaussian around the incumbent if \"crossover\", generates a config by crossover between a random sample     and the incumbent if \"mutation\", generates a config by perturbing each hyperparameter with     50% (mutation_rate=0.5) probability of selecting each hyperparmeter     for perturbation, sampling a deviation N(value, mutation_std=0.5))</p> <p> TYPE: <code>Literal['hypersphere', 'gaussian', 'crossover', 'mutation']</code> DEFAULT: <code>'mutation'</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(\n    self,\n    pipeline_space: SearchSpace,\n    inc_type: Literal[\n        \"hypersphere\", \"gaussian\", \"crossover\", \"mutation\"\n    ] = \"mutation\",\n):\n    \"\"\"Samples a policy as per its weights and performs the selected sampling.\n\n    Args:\n        pipeline_space: Space in which to search\n        inc_type: str\n            if \"hypersphere\", uniformly samples from around the incumbent within its\n                distance from the nearest neighbour in history\n            if \"gaussian\", samples from a gaussian around the incumbent\n            if \"crossover\", generates a config by crossover between a random sample\n                and the incumbent\n            if \"mutation\", generates a config by perturbing each hyperparameter with\n                50% (mutation_rate=0.5) probability of selecting each hyperparmeter\n                for perturbation, sampling a deviation N(value, mutation_std=0.5))\n    \"\"\"\n    super().__init__(pipeline_space=pipeline_space)\n    self.inc_type = inc_type\n    # setting all probabilities uniformly\n    self.policy_map = {\"random\": 0.33, \"prior\": 0.34, \"inc\": 0.33}\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.EnsemblePolicy.sample","title":"sample","text":"<pre><code>sample(\n    inc: SearchSpace | None = None,\n    weights: dict[str, float] | None = None,\n    *args: Any,\n    **kwargs: Any\n) -&gt; SearchSpace\n</code></pre> <p>Samples from the prior with a certain probability.</p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>[description]</p> <p> TYPE: <code>SearchSpace</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample(  # noqa: PLR0912, C901, PLR0915\n    self,\n    inc: SearchSpace | None = None,\n    weights: dict[str, float] | None = None,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; SearchSpace:\n    \"\"\"Samples from the prior with a certain probability.\n\n    Returns:\n        SearchSpace: [description]\n    \"\"\"\n    from neps.optimizers.multi_fidelity_prior.utils import (\n        custom_crossover,\n        local_mutation,\n    )\n\n    if weights is not None:\n        for key, value in sorted(weights.items()):\n            self.policy_map[key] = value\n    else:\n        logger.info(f\"Using default policy weights: {self.policy_map}\")\n    prob_weights = [v for _, v in sorted(self.policy_map.items())]\n    policy_idx = np.random.choice(range(len(prob_weights)), p=prob_weights)\n    policy = sorted(self.policy_map.keys())[policy_idx]\n\n    logger.info(f\"Sampling from {policy} with weights (i, p, r)={prob_weights}\")\n\n    if policy == \"prior\":\n        config = sample_one_old(\n            self.pipeline_space,\n            patience=self.patience,\n            user_priors=True,\n            ignore_fidelity=True,\n        )\n    elif policy == \"inc\":\n        if (\n            hasattr(self.pipeline_space, \"has_prior\")\n            and self.pipeline_space.has_prior\n        ):\n            user_priors = True\n        else:\n            user_priors = False\n\n        if inc is None:\n            inc = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n            logger.warning(\n                \"No incumbent config found, using default as the incumbent.\"\n            )\n\n        if self.inc_type == \"hypersphere\":\n            distance = kwargs[\"distance\"]\n            config = self.sample_neighbour(inc, distance)\n        elif self.inc_type == \"gaussian\":\n            # TODO: These could be lifted higher, ideall we pass\n            # down the encoder we want, where we want it. Also passing\n            # around a `Prior` should be the evidence that we want to use\n            # a prior, not whether the searchspace has a flag active or not.\n            encoder = ConfigEncoder.from_space(inc)\n            sampler = (\n                Prior.from_space(inc)\n                if user_priors\n                else Sampler.uniform(ndim=encoder.ncols)\n            )\n\n            config_tensor = sampler.sample(1, to=encoder.domains)\n            config_dict = encoder.decode(config_tensor)[0]\n            _fids = {fid_name: fid.value for fid_name, fid in inc.fidelities.items()}\n\n            config = inc.from_dict({**config_dict, **_fids})\n\n        elif self.inc_type == \"crossover\":\n            # choosing the configuration for crossover with incumbent\n            # the weight distributed across prior adnd inc\n            _w_priors = 1 - self.policy_map[\"random\"]\n            # re-calculate normalized score ratio for prior-inc\n            w_prior = np.clip(self.policy_map[\"prior\"] / _w_priors, a_min=0, a_max=1)\n            w_inc = np.clip(self.policy_map[\"inc\"] / _w_priors, a_min=0, a_max=1)\n            # calculating difference of prior and inc score\n            score_diff = np.abs(w_prior - w_inc)\n            # using the difference in score as the weight of what to sample when\n            # if the score difference is small, crossover between incumbent and prior\n            # if the score difference is large, crossover between incumbent and random\n            probs = [1 - score_diff, score_diff]  # the order is [prior, random]\n            if (\n                hasattr(self.pipeline_space, \"has_prior\")\n                and not self.pipeline_space.has_prior\n            ):\n                user_priors = False\n            else:\n                user_priors = np.random.choice([True, False], p=probs)\n            logger.info(\n                f\"Crossing over with user_priors={user_priors} with p={probs}\"\n            )\n            # sampling a configuration either randomly or from a prior\n            _config = sample_one_old(\n                self.pipeline_space,\n                patience=self.patience,\n                user_priors=user_priors,\n                ignore_fidelity=True,\n            )\n            # injecting hyperparameters from the sampled config into the incumbent\n            # TODO: ideally lower crossover prob overtime\n            config = custom_crossover(inc, _config, crossover_prob=0.5)\n        elif self.inc_type == \"mutation\":\n            if \"inc_mutation_rate\" in kwargs:\n                config = local_mutation(\n                    inc,\n                    mutation_rate=kwargs[\"inc_mutation_rate\"],\n                    std=kwargs[\"inc_mutation_std\"],\n                )\n            else:\n                config = local_mutation(inc)\n        else:\n            raise ValueError(\n                f\"{self.inc_type} is not in \"\n                f\"{{'mutation', 'crossover', 'hypersphere', 'gaussian'}}\"\n            )\n    else:\n        config = sample_one_old(\n            self.pipeline_space,\n            patience=self.patience,\n            user_priors=False,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.EnsemblePolicy.sample_neighbour","title":"sample_neighbour","text":"<pre><code>sample_neighbour(\n    incumbent: SearchSpace,\n    distance: float,\n    tolerance: float = TOLERANCE,\n) -&gt; SearchSpace\n</code></pre> <p>Samples a config from around the <code>incumbent</code> within radius as <code>distance</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample_neighbour(\n    self,\n    incumbent: SearchSpace,\n    distance: float,\n    tolerance: float = TOLERANCE,\n) -&gt; SearchSpace:\n    \"\"\"Samples a config from around the `incumbent` within radius as `distance`.\"\"\"\n    # TODO: how does tolerance affect optimization on landscapes of different scale\n    sample_counter = 0\n    from neps.optimizers.multi_fidelity_prior.utils import (\n        compute_config_dist,\n    )\n\n    while True:\n        config = sample_one_old(\n            self.pipeline_space,\n            patience=self.patience,\n            user_priors=False,\n            ignore_fidelity=False,\n        )\n        # computing distance from incumbent\n        d = compute_config_dist(config, incumbent)\n        # checking if sample is within the hypersphere around the incumbent\n        if d &lt; max(distance, tolerance):\n            # accept sample\n            break\n        sample_counter += 1\n        if sample_counter &gt; SAMPLE_THRESHOLD:\n            # reset counter for next increased radius for hypersphere\n            sample_counter = 0\n            # if no sample falls within the radius, increase the threshold radius 1%\n            distance += distance * DELTA_THRESHOLD\n    # end of while\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.FixedPriorPolicy","title":"FixedPriorPolicy","text":"<pre><code>FixedPriorPolicy(\n    pipeline_space: SearchSpace,\n    fraction_from_prior: float = 1,\n)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>A random policy for sampling configuration, i.e. the default for SH but samples a fixed fraction from the prior.</p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(self, pipeline_space: SearchSpace, fraction_from_prior: float = 1):\n    super().__init__(pipeline_space=pipeline_space)\n    assert 0 &lt;= fraction_from_prior &lt;= 1\n    self.fraction_from_prior = fraction_from_prior\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.FixedPriorPolicy.sample","title":"sample","text":"<pre><code>sample(*args: Any, **kwargs: Any) -&gt; SearchSpace\n</code></pre> <p>Samples from the prior with a certain probabiliyu.</p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>[description]</p> <p> TYPE: <code>SearchSpace</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample(self, *args: Any, **kwargs: Any) -&gt; SearchSpace:\n    \"\"\"Samples from the prior with a certain probabiliyu.\n\n    Returns:\n        SearchSpace: [description]\n    \"\"\"\n    user_priors = False\n    if np.random.uniform() &lt; self.fraction_from_prior:\n        user_priors = True\n\n    return sample_one_old(\n        self.pipeline_space,\n        patience=self.patience,\n        user_priors=user_priors,\n        ignore_fidelity=True,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.ModelPolicy","title":"ModelPolicy","text":"<pre><code>ModelPolicy(\n    *,\n    pipeline_space: SearchSpace,\n    prior: Prior | None = None,\n    use_cost: bool = False,\n    device: device | None = None\n)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>A policy for sampling configuration, i.e. the default for SH / hyperband.</p> PARAMETER DESCRIPTION <code>SamplingPolicy</code> <p>[description]</p> <p> TYPE: <code>[type]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    prior: Prior | None = None,\n    use_cost: bool = False,\n    device: torch.device | None = None,\n):\n    if use_cost:\n        raise NotImplementedError(\"Cost is not implemented yet.\")\n\n    super().__init__(pipeline_space=pipeline_space)\n    self.device = device\n    self.prior = prior\n    self._encoder = ConfigEncoder.from_space(\n        pipeline_space,\n        include_constants_when_decoding=True,\n    )\n    self._model: SingleTaskGP | None = None\n    self._acq: AcquisitionFunction | None = None\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.ModelPolicy.sample","title":"sample","text":"<pre><code>sample(\n    active_max_fidelity: int | None = None,\n    fidelity: int | None = None,\n    **kwargs: Any\n) -&gt; SearchSpace\n</code></pre> <p>Performs the equivalent of optimizing the acquisition function.</p> Performs 2 strategies as per the arguments passed <ul> <li>If fidelity is not None, triggers the case when the surrogate has been   trained jointly with the fidelity dimension, i.e., all observations ever   recorded. In this case, the EI for random samples is evaluated at the   <code>fidelity</code> where the new sample will be evaluated. The top-10 are selected,   and the EI for them is evaluated at the target/mmax fidelity.</li> <li>If active_max_fidelity is not None, triggers the case when a surrogate is   trained per fidelity. In this case, all samples have their fidelity   variable set to the same value. This value is same as that of the fidelity   value of the configs in the training data.</li> </ul> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def sample(\n    self,\n    active_max_fidelity: int | None = None,\n    fidelity: int | None = None,\n    **kwargs: Any,\n) -&gt; SearchSpace:\n    \"\"\"Performs the equivalent of optimizing the acquisition function.\n\n    Performs 2 strategies as per the arguments passed:\n        * If fidelity is not None, triggers the case when the surrogate has been\n          trained jointly with the fidelity dimension, i.e., all observations ever\n          recorded. In this case, the EI for random samples is evaluated at the\n          `fidelity` where the new sample will be evaluated. The top-10 are selected,\n          and the EI for them is evaluated at the target/mmax fidelity.\n        * If active_max_fidelity is not None, triggers the case when a surrogate is\n          trained per fidelity. In this case, all samples have their fidelity\n          variable set to the same value. This value is same as that of the fidelity\n          value of the configs in the training data.\n    \"\"\"\n    # sampling random configurations\n    samples = [\n        sample_one_old(self.pipeline_space, user_priors=False, ignore_fidelity=True)\n        for _ in range(SAMPLE_THRESHOLD)\n    ]\n\n    if fidelity is not None:\n        # w/o setting this flag, the AF eval will set all fidelities to max\n        self.acquisition.optimize_on_max_fidelity = False\n        _inc_copy = self.acquisition.incumbent\n        # TODO: better design required, for example, not import torch\n        #  right now this case handles the 2-step acquisition in `sample`\n        if \"incumbent\" in kwargs:\n            # sets the incumbent to the best score at the required fidelity for\n            # correct computation of EI scores\n            self.acquisition.incumbent = torch.tensor(kwargs[\"incumbent\"])\n        # updating the fidelity of the sampled configurations\n        samples = list(map(update_fidelity, samples, [fidelity] * len(samples)))\n        # computing EI at the given `fidelity`\n        eis = self.acquisition.eval(x=samples, asscalar=True)\n        # extracting the 10 highest scores\n        _ids = np.argsort(eis)[-TOP_EI_SAMPLE_COUNT:]\n        samples = pd.Series(samples).iloc[_ids].values.tolist()\n        # setting the fidelity to the maximum fidelity\n        self.acquisition.optimize_on_max_fidelity = True\n        self.acquisition.incumbent = _inc_copy\n\n    if active_max_fidelity is not None:\n        # w/o setting this flag, the AF eval will set all fidelities to max\n        self.acquisition.optimize_on_max_fidelity = False\n        fidelity = active_max_fidelity\n        samples = list(map(update_fidelity, samples, [fidelity] * len(samples)))\n\n    # computes the EI for all `samples`\n    eis = self.acquisition.eval(x=samples, asscalar=True)\n    # extracting the highest scored sample\n    return samples[np.argmax(eis)]\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.RandomUniformPolicy","title":"RandomUniformPolicy","text":"<pre><code>RandomUniformPolicy(pipeline_space: SearchSpace)\n</code></pre> <p>               Bases: <code>SamplingPolicy</code></p> <p>A random policy for sampling configuration, i.e. the default for SH / hyperband.</p> PARAMETER DESCRIPTION <code>SamplingPolicy</code> <p>[description]</p> <p> TYPE: <code>[type]</code> </p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(self, pipeline_space: SearchSpace):\n    super().__init__(pipeline_space=pipeline_space)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/sampling_policy/#neps.optimizers.multi_fidelity.sampling_policy.SamplingPolicy","title":"SamplingPolicy","text":"<pre><code>SamplingPolicy(\n    pipeline_space: SearchSpace, patience: int = 100\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for implementing a sampling strategy for SH and its subclasses.</p> Source code in <code>neps/optimizers/multi_fidelity/sampling_policy.py</code> <pre><code>def __init__(self, pipeline_space: SearchSpace, patience: int = 100):\n    self.pipeline_space = pipeline_space\n    self.patience = patience\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/","title":"Successive halving","text":""},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving","title":"neps.optimizers.multi_fidelity.successive_halving","text":""},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving","title":"AsynchronousSuccessiveHalving","text":"<pre><code>AsynchronousSuccessiveHalving(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | None\n    ) = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalvingBase</code></p> <p>Implements ASHA with a sampling and asynchronous promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,  # key difference from SH\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] | None = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        use_priors=use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.ask","title":"ask","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>This is basically the fit method.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>@override\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"This is basically the fit method.\"\"\"\n    completed: dict[str, ConfigResult] = {\n        trial_id: trial.into_config_result(self.pipeline_space.from_dict)\n        for trial_id, trial in trials.items()\n        if trial.report is not None\n    }\n    pending: dict[str, SearchSpace] = {\n        trial_id: self.pipeline_space.from_dict(trial.config)\n        for trial_id, trial in trials.items()\n        if trial.report is None\n    }\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(completed)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    config, _id, previous_id = self.get_config_and_ids()\n    return SampledConfig(id=_id, config=config, previous_config_id=previous_id)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    fidelity_name = self.pipeline_space.fidelity_name\n    assert fidelity_name is not None\n\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n\n        config_values = config._values\n        config_values[fidelity_name] = self.rung_map[rung]\n\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        rng = random.Random(None)  # TODO: Seeding\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                logger.info(\"Next config will be evaluated at target fidelity.\")\n            logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        elif rng.random() &lt; self.random_interleave_prob:\n            config = sample_one_old(\n                self.pipeline_space,\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config_values = config._values\n        config_values[fidelity_name] = fidelity_value\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config_values, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalving.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors","title":"AsynchronousSuccessiveHalvingWithPriors","text":"<pre><code>AsynchronousSuccessiveHalvingWithPriors(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>AsynchronousSuccessiveHalving</code></p> <p>Implements ASHA with a sampling and asynchronous promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,  # key difference from SH\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.ask","title":"ask","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>This is basically the fit method.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>@override\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"This is basically the fit method.\"\"\"\n    completed: dict[str, ConfigResult] = {\n        trial_id: trial.into_config_result(self.pipeline_space.from_dict)\n        for trial_id, trial in trials.items()\n        if trial.report is not None\n    }\n    pending: dict[str, SearchSpace] = {\n        trial_id: self.pipeline_space.from_dict(trial.config)\n        for trial_id, trial in trials.items()\n        if trial.report is None\n    }\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(completed)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    config, _id, previous_id = self.get_config_and_ids()\n    return SampledConfig(id=_id, config=config, previous_config_id=previous_id)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    fidelity_name = self.pipeline_space.fidelity_name\n    assert fidelity_name is not None\n\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n\n        config_values = config._values\n        config_values[fidelity_name] = self.rung_map[rung]\n\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        rng = random.Random(None)  # TODO: Seeding\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                logger.info(\"Next config will be evaluated at target fidelity.\")\n            logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        elif rng.random() &lt; self.random_interleave_prob:\n            config = sample_one_old(\n                self.pipeline_space,\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config_values = config._values\n        config_values[fidelity_name] = fidelity_value\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config_values, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.AsynchronousSuccessiveHalvingWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving","title":"SuccessiveHalving","text":"<pre><code>SuccessiveHalving(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int | None = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | None\n    ) = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalvingBase</code></p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>eta</code> <p>The reduction factor used by SH</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>early_stopping_rate</code> <p>Determines the number of rungs in an SH bracket Choosing 0 creates maximal rungs given the fidelity bounds</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>initial_design_type</code> <p>Type of initial design to switch to BO Legacy parameter from NePS BO design. Could be used to extend to MF-BO.</p> <p> TYPE: <code>Literal['max_budget', 'unique_configs']</code> DEFAULT: <code>'max_budget'</code> </p> <code>use_priors</code> <p>Allows random samples to be generated from a default Samples generated from a Gaussian centered around the default value</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sampling_policy</code> <p>The type of sampling procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>RandomUniformPolicy</code> </p> <code>promotion_policy</code> <p>The type of promotion procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>SyncPromotionPolicy</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>prior_confidence</code> <p>The range of confidence to have on the prior The higher the confidence, the smaller is the standard deviation of the prior distribution centered around the default</p> <p> TYPE: <code>Literal['low', 'medium', 'high'] | None</code> DEFAULT: <code>None</code> </p> <code>random_interleave_prob</code> <p>Chooses the fraction of samples from random vs prior</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sample_default_at_target</code> <p>Whether to evaluate the default configuration at the target fidelity or max budget</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int | None = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] | None = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    \"\"\"Initialise an SH bracket.\n\n    Args:\n        pipeline_space: Space in which to search\n        budget: Maximum budget\n        eta: The reduction factor used by SH\n        early_stopping_rate: Determines the number of rungs in an SH bracket\n            Choosing 0 creates maximal rungs given the fidelity bounds\n        initial_design_type: Type of initial design to switch to BO\n            Legacy parameter from NePS BO design. Could be used to extend to MF-BO.\n        use_priors: Allows random samples to be generated from a default\n            Samples generated from a Gaussian centered around the default value\n        sampling_policy: The type of sampling procedure to use\n        promotion_policy: The type of promotion procedure to use\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        prior_confidence: The range of confidence to have on the prior\n            The higher the confidence, the smaller is the standard deviation of the\n            prior distribution centered around the default\n        random_interleave_prob: Chooses the fraction of samples from random vs prior\n        sample_default_first: Whether to sample the default configuration first\n        sample_default_at_target: Whether to evaluate the default configuration at\n            the target fidelity or max budget\n    \"\"\"\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n    )\n    if random_interleave_prob &lt; 0 or random_interleave_prob &gt; 1:\n        raise ValueError(\"random_interleave_prob should be in [0.0, 1.0]\")\n    self.random_interleave_prob = random_interleave_prob\n    self.sample_default_first = sample_default_first\n    self.sample_default_at_target = sample_default_at_target\n\n    assert self.pipeline_space.fidelity is not None\n    self.min_budget = self.pipeline_space.fidelity.lower\n    self.max_budget = self.pipeline_space.fidelity.upper\n    self.eta = eta\n    # SH implicitly sets early_stopping_rate to 0\n    # the parameter is exposed to allow HB to call SH with different stopping rates\n    self.early_stopping_rate = early_stopping_rate\n    self.sampling_policy = sampling_policy(pipeline_space=self.pipeline_space)\n    self.promotion_policy = promotion_policy(self.eta)\n\n    # `max_budget_init` checks for the number of configurations that have been\n    # evaluated at the target budget\n    self.initial_design_type = initial_design_type\n    self.use_priors = use_priors\n\n    # check to ensure no rung ID is negative\n    # equivalent to s_max in https://arxiv.org/pdf/1603.06560.pdf\n    self.stopping_rate_limit = np.floor(\n        np.log(self.max_budget / self.min_budget) / np.log(self.eta)\n    ).astype(int)\n    assert self.early_stopping_rate &lt;= self.stopping_rate_limit\n\n    # maps rungs to a fidelity value for an SH bracket with `early_stopping_rate`\n    self.rung_map = self._get_rung_map(self.early_stopping_rate)\n    self.config_map = self._get_config_map(self.early_stopping_rate)\n\n    self.min_rung = min(list(self.rung_map.keys()))\n    self.max_rung = max(list(self.rung_map.keys()))\n\n    # placeholder args for varying promotion and sampling policies\n    self.promotion_policy_kwargs: dict = {}\n    self.promotion_policy_kwargs.update({\"config_map\": self.config_map})\n    self.sampling_args: dict = {}\n\n    self.fidelities = list(self.rung_map.values())\n    # stores the observations made and the corresponding fidelity explored\n    # crucial data structure used for determining promotion candidates\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n    # stores which configs occupy each rung at any time\n    self.rung_members: dict = {}  # stores config IDs per rung\n    self.rung_members_performance: dict = {}  # performances recorded per rung\n    self.rung_promotions: dict = {}  # records a promotable config per rung\n\n    # setup SH state counter\n    self.full_rung_trace = SuccessiveHalving._get_rung_trace(\n        self.rung_map, self.config_map\n    )\n\n    #############################\n    # Setting prior confidences #\n    #############################\n    # the std. dev or peakiness of distribution\n    self.prior_confidence = prior_confidence\n    self._enhance_priors()\n    self.rung_histories: dict[\n        int, dict[Literal[\"config\", \"perf\"], list[int | float]]\n    ] = {}\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.ask","title":"ask","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>This is basically the fit method.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>@override\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"This is basically the fit method.\"\"\"\n    completed: dict[str, ConfigResult] = {\n        trial_id: trial.into_config_result(self.pipeline_space.from_dict)\n        for trial_id, trial in trials.items()\n        if trial.report is not None\n    }\n    pending: dict[str, SearchSpace] = {\n        trial_id: self.pipeline_space.from_dict(trial.config)\n        for trial_id, trial in trials.items()\n        if trial.report is None\n    }\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(completed)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    config, _id, previous_id = self.get_config_and_ids()\n    return SampledConfig(id=_id, config=config, previous_config_id=previous_id)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. The key to simulating reset of rungs like in vanilla SH is by subsetting only the relevant part of the observation history that corresponds to one SH bracket. Under a parallel run, multiple SH brackets can be spawned. The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket is spawned. There are no waiting or blocking calls.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    The key to simulating reset of rungs like in vanilla SH is by subsetting only the\n    relevant part of the observation history that corresponds to one SH bracket.\n    Under a parallel run, multiple SH brackets can be spawned. The oldest, active,\n    incomplete SH bracket is searched for to choose the next evaluation. If either\n    all brackets are over or waiting, a new SH bracket is spawned.\n    There are no waiting or blocking calls.\n    \"\"\"\n    # indexes to mark separate brackets\n    start = 0\n    end = self.config_map[self.min_rung]  # length of lowest rung in a bracket\n    if self.sample_default_at_target and self.sample_default_first:\n        start += 1\n        end += 1\n    # iterates over the different SH brackets which span start-end by index\n    while end &lt;= len(self.observed_configs):\n        # for the SH bracket in start-end, calculate total SH budget used\n\n        # TODO(eddiebergman): Not idea what the type is of the stuff in the deepcopy\n        # but should work on removing the deepcopy\n        bracket_budget_used = self._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than a SH bracket budget then still an active bracket\n        if bracket_budget_used &lt; sum(self.full_rung_trace):\n            # subsetting only this SH bracket from the history\n            self._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, and signals to starts a new SH bracket\n            self._handle_promotions()\n            promotion_count = 0\n            for _, promotions in self.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found\n                return\n        # else move to next SH bracket recorded by an offset (= lowest rung length)\n        start = end\n        end = start + self.config_map[self.min_rung]\n\n    # updates rung info with the latest active, incomplete bracket\n    self._get_rungs_state(self.observed_configs.iloc[start:end])\n    # _handle_promotion() need not be called as it is called by load_results()\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    fidelity_name = self.pipeline_space.fidelity_name\n    assert fidelity_name is not None\n\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n\n        config_values = config._values\n        config_values[fidelity_name] = self.rung_map[rung]\n\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        rng = random.Random(None)  # TODO: Seeding\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                logger.info(\"Next config will be evaluated at target fidelity.\")\n            logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        elif rng.random() &lt; self.random_interleave_prob:\n            config = sample_one_old(\n                self.pipeline_space,\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config_values = config._values\n        config_values[fidelity_name] = fidelity_value\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config_values, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalving.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase","title":"SuccessiveHalvingBase","text":"<pre><code>SuccessiveHalvingBase(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int | None = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: (\n        Literal[\"low\", \"medium\", \"high\"] | None\n    ) = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Implements a SuccessiveHalving procedure with a sampling and promotion policy.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>Space in which to search</p> <p> TYPE: <code>SearchSpace</code> </p> <code>budget</code> <p>Maximum budget</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>eta</code> <p>The reduction factor used by SH</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>early_stopping_rate</code> <p>Determines the number of rungs in an SH bracket Choosing 0 creates maximal rungs given the fidelity bounds</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>initial_design_type</code> <p>Type of initial design to switch to BO Legacy parameter from NePS BO design. Could be used to extend to MF-BO.</p> <p> TYPE: <code>Literal['max_budget', 'unique_configs']</code> DEFAULT: <code>'max_budget'</code> </p> <code>use_priors</code> <p>Allows random samples to be generated from a default Samples generated from a Gaussian centered around the default value</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sampling_policy</code> <p>The type of sampling procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>RandomUniformPolicy</code> </p> <code>promotion_policy</code> <p>The type of promotion procedure to use</p> <p> TYPE: <code>Any</code> DEFAULT: <code>SyncPromotionPolicy</code> </p> <code>loss_value_on_error</code> <p>Setting this and cost_value_on_error to any float will supress any error during bayesian optimization and will use given loss value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>cost_value_on_error</code> <p>Setting this and loss_value_on_error to any float will supress any error during bayesian optimization and will use given cost value instead. default: None</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> <code>prior_confidence</code> <p>The range of confidence to have on the prior The higher the confidence, the smaller is the standard deviation of the prior distribution centered around the default</p> <p> TYPE: <code>Literal['low', 'medium', 'high'] | None</code> DEFAULT: <code>None</code> </p> <code>random_interleave_prob</code> <p>Chooses the fraction of samples from random vs prior</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sample_default_first</code> <p>Whether to sample the default configuration first</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sample_default_at_target</code> <p>Whether to evaluate the default configuration at the target fidelity or max budget</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int | None = None,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    use_priors: bool = False,\n    sampling_policy: Any = RandomUniformPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] | None = None,\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    \"\"\"Initialise an SH bracket.\n\n    Args:\n        pipeline_space: Space in which to search\n        budget: Maximum budget\n        eta: The reduction factor used by SH\n        early_stopping_rate: Determines the number of rungs in an SH bracket\n            Choosing 0 creates maximal rungs given the fidelity bounds\n        initial_design_type: Type of initial design to switch to BO\n            Legacy parameter from NePS BO design. Could be used to extend to MF-BO.\n        use_priors: Allows random samples to be generated from a default\n            Samples generated from a Gaussian centered around the default value\n        sampling_policy: The type of sampling procedure to use\n        promotion_policy: The type of promotion procedure to use\n        loss_value_on_error: Setting this and cost_value_on_error to any float will\n            supress any error during bayesian optimization and will use given loss\n            value instead. default: None\n        cost_value_on_error: Setting this and loss_value_on_error to any float will\n            supress any error during bayesian optimization and will use given cost\n            value instead. default: None\n        prior_confidence: The range of confidence to have on the prior\n            The higher the confidence, the smaller is the standard deviation of the\n            prior distribution centered around the default\n        random_interleave_prob: Chooses the fraction of samples from random vs prior\n        sample_default_first: Whether to sample the default configuration first\n        sample_default_at_target: Whether to evaluate the default configuration at\n            the target fidelity or max budget\n    \"\"\"\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n    )\n    if random_interleave_prob &lt; 0 or random_interleave_prob &gt; 1:\n        raise ValueError(\"random_interleave_prob should be in [0.0, 1.0]\")\n    self.random_interleave_prob = random_interleave_prob\n    self.sample_default_first = sample_default_first\n    self.sample_default_at_target = sample_default_at_target\n\n    assert self.pipeline_space.fidelity is not None\n    self.min_budget = self.pipeline_space.fidelity.lower\n    self.max_budget = self.pipeline_space.fidelity.upper\n    self.eta = eta\n    # SH implicitly sets early_stopping_rate to 0\n    # the parameter is exposed to allow HB to call SH with different stopping rates\n    self.early_stopping_rate = early_stopping_rate\n    self.sampling_policy = sampling_policy(pipeline_space=self.pipeline_space)\n    self.promotion_policy = promotion_policy(self.eta)\n\n    # `max_budget_init` checks for the number of configurations that have been\n    # evaluated at the target budget\n    self.initial_design_type = initial_design_type\n    self.use_priors = use_priors\n\n    # check to ensure no rung ID is negative\n    # equivalent to s_max in https://arxiv.org/pdf/1603.06560.pdf\n    self.stopping_rate_limit = np.floor(\n        np.log(self.max_budget / self.min_budget) / np.log(self.eta)\n    ).astype(int)\n    assert self.early_stopping_rate &lt;= self.stopping_rate_limit\n\n    # maps rungs to a fidelity value for an SH bracket with `early_stopping_rate`\n    self.rung_map = self._get_rung_map(self.early_stopping_rate)\n    self.config_map = self._get_config_map(self.early_stopping_rate)\n\n    self.min_rung = min(list(self.rung_map.keys()))\n    self.max_rung = max(list(self.rung_map.keys()))\n\n    # placeholder args for varying promotion and sampling policies\n    self.promotion_policy_kwargs: dict = {}\n    self.promotion_policy_kwargs.update({\"config_map\": self.config_map})\n    self.sampling_args: dict = {}\n\n    self.fidelities = list(self.rung_map.values())\n    # stores the observations made and the corresponding fidelity explored\n    # crucial data structure used for determining promotion candidates\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n    # stores which configs occupy each rung at any time\n    self.rung_members: dict = {}  # stores config IDs per rung\n    self.rung_members_performance: dict = {}  # performances recorded per rung\n    self.rung_promotions: dict = {}  # records a promotable config per rung\n\n    # setup SH state counter\n    self.full_rung_trace = SuccessiveHalving._get_rung_trace(\n        self.rung_map, self.config_map\n    )\n\n    #############################\n    # Setting prior confidences #\n    #############################\n    # the std. dev or peakiness of distribution\n    self.prior_confidence = prior_confidence\n    self._enhance_priors()\n    self.rung_histories: dict[\n        int, dict[Literal[\"config\", \"perf\"], list[int | float]]\n    ] = {}\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.ask","title":"ask","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>This is basically the fit method.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>@override\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"This is basically the fit method.\"\"\"\n    completed: dict[str, ConfigResult] = {\n        trial_id: trial.into_config_result(self.pipeline_space.from_dict)\n        for trial_id, trial in trials.items()\n        if trial.report is not None\n    }\n    pending: dict[str, SearchSpace] = {\n        trial_id: self.pipeline_space.from_dict(trial.config)\n        for trial_id, trial in trials.items()\n        if trial.report is None\n    }\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(completed)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    config, _id, previous_id = self.get_config_and_ids()\n    return SampledConfig(id=_id, config=config, previous_config_id=previous_id)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    fidelity_name = self.pipeline_space.fidelity_name\n    assert fidelity_name is not None\n\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n\n        config_values = config._values\n        config_values[fidelity_name] = self.rung_map[rung]\n\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        rng = random.Random(None)  # TODO: Seeding\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                logger.info(\"Next config will be evaluated at target fidelity.\")\n            logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        elif rng.random() &lt; self.random_interleave_prob:\n            config = sample_one_old(\n                self.pipeline_space,\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config_values = config._values\n        config_values[fidelity_name] = fidelity_value\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config_values, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingBase.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors","title":"SuccessiveHalvingWithPriors","text":"<pre><code>SuccessiveHalvingWithPriors(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalving</code></p> <p>Implements a SuccessiveHalving procedure with a sampling and promotion policy.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = FixedPriorPolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",  # medium = 0.25\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = False,\n    sample_default_at_target: bool = False,\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        use_priors=self.use_priors,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.ask","title":"ask","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>This is basically the fit method.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>@override\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"This is basically the fit method.\"\"\"\n    completed: dict[str, ConfigResult] = {\n        trial_id: trial.into_config_result(self.pipeline_space.from_dict)\n        for trial_id, trial in trials.items()\n        if trial.report is not None\n    }\n    pending: dict[str, SearchSpace] = {\n        trial_id: self.pipeline_space.from_dict(trial.config)\n        for trial_id, trial in trials.items()\n        if trial.report is None\n    }\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(completed)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    config, _id, previous_id = self.get_config_and_ids()\n    return SampledConfig(id=_id, config=config, previous_config_id=previous_id)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. The key to simulating reset of rungs like in vanilla SH is by subsetting only the relevant part of the observation history that corresponds to one SH bracket. Under a parallel run, multiple SH brackets can be spawned. The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket is spawned. There are no waiting or blocking calls.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    The key to simulating reset of rungs like in vanilla SH is by subsetting only the\n    relevant part of the observation history that corresponds to one SH bracket.\n    Under a parallel run, multiple SH brackets can be spawned. The oldest, active,\n    incomplete SH bracket is searched for to choose the next evaluation. If either\n    all brackets are over or waiting, a new SH bracket is spawned.\n    There are no waiting or blocking calls.\n    \"\"\"\n    # indexes to mark separate brackets\n    start = 0\n    end = self.config_map[self.min_rung]  # length of lowest rung in a bracket\n    if self.sample_default_at_target and self.sample_default_first:\n        start += 1\n        end += 1\n    # iterates over the different SH brackets which span start-end by index\n    while end &lt;= len(self.observed_configs):\n        # for the SH bracket in start-end, calculate total SH budget used\n\n        # TODO(eddiebergman): Not idea what the type is of the stuff in the deepcopy\n        # but should work on removing the deepcopy\n        bracket_budget_used = self._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than a SH bracket budget then still an active bracket\n        if bracket_budget_used &lt; sum(self.full_rung_trace):\n            # subsetting only this SH bracket from the history\n            self._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, and signals to starts a new SH bracket\n            self._handle_promotions()\n            promotion_count = 0\n            for _, promotions in self.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found\n                return\n        # else move to next SH bracket recorded by an offset (= lowest rung length)\n        start = end\n        end = start + self.config_map[self.min_rung]\n\n    # updates rung info with the latest active, incomplete bracket\n    self._get_rungs_state(self.observed_configs.iloc[start:end])\n    # _handle_promotion() need not be called as it is called by load_results()\n    return\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    fidelity_name = self.pipeline_space.fidelity_name\n    assert fidelity_name is not None\n\n    rung_to_promote = self.is_promotable()\n    if rung_to_promote is not None:\n        # promotes the first recorded promotable config in the argsort-ed rung\n        row = self.observed_configs.iloc[self.rung_promotions[rung_to_promote][0]]\n        config = row[\"config\"].clone()\n        rung = rung_to_promote + 1\n        # assigning the fidelity to evaluate the config at\n\n        config_values = config._values\n        config_values[fidelity_name] = self.rung_map[rung]\n\n        # updating config IDs\n        previous_config_id = f\"{row.name}_{rung_to_promote}\"\n        config_id = f\"{row.name}_{rung}\"\n    else:\n        rung_id = self.min_rung\n        # using random instead of np.random to be consistent with NePS BO\n        rng = random.Random(None)  # TODO: Seeding\n        if (\n            self.use_priors\n            and self.sample_default_first\n            and len(self.observed_configs) == 0\n        ):\n            if self.sample_default_at_target:\n                # sets the default config to be evaluated at the target fidelity\n                rung_id = self.max_rung\n                logger.info(\"Next config will be evaluated at target fidelity.\")\n            logger.info(\"Sampling the default configuration...\")\n            config = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        elif rng.random() &lt; self.random_interleave_prob:\n            config = sample_one_old(\n                self.pipeline_space,\n                patience=self.patience,\n                user_priors=False,  # sample uniformly random\n                ignore_fidelity=True,\n            )\n        else:\n            config = self.sample_new_config(rung=rung_id)\n\n        fidelity_value = self.rung_map[rung_id]\n        config_values = config._values\n        config_values[fidelity_name] = fidelity_value\n\n        previous_config_id = None\n        config_id = f\"{self._generate_new_config_id()}_{rung_id}\"\n\n    return config_values, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/successive_halving/#neps.optimizers.multi_fidelity.successive_halving.SuccessiveHalvingWithPriors.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/","title":"Utils","text":""},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils","title":"neps.optimizers.multi_fidelity.utils","text":""},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData","title":"MFObservedData","text":"<pre><code>MFObservedData(\n    columns: list[str] | None = None,\n    index_names: list[str] | None = None,\n)\n</code></pre> <p>(Under development).</p> <p>This module is used to unify the data access across different Multi-Fidelity optimizers. It stores column names and index names. Possible optimizations and extensions of the observed data should be handled by this class.</p> <p>So far this is just a draft class containing the DataFrame and some properties.</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def __init__(\n    self,\n    columns: list[str] | None = None,\n    index_names: list[str] | None = None,\n):\n    if columns is None:\n        columns = [self.default_config_col, self.default_perf_col]\n    if index_names is None:\n        index_names = [self.default_config_idx, self.default_budget_idx]\n\n    self.config_col = columns[0]\n    self.perf_col = columns[1]\n\n    if len(columns) &gt; 2:\n        self.lc_col_name = columns[2]\n    else:\n        self.lc_col_name = self.default_lc_col\n\n    if len(index_names) == 1:\n        index_names += [\"budget_id\"]\n\n    self.config_idx = index_names[0]\n    self.budget_idx = index_names[1]\n    self.index_names = index_names\n\n    index = pd.MultiIndex.from_tuples([], names=index_names)\n\n    self.df = pd.DataFrame([], columns=columns, index=index)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.add_data","title":"add_data","text":"<pre><code>add_data(\n    data: list[Any] | list[list[Any]],\n    index: (\n        tuple[int, ...]\n        | Sequence[tuple[int, ...]]\n        | Sequence[int]\n        | int\n    ),\n    *,\n    error: bool = False\n) -&gt; None\n</code></pre> <p>Add data only if none of the indices are already existing in the DataFrame.</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def add_data(\n    self,\n    data: list[Any] | list[list[Any]],\n    index: tuple[int, ...] | Sequence[tuple[int, ...]] | Sequence[int] | int,\n    *,\n    error: bool = False,\n) -&gt; None:\n    \"\"\"Add data only if none of the indices are already existing in the DataFrame.\"\"\"\n    # TODO: If index is only config_id extend it\n    if not isinstance(index, list):\n        index_list = [index]\n        data_list = [data]\n    else:\n        index_list = index\n        data_list = data\n\n    if not self.df.index.isin(index_list).any():\n        index = pd.MultiIndex.from_tuples(index_list, names=self.index_names)\n        _df = pd.DataFrame(data_list, columns=self.df.columns, index=index)\n        self.df = _df.copy() if self.df.empty else pd.concat((self.df, _df))\n    elif error:\n        raise ValueError(\n            f\"Data with at least one of the given indices already \"\n            f\"exists: {self.df[self.df.index.isin(index_list)]}\\n\"\n            f\"Given indices: {index_list}\"\n        )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.get_best_learning_curve_id","title":"get_best_learning_curve_id","text":"<pre><code>get_best_learning_curve_id(\n    *, maximize: bool = False\n) -&gt; int\n</code></pre> <p>Returns a single configuration id of the best observed performance.</p> this will always return the single best lowest ID <p>if two configurations has the same performance</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def get_best_learning_curve_id(self, *, maximize: bool = False) -&gt; int:\n    \"\"\"Returns a single configuration id of the best observed performance.\n\n    Note: this will always return the single best lowest ID\n          if two configurations has the same performance\n    \"\"\"\n    learning_curves = self.get_learning_curves()\n    if maximize:\n        return learning_curves.max(axis=1).idxmax()\n    return learning_curves.min(axis=1).idxmin()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.get_best_performance_per_config","title":"get_best_performance_per_config","text":"<pre><code>get_best_performance_per_config(\n    *, maximize: bool = False\n) -&gt; Series\n</code></pre> <p>Returns the best score recorded per config across fidelities seen.</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def get_best_performance_per_config(self, *, maximize: bool = False) -&gt; pd.Series:\n    \"\"\"Returns the best score recorded per config across fidelities seen.\"\"\"\n    op = np.max if maximize else np.min\n    return (\n        self.df.sort_values(\n            \"budget_id\", ascending=False\n        )  # sorts with largest budget first\n        .groupby(\"config_id\")  # retains only config_id\n        .first()  # retrieves the largest budget seen for each config_id\n        .learning_curves.apply(  # extracts all values seen till largest budget\n            op\n        )  # finds the minimum over per-config learning curve\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.get_max_observed_fidelity_level_per_config","title":"get_max_observed_fidelity_level_per_config","text":"<pre><code>get_max_observed_fidelity_level_per_config() -&gt; Series\n</code></pre> <p>Returns the highest fidelity level recorded per config seen.</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def get_max_observed_fidelity_level_per_config(self) -&gt; pd.Series:\n    \"\"\"Returns the highest fidelity level recorded per config seen.\"\"\"\n    max_z_observed = {\n        _id: self.df.loc[_id, :].index.sort_values()[-1]\n        for _id in self.df.index.get_level_values(\"config_id\").sort_values()\n    }\n    return pd.Series(max_z_observed)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity/utils/#neps.optimizers.multi_fidelity.utils.MFObservedData.update_data","title":"update_data","text":"<pre><code>update_data(\n    data_dict: dict[str, list[Any]],\n    index: (\n        tuple[int, ...]\n        | Sequence[tuple[int, ...]]\n        | Sequence[int]\n        | int\n    ),\n    *,\n    error: bool = False\n) -&gt; None\n</code></pre> <p>Update data if all the indices already exist in the DataFrame.</p> Source code in <code>neps/optimizers/multi_fidelity/utils.py</code> <pre><code>def update_data(\n    self,\n    data_dict: dict[str, list[Any]],\n    index: tuple[int, ...] | Sequence[tuple[int, ...]] | Sequence[int] | int,\n    *,\n    error: bool = False,\n) -&gt; None:\n    \"\"\"Update data if all the indices already exist in the DataFrame.\"\"\"\n    index_list = [index] if not isinstance(index, list) else index\n    if self.df.index.isin(index_list).sum() == len(index_list):\n        column_names, data = zip(*data_dict.items(), strict=False)\n        data = list(zip(*data, strict=False))\n        self.df.loc[index_list, list(column_names)] = data\n\n    elif error:\n        raise ValueError(\n            f\"Data with at least one of the given indices doesn't \"\n            f\"exist.\\n Existing indices: {self.df.index}\\n\"\n            f\"Given indices: {index_list}\"\n        )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/","title":"Async priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband","title":"neps.optimizers.multi_fidelity_prior.async_priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha","title":"PriorBandAsha","text":"<pre><code>PriorBandAsha(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: Literal[\n        \"geometric\", \"linear\", \"50-50\"\n    ] = \"geometric\",\n    inc_sample_type: Literal[\n        \"crossover\", \"gaussian\", \"hypersphere\", \"mutation\"\n    ] = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: Literal[\n        \"dynamic\", \"constant\", \"decay\"\n    ] = \"dynamic\",\n    model_based: bool = False,\n    modelling_type: Literal[\"joint\", \"rung\"] = \"joint\",\n    initial_design_size: int | None = None,\n    model_policy: Any = ModelPolicy,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str | None = None,\n    hp_kernels: list | None = None,\n    surrogate_model_args: dict | None = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str = \"random\"\n)\n</code></pre> <p>               Bases: <code>MFBOBase</code>, <code>PriorBandBase</code>, <code>AsynchronousSuccessiveHalvingWithPriors</code></p> <p>Implements a PriorBand on top of ASHA.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    early_stopping_rate: int = 0,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,  # key difference to ASHA\n    promotion_policy: Any = AsyncPromotionPolicy,  # key difference from SH\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: Literal[\"geometric\", \"linear\", \"50-50\"] = \"geometric\",\n    inc_sample_type: Literal[\n        \"crossover\", \"gaussian\", \"hypersphere\", \"mutation\"\n    ] = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: Literal[\"dynamic\", \"constant\", \"decay\"] = \"dynamic\",\n    # arguments for model\n    model_based: bool = False,  # crucial argument to set to allow model-search\n    modelling_type: Literal[\"joint\", \"rung\"] = \"joint\",\n    initial_design_size: int | None = None,\n    model_policy: Any = ModelPolicy,\n    # TODO: Remove these when fixing model policy\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str | None = None,\n    hp_kernels: list | None = None,\n    surrogate_model_args: dict | None = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str = \"random\",\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        early_stopping_rate=early_stopping_rate,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    self.prior_weight_type = prior_weight_type\n    self.inc_sample_type = inc_sample_type\n    self.inc_mutation_rate = inc_mutation_rate\n    self.inc_mutation_std = inc_mutation_std\n    self.sampling_policy = sampling_policy(\n        pipeline_space=pipeline_space, inc_type=self.inc_sample_type\n    )\n    # determines the kind of trade-off between incumbent and prior weightage\n    self.inc_style = inc_style  # used by PriorBandBase\n    self.sampling_args = {\n        \"inc\": None,\n        \"weights\": {\n            \"prior\": 1,  # begin with only prior sampling\n            \"inc\": 0,\n            \"random\": 0,\n        },\n    }\n\n    self.model_based = model_based\n    self.modelling_type = modelling_type\n    self.initial_design_size = initial_design_size\n    # counting non-fidelity dimensions in search space\n    ndims = sum(\n        1\n        for _, hp in self.pipeline_space.hyperparameters.items()\n        if not hp.is_fidelity\n    )\n    n_min = ndims + 1\n    self.init_size = n_min + 1  # in BOHB: init_design &gt;= N_dim + 2\n    if self.modelling_type == \"joint\" and self.initial_design_size is not None:\n        self.init_size = self.initial_design_size\n\n    prior_dist = Prior.from_space(self.pipeline_space)\n    self.model_policy = model_policy(pipeline_space=pipeline_space, prior=prior_dist)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.ask","title":"ask","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>This is basically the fit method.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>@override\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"This is basically the fit method.\"\"\"\n    completed: dict[str, ConfigResult] = {\n        trial_id: trial.into_config_result(self.pipeline_space.from_dict)\n        for trial_id, trial in trials.items()\n        if trial.report is not None\n    }\n    pending: dict[str, SearchSpace] = {\n        trial_id: self.pipeline_space.from_dict(trial.config)\n        for trial_id, trial in trials.items()\n        if trial.report is None\n    }\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(completed)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    config, _id, previous_id = self.get_config_and_ids()\n    return SampledConfig(id=_id, config=config, previous_config_id=previous_id)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung: int) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung: int) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    return {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; float\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent: SearchSpace) -&gt; float:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    return min(distances)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; list[float]\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent: SearchSpace) -&gt; list[float]:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    return [d for d in distances if d &gt; 0]\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int | None = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int | None = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            logger.warning(f\"{rung} not in {np.unique(idxs)}\")  # type: ignore\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n        assert isinstance(inc, SearchSpace)\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        logger.warning(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def get_config_and_ids(\n    self,\n) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    rung_to_promote = self.is_promotable()\n    rung = rung_to_promote + 1 if rung_to_promote is not None else self.min_rung\n    self._set_sampling_weights_and_inc(rung=rung)\n    # performs standard ASHA but sampling happens as per the EnsemblePolicy\n    return super().get_config_and_ids()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    sh_brackets = getattr(self, \"sh_brackets\", None)\n    if sh_brackets is not None and len(sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n\n    assert isinstance(bracket, SuccessiveHalvingBase)\n\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(bracket.rung_map.keys(), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    valid_perf_mask = self.observed_configs[\"perf\"].notna()\n    rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n    resources_used = sum(self.rung_map[r] for r in rungs)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        total_resources = sum(self.rung_map[r] for r in rungs)\n        resources = total_resources / self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; tuple[float, float]\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; tuple[float, float]:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    if self.inc_style == \"decay\":\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        resources = sum(self.rung_map[r] for r in rungs)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    if self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAsha.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(\n    rung: int | None = None, **kwargs: Any\n) -&gt; SearchSpace\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int | None = None,\n    **kwargs: Any,\n) -&gt; SearchSpace:\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            raise ValueError(\"Choice of modelling_type not in 'rung', 'joint'\")\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = sample_one_old(\n            self.pipeline_space,\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB","title":"PriorBandAshaHB","text":"<pre><code>PriorBandAshaHB(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = AsyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: Literal[\n        \"geometric\", \"linear\", \"50-50\"\n    ] = \"geometric\",\n    inc_sample_type: Literal[\n        \"crossover\", \"gaussian\", \"hypersphere\", \"mutation\"\n    ] = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: Literal[\n        \"dynamic\", \"constant\", \"decay\"\n    ] = \"dynamic\",\n    model_based: bool = False,\n    modelling_type: Literal[\"joint\", \"rung\"] = \"joint\",\n    initial_design_size: int | None = None,\n    model_policy: Any = ModelPolicy,\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str | None = None,\n    hp_kernels: list | None = None,\n    surrogate_model_args: dict | None = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str = \"random\"\n)\n</code></pre> <p>               Bases: <code>PriorBandAsha</code></p> <p>Implements a PriorBand on top of ASHA-HB (Mobster).</p> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,  # key difference to ASHA\n    promotion_policy: Any = AsyncPromotionPolicy,  # key difference from PB\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: Literal[\"geometric\", \"linear\", \"50-50\"] = \"geometric\",\n    inc_sample_type: Literal[\n        \"crossover\", \"gaussian\", \"hypersphere\", \"mutation\"\n    ] = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: Literal[\"dynamic\", \"constant\", \"decay\"] = \"dynamic\",\n    # arguments for model\n    model_based: bool = False,  # crucial argument to set to allow model-search\n    modelling_type: Literal[\"joint\", \"rung\"] = \"joint\",\n    initial_design_size: int | None = None,\n    model_policy: Any = ModelPolicy,\n    # TODO: Remove these when fixing model policy\n    surrogate_model: str | Any = \"gp\",\n    domain_se_kernel: str | None = None,\n    hp_kernels: list | None = None,\n    surrogate_model_args: dict | None = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str = \"random\",\n):\n    # collecting arguments required by ASHA\n    args: dict[str, Any] = {\n        \"pipeline_space\": pipeline_space,\n        \"budget\": budget,\n        \"eta\": eta,\n        \"early_stopping_rate\": self.early_stopping_rate,\n        \"initial_design_type\": initial_design_type,\n        \"sampling_policy\": sampling_policy,\n        \"promotion_policy\": promotion_policy,\n        \"loss_value_on_error\": loss_value_on_error,\n        \"cost_value_on_error\": cost_value_on_error,\n        \"ignore_errors\": ignore_errors,\n        \"prior_confidence\": prior_confidence,\n        \"random_interleave_prob\": random_interleave_prob,\n        \"sample_default_first\": sample_default_first,\n        \"sample_default_at_target\": sample_default_at_target,\n    }\n    super().__init__(\n        **args,\n        prior_weight_type=prior_weight_type,\n        inc_sample_type=inc_sample_type,\n        inc_mutation_rate=inc_mutation_rate,\n        inc_mutation_std=inc_mutation_std,\n        inc_style=inc_style,\n        model_based=model_based,\n        modelling_type=modelling_type,\n        initial_design_size=initial_design_size,\n        model_policy=model_policy,\n    )\n\n    # Creating the ASHA (SH) brackets that Hyperband iterates over\n    self.sh_brackets = {}\n    for s in range(self.max_rung + 1):\n        args.update({\"early_stopping_rate\": s})\n        # key difference from vanilla HB where it runs synchronous SH brackets\n        self.sh_brackets[s] = AsynchronousSuccessiveHalvingWithPriors(**args)\n        self.sh_brackets[s].sampling_policy = self.sampling_policy\n        self.sh_brackets[s].sampling_args = self.sampling_args\n        self.sh_brackets[s].model_policy = self.model_policy  # type: ignore\n        self.sh_brackets[s].sample_new_config = self.sample_new_config  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.ask","title":"ask","text":"<pre><code>ask(\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig\n</code></pre> <p>This is basically the fit method.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>@override\ndef ask(\n    self,\n    trials: Mapping[str, Trial],\n    budget_info: BudgetInfo | None,\n) -&gt; SampledConfig:\n    \"\"\"This is basically the fit method.\"\"\"\n    completed: dict[str, ConfigResult] = {\n        trial_id: trial.into_config_result(self.pipeline_space.from_dict)\n        for trial_id, trial in trials.items()\n        if trial.report is not None\n    }\n    pending: dict[str, SearchSpace] = {\n        trial_id: self.pipeline_space.from_dict(trial.config)\n        for trial_id, trial in trials.items()\n        if trial.report is None\n    }\n\n    self.rung_histories = {\n        rung: {\"config\": [], \"perf\": []}\n        for rung in range(self.min_rung, self.max_rung + 1)\n    }\n\n    self.observed_configs = pd.DataFrame([], columns=(\"config\", \"rung\", \"perf\"))\n\n    # previous optimization run exists and needs to be loaded\n    self._load_previous_observations(completed)\n\n    # account for pending evaluations\n    self._handle_pending_evaluations(pending)\n\n    # process optimization state and bucket observations per rung\n    self._get_rungs_state()\n\n    # filter/reset old SH brackets\n    self.clear_old_brackets()\n\n    # identifying promotion list per rung\n    self._handle_promotions()\n\n    # fit any model/surrogates\n    self._fit_models()\n\n    # important for the global HB to run the right SH\n    self._update_sh_bracket_state()\n\n    config, _id, previous_id = self.get_config_and_ids()\n    return SampledConfig(id=_id, config=config, previous_config_id=previous_id)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung: int) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung: int) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    return {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; float\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent: SearchSpace) -&gt; float:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    return min(distances)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; list[float]\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent: SearchSpace) -&gt; list[float]:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    return [d for d in distances if d &gt; 0]\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int | None = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int | None = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            logger.warning(f\"{rung} not in {np.unique(idxs)}\")  # type: ignore\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n        assert isinstance(inc, SearchSpace)\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        logger.warning(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/async_priorband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    # the rung to sample at\n    bracket_to_run = self._get_bracket_to_run()\n\n    self._set_sampling_weights_and_inc(rung=bracket_to_run)\n    self.sh_brackets[bracket_to_run].sampling_args = self.sampling_args\n    config, config_id, previous_config_id = self.sh_brackets[\n        bracket_to_run\n    ].get_config_and_ids()\n    return config, config_id, previous_config_id\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    sh_brackets = getattr(self, \"sh_brackets\", None)\n    if sh_brackets is not None and len(sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n\n    assert isinstance(bracket, SuccessiveHalvingBase)\n\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(bracket.rung_map.keys(), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    valid_perf_mask = self.observed_configs[\"perf\"].notna()\n    rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n    resources_used = sum(self.rung_map[r] for r in rungs)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        total_resources = sum(self.rung_map[r] for r in rungs)\n        resources = total_resources / self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; tuple[float, float]\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; tuple[float, float]:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    if self.inc_style == \"decay\":\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        resources = sum(self.rung_map[r] for r in rungs)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    if self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/async_priorband/#neps.optimizers.multi_fidelity_prior.async_priorband.PriorBandAshaHB.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(\n    rung: int | None = None, **kwargs: Any\n) -&gt; SearchSpace\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int | None = None,\n    **kwargs: Any,\n) -&gt; SearchSpace:\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            raise ValueError(\"Choice of modelling_type not in 'rung', 'joint'\")\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = sample_one_old(\n            self.pipeline_space,\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/","title":"Priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband","title":"neps.optimizers.multi_fidelity_prior.priorband","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand","title":"PriorBand","text":"<pre><code>PriorBand(\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\n        \"max_budget\", \"unique_configs\"\n    ] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: Literal[\n        \"geometric\", \"linear\", \"50-50\"\n    ] = \"geometric\",\n    inc_sample_type: Literal[\n        \"hypersphere\", \"mutation\", \"crossover\", \"gaussian\"\n    ] = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: Literal[\n        \"dynamic\", \"decay\", \"constant\"\n    ] = \"dynamic\",\n    model_based: bool = False,\n    modelling_type: Literal[\"joint\", \"rung\"] = \"joint\",\n    initial_design_size: int | None = None,\n    model_policy: Any = ModelPolicy,\n    surrogate_model: str | Any = \"gp\",\n    surrogate_model_args: dict | None = None,\n    acquisition: str | BaseAcquisition = \"EI\",\n    log_prior_weighted: bool = False,\n    acquisition_sampler: str = \"random\"\n)\n</code></pre> <p>               Bases: <code>MFBOBase</code>, <code>HyperbandCustomDefault</code>, <code>PriorBandBase</code></p> <p>PriorBand optimizer for multi-fidelity optimization.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    budget: int,\n    eta: int = 3,\n    initial_design_type: Literal[\"max_budget\", \"unique_configs\"] = \"max_budget\",\n    sampling_policy: Any = EnsemblePolicy,\n    promotion_policy: Any = SyncPromotionPolicy,\n    loss_value_on_error: None | float = None,\n    cost_value_on_error: None | float = None,\n    ignore_errors: bool = False,\n    prior_confidence: Literal[\"low\", \"medium\", \"high\"] = \"medium\",\n    random_interleave_prob: float = 0.0,\n    sample_default_first: bool = True,\n    sample_default_at_target: bool = True,\n    prior_weight_type: Literal[\"geometric\", \"linear\", \"50-50\"] = \"geometric\",\n    inc_sample_type: Literal[\n        \"hypersphere\", \"mutation\", \"crossover\", \"gaussian\"\n    ] = \"mutation\",\n    inc_mutation_rate: float = 0.5,\n    inc_mutation_std: float = 0.25,\n    inc_style: Literal[\"dynamic\", \"decay\", \"constant\"] = \"dynamic\",\n    # arguments for model\n    model_based: bool = False,  # crucial argument to set to allow model-search\n    modelling_type: Literal[\"joint\", \"rung\"] = \"joint\",\n    initial_design_size: int | None = None,\n    model_policy: Any = ModelPolicy,\n    # TODO: Remove these when fixing ModelPolicy\n    surrogate_model: str | Any = \"gp\",\n    surrogate_model_args: dict | None = None,  # TODO: Remove\n    acquisition: str | BaseAcquisition = \"EI\",  # TODO: Remove\n    log_prior_weighted: bool = False,  # TODO: Remove\n    acquisition_sampler: str = \"random\",  # TODO: Remove\n):\n    super().__init__(\n        pipeline_space=pipeline_space,\n        budget=budget,\n        eta=eta,\n        initial_design_type=initial_design_type,\n        sampling_policy=sampling_policy,\n        promotion_policy=promotion_policy,\n        loss_value_on_error=loss_value_on_error,\n        cost_value_on_error=cost_value_on_error,\n        ignore_errors=ignore_errors,\n        prior_confidence=prior_confidence,\n        random_interleave_prob=random_interleave_prob,\n        sample_default_first=sample_default_first,\n        sample_default_at_target=sample_default_at_target,\n    )\n    self.prior_weight_type = prior_weight_type\n    self.inc_sample_type = inc_sample_type\n    self.inc_mutation_rate = inc_mutation_rate\n    self.inc_mutation_std = inc_mutation_std\n    self.sampling_policy = sampling_policy(\n        pipeline_space=pipeline_space, inc_type=self.inc_sample_type\n    )\n    # determines the kind of trade-off between incumbent and prior weightage\n    self.inc_style = inc_style  # used by PriorBandBase\n    self.sampling_args: dict[str, Any] = {\n        \"inc\": None,\n        \"weights\": {\n            \"prior\": 1,  # begin with only prior sampling\n            \"inc\": 0,\n            \"random\": 0,\n        },\n    }\n\n    self.model_based = model_based\n    self.modelling_type = modelling_type\n    self.initial_design_size = initial_design_size\n    # counting non-fidelity dimensions in search space\n    ndims = sum(\n        1\n        for _, hp in self.pipeline_space.hyperparameters.items()\n        if not hp.is_fidelity\n    )\n    n_min = ndims + 1\n    self.init_size = n_min + 1  # in BOHB: init_design &gt;= N_min + 2\n    if self.modelling_type == \"joint\" and self.initial_design_size is not None:\n        self.init_size = self.initial_design_size\n\n    # TODO: We also create a prior later inside of `compute_scores()`,\n    # in which we should really just pass in the prior dist as it does not move\n    # around in the space.\n    prior_dist = Prior.from_space(self.pipeline_space)\n    self.model_policy = model_policy(pipeline_space=pipeline_space, prior=prior_dist)\n\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_policy = self.sampling_policy\n        sh.sampling_args = self.sampling_args\n        sh.model_policy = self.model_policy  # type: ignore\n        sh.sample_new_config = self.sample_new_config  # type: ignore\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung: int) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung: int) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    return {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.clear_old_brackets","title":"clear_old_brackets","text":"<pre><code>clear_old_brackets() -&gt; None\n</code></pre> <p>Enforces reset at each new bracket.</p> <p>The _get_rungs_state() function creates the <code>rung_promotions</code> dict mapping which is used by the promotion policies to determine the next step: promotion/sample. To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a series of SH brackets, where the SH brackets comprising HB is repeated. This is done by iterating over the closed loop of possible SH brackets (self.sh_brackets). The oldest, active, incomplete SH bracket is searched for to choose the next evaluation. If either all brackets are over or waiting, a new SH bracket, corresponding to the SH bracket under HB as registered by <code>current_SH_bracket</code>.</p> Source code in <code>neps/optimizers/multi_fidelity/hyperband.py</code> <pre><code>def clear_old_brackets(self) -&gt; None:\n    \"\"\"Enforces reset at each new bracket.\n\n    The _get_rungs_state() function creates the `rung_promotions` dict mapping which\n    is used by the promotion policies to determine the next step: promotion/sample.\n    To simulate reset of rungs like in vanilla HB, the algorithm is viewed as a\n    series of SH brackets, where the SH brackets comprising HB is repeated. This is\n    done by iterating over the closed loop of possible SH brackets (self.sh_brackets).\n    The oldest, active, incomplete SH bracket is searched for to choose the next\n    evaluation. If either all brackets are over or waiting, a new SH bracket,\n    corresponding to the SH bracket under HB as registered by `current_SH_bracket`.\n    \"\"\"\n    n_sh_brackets = len(self.sh_brackets)\n    # iterates over the different SH brackets\n    self.current_sh_bracket = 0  # indexing from range(0, n_sh_brackets)\n    start = 0\n    _min_rung = self.sh_brackets[self.current_sh_bracket].min_rung\n    end = self.sh_brackets[self.current_sh_bracket].config_map[_min_rung]\n\n    if self.sample_default_first and self.sample_default_at_target:\n        start += 1\n        end += 1\n\n    # stores the base rung size for each SH bracket in HB\n    base_rung_sizes = []  # sorted(self.config_map.values(), reverse=True)\n    for bracket in self.sh_brackets.values():\n        base_rung_sizes.append(sorted(bracket.config_map.values(), reverse=True)[0])\n    while end &lt;= len(self.observed_configs):\n        # subsetting only this SH bracket from the history\n        sh_bracket = self.sh_brackets[self.current_sh_bracket]\n        sh_bracket.clean_rung_information()\n        # for the SH bracket in start-end, calculate total SH budget used, from the\n        # correct SH bracket object to make the right budget calculations\n\n        assert isinstance(sh_bracket, SuccessiveHalving)\n        bracket_budget_used = sh_bracket._calc_budget_used_in_bracket(\n            deepcopy(self.observed_configs.rung.values[start:end])\n        )\n        # if budget used is less than the total SH budget then still an active bracket\n        current_bracket_full_budget = sum(sh_bracket.full_rung_trace)\n        if bracket_budget_used &lt; current_bracket_full_budget:\n            # updating rung information of the current bracket\n\n            sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n            # extra call to use the updated rung member info to find promotions\n            # SyncPromotion signals a wait if a rung is full but with\n            # incomplete/pending evaluations, signals to starts a new SH bracket\n            sh_bracket._handle_promotions()\n            promotion_count = 0\n            for _, promotions in sh_bracket.rung_promotions.items():\n                promotion_count += len(promotions)\n            # if no promotion candidates are returned, then the current bracket\n            # is active and waiting\n            if promotion_count:\n                # returns the oldest active bracket if a promotion found which is the\n                # current SH bracket at this scope\n                return\n            # if no promotions, ensure an empty state explicitly to disable bracket\n            sh_bracket.clean_rung_information()\n        start = end\n        # updating pointer to the next SH bracket in HB\n        self.current_sh_bracket = (self.current_sh_bracket + 1) % n_sh_brackets\n        end = start + base_rung_sizes[self.current_sh_bracket]\n    # reaches here if all old brackets are either waiting or finished\n\n    # updates rung info with the latest active, incomplete bracket\n    sh_bracket = self.sh_brackets[self.current_sh_bracket]\n\n    sh_bracket._get_rungs_state(self.observed_configs.iloc[start:end])\n    sh_bracket._handle_promotions()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; float\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent: SearchSpace) -&gt; float:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    return min(distances)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; list[float]\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent: SearchSpace) -&gt; list[float]:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    return [d for d in distances if d &gt; 0]\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int | None = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int | None = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            logger.warning(f\"{rung} not in {np.unique(idxs)}\")  # type: ignore\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n        assert isinstance(inc, SearchSpace)\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        logger.warning(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.get_config_and_ids","title":"get_config_and_ids","text":"<pre><code>get_config_and_ids() -&gt; tuple[RawConfig, str, str | None]\n</code></pre> <p>...and this is the method that decides which point to query.</p> RETURNS DESCRIPTION <code>tuple[RawConfig, str, str | None]</code> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def get_config_and_ids(self) -&gt; tuple[RawConfig, str, str | None]:\n    \"\"\"...and this is the method that decides which point to query.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    self._set_sampling_weights_and_inc(rung=self.current_sh_bracket)\n\n    for _, sh in self.sh_brackets.items():\n        sh.sampling_args = self.sampling_args\n    return super().get_config_and_ids()\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    sh_brackets = getattr(self, \"sh_brackets\", None)\n    if sh_brackets is not None and len(sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n\n    assert isinstance(bracket, SuccessiveHalvingBase)\n\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(bracket.rung_map.keys(), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    valid_perf_mask = self.observed_configs[\"perf\"].notna()\n    rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n    resources_used = sum(self.rung_map[r] for r in rungs)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.is_init_phase","title":"is_init_phase","text":"<pre><code>is_init_phase() -&gt; bool\n</code></pre> <p>Returns True is in the warmstart phase and False under model-based search.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def is_init_phase(self) -&gt; bool:\n    \"\"\"Returns True is in the warmstart phase and False under model-based search.\"\"\"\n    if self.modelling_type == \"rung\":\n        # build a model per rung or per fidelity\n        # in this case, the initial design checks if `init_size` number of\n        # configurations have finished at a rung or not and the highest such rung is\n        # chosen for model building at teh current iteration\n        if self._active_rung() is None:\n            return True\n    elif self.modelling_type == \"joint\":\n        # builds a model across all fidelities with the fidelity as a dimension\n        # in this case, calculate the total number of function evaluations spent\n        # and in vanilla BO fashion use that to compare with the initital design size\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        total_resources = sum(self.rung_map[r] for r in rungs)\n        resources = total_resources / self.max_budget\n        if resources &lt; self.init_size:\n            return True\n    else:\n        raise ValueError(\"Choice of modelling_type not in {{'rung', 'joint'}}\")\n    return False\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.is_promotable","title":"is_promotable","text":"<pre><code>is_promotable() -&gt; int | None\n</code></pre> <p>Returns an int if a rung can be promoted, else a None.</p> Source code in <code>neps/optimizers/multi_fidelity/successive_halving.py</code> <pre><code>def is_promotable(self) -&gt; int | None:\n    \"\"\"Returns an int if a rung can be promoted, else a None.\"\"\"\n    rung_to_promote = None\n    # # iterates starting from the highest fidelity promotable to the lowest fidelity\n    for rung in reversed(range(self.min_rung, self.max_rung)):\n        if len(self.rung_promotions[rung]) &gt; 0:\n            rung_to_promote = rung\n            # stop checking when a promotable config found\n            # no need to search at lower fidelities\n            break\n    return rung_to_promote\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; tuple[float, float]\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; tuple[float, float]:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    if self.inc_style == \"decay\":\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        resources = sum(self.rung_map[r] for r in rungs)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    if self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBand.sample_new_config","title":"sample_new_config","text":"<pre><code>sample_new_config(\n    rung: int | None = None, **kwargs: Any\n) -&gt; SearchSpace\n</code></pre> <p>Samples configuration from policies or random.</p> Source code in <code>neps/optimizers/multi_fidelity/mf_bo.py</code> <pre><code>def sample_new_config(\n    self,\n    rung: int | None = None,\n    **kwargs: Any,\n) -&gt; SearchSpace:\n    \"\"\"Samples configuration from policies or random.\"\"\"\n    if self.model_based and not self.is_init_phase():\n        incumbent = None\n        if self.modelling_type == \"rung\":\n            # `rung` should not be None when not in init phase\n            active_max_rung = self._active_rung()\n            fidelity = None\n            active_max_fidelity = self.rung_map[active_max_rung]\n        elif self.modelling_type == \"joint\":\n            fidelity = self.rung_map[rung]\n            active_max_fidelity = None\n            # IMPORTANT step for correct 2-step acquisition\n            incumbent = min(self.rung_histories[rung][\"perf\"])\n        else:\n            raise ValueError(\"Choice of modelling_type not in 'rung', 'joint'\")\n        assert (\n            (fidelity is None and active_max_fidelity is not None)\n            or (active_max_fidelity is None and fidelity is not None)\n            or (active_max_fidelity is not None and fidelity is not None)\n        ), \"Either condition needs to be not None!\"\n        config = self.model_policy.sample(\n            active_max_fidelity=active_max_fidelity,\n            fidelity=fidelity,\n            incumbent=incumbent,\n            **self.sampling_args,\n        )\n    elif self.sampling_policy is not None:\n        config = self.sampling_policy.sample(**self.sampling_args)\n    else:\n        config = sample_one_old(\n            self.pipeline_space,\n            patience=self.patience,\n            user_priors=self.use_priors,\n            ignore_fidelity=True,\n        )\n    return config\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase","title":"PriorBandBase","text":"<p>Class that defines essential properties needed by PriorBand.</p> <p>Designed to work with the topmost parent class as SuccessiveHalvingBase.</p>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.calc_sampling_args","title":"calc_sampling_args","text":"<pre><code>calc_sampling_args(rung: int) -&gt; dict\n</code></pre> <p>Sets the weights for each of the sampling techniques.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def calc_sampling_args(self, rung: int) -&gt; dict:\n    \"\"\"Sets the weights for each of the sampling techniques.\"\"\"\n    if self.prior_weight_type == \"geometric\":\n        _w_random = 1\n        # scales weight of prior by eta raised to the current rung level\n        # at the base rung thus w_prior = w_random\n        # at the max rung r, w_prior = eta^r * w_random\n        _w_prior = (self.eta**rung) * _w_random\n    elif self.prior_weight_type == \"linear\":\n        _w_random = 1\n        w_prior_min_rung = 1 * _w_random\n        w_prior_max_rung = self.eta * _w_random\n        num_rungs = len(self.rung_map)\n        # linearly increasing prior weight such that\n        # at base rung, w_prior = w_random\n        # at max rung, w_prior = self.eta * w_random\n        _w_prior = np.linspace(\n            start=w_prior_min_rung,\n            stop=w_prior_max_rung,\n            endpoint=True,\n            num=num_rungs,\n        )[rung]\n    elif self.prior_weight_type == \"50-50\":\n        _w_random = 1\n        _w_prior = 1\n    else:\n        raise ValueError(f\"{self.prior_weight_type} not in {{'linear', 'geometric'}}\")\n\n    # normalizing weights of random and prior sampling\n    w_prior = _w_prior / (_w_prior + _w_random)\n    w_random = _w_random / (_w_prior + _w_random)\n    # calculating ratio of prior and incumbent weights\n    _w_prior, _w_inc = self.prior_to_incumbent_ratio()\n    # scaling back such that w_random + w_prior + w_inc = 1\n    w_inc = _w_inc * w_prior\n    w_prior = _w_prior * w_prior\n\n    return {\n        \"prior\": w_prior,\n        \"inc\": w_inc,\n        \"random\": w_random,\n    }\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.find_1nn_distance_from_incumbent","title":"find_1nn_distance_from_incumbent","text":"<pre><code>find_1nn_distance_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; float\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_1nn_distance_from_incumbent(self, incumbent: SearchSpace) -&gt; float:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    distances = self.find_all_distances_from_incumbent(incumbent)\n    return min(distances)\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.find_all_distances_from_incumbent","title":"find_all_distances_from_incumbent","text":"<pre><code>find_all_distances_from_incumbent(\n    incumbent: SearchSpace,\n) -&gt; list[float]\n</code></pre> <p>Finds the distance to the nearest neighbour.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_all_distances_from_incumbent(self, incumbent: SearchSpace) -&gt; list[float]:\n    \"\"\"Finds the distance to the nearest neighbour.\"\"\"\n    dist = lambda x: compute_config_dist(incumbent, x)\n    # computing distance of incumbent from all seen points in history\n    distances = [dist(config) for config in self.observed_configs.config]\n    # ensuring the distances exclude 0 or the distance from itself\n    return [d for d in distances if d &gt; 0]\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.find_incumbent","title":"find_incumbent","text":"<pre><code>find_incumbent(rung: int | None = None) -&gt; SearchSpace\n</code></pre> <p>Find the best performing configuration seen so far.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def find_incumbent(self, rung: int | None = None) -&gt; SearchSpace:\n    \"\"\"Find the best performing configuration seen so far.\"\"\"\n    rungs = self.observed_configs.rung.values\n    idxs = self.observed_configs.index.values\n    while rung is not None:\n        # enters this scope is `rung` argument passed and not left empty or None\n        if rung not in rungs:\n            logger.warning(f\"{rung} not in {np.unique(idxs)}\")  # type: ignore\n        # filtering by rung based on argument passed\n        idxs = self.observed_configs.rung.values == rung\n        # checking width of current rung\n        if len(idxs) &lt; self.eta:\n            logger.warn(\n                f\"Selecting incumbent from a rung with width less than {self.eta}\"\n            )\n    # extracting the incumbent configuration\n    if len(idxs):\n        # finding the config with the lowest recorded performance\n        _perfs = self.observed_configs.loc[idxs].perf.values\n        inc_idx = np.nanargmin([np.nan if t is None else t for t in _perfs])\n        inc = self.observed_configs.loc[idxs].iloc[inc_idx].config\n        assert isinstance(inc, SearchSpace)\n    else:\n        # THIS block should not ever execute, but for runtime anomalies, if no\n        # incumbent can be extracted, the prior is treated as the incumbent\n        inc = self.pipeline_space.from_dict(self.pipeline_space.default_config)\n        logger.warning(\n            \"Treating the prior as the incumbent. \"\n            \"Please check if this should not happen.\"\n        )\n    return inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.is_activate_inc","title":"is_activate_inc","text":"<pre><code>is_activate_inc() -&gt; bool\n</code></pre> <p>Function to check optimization state to allow/disallow incumbent sampling.</p> <p>This function checks if the total resources used for the finished evaluations sums to the budget of one full SH bracket.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def is_activate_inc(self) -&gt; bool:\n    \"\"\"Function to check optimization state to allow/disallow incumbent sampling.\n\n    This function checks if the total resources used for the finished evaluations\n    sums to the budget of one full SH bracket.\n    \"\"\"\n    activate_inc = False\n\n    # calculate total resource cost required for the first SH bracket in HB\n    sh_brackets = getattr(self, \"sh_brackets\", None)\n    if sh_brackets is not None and len(sh_brackets) &gt; 1:\n        # for HB or AsyncHB which invokes multiple SH brackets\n        bracket = sh_brackets[self.min_rung]\n    else:\n        # for SH or ASHA which do not invoke multiple SH brackets\n        bracket = self\n\n    assert isinstance(bracket, SuccessiveHalvingBase)\n\n    # calculating the total resources spent in the first SH bracket, taking into\n    # account the continuations, that is, the resources spent on a promoted config is\n    # not fidelity[rung] but (fidelity[rung] - fidelity[rung - 1])\n    continuation_resources = bracket.rung_map[bracket.min_rung]\n    resources = bracket.config_map[bracket.min_rung] * continuation_resources\n    for r in range(1, len(bracket.rung_map)):\n        rung = sorted(bracket.rung_map.keys(), reverse=False)[r]\n        continuation_resources = bracket.rung_map[rung] - bracket.rung_map[rung - 1]\n        resources += bracket.config_map[rung] * continuation_resources\n\n    # find resources spent so far for all finished evaluations\n    valid_perf_mask = self.observed_configs[\"perf\"].notna()\n    rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n    resources_used = sum(self.rung_map[r] for r in rungs)\n\n    if resources_used &gt;= resources and len(\n        self.rung_histories[self.max_rung][\"config\"]\n    ):\n        # activate incumbent-based sampling if a total resources is at least\n        # equivalent to one SH bracket resource usage, and additionally, for the\n        # asynchronous case with large number of workers, the check enforces that\n        # at least one configuration has been evaluated at the highest fidelity\n        activate_inc = True\n    return activate_inc\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/priorband/#neps.optimizers.multi_fidelity_prior.priorband.PriorBandBase.prior_to_incumbent_ratio","title":"prior_to_incumbent_ratio","text":"<pre><code>prior_to_incumbent_ratio() -&gt; tuple[float, float]\n</code></pre> <p>Calculates the normalized weight distribution between prior and incumbent.</p> <p>Sum of the weights should be 1.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/priorband.py</code> <pre><code>def prior_to_incumbent_ratio(self) -&gt; tuple[float, float]:\n    \"\"\"Calculates the normalized weight distribution between prior and incumbent.\n\n    Sum of the weights should be 1.\n    \"\"\"\n    if self.inc_style == \"constant\":\n        return self._prior_to_incumbent_ratio_constant()\n    if self.inc_style == \"decay\":\n        valid_perf_mask = self.observed_configs[\"perf\"].notna()\n        rungs = self.observed_configs.loc[valid_perf_mask, \"rung\"]\n        resources = sum(self.rung_map[r] for r in rungs)\n        return self._prior_to_incumbent_ratio_decay(\n            resources, self.eta, self.min_budget, self.max_budget\n        )\n    if self.inc_style == \"dynamic\":\n        return self._prior_to_incumbent_ratio_dynamic(self.max_rung)\n    raise ValueError(f\"Invalid option {self.inc_style}\")\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/","title":"Utils","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils","title":"neps.optimizers.multi_fidelity_prior.utils","text":""},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.compute_config_dist","title":"compute_config_dist","text":"<pre><code>compute_config_dist(\n    config1: SearchSpace, config2: SearchSpace\n) -&gt; float\n</code></pre> <p>Computes distance between two configurations.</p> <p>Divides the search space into continuous and categorical subspaces. Normalizes all the continuous values while gives numerical encoding to categories. Distance returned is the sum of the Euclidean distance of the continous subspace and the Hamming distance of the categorical subspace.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def compute_config_dist(config1: SearchSpace, config2: SearchSpace) -&gt; float:\n    \"\"\"Computes distance between two configurations.\n\n    Divides the search space into continuous and categorical subspaces.\n    Normalizes all the continuous values while gives numerical encoding to categories.\n    Distance returned is the sum of the Euclidean distance of the continous subspace and\n    the Hamming distance of the categorical subspace.\n    \"\"\"\n    encoder = ConfigEncoder.from_parameters({**config1.numerical, **config1.categoricals})\n    configs = encoder.encode([config1._values, config2._values])\n    dist = pairwise_dist(configs, encoder, square_form=False)\n    return float(dist.item())\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.compute_scores","title":"compute_scores","text":"<pre><code>compute_scores(\n    config: SearchSpace,\n    prior: SearchSpace,\n    inc: SearchSpace,\n    *,\n    include_fidelity: bool = False\n) -&gt; tuple[float, float]\n</code></pre> <p>Scores the config by a Gaussian around the prior and the incumbent.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def compute_scores(\n    config: SearchSpace,\n    prior: SearchSpace,\n    inc: SearchSpace,\n    *,\n    include_fidelity: bool = False,\n) -&gt; tuple[float, float]:\n    \"\"\"Scores the config by a Gaussian around the prior and the incumbent.\"\"\"\n    # TODO: This could lifted up and just done in the class itself\n    # in a vectorized form.\n    encoder = ConfigEncoder.from_space(config, include_fidelity=include_fidelity)\n    encoded_config = encoder.encode([config._values])\n\n    prior_dist = Prior.from_space(\n        prior,\n        center_values=prior._values,\n        include_fidelity=include_fidelity,\n    )\n    inc_dist = Prior.from_space(\n        inc,\n        center_values=inc._values,\n        include_fidelity=include_fidelity,\n    )\n\n    prior_score = prior_dist.pdf(encoded_config, frm=encoder).item()\n    inc_score = inc_dist.pdf(encoded_config, frm=encoder).item()\n    return prior_score, inc_score\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.custom_crossover","title":"custom_crossover","text":"<pre><code>custom_crossover(\n    config1: SearchSpace,\n    config2: SearchSpace,\n    crossover_prob: float = 0.5,\n    patience: int = 50,\n) -&gt; SearchSpace\n</code></pre> <p>Performs a crossover of config2 into config1.</p> <p>Returns a configuration where each HP in config1 has <code>crossover_prob</code>% chance of getting config2's value of the corresponding HP. By default, crossover rate is 50%.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def custom_crossover(\n    config1: SearchSpace,\n    config2: SearchSpace,\n    crossover_prob: float = 0.5,\n    patience: int = 50,\n) -&gt; SearchSpace:\n    \"\"\"Performs a crossover of config2 into config1.\n\n    Returns a configuration where each HP in config1 has `crossover_prob`% chance of\n    getting config2's value of the corresponding HP. By default, crossover rate is 50%.\n    \"\"\"\n    _existing = config1._values\n\n    for _ in range(patience):\n        child_config = {}\n        for key, hyperparameter in config1.items():\n            if not hyperparameter.is_fidelity and np.random.random() &lt; crossover_prob:\n                child_config[key] = config2[key].value\n            else:\n                child_config[key] = hyperparameter.value\n\n        if _existing != child_config:\n            return config1.from_dict(child_config)\n\n    # fail safe check to handle edge cases where config1=config2 or\n    # config1 extremely local to config2 such that crossover fails to\n    # generate new config in a discrete (sub-)space\n    return sample_one_old(\n        config1,\n        patience=patience,\n        user_priors=False,\n        ignore_fidelity=True,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.get_prior_weight_for_decay","title":"get_prior_weight_for_decay","text":"<pre><code>get_prior_weight_for_decay(\n    resources_used: float,\n    eta: int,\n    min_budget: int | float,\n    max_budget: int | float,\n) -&gt; float\n</code></pre> <p>Creates a step function schedule for the prior weight decay.</p> <p>The prior weight ratio is decayed every time the total resources used is equivalent to the cost of one successive halving bracket within the HB schedule. This is approximately eta \\times max_budget resources for one evaluation.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def get_prior_weight_for_decay(\n    resources_used: float, eta: int, min_budget: int | float, max_budget: int | float\n) -&gt; float:\n    r\"\"\"Creates a step function schedule for the prior weight decay.\n\n    The prior weight ratio is decayed every time the total resources used is\n    equivalent to the cost of one successive halving bracket within the HB schedule.\n    This is approximately eta \\times max_budget resources for one evaluation.\n    \"\"\"\n    # decay factor for the prior\n    decay = 2\n    unit_resources = eta * max_budget\n    idx = resources_used // unit_resources\n    return 1 / decay**idx\n</code></pre>"},{"location":"api/neps/optimizers/multi_fidelity_prior/utils/#neps.optimizers.multi_fidelity_prior.utils.local_mutation","title":"local_mutation","text":"<pre><code>local_mutation(\n    config: SearchSpace,\n    std: float = 0.25,\n    mutation_rate: float = 0.5,\n    patience: int = 50,\n) -&gt; SearchSpace\n</code></pre> <p>Performs a local search by mutating randomly chosen hyperparameters.</p> Source code in <code>neps/optimizers/multi_fidelity_prior/utils.py</code> <pre><code>def local_mutation(\n    config: SearchSpace,\n    std: float = 0.25,\n    mutation_rate: float = 0.5,\n    patience: int = 50,\n) -&gt; SearchSpace:\n    \"\"\"Performs a local search by mutating randomly chosen hyperparameters.\"\"\"\n    # Used to check uniqueness later.\n    # TODO: Seeding\n    space = config\n    parameters_to_keep = {}\n    parameters_to_mutate = {}\n\n    for name, parameter in space.hyperparameters.items():\n        if (\n            parameter.is_fidelity\n            or isinstance(parameter, Constant)\n            or np.random.uniform() &gt; mutation_rate\n        ):\n            parameters_to_keep[name] = parameter.value\n        else:\n            parameters_to_mutate[name] = parameter\n\n    if len(parameters_to_mutate) == 0:\n        return space.from_dict(parameters_to_keep)\n\n    new_config: dict[str, Any] = {}\n\n    for hp_name, hp in parameters_to_mutate.items():\n        match hp:\n            case Categorical():\n                assert hp._value_index is not None\n                perm: list[int] = torch.randperm(len(hp.choices)).tolist()\n                ix = perm[0] if perm[0] != hp._value_index else perm[1]\n                new_config[hp_name] = hp.choices[ix]\n            case GraphParameter():\n                new_config[hp_name] = hp.mutate(mutation_strategy=\"bananas\")\n            case Integer() | Float():\n                prior = Prior.from_parameters(\n                    {hp_name: hp},\n                    confidence_values={hp_name: (1 - std)},\n                )\n\n                for _ in range(patience):\n                    sample = prior.sample(1, to=hp.domain).item()\n                    if sample != hp.value:\n                        new_config[hp_name] = hp.value\n                        break\n                else:\n                    raise ValueError(\n                        f\"Exhausted patience trying to mutate parameter '{hp_name}'\"\n                        f\" with value {hp.value}\"\n                    )\n            case _:\n                raise NotImplementedError(f\"Unknown hp type for {hp_name}: {type(hp)}\")\n\n    return space.from_dict(new_config)\n</code></pre>"},{"location":"api/neps/optimizers/random_search/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/optimizers/random_search/optimizer/#neps.optimizers.random_search.optimizer","title":"neps.optimizers.random_search.optimizer","text":"<p>Random search optimizer.</p>"},{"location":"api/neps/optimizers/random_search/optimizer/#neps.optimizers.random_search.optimizer.RandomSearch","title":"RandomSearch","text":"<pre><code>RandomSearch(\n    *,\n    pipeline_space: SearchSpace,\n    use_priors: bool = False,\n    ignore_fidelity: bool = True,\n    seed: int | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>A simple random search optimizer.</p> PARAMETER DESCRIPTION <code>pipeline_space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>use_priors</code> <p>Whether to use priors when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore fidelity when sampling. In this case, the max fidelity is always used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>seed</code> <p>The seed for the random number generator.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/optimizers/random_search/optimizer.py</code> <pre><code>def __init__(\n    self,\n    *,\n    pipeline_space: SearchSpace,\n    use_priors: bool = False,\n    ignore_fidelity: bool = True,\n    seed: int | None = None,\n    **kwargs: Any,  # TODO: Remove\n):\n    \"\"\"Initialize the random search optimizer.\n\n    Args:\n        pipeline_space: The search space to sample from.\n        use_priors: Whether to use priors when sampling.\n        ignore_fidelity: Whether to ignore fidelity when sampling.\n            In this case, the max fidelity is always used.\n        seed: The seed for the random number generator.\n    \"\"\"\n    super().__init__(pipeline_space=pipeline_space)\n    self.use_priors = use_priors\n    self.ignore_fidelity = ignore_fidelity\n    if seed is not None:\n        raise NotImplementedError(\"Seed is not implemented yet for RandomSearch\")\n\n    self.seed = seed\n    self.encoder = ConfigEncoder.from_space(\n        pipeline_space,\n        include_fidelity=False,\n        include_constants_when_decoding=True,\n    )\n    self.sampler = UniformPrior(ndim=self.encoder.ncols)\n</code></pre>"},{"location":"api/neps/optimizers/random_search/optimizer/#neps.optimizers.random_search.optimizer.RandomSearch.get_cost","title":"get_cost","text":"<pre><code>get_cost(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_cost() and passes the error handling through. Please use self.get_cost() instead of get_cost() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_cost(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_cost() and passes the error handling through.\n    Please use self.get_cost() instead of get_cost() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_cost(\n        result,\n        cost_value_on_error=self.cost_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/optimizers/random_search/optimizer/#neps.optimizers.random_search.optimizer.RandomSearch.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    result: ERROR | ResultDict | float | Report,\n) -&gt; float | ERROR\n</code></pre> <p>Calls result.utils.get_loss() and passes the error handling through. Please use self.get_loss() instead of get_loss() in all optimizer classes.</p> Source code in <code>neps/optimizers/base_optimizer.py</code> <pre><code>def get_loss(self, result: ERROR | ResultDict | float | Report) -&gt; float | ERROR:\n    \"\"\"Calls result.utils.get_loss() and passes the error handling through.\n    Please use self.get_loss() instead of get_loss() in all optimizer classes.\n    \"\"\"\n    # TODO(eddiebergman): This is a forward change for whenever we can have optimizers\n    # use `Trial` and `Report`, they already take care of this and save having to do\n    # this `_get_loss` at every call. We can also then just use `None` instead of\n    # the string `\"error\"`\n    if isinstance(result, Report):\n        return result.loss if result.loss is not None else \"error\"\n\n    return _get_loss(\n        result,\n        loss_value_on_error=self.loss_value_on_error,\n        ignore_errors=self.ignore_errors,\n    )\n</code></pre>"},{"location":"api/neps/plot/plot/","title":"Plot","text":""},{"location":"api/neps/plot/plot/#neps.plot.plot","title":"neps.plot.plot","text":"<p>Plot results of a neural pipeline search run.</p>"},{"location":"api/neps/plot/plot/#neps.plot.plot.plot","title":"plot","text":"<pre><code>plot(\n    root_directory: str | Path,\n    *,\n    scientific_mode: bool = False,\n    key_to_extract: str | None = None,\n    benchmarks: list[str] | None = None,\n    algorithms: list[str] | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n    x_range: tuple | None = None,\n    log_x: bool = False,\n    log_y: bool = True,\n    filename: str = \"incumbent_trajectory\",\n    extension: str = \"png\",\n    dpi: int = 100\n) -&gt; None\n</code></pre> <p>Plot results of a neural pipeline search run.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The directory with neps results (see below).</p> <p> TYPE: <code>str | Path</code> </p> <code>scientific_mode</code> <p>If true, plot from a tree-structured root_directory: benchmark={}/algorithm={}/seed={}</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>key_to_extract</code> <p>The metric to be used on the x-axis (if active, make sure run_pipeline returns the metric in the info_dict)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>benchmarks</code> <p>List of benchmarks to plot</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>algorithms</code> <p>List of algorithms to plot</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>consider_continuations</code> <p>If true, toggle calculation of continuation costs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_workers</code> <p>Number of parallel processes of neps.run</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>x_range</code> <p>Bound x-axis (e.g. 1 10)</p> <p> TYPE: <code>tuple | None</code> DEFAULT: <code>None</code> </p> <code>log_x</code> <p>If true, toggle logarithmic scale on the x-axis</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>log_y</code> <p>If true, toggle logarithmic scale on the y-axis</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>filename</code> <p>Filename</p> <p> TYPE: <code>str</code> DEFAULT: <code>'incumbent_trajectory'</code> </p> <code>extension</code> <p>Image format</p> <p> TYPE: <code>str</code> DEFAULT: <code>'png'</code> </p> <code>dpi</code> <p>Image resolution</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the data to be plotted is not present.</p> Source code in <code>neps/plot/plot.py</code> <pre><code>def plot(  # noqa: C901, PLR0913\n    root_directory: str | Path,\n    *,\n    scientific_mode: bool = False,\n    key_to_extract: str | None = None,\n    benchmarks: list[str] | None = None,\n    algorithms: list[str] | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n    x_range: tuple | None = None,\n    log_x: bool = False,\n    log_y: bool = True,\n    filename: str = \"incumbent_trajectory\",\n    extension: str = \"png\",\n    dpi: int = 100,\n) -&gt; None:\n    \"\"\"Plot results of a neural pipeline search run.\n\n    Args:\n        root_directory: The directory with neps results (see below).\n        scientific_mode: If true, plot from a tree-structured root_directory:\n            benchmark={}/algorithm={}/seed={}\n        key_to_extract: The metric to be used on the x-axis\n            (if active, make sure run_pipeline returns the metric in the info_dict)\n        benchmarks: List of benchmarks to plot\n        algorithms: List of algorithms to plot\n        consider_continuations: If true, toggle calculation of continuation costs\n        n_workers: Number of parallel processes of neps.run\n        x_range: Bound x-axis (e.g. 1 10)\n        log_x: If true, toggle logarithmic scale on the x-axis\n        log_y: If true, toggle logarithmic scale on the y-axis\n        filename: Filename\n        extension: Image format\n        dpi: Image resolution\n\n    Raises:\n        FileNotFoundError: If the data to be plotted is not present.\n    \"\"\"\n    logger = logging.getLogger(\"neps\")\n    logger.info(f\"Starting neps.plot using working directory {root_directory}\")\n\n    if benchmarks is None:\n        benchmarks = [\"example\"]\n    if algorithms is None:\n        algorithms = [\"neps\"]\n\n    logger.info(\n        f\"Processing {len(benchmarks)} benchmark(s) \"\n        f\"and {len(algorithms)} algorithm(s)...\"\n    )\n\n    ncols = 1 if len(benchmarks) == 1 else 2\n    nrows = np.ceil(len(benchmarks) / ncols).astype(int)\n\n    fig, axs = _get_fig_and_axs(nrows=nrows, ncols=ncols)\n\n    base_path = Path(root_directory)\n\n    for benchmark_idx, benchmark in enumerate(benchmarks):\n        if scientific_mode:\n            _base_path = base_path / f\"benchmark={benchmark}\"\n            if not _base_path.is_dir():\n                raise FileNotFoundError(\n                    errno.ENOENT, os.strerror(errno.ENOENT), _base_path\n                )\n        else:\n            _base_path = None\n\n        for algorithm in algorithms:\n            seeds = [None]\n            if _base_path is not None:\n                assert scientific_mode\n                _path = _base_path / f\"algorithm={algorithm}\"\n                if not _path.is_dir():\n                    raise FileNotFoundError(\n                        errno.ENOENT, os.strerror(errno.ENOENT), _path\n                    )\n\n                seeds = sorted(os.listdir(_path))  # type: ignore\n            else:\n                _path = None\n\n            incumbents = []\n            costs = []\n            max_costs = []\n            for seed in seeds:\n                incumbent, cost, max_cost = process_seed(\n                    path=_path if _path is not None else base_path,\n                    seed=seed,\n                    key_to_extract=key_to_extract,\n                    consider_continuations=consider_continuations,\n                    n_workers=n_workers,\n                )\n                incumbents.append(incumbent)\n                costs.append(cost)\n                max_costs.append(max_cost)\n\n            is_last_row = benchmark_idx &gt;= (nrows - 1) * ncols\n            is_first_column = benchmark_idx % ncols == 0\n            xlabel = \"Evaluations\" if key_to_extract is None else key_to_extract.upper()\n            _plot_incumbent(\n                ax=_map_axs(\n                    axs,\n                    benchmark_idx,\n                    len(benchmarks),\n                    ncols,\n                ),\n                x=costs,\n                y=incumbents,\n                scale_x=max(max_costs) if key_to_extract == \"fidelity\" else None,\n                title=benchmark if scientific_mode else None,\n                xlabel=xlabel if is_last_row else None,\n                ylabel=\"Best error\" if is_first_column else None,\n                log_x=log_x,\n                log_y=log_y,\n                x_range=x_range,\n                label=algorithm,\n            )\n\n    if scientific_mode:\n        _set_legend(\n            fig,\n            axs,\n            benchmarks=benchmarks,\n            algorithms=algorithms,\n            nrows=nrows,\n            ncols=ncols,\n        )\n    _save_fig(fig, output_dir=base_path, filename=filename, extension=extension, dpi=dpi)\n    logger.info(f\"Saved to '{base_path}/{filename}.{extension}'\")\n</code></pre>"},{"location":"api/neps/plot/plot3D/","title":"plot3D","text":""},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D","title":"neps.plot.plot3D","text":"<p>Plot a 3D landscape of learning curves for a given run.</p>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D","title":"Plotter3D  <code>dataclass</code>","text":"<pre><code>Plotter3D(\n    loss_key: str = \"Loss\",\n    fidelity_key: str = \"epochs\",\n    run_path: str | Path | None = None,\n    scatter: bool = True,\n    footnote: bool = True,\n    alpha: float = 0.9,\n    scatter_size: float | int = 3,\n    bck_color_2d: tuple[float, float, float] = (\n        0.8,\n        0.82,\n        0.8,\n    ),\n    view_angle: tuple[float, float] = (15, -70),\n)\n</code></pre> <p>Plot a 3d landscape of learning curves for a given run.</p>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_color","title":"get_color  <code>staticmethod</code>","text":"<pre><code>get_color(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the color values for the plot.</p> Source code in <code>neps/plot/plot3D.py</code> <pre><code>@staticmethod\ndef get_color(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the color values for the plot.\"\"\"\n    return df.index.to_numpy()\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_x","title":"get_x  <code>staticmethod</code>","text":"<pre><code>get_x(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the x-axis values for the plot.</p> Source code in <code>neps/plot/plot3D.py</code> <pre><code>@staticmethod\ndef get_x(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the x-axis values for the plot.\"\"\"\n    return df[\"epochID\"].to_numpy()\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_y","title":"get_y  <code>staticmethod</code>","text":"<pre><code>get_y(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the y-axis values for the plot.</p> Source code in <code>neps/plot/plot3D.py</code> <pre><code>@staticmethod\ndef get_y(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the y-axis values for the plot.\"\"\"\n    y_ = df[\"configID\"].to_numpy()\n    return np.ones_like(y_) * y_[0]\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.get_z","title":"get_z  <code>staticmethod</code>","text":"<pre><code>get_z(df: DataFrame) -&gt; ndarray\n</code></pre> <p>Get the z-axis values for the plot.</p> Source code in <code>neps/plot/plot3D.py</code> <pre><code>@staticmethod\ndef get_z(df: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Get the z-axis values for the plot.\"\"\"\n    return df[\"result.loss\"].to_numpy()\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.plot3D","title":"plot3D","text":"<pre><code>plot3D(\n    data: DataFrame | None = None,\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None\n</code></pre> <p>Plot the 3D landscape of learning curves.</p> Source code in <code>neps/plot/plot3D.py</code> <pre><code>def plot3D(  # noqa: N802, PLR0915\n    self,\n    data: pd.DataFrame | None = None,\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None:\n    \"\"\"Plot the 3D landscape of learning curves.\"\"\"\n    data = self.prep_df(data)\n\n    # Create the figure and the axes for the plot\n    fig, (ax3D, ax, cax) = plt.subplots(\n        1, 3, figsize=(12, 5), width_ratios=(20, 20, 1)\n    )\n\n    # remove a 2D axis and replace with a 3D projection one\n    ax3D.remove()\n    ax3D = fig.add_subplot(131, projection=\"3d\")\n\n    # Create the normalizer to normalize the color values\n    norm = Normalize(self.get_color(data).min(), self.get_color(data).max())\n\n    # Counters to keep track of the configurations run for only a single fidelity\n    n_lines = 0\n    n_points = 0\n\n    data_groups = data.groupby(\"configID\", sort=False)\n\n    for idx, (_configID, data_) in enumerate(data_groups):\n        x = self.get_x(data_)\n        y = self.get_y(data_)\n        z = self.get_z(data_)\n\n        y = np.ones_like(y) * idx\n        color = self.get_color(data_)\n\n        if len(x) &lt; 2:\n            n_points += 1\n            if self.scatter:\n                # 3D points\n                ax3D.scatter(\n                    y,\n                    z,\n                    s=self.scatter_size,\n                    zs=0,\n                    zdir=\"x\",\n                    c=color,\n                    cmap=\"RdYlBu_r\",\n                    norm=norm,\n                    alpha=self.alpha * 0.8,\n                )\n                # 2D points\n                ax.scatter(\n                    x,\n                    z,\n                    s=self.scatter_size,\n                    c=color,\n                    cmap=\"RdYlBu_r\",\n                    norm=norm,\n                    alpha=self.alpha * 0.8,\n                )\n        else:\n            n_lines += 1\n\n            # Plot 3D\n            # Get segments for all lines\n            points3D = np.array([x, y, z]).T.reshape(-1, 1, 3)\n            segments3D = np.concatenate([points3D[:-1], points3D[1:]], axis=1)\n\n            # Construct lines from segments\n            lc3D = Line3DCollection(\n                segments3D,  # type: ignore\n                cmap=\"RdYlBu_r\",\n                norm=norm,\n                alpha=self.alpha,\n            )\n            lc3D.set_array(color)\n\n            # Draw lines\n            ax3D.add_collection3d(lc3D)  # type: ignore\n\n            # Plot 2D\n            # Get segments for all lines\n            points = np.array([x, z]).T.reshape(-1, 1, 2)\n            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n\n            # Construct lines from segments\n            lc = LineCollection(\n                segments,  # type: ignore\n                cmap=\"RdYlBu_r\",\n                norm=norm,\n                alpha=self.alpha,  # type: ignore\n            )\n            lc.set_array(color)\n\n            # Draw lines\n            ax.add_collection(lc)\n\n    assert self.loss_range is not None\n    assert self.epochs_range is not None\n\n    ax3D.axes.set_xlim3d(left=self.epochs_range[0], right=self.epochs_range[1])  # type: ignore\n    ax3D.axes.set_ylim3d(bottom=0, top=data_groups.ngroups)  # type: ignore\n    ax3D.axes.set_zlim3d(bottom=self.loss_range[0], top=self.loss_range[1])  # type: ignore\n\n    ax3D.set_xlabel(\"Epochs\")\n    ax3D.set_ylabel(\"Iteration sampled\")\n    ax3D.set_zlabel(f\"{self.loss_key}\")  # type: ignore\n\n    # set view angle\n    ax3D.view_init(elev=self.view_angle[0], azim=self.view_angle[1])  # type: ignore\n\n    ax.autoscale_view()\n    ax.set_xlabel(self.fidelity_key)\n    ax.set_ylabel(f\"{self.loss_key}\")\n    ax.set_facecolor(self.bck_color_2d)\n    fig.suptitle(\"ifBO run\")\n\n    if self.footnote:\n        fig.text(\n            0.01,\n            0.02,\n            f\"Total {n_lines + n_points} configs evaluated; for multiple budgets: \"\n            f\"{n_lines}, for single budget: {n_points}\",\n            ha=\"left\",\n            va=\"bottom\",\n            fontsize=10,\n        )\n\n    plt.colorbar(\n        cm.ScalarMappable(norm=norm, cmap=\"RdYlBu_r\"),\n        cax=cax,\n        label=\"Iteration\",\n        use_gridspec=True,\n        alpha=self.alpha,\n    )\n    fig.tight_layout()\n\n    self.save(save_path, filename)\n    plt.close(fig)\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.prep_df","title":"prep_df","text":"<pre><code>prep_df(df: DataFrame | None = None) -&gt; DataFrame\n</code></pre> <p>Prepare the dataframe for plotting.</p> Source code in <code>neps/plot/plot3D.py</code> <pre><code>def prep_df(self, df: pd.DataFrame | None = None) -&gt; pd.DataFrame:\n    \"\"\"Prepare the dataframe for plotting.\"\"\"\n    df = self.df if df is None else df\n\n    _fid_key = f\"config.{self.fidelity_key}\"\n    self.loss_range = (df[\"result.loss\"].min(), df[\"result.loss\"].max())  # type: ignore\n    self.epochs_range = (df[_fid_key].min(), df[_fid_key].max())  # type: ignore\n\n    split_values = np.array([[*index.split(\"_\")] for index in df.index])\n    df[[\"configID\", \"epochID\"]] = split_values\n    df.configID = df.configID.astype(int)\n    df.epochID = df.epochID.astype(int)\n    if df.epochID.min() == 0:\n        df.epochID += 1\n\n    # indices become sampling order\n    time_cols = [\"metadata.time_started\", \"metadata.time_end\"]\n    return df.sort_values(by=time_cols).reset_index(drop=True)\n</code></pre>"},{"location":"api/neps/plot/plot3D/#neps.plot.plot3D.Plotter3D.save","title":"save","text":"<pre><code>save(\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None\n</code></pre> <p>Save the plot to a file.</p> Source code in <code>neps/plot/plot3D.py</code> <pre><code>def save(\n    self,\n    save_path: str | Path | None = None,\n    filename: str = \"freeze_thaw\",\n) -&gt; None:\n    \"\"\"Save the plot to a file.\"\"\"\n    path = save_path if save_path is not None else self.run_path\n    assert path is not None\n\n    run_path = Path(path)\n    run_path.mkdir(parents=True, exist_ok=True)\n    assert run_path.is_dir()\n    plot_path = run_path / f\"Plot3D_{filename}.png\"\n\n    plt.savefig(plot_path, bbox_inches=\"tight\")\n</code></pre>"},{"location":"api/neps/plot/plotting/","title":"Plotting","text":""},{"location":"api/neps/plot/plotting/#neps.plot.plotting","title":"neps.plot.plotting","text":"<p>Plotting functions for incumbent trajectory plots.</p>"},{"location":"api/neps/plot/read_results/","title":"Read results","text":""},{"location":"api/neps/plot/read_results/#neps.plot.read_results","title":"neps.plot.read_results","text":"<p>Utility functions for reading and processing results.</p>"},{"location":"api/neps/plot/read_results/#neps.plot.read_results.process_seed","title":"process_seed","text":"<pre><code>process_seed(\n    *,\n    path: str | Path,\n    seed: str | int | None,\n    key_to_extract: str | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1\n) -&gt; tuple[list[float], list[float], float]\n</code></pre> <p>Reads and processes data per seed.</p> Source code in <code>neps/plot/read_results.py</code> <pre><code>def process_seed(\n    *,\n    path: str | Path,\n    seed: str | int | None,\n    key_to_extract: str | None = None,\n    consider_continuations: bool = False,\n    n_workers: int = 1,\n) -&gt; tuple[list[float], list[float], float]:\n    \"\"\"Reads and processes data per seed.\"\"\"\n    path = Path(path)\n    if seed is not None:\n        path = path / str(seed) / \"neps_root_directory\"\n\n    stats, _ = neps.status(path, print_summary=False)\n    sorted_stats = sorted(sorted(stats.items()), key=lambda x: len(x[0]))\n    stats = OrderedDict(sorted_stats)\n\n    # max_cost only relevant for scaling x-axis when using fidelity on the x-axis\n    max_cost: float = -1.0\n    if key_to_extract == \"fidelity\":\n        # TODO(eddiebergman): This can crash for a number of reasons, namely if the config\n        # crased and it's result is an error, or if the `\"info_dict\"` and/or\n        # `key_to_extract` doesn't exist\n        max_cost = max(s.result[\"info_dict\"][key_to_extract] for s in stats.values())  # type: ignore\n\n    global_start = stats[min(stats.keys())].metadata[\"time_sampled\"]\n\n    def get_cost(idx: str) -&gt; float:\n        if key_to_extract is not None:\n            # TODO(eddiebergman): This can crash for a number of reasons, namely if the\n            # config crased and it's result is an error, or if the `\"info_dict\"` and/or\n            # `key_to_extract` doesn't exist\n            return float(stats[idx].result[\"info_dict\"][key_to_extract])  # type: ignore\n\n        return 1.0\n\n    losses = []\n    costs = []\n\n    for config_id, config_result in stats.items():\n        config_cost = get_cost(config_id)\n        if consider_continuations:\n            if n_workers == 1:\n                # calculates continuation costs for MF algorithms NOTE: assumes that\n                # all recorded evaluations are black-box evaluations where\n                # continuations or freeze-thaw was not accounted for during optimization\n                if \"previous_config_id\" in config_result.metadata:\n                    previous_config_id = config_result.metadata[\"previous_config_id\"]\n                    config_cost -= get_cost(previous_config_id)\n            else:\n                config_cost = config_result.metadata[\"time_end\"] - global_start\n\n        # TODO(eddiebergman): Assumes it never crashed and there's a loss available,\n        # not fixing now but it should be addressed\n        losses.append(config_result.result[\"loss\"])  # type: ignore\n        costs.append(config_cost)\n\n    return list(np.minimum.accumulate(losses)), costs, max_cost\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/","title":"Tensorboard eval","text":""},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval","title":"neps.plot.tensorboard_eval","text":"<p>The tblogger module provides a simplified interface for logging to TensorBoard.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.SummaryWriter_","title":"SummaryWriter_","text":"<p>               Bases: <code>SummaryWriter</code></p> <p>This class inherits from the base SummaryWriter class and provides modifications to improve the logging. It simplifies the logging structure and ensures consistent tag formatting for metrics.</p> <p>Changes Made: - Avoids creating unnecessary subfolders in the log directory. - Ensures all logs are stored in the same 'tfevent' directory for   better organization. - Updates metric keys to have a consistent 'Summary/' prefix for clarity. - Improves the display of 'Loss' or 'Accuracy' on the Summary file.</p> <p>Methods: - add_hparams: Overrides the base method to log hyperparameters and metrics with better formatting.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.SummaryWriter_.add_hparams","title":"add_hparams","text":"<pre><code>add_hparams(\n    hparam_dict: dict[str, Any],\n    metric_dict: dict[str, Any],\n    global_step: int,\n) -&gt; None\n</code></pre> <p>Add a set of hyperparameters to be logged.</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@override\ndef add_hparams(  # type: ignore\n    self,\n    hparam_dict: dict[str, Any],\n    metric_dict: dict[str, Any],\n    global_step: int,\n) -&gt; None:\n    \"\"\"Add a set of hyperparameters to be logged.\"\"\"\n    if not isinstance(hparam_dict, dict) or not isinstance(metric_dict, dict):\n        raise TypeError(\"hparam_dict and metric_dict should be dictionary.\")\n    updated_metric = {f\"Summary/{key}\": val for key, val in metric_dict.items()}\n    exp, ssi, sei = hparams(hparam_dict, updated_metric)\n\n    assert self.file_writer is not None\n    self.file_writer.add_summary(exp)\n    self.file_writer.add_summary(ssi)\n    self.file_writer.add_summary(sei)\n    for k, v in updated_metric.items():\n        self.add_scalar(tag=k, scalar_value=v, global_step=global_step)\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger","title":"tblogger","text":"<p>The tblogger class provides a simplified interface for logging to tensorboard.</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.disable","title":"disable  <code>staticmethod</code>","text":"<pre><code>disable() -&gt; None\n</code></pre> <p>The function allows for disabling the logger functionality. When the logger is disabled, it will not perform logging operations.</p> <p>By default tblogger is enabled when used.</p> Example Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef disable() -&gt; None:\n    \"\"\"The function allows for disabling the logger functionality.\n    When the logger is disabled, it will not perform logging operations.\n\n    By default tblogger is enabled when used.\n\n    Example:\n        # Disable the logger\n        tblogger.disable()\n    \"\"\"\n    tblogger.disable_logging = True\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.disable--disable-the-logger","title":"Disable the logger","text":"<p>tblogger.disable()</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.enable","title":"enable  <code>staticmethod</code>","text":"<pre><code>enable() -&gt; None\n</code></pre> <p>The function allows for enabling the logger functionality. When the logger is enabled, it will perform the logging operations.</p> <p>By default this is enabled.</p> Example Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef enable() -&gt; None:\n    \"\"\"The function allows for enabling the logger functionality.\n    When the logger is enabled, it will perform the logging operations.\n\n    By default this is enabled.\n\n    Example:\n        # Enable the logger\n        tblogger.enable()\n    \"\"\"\n    tblogger.disable_logging = False\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.enable--enable-the-logger","title":"Enable the logger","text":"<p>tblogger.enable()</p>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.end_of_config","title":"end_of_config  <code>staticmethod</code>","text":"<pre><code>end_of_config(trial: Trial) -&gt; None\n</code></pre> <p>Closes the writer.</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef end_of_config(trial: Trial) -&gt; None:  # noqa: ARG004\n    \"\"\"Closes the writer.\"\"\"\n    if tblogger.config_writer:\n        # Close and reset previous config writers for consistent logging.\n        # Prevent conflicts by reinitializing writers when logging ongoing.\n        tblogger.config_writer.close()\n        tblogger.config_writer = None\n\n    if tblogger.write_incumbent:\n        tblogger._tracking_incumbent_api()\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.get_status","title":"get_status  <code>staticmethod</code>","text":"<pre><code>get_status() -&gt; bool\n</code></pre> <p>Returns the currect state of tblogger ie. whether the logger is enabled or not.</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef get_status() -&gt; bool:\n    \"\"\"Returns the currect state of tblogger ie. whether the logger is\n    enabled or not.\n    \"\"\"\n    return not tblogger.disable_logging\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.image_logging","title":"image_logging  <code>staticmethod</code>","text":"<pre><code>image_logging(\n    image: Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | RandomState | None = None\n) -&gt; tuple[\n    str,\n    Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | RandomState | None,\n]\n</code></pre> <p>Prepare an image tensor for logging.</p> PARAMETER DESCRIPTION <code>image</code> <p>Image tensor to be logged.</p> <p> TYPE: <code>Tensor</code> </p> <code>counter</code> <p>Counter value associated with the images.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>resize_images</code> <p>List of integers for image sizes after resizing.</p> <p> TYPE: <code>list[None | int] | None</code> DEFAULT: <code>None</code> </p> <code>random_images</code> <p>Images are randomly selected if True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_images</code> <p>Number of images to log.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>seed</code> <p>Seed value or RandomState instance to control randomness.</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[str, Tensor, int, list[None | int] | None, bool, int, int | RandomState | None]</code> <p>A tuple containing the logging mode and all the necessary parameters for image logging. Tuple: (logging_mode, img_tensor, counter, resize_images,                 random_images, num_images, seed).</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef image_logging(\n    image: torch.Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | np.random.RandomState | None = None,\n) -&gt; tuple[\n    str,\n    torch.Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | np.random.RandomState | None,\n]:\n    \"\"\"Prepare an image tensor for logging.\n\n    Args:\n        image: Image tensor to be logged.\n        counter: Counter value associated with the images.\n        resize_images: List of integers for image sizes after resizing.\n        random_images: Images are randomly selected if True.\n        num_images: Number of images to log.\n        seed: Seed value or RandomState instance to control randomness.\n\n    Returns:\n        A tuple containing the logging mode and all the necessary parameters for\n        image logging.\n        Tuple: (logging_mode, img_tensor, counter, resize_images,\n                        random_images, num_images, seed).\n    \"\"\"\n    logging_mode = \"image\"\n    return (\n        logging_mode,\n        image,\n        counter,\n        resize_images,\n        random_images,\n        num_images,\n        seed,\n    )\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.log","title":"log  <code>staticmethod</code>","text":"<pre><code>log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None\n) -&gt; None\n</code></pre> <p>Log experiment data to the logger, including scalar values, hyperparameters, and images.</p> PARAMETER DESCRIPTION <code>loss</code> <p>Current loss value.</p> <p> TYPE: <code>float</code> </p> <code>current_epoch</code> <p>Current epoch of the experiment (used as the global step).</p> <p> TYPE: <code>int</code> </p> <code>writer_config_scalar</code> <p>Displaying the loss or accuracy curve on tensorboard (default: True)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>writer_config_hparam</code> <p>Write hyperparameters logging of the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>write_summary_incumbent</code> <p>Set to <code>True</code> for a live incumbent trajectory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>extra_data</code> <p>Additional experiment data for logging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None,\n) -&gt; None:\n    \"\"\"Log experiment data to the logger, including scalar values,\n    hyperparameters, and images.\n\n    Args:\n        loss: Current loss value.\n        current_epoch: Current epoch of the experiment (used as the global step).\n        writer_config_scalar: Displaying the loss or accuracy\n            curve on tensorboard (default: True)\n        writer_config_hparam: Write hyperparameters logging of the configs.\n        write_summary_incumbent: Set to `True` for a live incumbent trajectory.\n        extra_data: Additional experiment data for logging.\n    \"\"\"\n    if tblogger.disable_logging:\n        return\n\n    tblogger.current_epoch = current_epoch\n    tblogger.loss = loss\n    tblogger.write_incumbent = write_summary_incumbent\n\n    tblogger._initiate_internal_configurations()\n\n    if writer_config_scalar:\n        tblogger._write_scalar_config(tag=\"Loss\", value=loss)\n\n    if writer_config_hparam:\n        tblogger._write_hparam_config()\n\n    if extra_data is not None:\n        for key in extra_data:\n            if extra_data[key][0] == \"scalar\":\n                tblogger._write_scalar_config(tag=str(key), value=extra_data[key][1])\n\n            elif extra_data[key][0] == \"image\":\n                tblogger._write_image_config(\n                    tag=str(key),\n                    image=extra_data[key][1],\n                    counter=extra_data[key][2],\n                    resize_images=extra_data[key][3],\n                    random_images=extra_data[key][4],\n                    num_images=extra_data[key][5],\n                    seed=extra_data[key][6],\n                )\n</code></pre>"},{"location":"api/neps/plot/tensorboard_eval/#neps.plot.tensorboard_eval.tblogger.scalar_logging","title":"scalar_logging  <code>staticmethod</code>","text":"<pre><code>scalar_logging(value: float) -&gt; tuple[str, float]\n</code></pre> <p>Prepare a scalar value for logging.</p> PARAMETER DESCRIPTION <code>value</code> <p>The scalar value to be logged.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tuple</code> <p>A tuple containing the logging mode and the value for logging.     The tuple format is (logging_mode, value).</p> <p> TYPE: <code>tuple[str, float]</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef scalar_logging(value: float) -&gt; tuple[str, float]:\n    \"\"\"Prepare a scalar value for logging.\n\n    Args:\n        value (float): The scalar value to be logged.\n\n    Returns:\n        Tuple: A tuple containing the logging mode and the value for logging.\n            The tuple format is (logging_mode, value).\n    \"\"\"\n    logging_mode = \"scalar\"\n    return (logging_mode, value)\n</code></pre>"},{"location":"api/neps/sampling/distributions/","title":"Distributions","text":""},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions","title":"neps.sampling.distributions","text":"<p>Custom distributions for NEPS.</p>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.TorchDistributionWithDomain","title":"TorchDistributionWithDomain  <code>dataclass</code>","text":"<pre><code>TorchDistributionWithDomain(\n    distribution: Distribution, domain: Domain\n)\n</code></pre> <p>A torch distribution with an associated domain it samples over.</p>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.TruncatedNormal","title":"TruncatedNormal","text":"<pre><code>TruncatedNormal(\n    loc: float | Tensor,\n    scale: float | Tensor,\n    a: float | Tensor,\n    b: float | Tensor,\n    validate_args: bool | None = None,\n    device: device | None = None,\n)\n</code></pre> <p>               Bases: <code>TruncatedStandardNormal</code></p> <p>Truncated Normal distribution.</p> <p>people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf</p> PARAMETER DESCRIPTION <code>loc</code> <p>The mean of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>scale</code> <p>The std of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>a</code> <p>The lower bound of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>b</code> <p>The upper bound of the distribution.</p> <p> TYPE: <code>float | Tensor</code> </p> <code>validate_args</code> <p>Whether to validate input.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to use.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/sampling/distributions.py</code> <pre><code>def __init__(\n    self,\n    loc: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    a: float | torch.Tensor,\n    b: float | torch.Tensor,\n    validate_args: bool | None = None,\n    device: torch.device | None = None,\n):\n    \"\"\"Initialize a truncated standard normal distribution.\n\n    Args:\n        loc: The mean of the distribution.\n        scale: The std of the distribution.\n        a: The lower bound of the distribution.\n        b: The upper bound of the distribution.\n        validate_args: Whether to validate input.\n        device: Device to use.\n    \"\"\"\n    scale = torch.as_tensor(scale, device=device)\n    scale = scale.clamp_min(self.eps)\n\n    self.loc, self.scale, a, b = broadcast_all(loc, scale, a, b)\n    a = a.to(device)  # type: ignore\n    b = b.to(device)  # type: ignore\n    self._non_std_a = a\n    self._non_std_b = b\n    a = (a - self.loc) / self.scale\n    b = (b - self.loc) / self.scale\n    super().__init__(a, b, validate_args=validate_args)  # type: ignore\n    self._log_scale = self.scale.log()\n    self._mean = self._mean * self.scale + self.loc\n    self._variance = self._variance * self.scale**2\n    self._entropy += self._log_scale\n</code></pre>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.TruncatedStandardNormal","title":"TruncatedStandardNormal","text":"<pre><code>TruncatedStandardNormal(\n    a: Tensor,\n    b: Tensor,\n    validate_args: bool | None = None,\n    device: device | None = None,\n)\n</code></pre> <p>               Bases: <code>Distribution</code></p> <p>Truncated Standard Normal distribution.</p> <p>Source: people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf</p> PARAMETER DESCRIPTION <code>a</code> <p>Lower truncation bound.</p> <p> TYPE: <code>Tensor</code> </p> <code>b</code> <p>Upper truncation bound.</p> <p> TYPE: <code>Tensor</code> </p> <code>validate_args</code> <p>Whether to validate input.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to use.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/sampling/distributions.py</code> <pre><code>def __init__(\n    self,\n    a: torch.Tensor,\n    b: torch.Tensor,\n    validate_args: bool | None = None,\n    device: torch.device | None = None,\n):\n    \"\"\"Initialize a truncated standard normal distribution.\n\n    Args:\n        a: Lower truncation bound.\n        b: Upper truncation bound.\n        validate_args: Whether to validate input.\n        device: Device to use.\n    \"\"\"\n    self.a, self.b = broadcast_all(a, b)\n    self.a = self.a.to(device)\n    self.b = self.b.to(device)\n\n    if isinstance(a, Number) and isinstance(b, Number):\n        batch_shape = torch.Size()\n    else:\n        batch_shape = self.a.size()\n\n    super().__init__(batch_shape, validate_args=validate_args)\n\n    if self.a.dtype != self.b.dtype:\n        raise ValueError(\"Truncation bounds types are different\")\n\n    if any((self.a &gt;= self.b).view(-1).tolist()):\n        raise ValueError(\"Incorrect truncation range\")\n\n    eps = self.eps\n    self._dtype_min_gt_0 = eps\n    self._dtype_max_lt_1 = 1 - eps\n    self._little_phi_a = self._little_phi(self.a)\n    self._little_phi_b = self._little_phi(self.b)\n    self._big_phi_a = self._big_phi(self.a)\n    self._big_phi_b = self._big_phi(self.b)\n    self._Z = (self._big_phi_b - self._big_phi_a).clamp(eps, 1 - eps)\n    self._log_Z = self._Z.log()\n    little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n    little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n    self._lpbb_m_lpaa_d_Z = (\n        self._little_phi_b * little_phi_coeff_b\n        - self._little_phi_a * little_phi_coeff_a\n    ) / self._Z\n    self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n    self._variance = (\n        1\n        - self._lpbb_m_lpaa_d_Z\n        - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n    )\n    self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n</code></pre>"},{"location":"api/neps/sampling/distributions/#neps.sampling.distributions.UniformWithUpperBound","title":"UniformWithUpperBound","text":"<p>               Bases: <code>Uniform</code></p> <p>Uniform distribution with upper bound inclusive.</p> <p>This is mostly a hack because torch's version of Uniform does not include the upper bound which only causes a problem when considering the log_prob. Otherwise the upper bound works with every other method.</p>"},{"location":"api/neps/sampling/priors/","title":"Priors","text":""},{"location":"api/neps/sampling/priors/#neps.sampling.priors","title":"neps.sampling.priors","text":"<p>Priors for search spaces.</p> <p>Loosely speaking, they are joint distributions over multiple independent variables, i.e. each column of a tensor is assumed to be independent and can be acted on independently.</p> <p>See the class doc description of <code>Prior</code> for more details.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior","title":"CenteredPrior  <code>dataclass</code>","text":"<pre><code>CenteredPrior(\n    distributions: list[TorchDistributionWithDomain],\n)\n</code></pre> <p>               Bases: <code>Prior</code></p> <p>A prior that is centered around a given value with a given confidence.</p> <p>This prior is useful for creating priors for search spaces where the values are centered around a given value with a given confidence level.</p> <p>You can use a <code>torch.distribution.Uniform</code> for any values which do not have a center and confidence level, i.e. no prior information.</p> <p>You can create this class more easily using <code>Prior.make_centered()</code>.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.distributions","title":"distributions  <code>instance-attribute</code>","text":"<pre><code>distributions: list[TorchDistributionWithDomain]\n</code></pre> <p>Distributions along with the corresponding domains they sample from.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.from_domains_and_centers","title":"from_domains_and_centers  <code>classmethod</code>","text":"<pre><code>from_domains_and_centers(\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: device | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior for a given list of domains.</p> <p>Will use a <code>TruncatedNormal</code> distribution for all parameters, except those who have a domain marked with <code>is_categorical=True</code>, using a <code>Categorical</code> distribution instead. If the center for a given domain is <code>None</code>, a uniform prior will be used instead.</p> <p>For non-categoricals, this will be interpreted as the mean and std <code>(1 - confidence)</code> for a truncnorm. For categorical values, the center will contain a probability mass of <code>confidence</code> with the remaining <code>(1 - confidence)</code> probability mass distributed uniformly amongest the other choices.</p> <p>The order of the items in <code>domains</code> matters and should align with any tensors that you will use to evaluate from the prior. I.e. the first domain in <code>domains</code> will be the first column of a tensor that this prior can be used on.</p> PARAMETER DESCRIPTION <code>domains</code> <p>domains over which to have a centered prior.</p> <p> TYPE: <code>Iterable[Domain] | ConfigEncoder</code> </p> <code>centers</code> <p>centers for the priors, i.e. the mode of the prior for that domain, along with the confidence of that mode, which get's re-interpreted as the std of the truncnorm or the probability mass for the categorical.</p> <p>If <code>None</code>, a uniform prior will be used.</p> <p>Warning</p> <p>The values contained in centers should be contained within the domain. All confidence levels should be within the <code>[0, 1]</code> range.</p> <p> TYPE: <code>Iterable[None | tuple[int | float, float]]</code> </p> <code>confidence</code> <p>The confidence level for the center. Entries containing <code>None</code> should match with <code>centers</code> that are <code>None</code>. If not, this is considered an error.</p> <p> </p> <code>device</code> <p>Device to place the tensors on for distributions.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>A prior for the search space.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_domains_and_centers(\n    cls,\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: torch.device | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior for a given list of domains.\n\n    Will use a `TruncatedNormal` distribution for all parameters,\n    except those who have a domain marked with `is_categorical=True`,\n    using a `Categorical` distribution instead.\n    If the center for a given domain is `None`, a uniform prior\n    will be used instead.\n\n    For non-categoricals, this will be interpreted as the mean and\n    std `(1 - confidence)` for a truncnorm. For categorical values,\n    the _center_ will contain a probability mass of `confidence` with\n    the remaining `(1 - confidence)` probability mass distributed uniformly\n    amongest the other choices.\n\n    The order of the items in `domains` matters and should align\n    with any tensors that you will use to evaluate from the prior.\n    I.e. the first domain in `domains` will be the first column\n    of a tensor that this prior can be used on.\n\n    Args:\n        domains: domains over which to have a centered prior.\n        centers: centers for the priors, i.e. the mode of the prior for that\n            domain, along with the confidence of that mode, which get's\n            re-interpreted as the std of the truncnorm or the probability\n            mass for the categorical.\n\n            If `None`, a uniform prior will be used.\n\n            !!! warning\n\n                The values contained in centers should be contained within the\n                domain. All confidence levels should be within the `[0, 1]` range.\n\n        confidence: The confidence level for the center. Entries containing `None`\n            should match with `centers` that are `None`. If not, this is considered an\n            error.\n        device: Device to place the tensors on for distributions.\n\n    Returns:\n        A prior for the search space.\n    \"\"\"\n    match domains:\n        case ConfigEncoder():\n            domains = domains.domains\n        case _:\n            domains = list(domains)\n\n    distributions: list[TorchDistributionWithDomain] = []\n    for domain, center_conf in zip(domains, centers, strict=True):\n        # If the center is None, we use a uniform distribution. We try to match\n        # the distributions to all be unit uniform as it can speed up sampling when\n        # consistentaly the same. This still works for categoricals\n        if center_conf is None:\n            distributions.append(UNIT_UNIFORM_DIST)\n            continue\n\n        center, conf = center_conf\n        assert 0 &lt;= conf &lt;= 1\n\n        # If categorical, treat it as a weighted distribution over integers\n        if domain.is_categorical:\n            domain_as_ints = domain.as_integer_domain()\n            assert domain_as_ints.cardinality is not None\n\n            weight_for_choice = conf\n            remaining_weight = 1 - weight_for_choice\n\n            distributed_weight = remaining_weight / (domain_as_ints.cardinality - 1)\n            weights = torch.full(\n                (domain_as_ints.cardinality,),\n                distributed_weight,\n                device=device,\n                dtype=torch.float64,\n            )\n            center_index = domain_as_ints.cast_one(center, frm=domain)\n            weights[int(center_index)] = conf\n\n            dist = TorchDistributionWithDomain(\n                distribution=torch.distributions.Categorical(\n                    probs=weights, validate_args=False\n                ),\n                domain=domain,\n            )\n            distributions.append(dist)\n            continue\n\n        # Otherwise, we use a continuous truncnorm\n        unit_center = domain.to_unit_one(center)\n        scale = torch.tensor(1 - conf, device=device, dtype=torch.float64)\n        a = torch.tensor(0.0, device=device, dtype=torch.float64)\n        b = torch.tensor(1.0, device=device, dtype=torch.float64)\n        dist = TorchDistributionWithDomain(\n            distribution=TruncatedNormal(\n                loc=unit_center,\n                scale=scale,\n                a=a,\n                b=b,\n                device=device,\n                validate_args=False,\n            ),\n            domain=UNIT_FLOAT_DOMAIN,\n        )\n        distributions.append(dist)\n\n    return CenteredPrior(distributions=distributions)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Please refer to <code>from_space()</code> for more details.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Please refer to [`from_space()`][neps.priors.Prior.from_space]\n    for more details.\n    \"\"\"\n    # TODO: This needs to be moved to the search space class, however\n    # to not break the current prior based APIs used elsewhere, we can\n    # just manually create this here.\n    # We use confidence here where `0` means no confidence and `1` means\n    # absolute confidence. This gets translated in to std's and weights\n    # accordingly in a `CenteredPrior`\n    _mapping = {\"low\": 0.25, \"medium\": 0.5, \"high\": 0.75}\n\n    center_values = center_values or {}\n    confidence_values = confidence_values or {}\n    domains: list[Domain] = []\n    centers: list[tuple[Any, float] | None] = []\n    for name, hp in parameters.items():\n        domains.append(hp.domain)\n\n        default = center_values.get(name, hp.default)\n        if default is None:\n            centers.append(None)\n            continue\n\n        confidence_score = confidence_values.get(\n            name,\n            _mapping[hp.default_confidence_choice],\n        )\n        center = hp.choices.index(default) if isinstance(hp, Categorical) else default\n        centers.append((center, confidence_score))\n\n    return Prior.from_domains_and_centers(domains=domains, centers=centers)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.from_space","title":"from_space  <code>classmethod</code>","text":"<pre><code>from_space(\n    space: SearchSpace,\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n    include_fidelity: bool = False\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior distribution from a search space.</p> <p>Takes care to insert things in the correct order.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to createa a prior from. Will look at the <code>.default</code> and <code>.default_confidence</code> of the parameters to create a truncated normal. Any parameters that do not have a <code>.default</code> will be covered by a uniform distribution.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>center_values</code> <p>Any additional values that should be used for centering the prior. Overwrites whatever is set by default in the <code>space</code></p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>confidence_values</code> <p>Any additional values that should be used for determining the strength of the prior. Values should be between 0 and 1. Overwrites whatever is set by default in the <code>space</code>.</p> <p> TYPE: <code>Mapping[str, float] | None</code> DEFAULT: <code>None</code> </p> <code>include_fidelity</code> <p>Whether to include computing the prior over the fidelity of te search space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>The prior distribution</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_space(\n    cls,\n    space: SearchSpace,\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n    include_fidelity: bool = False,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior distribution from a search space.\n\n    Takes care to insert things in the correct order.\n\n    Args:\n        space: The search space to createa a prior from. Will look\n            at the `.default` and `.default_confidence` of the parameters\n            to create a truncated normal.\n            Any parameters that do not have a `.default` will be covered by\n            a uniform distribution.\n        center_values: Any additional values that should be used\n            for centering the prior. Overwrites whatever is set by default\n            in the `space`\n        confidence_values: Any additional values that should be\n            used for determining the strength of the prior. Values should\n            be between 0 and 1. Overwrites whatever is set by default in\n            the `space`.\n        include_fidelity: Whether to include computing the prior over the\n            fidelity of te search space.\n\n    Returns:\n        The prior distribution\n    \"\"\"\n    params = {**space.numerical, **space.categoricals}\n    if include_fidelity:\n        params.update(space.fidelities)\n\n    return Prior.from_parameters(\n        params,\n        center_values=center_values,\n        confidence_values=confidence_values,\n    )\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.pdf","title":"pdf","text":"<pre><code>pdf(\n    x: Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>def pdf(\n    self, x: torch.Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.priors.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return torch.exp(self.log_pdf(x, frm=frm))\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.sample_one","title":"sample_one","text":"<pre><code>sample_one(\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample a single point and convert it to the given domain.</p> <p>The configuration will be a single dimensional tensor of shape <code>(ncols,)</code>.</p> <p>Please see <code>sample</code> for more details.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>def sample_one(\n    self,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a single point and convert it to the given domain.\n\n    The configuration will be a single dimensional tensor of shape\n    `(ncols,)`.\n\n    Please see [`sample`][neps.samplers.Sampler.sample] for more details.\n    \"\"\"\n    return self.sample(1, to=to, seed=seed, device=device, dtype=dtype).squeeze(0)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.CenteredPrior.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ncols: int) -&gt; UniformPrior\n</code></pre> <p>Create a uniform prior for a given list of domains.</p> PARAMETER DESCRIPTION <code>ncols</code> <p>The number of columns in the tensor to sample.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef uniform(cls, ncols: int) -&gt; UniformPrior:\n    \"\"\"Create a uniform prior for a given list of domains.\n\n    Args:\n        ncols: The number of columns in the tensor to sample.\n    \"\"\"\n    return UniformPrior(ndim=ncols)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior","title":"Prior","text":"<p>               Bases: <code>Sampler</code></p> <p>A protocol for priors over search spaces.</p> <p>Extends from the <code>Sampler</code> protocol.</p> <p>At it's core, the two methods that need to be implemented are <code>log_pdf</code> and <code>sample</code>. The <code>log_pdf</code> method should return the log probability of a given tensor of samples under its distribution. The <code>sample</code> method should return a tensor of samples from distribution.</p> <p>All values given to the <code>log_pdf</code> and the ones returned from the <code>sample</code> method are assumed to be in the value domain of the prior, i.e. the <code>.domains</code> attribute.</p> <p>Warning</p> <p>The domain in which samples are actually drawn from not necessarily need to match that of the value domain. For example, the <code>UniformPrior</code> class uses a unit uniform distribution to sample from the unit interval before converting samples to the value domain.</p> <p>As a result, the <code>log_pdf</code> and <code>pdf</code> method may not give the same values as you might expect for a distribution over the value domain.</p> <p>For example, consider a value domain <code>[0, 1e9]</code>. You might expect the <code>pdf</code> to be <code>1e-9</code> (1 / 1e9) for any given value inside the domain. However, since the <code>UniformPrior</code> samples from the unit interval, the <code>pdf</code> will actually be <code>1</code> (1 / 1) for any value inside the domain.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.ncols","title":"ncols  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ncols: int\n</code></pre> <p>The number of columns in the samples produced by this sampler.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.from_domains_and_centers","title":"from_domains_and_centers  <code>classmethod</code>","text":"<pre><code>from_domains_and_centers(\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: device | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior for a given list of domains.</p> <p>Will use a <code>TruncatedNormal</code> distribution for all parameters, except those who have a domain marked with <code>is_categorical=True</code>, using a <code>Categorical</code> distribution instead. If the center for a given domain is <code>None</code>, a uniform prior will be used instead.</p> <p>For non-categoricals, this will be interpreted as the mean and std <code>(1 - confidence)</code> for a truncnorm. For categorical values, the center will contain a probability mass of <code>confidence</code> with the remaining <code>(1 - confidence)</code> probability mass distributed uniformly amongest the other choices.</p> <p>The order of the items in <code>domains</code> matters and should align with any tensors that you will use to evaluate from the prior. I.e. the first domain in <code>domains</code> will be the first column of a tensor that this prior can be used on.</p> PARAMETER DESCRIPTION <code>domains</code> <p>domains over which to have a centered prior.</p> <p> TYPE: <code>Iterable[Domain] | ConfigEncoder</code> </p> <code>centers</code> <p>centers for the priors, i.e. the mode of the prior for that domain, along with the confidence of that mode, which get's re-interpreted as the std of the truncnorm or the probability mass for the categorical.</p> <p>If <code>None</code>, a uniform prior will be used.</p> <p>Warning</p> <p>The values contained in centers should be contained within the domain. All confidence levels should be within the <code>[0, 1]</code> range.</p> <p> TYPE: <code>Iterable[None | tuple[int | float, float]]</code> </p> <code>confidence</code> <p>The confidence level for the center. Entries containing <code>None</code> should match with <code>centers</code> that are <code>None</code>. If not, this is considered an error.</p> <p> </p> <code>device</code> <p>Device to place the tensors on for distributions.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>A prior for the search space.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_domains_and_centers(\n    cls,\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: torch.device | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior for a given list of domains.\n\n    Will use a `TruncatedNormal` distribution for all parameters,\n    except those who have a domain marked with `is_categorical=True`,\n    using a `Categorical` distribution instead.\n    If the center for a given domain is `None`, a uniform prior\n    will be used instead.\n\n    For non-categoricals, this will be interpreted as the mean and\n    std `(1 - confidence)` for a truncnorm. For categorical values,\n    the _center_ will contain a probability mass of `confidence` with\n    the remaining `(1 - confidence)` probability mass distributed uniformly\n    amongest the other choices.\n\n    The order of the items in `domains` matters and should align\n    with any tensors that you will use to evaluate from the prior.\n    I.e. the first domain in `domains` will be the first column\n    of a tensor that this prior can be used on.\n\n    Args:\n        domains: domains over which to have a centered prior.\n        centers: centers for the priors, i.e. the mode of the prior for that\n            domain, along with the confidence of that mode, which get's\n            re-interpreted as the std of the truncnorm or the probability\n            mass for the categorical.\n\n            If `None`, a uniform prior will be used.\n\n            !!! warning\n\n                The values contained in centers should be contained within the\n                domain. All confidence levels should be within the `[0, 1]` range.\n\n        confidence: The confidence level for the center. Entries containing `None`\n            should match with `centers` that are `None`. If not, this is considered an\n            error.\n        device: Device to place the tensors on for distributions.\n\n    Returns:\n        A prior for the search space.\n    \"\"\"\n    match domains:\n        case ConfigEncoder():\n            domains = domains.domains\n        case _:\n            domains = list(domains)\n\n    distributions: list[TorchDistributionWithDomain] = []\n    for domain, center_conf in zip(domains, centers, strict=True):\n        # If the center is None, we use a uniform distribution. We try to match\n        # the distributions to all be unit uniform as it can speed up sampling when\n        # consistentaly the same. This still works for categoricals\n        if center_conf is None:\n            distributions.append(UNIT_UNIFORM_DIST)\n            continue\n\n        center, conf = center_conf\n        assert 0 &lt;= conf &lt;= 1\n\n        # If categorical, treat it as a weighted distribution over integers\n        if domain.is_categorical:\n            domain_as_ints = domain.as_integer_domain()\n            assert domain_as_ints.cardinality is not None\n\n            weight_for_choice = conf\n            remaining_weight = 1 - weight_for_choice\n\n            distributed_weight = remaining_weight / (domain_as_ints.cardinality - 1)\n            weights = torch.full(\n                (domain_as_ints.cardinality,),\n                distributed_weight,\n                device=device,\n                dtype=torch.float64,\n            )\n            center_index = domain_as_ints.cast_one(center, frm=domain)\n            weights[int(center_index)] = conf\n\n            dist = TorchDistributionWithDomain(\n                distribution=torch.distributions.Categorical(\n                    probs=weights, validate_args=False\n                ),\n                domain=domain,\n            )\n            distributions.append(dist)\n            continue\n\n        # Otherwise, we use a continuous truncnorm\n        unit_center = domain.to_unit_one(center)\n        scale = torch.tensor(1 - conf, device=device, dtype=torch.float64)\n        a = torch.tensor(0.0, device=device, dtype=torch.float64)\n        b = torch.tensor(1.0, device=device, dtype=torch.float64)\n        dist = TorchDistributionWithDomain(\n            distribution=TruncatedNormal(\n                loc=unit_center,\n                scale=scale,\n                a=a,\n                b=b,\n                device=device,\n                validate_args=False,\n            ),\n            domain=UNIT_FLOAT_DOMAIN,\n        )\n        distributions.append(dist)\n\n    return CenteredPrior(distributions=distributions)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Please refer to <code>from_space()</code> for more details.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Please refer to [`from_space()`][neps.priors.Prior.from_space]\n    for more details.\n    \"\"\"\n    # TODO: This needs to be moved to the search space class, however\n    # to not break the current prior based APIs used elsewhere, we can\n    # just manually create this here.\n    # We use confidence here where `0` means no confidence and `1` means\n    # absolute confidence. This gets translated in to std's and weights\n    # accordingly in a `CenteredPrior`\n    _mapping = {\"low\": 0.25, \"medium\": 0.5, \"high\": 0.75}\n\n    center_values = center_values or {}\n    confidence_values = confidence_values or {}\n    domains: list[Domain] = []\n    centers: list[tuple[Any, float] | None] = []\n    for name, hp in parameters.items():\n        domains.append(hp.domain)\n\n        default = center_values.get(name, hp.default)\n        if default is None:\n            centers.append(None)\n            continue\n\n        confidence_score = confidence_values.get(\n            name,\n            _mapping[hp.default_confidence_choice],\n        )\n        center = hp.choices.index(default) if isinstance(hp, Categorical) else default\n        centers.append((center, confidence_score))\n\n    return Prior.from_domains_and_centers(domains=domains, centers=centers)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.from_space","title":"from_space  <code>classmethod</code>","text":"<pre><code>from_space(\n    space: SearchSpace,\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n    include_fidelity: bool = False\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior distribution from a search space.</p> <p>Takes care to insert things in the correct order.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to createa a prior from. Will look at the <code>.default</code> and <code>.default_confidence</code> of the parameters to create a truncated normal. Any parameters that do not have a <code>.default</code> will be covered by a uniform distribution.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>center_values</code> <p>Any additional values that should be used for centering the prior. Overwrites whatever is set by default in the <code>space</code></p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>confidence_values</code> <p>Any additional values that should be used for determining the strength of the prior. Values should be between 0 and 1. Overwrites whatever is set by default in the <code>space</code>.</p> <p> TYPE: <code>Mapping[str, float] | None</code> DEFAULT: <code>None</code> </p> <code>include_fidelity</code> <p>Whether to include computing the prior over the fidelity of te search space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>The prior distribution</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_space(\n    cls,\n    space: SearchSpace,\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n    include_fidelity: bool = False,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior distribution from a search space.\n\n    Takes care to insert things in the correct order.\n\n    Args:\n        space: The search space to createa a prior from. Will look\n            at the `.default` and `.default_confidence` of the parameters\n            to create a truncated normal.\n            Any parameters that do not have a `.default` will be covered by\n            a uniform distribution.\n        center_values: Any additional values that should be used\n            for centering the prior. Overwrites whatever is set by default\n            in the `space`\n        confidence_values: Any additional values that should be\n            used for determining the strength of the prior. Values should\n            be between 0 and 1. Overwrites whatever is set by default in\n            the `space`.\n        include_fidelity: Whether to include computing the prior over the\n            fidelity of te search space.\n\n    Returns:\n        The prior distribution\n    \"\"\"\n    params = {**space.numerical, **space.categoricals}\n    if include_fidelity:\n        params.update(space.fidelities)\n\n    return Prior.from_parameters(\n        params,\n        center_values=center_values,\n        confidence_values=confidence_values,\n    )\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.log_pdf","title":"log_pdf  <code>abstractmethod</code>","text":"<pre><code>log_pdf(\n    x: Tensor, *, frm: ConfigEncoder | list[Domain] | Domain\n) -&gt; Tensor\n</code></pre> <p>Compute the log pdf of values in <code>x</code> under a prior.</p> <p>The last dimenion of <code>x</code> is assumed to be independent, such that the log pdf of the entire tensor is the sum of the log pdf of each element in that dimension.</p> <p>For example, if <code>x</code> is of shape <code>(n_samples, n_dims)</code>, then the you will be given back a tensor of shape <code>(n_samples,)</code> with the each entry being the log pdf of the corresponding sample.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of shape (..., n_dims) In the case of a 1D tensor, the shape is assumed to be (n_dims,)</p> <p> TYPE: <code>Tensor</code> </p> <code>frm</code> <p>The domain of the values in <code>x</code>. If a single domain, then all the values are assumed to be from that domain, otherwise each column <code>n_dims</code> in (n_samples, n_dims) is from the corresponding domain. If a <code>ConfigEncoder</code> is passed in, it will just take it's domains for use.</p> <p> TYPE: <code>ConfigEncoder | list[Domain] | Domain</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of shape (...,), with the last dimension reduced out. In the case that only single dimensional tensor is passed, the returns value is a scalar.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@abstractmethod\ndef log_pdf(\n    self,\n    x: torch.Tensor,\n    *,\n    frm: ConfigEncoder | list[Domain] | Domain,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the log pdf of values in `x` under a prior.\n\n    The last dimenion of `x` is assumed to be independent, such that the\n    log pdf of the entire tensor is the sum of the log\n    pdf of each element in that dimension.\n\n    For example, if `x` is of shape `(n_samples, n_dims)`, then the\n    you will be given back a tensor of shape `(n_samples,)` with the\n    each entry being the log pdf of the corresponding sample.\n\n    Args:\n        x: Tensor of shape (..., n_dims)\n            In the case of a 1D tensor, the shape is assumed to be (n_dims,)\n        frm: The domain of the values in `x`. If a single domain, then all the\n            values are assumed to be from that domain, otherwise each column\n            `n_dims` in (n_samples, n_dims) is from the corresponding domain.\n            If a `ConfigEncoder` is passed in, it will just take it's domains\n            for use.\n\n    Returns:\n        Tensor of shape (...,), with the last dimension reduced out. In the\n        case that only single dimensional tensor is passed, the returns value\n        is a scalar.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.pdf","title":"pdf","text":"<pre><code>pdf(\n    x: Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>def pdf(\n    self, x: torch.Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.priors.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return torch.exp(self.log_pdf(x, frm=frm))\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(\n    n: int | Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample <code>n</code> points and convert them to the given domain.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of points to sample. If a torch.Size, an additional dimension will be added with <code>.ncols</code>. For example, if <code>n = 5</code>, the output will be <code>(5, ncols)</code>. If <code>n = (5, 3)</code>, the output will be <code>(5, 3, ncols)</code>.</p> <p> TYPE: <code>int | Size</code> </p> <code>to</code> <p>If a single domain, <code>.ncols</code> columns will be produced form that one domain. If a list of domains, then it must have the same length as the number of columns, with each column being in the corresponding domain.</p> <p> TYPE: <code>Domain | list[Domain] | ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the output tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device to cast the samples to.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor of (n, ndim) points sampled cast to the given domain.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    n: int | torch.Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample `n` points and convert them to the given domain.\n\n    Args:\n        n: The number of points to sample. If a torch.Size, an additional dimension\n            will be added with [`.ncols`][neps.samplers.Sampler.ncols].\n            For example, if `n = 5`, the output will be `(5, ncols)`. If\n            `n = (5, 3)`, the output will be `(5, 3, ncols)`.\n        to: If a single domain, `.ncols` columns will be produced form that one\n            domain. If a list of domains, then it must have the same length as the\n            number of columns, with each column being in the corresponding domain.\n        seed: The seed generator\n        dtype: The dtype of the output tensor.\n        device: The device to cast the samples to.\n\n    Returns:\n        A tensor of (n, ndim) points sampled cast to the given domain.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.sample_one","title":"sample_one","text":"<pre><code>sample_one(\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample a single point and convert it to the given domain.</p> <p>The configuration will be a single dimensional tensor of shape <code>(ncols,)</code>.</p> <p>Please see <code>sample</code> for more details.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>def sample_one(\n    self,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a single point and convert it to the given domain.\n\n    The configuration will be a single dimensional tensor of shape\n    `(ncols,)`.\n\n    Please see [`sample`][neps.samplers.Sampler.sample] for more details.\n    \"\"\"\n    return self.sample(1, to=to, seed=seed, device=device, dtype=dtype).squeeze(0)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.Prior.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ncols: int) -&gt; UniformPrior\n</code></pre> <p>Create a uniform prior for a given list of domains.</p> PARAMETER DESCRIPTION <code>ncols</code> <p>The number of columns in the tensor to sample.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef uniform(cls, ncols: int) -&gt; UniformPrior:\n    \"\"\"Create a uniform prior for a given list of domains.\n\n    Args:\n        ncols: The number of columns in the tensor to sample.\n    \"\"\"\n    return UniformPrior(ndim=ncols)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior","title":"UniformPrior  <code>dataclass</code>","text":"<pre><code>UniformPrior(ndim: int)\n</code></pre> <p>               Bases: <code>Prior</code></p> <p>A prior that is uniform over a given domain.</p> <p>Uses a UnitUniform under the hood before converting to the value domain.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.ndim","title":"ndim  <code>instance-attribute</code>","text":"<pre><code>ndim: int\n</code></pre> <p>The number of columns in the tensor to sample from.</p>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.from_domains_and_centers","title":"from_domains_and_centers  <code>classmethod</code>","text":"<pre><code>from_domains_and_centers(\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: device | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior for a given list of domains.</p> <p>Will use a <code>TruncatedNormal</code> distribution for all parameters, except those who have a domain marked with <code>is_categorical=True</code>, using a <code>Categorical</code> distribution instead. If the center for a given domain is <code>None</code>, a uniform prior will be used instead.</p> <p>For non-categoricals, this will be interpreted as the mean and std <code>(1 - confidence)</code> for a truncnorm. For categorical values, the center will contain a probability mass of <code>confidence</code> with the remaining <code>(1 - confidence)</code> probability mass distributed uniformly amongest the other choices.</p> <p>The order of the items in <code>domains</code> matters and should align with any tensors that you will use to evaluate from the prior. I.e. the first domain in <code>domains</code> will be the first column of a tensor that this prior can be used on.</p> PARAMETER DESCRIPTION <code>domains</code> <p>domains over which to have a centered prior.</p> <p> TYPE: <code>Iterable[Domain] | ConfigEncoder</code> </p> <code>centers</code> <p>centers for the priors, i.e. the mode of the prior for that domain, along with the confidence of that mode, which get's re-interpreted as the std of the truncnorm or the probability mass for the categorical.</p> <p>If <code>None</code>, a uniform prior will be used.</p> <p>Warning</p> <p>The values contained in centers should be contained within the domain. All confidence levels should be within the <code>[0, 1]</code> range.</p> <p> TYPE: <code>Iterable[None | tuple[int | float, float]]</code> </p> <code>confidence</code> <p>The confidence level for the center. Entries containing <code>None</code> should match with <code>centers</code> that are <code>None</code>. If not, this is considered an error.</p> <p> </p> <code>device</code> <p>Device to place the tensors on for distributions.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>A prior for the search space.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_domains_and_centers(\n    cls,\n    domains: Iterable[Domain] | ConfigEncoder,\n    centers: Iterable[None | tuple[int | float, float]],\n    *,\n    device: torch.device | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior for a given list of domains.\n\n    Will use a `TruncatedNormal` distribution for all parameters,\n    except those who have a domain marked with `is_categorical=True`,\n    using a `Categorical` distribution instead.\n    If the center for a given domain is `None`, a uniform prior\n    will be used instead.\n\n    For non-categoricals, this will be interpreted as the mean and\n    std `(1 - confidence)` for a truncnorm. For categorical values,\n    the _center_ will contain a probability mass of `confidence` with\n    the remaining `(1 - confidence)` probability mass distributed uniformly\n    amongest the other choices.\n\n    The order of the items in `domains` matters and should align\n    with any tensors that you will use to evaluate from the prior.\n    I.e. the first domain in `domains` will be the first column\n    of a tensor that this prior can be used on.\n\n    Args:\n        domains: domains over which to have a centered prior.\n        centers: centers for the priors, i.e. the mode of the prior for that\n            domain, along with the confidence of that mode, which get's\n            re-interpreted as the std of the truncnorm or the probability\n            mass for the categorical.\n\n            If `None`, a uniform prior will be used.\n\n            !!! warning\n\n                The values contained in centers should be contained within the\n                domain. All confidence levels should be within the `[0, 1]` range.\n\n        confidence: The confidence level for the center. Entries containing `None`\n            should match with `centers` that are `None`. If not, this is considered an\n            error.\n        device: Device to place the tensors on for distributions.\n\n    Returns:\n        A prior for the search space.\n    \"\"\"\n    match domains:\n        case ConfigEncoder():\n            domains = domains.domains\n        case _:\n            domains = list(domains)\n\n    distributions: list[TorchDistributionWithDomain] = []\n    for domain, center_conf in zip(domains, centers, strict=True):\n        # If the center is None, we use a uniform distribution. We try to match\n        # the distributions to all be unit uniform as it can speed up sampling when\n        # consistentaly the same. This still works for categoricals\n        if center_conf is None:\n            distributions.append(UNIT_UNIFORM_DIST)\n            continue\n\n        center, conf = center_conf\n        assert 0 &lt;= conf &lt;= 1\n\n        # If categorical, treat it as a weighted distribution over integers\n        if domain.is_categorical:\n            domain_as_ints = domain.as_integer_domain()\n            assert domain_as_ints.cardinality is not None\n\n            weight_for_choice = conf\n            remaining_weight = 1 - weight_for_choice\n\n            distributed_weight = remaining_weight / (domain_as_ints.cardinality - 1)\n            weights = torch.full(\n                (domain_as_ints.cardinality,),\n                distributed_weight,\n                device=device,\n                dtype=torch.float64,\n            )\n            center_index = domain_as_ints.cast_one(center, frm=domain)\n            weights[int(center_index)] = conf\n\n            dist = TorchDistributionWithDomain(\n                distribution=torch.distributions.Categorical(\n                    probs=weights, validate_args=False\n                ),\n                domain=domain,\n            )\n            distributions.append(dist)\n            continue\n\n        # Otherwise, we use a continuous truncnorm\n        unit_center = domain.to_unit_one(center)\n        scale = torch.tensor(1 - conf, device=device, dtype=torch.float64)\n        a = torch.tensor(0.0, device=device, dtype=torch.float64)\n        b = torch.tensor(1.0, device=device, dtype=torch.float64)\n        dist = TorchDistributionWithDomain(\n            distribution=TruncatedNormal(\n                loc=unit_center,\n                scale=scale,\n                a=a,\n                b=b,\n                device=device,\n                validate_args=False,\n            ),\n            domain=UNIT_FLOAT_DOMAIN,\n        )\n        distributions.append(dist)\n\n    return CenteredPrior(distributions=distributions)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None\n) -&gt; CenteredPrior\n</code></pre> <p>Please refer to <code>from_space()</code> for more details.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Categorical | Float | Integer],\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n) -&gt; CenteredPrior:\n    \"\"\"Please refer to [`from_space()`][neps.priors.Prior.from_space]\n    for more details.\n    \"\"\"\n    # TODO: This needs to be moved to the search space class, however\n    # to not break the current prior based APIs used elsewhere, we can\n    # just manually create this here.\n    # We use confidence here where `0` means no confidence and `1` means\n    # absolute confidence. This gets translated in to std's and weights\n    # accordingly in a `CenteredPrior`\n    _mapping = {\"low\": 0.25, \"medium\": 0.5, \"high\": 0.75}\n\n    center_values = center_values or {}\n    confidence_values = confidence_values or {}\n    domains: list[Domain] = []\n    centers: list[tuple[Any, float] | None] = []\n    for name, hp in parameters.items():\n        domains.append(hp.domain)\n\n        default = center_values.get(name, hp.default)\n        if default is None:\n            centers.append(None)\n            continue\n\n        confidence_score = confidence_values.get(\n            name,\n            _mapping[hp.default_confidence_choice],\n        )\n        center = hp.choices.index(default) if isinstance(hp, Categorical) else default\n        centers.append((center, confidence_score))\n\n    return Prior.from_domains_and_centers(domains=domains, centers=centers)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.from_space","title":"from_space  <code>classmethod</code>","text":"<pre><code>from_space(\n    space: SearchSpace,\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n    include_fidelity: bool = False\n) -&gt; CenteredPrior\n</code></pre> <p>Create a prior distribution from a search space.</p> <p>Takes care to insert things in the correct order.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to createa a prior from. Will look at the <code>.default</code> and <code>.default_confidence</code> of the parameters to create a truncated normal. Any parameters that do not have a <code>.default</code> will be covered by a uniform distribution.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>center_values</code> <p>Any additional values that should be used for centering the prior. Overwrites whatever is set by default in the <code>space</code></p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>confidence_values</code> <p>Any additional values that should be used for determining the strength of the prior. Values should be between 0 and 1. Overwrites whatever is set by default in the <code>space</code>.</p> <p> TYPE: <code>Mapping[str, float] | None</code> DEFAULT: <code>None</code> </p> <code>include_fidelity</code> <p>Whether to include computing the prior over the fidelity of te search space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>CenteredPrior</code> <p>The prior distribution</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef from_space(\n    cls,\n    space: SearchSpace,\n    *,\n    center_values: Mapping[str, Any] | None = None,\n    confidence_values: Mapping[str, float] | None = None,\n    include_fidelity: bool = False,\n) -&gt; CenteredPrior:\n    \"\"\"Create a prior distribution from a search space.\n\n    Takes care to insert things in the correct order.\n\n    Args:\n        space: The search space to createa a prior from. Will look\n            at the `.default` and `.default_confidence` of the parameters\n            to create a truncated normal.\n            Any parameters that do not have a `.default` will be covered by\n            a uniform distribution.\n        center_values: Any additional values that should be used\n            for centering the prior. Overwrites whatever is set by default\n            in the `space`\n        confidence_values: Any additional values that should be\n            used for determining the strength of the prior. Values should\n            be between 0 and 1. Overwrites whatever is set by default in\n            the `space`.\n        include_fidelity: Whether to include computing the prior over the\n            fidelity of te search space.\n\n    Returns:\n        The prior distribution\n    \"\"\"\n    params = {**space.numerical, **space.categoricals}\n    if include_fidelity:\n        params.update(space.fidelities)\n\n    return Prior.from_parameters(\n        params,\n        center_values=center_values,\n        confidence_values=confidence_values,\n    )\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.pdf","title":"pdf","text":"<pre><code>pdf(\n    x: Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; Tensor\n</code></pre> <p>Compute the pdf of values in <code>x</code> under a prior.</p> <p>See <code>log_pdf()</code> for details on shapes.</p> Source code in <code>neps/sampling/priors.py</code> <pre><code>def pdf(\n    self, x: torch.Tensor, *, frm: ConfigEncoder | Domain | list[Domain]\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pdf of values in `x` under a prior.\n\n    See [`log_pdf()`][neps.priors.Prior.log_pdf] for details on shapes.\n    \"\"\"\n    return torch.exp(self.log_pdf(x, frm=frm))\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.sample_one","title":"sample_one","text":"<pre><code>sample_one(\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample a single point and convert it to the given domain.</p> <p>The configuration will be a single dimensional tensor of shape <code>(ncols,)</code>.</p> <p>Please see <code>sample</code> for more details.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>def sample_one(\n    self,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a single point and convert it to the given domain.\n\n    The configuration will be a single dimensional tensor of shape\n    `(ncols,)`.\n\n    Please see [`sample`][neps.samplers.Sampler.sample] for more details.\n    \"\"\"\n    return self.sample(1, to=to, seed=seed, device=device, dtype=dtype).squeeze(0)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/priors/#neps.sampling.priors.UniformPrior.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ncols: int) -&gt; UniformPrior\n</code></pre> <p>Create a uniform prior for a given list of domains.</p> PARAMETER DESCRIPTION <code>ncols</code> <p>The number of columns in the tensor to sample.</p> <p> TYPE: <code>int</code> </p> Source code in <code>neps/sampling/priors.py</code> <pre><code>@classmethod\ndef uniform(cls, ncols: int) -&gt; UniformPrior:\n    \"\"\"Create a uniform prior for a given list of domains.\n\n    Args:\n        ncols: The number of columns in the tensor to sample.\n    \"\"\"\n    return UniformPrior(ndim=ncols)\n</code></pre>"},{"location":"api/neps/sampling/samplers/","title":"Samplers","text":""},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers","title":"neps.sampling.samplers","text":"<p>Samplers for generating points in a search space.</p> <p>These are similar to <code>Prior</code> objects, but they do not necessarily have an easily definable pdf.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler","title":"BorderSampler  <code>dataclass</code>","text":"<pre><code>BorderSampler(ndim: int)\n</code></pre> <p>               Bases: <code>Sampler</code></p> <p>A sampler that samples from the border of a hypercube.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.n_possible","title":"n_possible  <code>property</code>","text":"<pre><code>n_possible: int\n</code></pre> <p>The amount of possible border configurations.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.sample_one","title":"sample_one","text":"<pre><code>sample_one(\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample a single point and convert it to the given domain.</p> <p>The configuration will be a single dimensional tensor of shape <code>(ncols,)</code>.</p> <p>Please see <code>sample</code> for more details.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>def sample_one(\n    self,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a single point and convert it to the given domain.\n\n    The configuration will be a single dimensional tensor of shape\n    `(ncols,)`.\n\n    Please see [`sample`][neps.samplers.Sampler.sample] for more details.\n    \"\"\"\n    return self.sample(1, to=to, seed=seed, device=device, dtype=dtype).squeeze(0)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.BorderSampler.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; UniformPrior\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>UniformPrior</code> <p>A uniform sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; UniformPrior:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import UniformPrior\n\n    return UniformPrior(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler","title":"Sampler","text":"<p>               Bases: <code>ABC</code></p> <p>A protocol for sampling tensors and vonerting them to a given domain.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.ncols","title":"ncols  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ncols: int\n</code></pre> <p>The number of columns in the samples produced by this sampler.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(\n    n: int | Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample <code>n</code> points and convert them to the given domain.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of points to sample. If a torch.Size, an additional dimension will be added with <code>.ncols</code>. For example, if <code>n = 5</code>, the output will be <code>(5, ncols)</code>. If <code>n = (5, 3)</code>, the output will be <code>(5, 3, ncols)</code>.</p> <p> TYPE: <code>int | Size</code> </p> <code>to</code> <p>If a single domain, <code>.ncols</code> columns will be produced form that one domain. If a list of domains, then it must have the same length as the number of columns, with each column being in the corresponding domain.</p> <p> TYPE: <code>Domain | list[Domain] | ConfigEncoder</code> </p> <code>seed</code> <p>The seed generator</p> <p> TYPE: <code>Generator | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the output tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device to cast the samples to.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor of (n, ndim) points sampled cast to the given domain.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    n: int | torch.Size,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample `n` points and convert them to the given domain.\n\n    Args:\n        n: The number of points to sample. If a torch.Size, an additional dimension\n            will be added with [`.ncols`][neps.samplers.Sampler.ncols].\n            For example, if `n = 5`, the output will be `(5, ncols)`. If\n            `n = (5, 3)`, the output will be `(5, 3, ncols)`.\n        to: If a single domain, `.ncols` columns will be produced form that one\n            domain. If a list of domains, then it must have the same length as the\n            number of columns, with each column being in the corresponding domain.\n        seed: The seed generator\n        dtype: The dtype of the output tensor.\n        device: The device to cast the samples to.\n\n    Returns:\n        A tensor of (n, ndim) points sampled cast to the given domain.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.sample_one","title":"sample_one","text":"<pre><code>sample_one(\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample a single point and convert it to the given domain.</p> <p>The configuration will be a single dimensional tensor of shape <code>(ncols,)</code>.</p> <p>Please see <code>sample</code> for more details.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>def sample_one(\n    self,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a single point and convert it to the given domain.\n\n    The configuration will be a single dimensional tensor of shape\n    `(ncols,)`.\n\n    Please see [`sample`][neps.samplers.Sampler.sample] for more details.\n    \"\"\"\n    return self.sample(1, to=to, seed=seed, device=device, dtype=dtype).squeeze(0)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sampler.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; UniformPrior\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>UniformPrior</code> <p>A uniform sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; UniformPrior:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import UniformPrior\n\n    return UniformPrior(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol","title":"Sobol  <code>dataclass</code>","text":"<pre><code>Sobol(ndim: int, scramble: bool = True)\n</code></pre> <p>               Bases: <code>Sampler</code></p> <p>Sample from a Sobol sequence.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.ndim","title":"ndim  <code>instance-attribute</code>","text":"<pre><code>ndim: int\n</code></pre> <p>The number of dimensions to sample for.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.scramble","title":"scramble  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scramble: bool = True\n</code></pre> <p>Whether to scramble the Sobol sequence.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.sample_one","title":"sample_one","text":"<pre><code>sample_one(\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample a single point and convert it to the given domain.</p> <p>The configuration will be a single dimensional tensor of shape <code>(ncols,)</code>.</p> <p>Please see <code>sample</code> for more details.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>def sample_one(\n    self,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a single point and convert it to the given domain.\n\n    The configuration will be a single dimensional tensor of shape\n    `(ncols,)`.\n\n    Please see [`sample`][neps.samplers.Sampler.sample] for more details.\n    \"\"\"\n    return self.sample(1, to=to, seed=seed, device=device, dtype=dtype).squeeze(0)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.Sobol.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; UniformPrior\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>UniformPrior</code> <p>A uniform sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; UniformPrior:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import UniformPrior\n\n    return UniformPrior(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler","title":"WeightedSampler  <code>dataclass</code>","text":"<pre><code>WeightedSampler(\n    samplers: Sequence[Sampler], weights: Tensor\n)\n</code></pre> <p>               Bases: <code>Sampler</code></p> <p>A sampler that samples from a weighted combination of samplers.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.sampler_probabilities","title":"sampler_probabilities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sampler_probabilities: Tensor = field(\n    init=False, repr=False\n)\n</code></pre> <p>The probabilities for each sampler. Normalized weights.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.samplers","title":"samplers  <code>instance-attribute</code>","text":"<pre><code>samplers: Sequence[Sampler]\n</code></pre> <p>The samplers to sample from.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights: Tensor\n</code></pre> <p>The weights for each sampler.</p>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.borders","title":"borders  <code>classmethod</code>","text":"<pre><code>borders(ndim: int) -&gt; BorderSampler\n</code></pre> <p>Create a border sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of dimensions to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BorderSampler</code> <p>A border sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef borders(cls, ndim: int) -&gt; BorderSampler:\n    \"\"\"Create a border sampler.\n\n    Args:\n        ndim: The number of dimensions to sample.\n\n    Returns:\n        A border sampler.\n    \"\"\"\n    return BorderSampler(ndim=ndim)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.sample_one","title":"sample_one","text":"<pre><code>sample_one(\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: Generator | None = None,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Sample a single point and convert it to the given domain.</p> <p>The configuration will be a single dimensional tensor of shape <code>(ncols,)</code>.</p> <p>Please see <code>sample</code> for more details.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>def sample_one(\n    self,\n    *,\n    to: Domain | list[Domain] | ConfigEncoder,\n    seed: torch.Generator | None = None,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a single point and convert it to the given domain.\n\n    The configuration will be a single dimensional tensor of shape\n    `(ncols,)`.\n\n    Please see [`sample`][neps.samplers.Sampler.sample] for more details.\n    \"\"\"\n    return self.sample(1, to=to, seed=seed, device=device, dtype=dtype).squeeze(0)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.sobol","title":"sobol  <code>classmethod</code>","text":"<pre><code>sobol(ndim: int, *, scramble: bool = True) -&gt; Sobol\n</code></pre> <p>Create a Sobol sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> <code>scramble</code> <p>Whether to scramble the Sobol sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sobol</code> <p>A Sobol sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef sobol(cls, ndim: int, *, scramble: bool = True) -&gt; Sobol:\n    \"\"\"Create a Sobol sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n        scramble: Whether to scramble the Sobol sequence.\n\n    Returns:\n        A Sobol sampler.\n    \"\"\"\n    return Sobol(ndim=ndim, scramble=scramble)\n</code></pre>"},{"location":"api/neps/sampling/samplers/#neps.sampling.samplers.WeightedSampler.uniform","title":"uniform  <code>classmethod</code>","text":"<pre><code>uniform(ndim: int) -&gt; UniformPrior\n</code></pre> <p>Create a uniform sampler.</p> PARAMETER DESCRIPTION <code>ndim</code> <p>The number of columns to sample.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>UniformPrior</code> <p>A uniform sampler.</p> Source code in <code>neps/sampling/samplers.py</code> <pre><code>@classmethod\ndef uniform(cls, ndim: int) -&gt; UniformPrior:\n    \"\"\"Create a uniform sampler.\n\n    Args:\n        ndim: The number of columns to sample.\n\n    Returns:\n        A uniform sampler.\n    \"\"\"\n    from neps.sampling.priors import UniformPrior\n\n    return UniformPrior(ndim=ndim)\n</code></pre>"},{"location":"api/neps/search_spaces/domain/","title":"Domain","text":""},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain","title":"neps.search_spaces.domain","text":"<p>A class representing a domain, a range for a value + properties.</p> <p>Some properties include:</p> <ul> <li>The lower and upper bounds of the domain.</li> <li>Whether the domain is a log domain.</li> <li>Whether the domain is float/int.</li> <li>Whether the domain is split into bins.</li> </ul> <p>With that, the primary method of a domain is to be able to <code>cast()</code> a tensor of values from one to domain to another, e.g. <code>values_a = domain_a.cast(values_b, frm=domain_b)</code>.</p> <p>This can be used to convert float samples to integers, integers to log space, etc.</p> <p>The core method to do so is to be able to cast <code>to_unit()</code> which takes values to a unit interval [0, 1], and then to be able to cast values in [0, 1] to the new domain with <code>from_unit()</code>.</p> <p>There are some shortcuts implemented in <code>cast</code>, such as skipping going through the unit interval if the domains are the same, as no transformation is needed.</p> <p>The primary methods for creating a domain are</p> <ul> <li><code>Domain.float(l, u, ...)</code> -     Used for modelling float ranges</li> <li><code>Domain.int(l, u, ...)</code> -     Used for modelling integer ranges</li> <li><code>Domain.indices(n)</code> -     Primarly used to model categorical choices</li> </ul> <p>If you have a tensor of values, where each column corresponds to a different domain, you can take a look at <code>Domain.translate()</code></p> <p>If you need a unit-interval domain, please use the <code>Domain.unit_float()</code> or <code>UNIT_FLOAT_DOMAIN</code> constant.</p>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain","title":"Domain  <code>dataclass</code>","text":"<pre><code>Domain(\n    lower: V,\n    upper: V,\n    round: bool,\n    log_bounds: tuple[float, float] | None = None,\n    bins: int | None = None,\n    is_categorical: bool = False,\n)\n</code></pre> <p>               Bases: <code>Generic[V]</code></p> <p>A domain for a value.</p> <p>The primary methods for creating a domain are</p> <ul> <li><code>Domain.float(l, u, ...)</code> -     Used for modelling float ranges</li> <li><code>Domain.int(l, u, ...)</code> -     Used for modelling integer ranges</li> <li><code>Domain.indices(n)</code> -     Primarly used to model categorical choices</li> </ul>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.bins","title":"bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bins: int | None = None\n</code></pre> <p>The number of discrete bins to split the domain into.</p> <p>Includes both endpoints of the domain and values are rounded to the nearest bin value.</p>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.is_categorical","title":"is_categorical  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_categorical: bool = False\n</code></pre> <p>Whether the domain is representing a categorical.</p> <p>The domain does not use this information directly, but it can be useful for external classes that consume Domain objects. This can only be set to <code>True</code> if the <code>cardinality</code> of the domain is finite, i.e. <code>bins</code> is not <code>None</code> OR <code>round</code> is <code>True</code> or the boundaries are both integers.</p>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.log_bounds","title":"log_bounds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_bounds: tuple[float, float] | None = None\n</code></pre> <p>The log bounds of the domain, if the domain is in log space.</p>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.lower","title":"lower  <code>instance-attribute</code>","text":"<pre><code>lower: V\n</code></pre> <p>The lower bound of the domain.</p>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.round","title":"round  <code>instance-attribute</code>","text":"<pre><code>round: bool\n</code></pre> <p>Whether to round the values to the nearest integer.</p>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.upper","title":"upper  <code>instance-attribute</code>","text":"<pre><code>upper: V\n</code></pre> <p>The upper bound of the domain.</p>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.as_integer_domain","title":"as_integer_domain","text":"<pre><code>as_integer_domain() -&gt; Domain\n</code></pre> <p>Get the integer version of this domain.</p> <p>Warning</p> <p>This is only possible if this domain has a finite cardinality</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>def as_integer_domain(self) -&gt; Domain:\n    \"\"\"Get the integer version of this domain.\n\n    !!! warning\n\n        This is only possible if this domain has a finite cardinality\n    \"\"\"\n    if self.cardinality is None:\n        raise ValueError(\n            \"Cannot get integer representation of this domain as its\"\n            \" cardinality is non-finite.\"\n        )\n\n    return Domain.indices(self.cardinality, is_categorical=self.is_categorical)\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.cast","title":"cast","text":"<pre><code>cast(\n    x: Tensor, frm: Domain, *, dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Cast a tensor of values frm the domain <code>frm</code> to this domain.</p> <p>If you need to cast a tensor of mixed domains, use <code>Domain.translate()</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of values in the <code>frm</code> domain to cast to this domain.</p> <p> TYPE: <code>Tensor</code> </p> <code>frm</code> <p>The domain to cast from.</p> <p> TYPE: <code>Domain</code> </p> <code>dtype</code> <p>The dtype to convert to</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Same shape tensor with the values cast to this domain.</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>def cast(self, x: Tensor, frm: Domain, *, dtype: torch.dtype | None = None) -&gt; Tensor:\n    \"\"\"Cast a tensor of values frm the domain `frm` to this domain.\n\n    If you need to cast a tensor of mixed domains, use\n    [`Domain.translate()`][neps.search_spaces.domain.Domain.translate].\n\n    Args:\n        x: Tensor of values in the `frm` domain to cast to this domain.\n        frm: The domain to cast from.\n        dtype: The dtype to convert to\n\n    Returns:\n        Same shape tensor with the values cast to this domain.\n    \"\"\"\n    dtype = dtype or self.preffered_dtype\n    # NOTE: In general, we should always be able to go through the unit interval\n    # [0, 1] to be able to transform between domains. However sometimes we can\n    # bypass some steps, dependant on the domains, hence the ugliness...\n\n    # Shortcut 1. (Same Domain)\n    # We can shortcut out going through normalized space if all the boundaries and\n    # they live on the same scale. However, if their bins don't line up, we will\n    # have to go through unit space to figure out the bins\n    same_bounds = self.lower == frm.lower and self.upper == frm.upper\n    same_log_bounds = self.log_bounds == frm.log_bounds\n    same_cardinality = self.cardinality == frm.cardinality\n    if same_bounds and same_log_bounds and same_cardinality:\n        if self.round:\n            x = torch.round(x)\n        return x.type(dtype)\n\n    # Shortcut 2. (From normalized)\n    # The domain we are coming from is already normalized, we only need to lift\n    if frm.is_unit_float:\n        return self.from_unit(x, dtype=dtype)  # type: ignore\n\n    # Shortcut 3. (Log lift)\n    # We can also shortcut out if the only diffrence is that we are coming frm the\n    # log bounds of this domain. We dont care if where we came from was binned or not,\n    # we just lift it up with `np.exp` and round if needed\n    if (self.lower, self.upper) == frm.log_bounds and self.cardinality is None:\n        x = torch.exp(x)\n        if self.round:\n            x = torch.round(x)\n        return x.type(dtype)\n\n    # Otherwise, through the unit interval we go\n    lift = self.from_unit(frm.to_unit(x), dtype=dtype)\n    return lift  # noqa: RET504\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.cast_one","title":"cast_one","text":"<pre><code>cast_one(x: float | int, frm: Domain) -&gt; float | int\n</code></pre> <p>Cast a single value from the domain <code>frm</code> to this domain.</p> PARAMETER DESCRIPTION <code>x</code> <p>Value in the <code>frm</code> domain to cast to this domain.</p> <p> TYPE: <code>float | int</code> </p> <code>frm</code> <p>The domain to cast from.</p> <p> TYPE: <code>Domain</code> </p> RETURNS DESCRIPTION <code>float | int</code> <p>Value cast to this domain.</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>def cast_one(self, x: float | int, frm: Domain) -&gt; float | int:\n    \"\"\"Cast a single value from the domain `frm` to this domain.\n\n    Args:\n        x: Value in the `frm` domain to cast to this domain.\n        frm: The domain to cast from.\n\n    Returns:\n        Value cast to this domain.\n    \"\"\"\n    return self.cast(torch.tensor(x), frm=frm).item()\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.floating","title":"floating  <code>classmethod</code>","text":"<pre><code>floating(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False\n) -&gt; Domain[float]\n</code></pre> <p>Create a domain for a range of float values.</p> PARAMETER DESCRIPTION <code>lower</code> <p>The lower bound of the domain.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>The upper bound of the domain.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>Whether the domain is in log space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>bins</code> <p>The number of discrete bins to split the domain into.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>is_categorical</code> <p>Whether the domain is representing a categorical.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Domain[float]</code> <p>A domain for a range of float values.</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>@classmethod\ndef floating(\n    cls,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False,\n) -&gt; Domain[float]:\n    \"\"\"Create a domain for a range of float values.\n\n    Args:\n        lower: The lower bound of the domain.\n        upper: The upper bound of the domain.\n        log: Whether the domain is in log space.\n        bins: The number of discrete bins to split the domain into.\n        is_categorical: Whether the domain is representing a categorical.\n\n    Returns:\n        A domain for a range of float values.\n    \"\"\"\n    return Domain(\n        lower=float(lower),\n        upper=float(upper),\n        log_bounds=(math.log(lower), math.log(upper)) if log else None,\n        bins=bins,\n        round=False,\n        is_categorical=is_categorical,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.from_unit","title":"from_unit","text":"<pre><code>from_unit(\n    x: Tensor, *, dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Transform a tensor of values from the unit interval [0, 1] to this domain.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of values in the unit interval [0, 1] to convert.</p> <p> TYPE: <code>Tensor</code> </p> <code>dtype</code> <p>The dtype to convert to</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Same shape tensor with the lifted into this domain.</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>def from_unit(self, x: Tensor, *, dtype: torch.dtype | None = None) -&gt; Tensor:\n    \"\"\"Transform a tensor of values from the unit interval [0, 1] to this domain.\n\n    Args:\n        x: A tensor of values in the unit interval [0, 1] to convert.\n        dtype: The dtype to convert to\n\n    Returns:\n        Same shape tensor with the lifted into this domain.\n    \"\"\"\n    dtype = dtype or self.preffered_dtype\n    if self.is_unit_float:\n        return x.to(dtype)\n\n    q = self.cardinality\n    if q is not None:\n        quantization_levels = torch.floor(x * q).clip(0, q - 1)\n        x = quantization_levels / (q - 1)\n\n    # Now we scale to the new domain\n    if self.log_bounds is not None:\n        lower, upper = self.log_bounds\n        x = x * (upper - lower) + lower\n        x = torch.exp(x)\n    else:\n        lower, upper = self.lower, self.upper\n        x = x * (upper - lower) + lower\n\n    if self.round:\n        x = torch.round(x)\n\n    return x.type(dtype)\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.indices","title":"indices  <code>classmethod</code>","text":"<pre><code>indices(\n    n: int, *, is_categorical: bool = False\n) -&gt; Domain[int]\n</code></pre> <p>Create a domain for a range of indices.</p> <p>Like range based functions this domain is inclusive of the lower bound and exclusive of the upper bound.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices.</p> <p> TYPE: <code>int</code> </p> <code>is_categorical</code> <p>Whether the domain is representing a categorical.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Domain[int]</code> <p>A domain for a range of indices.</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>@classmethod\ndef indices(cls, n: int, *, is_categorical: bool = False) -&gt; Domain[int]:\n    \"\"\"Create a domain for a range of indices.\n\n    Like range based functions this domain is inclusive of the lower bound\n    and exclusive of the upper bound.\n\n    Args:\n        n: The number of indices.\n        is_categorical: Whether the domain is representing a categorical.\n\n    Returns:\n        A domain for a range of indices.\n    \"\"\"\n    return Domain.integer(0, n - 1, is_categorical=is_categorical)\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.integer","title":"integer  <code>classmethod</code>","text":"<pre><code>integer(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False\n) -&gt; Domain[int]\n</code></pre> <p>Create a domain for a range of integer values.</p> PARAMETER DESCRIPTION <code>lower</code> <p>The lower bound of the domain.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>The upper bound of the domain (inclusive).</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>Whether the domain is in log space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>bins</code> <p>The number of discrete bins to split the domain into.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>is_categorical</code> <p>Whether the domain is representing a categorical.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Domain[int]</code> <p>A domain for a range of integer values.</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>@classmethod\ndef integer(\n    cls,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    bins: int | None = None,\n    is_categorical: bool = False,\n) -&gt; Domain[int]:\n    \"\"\"Create a domain for a range of integer values.\n\n    Args:\n        lower: The lower bound of the domain.\n        upper: The upper bound of the domain (inclusive).\n        log: Whether the domain is in log space.\n        bins: The number of discrete bins to split the domain into.\n        is_categorical: Whether the domain is representing a categorical.\n\n    Returns:\n        A domain for a range of integer values.\n    \"\"\"\n    return Domain(\n        lower=int(round(lower)),\n        upper=int(round(upper)),\n        log_bounds=(math.log(lower), math.log(upper)) if log else None,\n        round=True,\n        bins=bins,\n        is_categorical=is_categorical,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.to_unit","title":"to_unit","text":"<pre><code>to_unit(x: Tensor, *, dtype: dtype | None = None) -&gt; Tensor\n</code></pre> <p>Transform a tensor of values from this domain to the unit interval [0, 1].</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of values in this domain to convert.</p> <p> TYPE: <code>Tensor</code> </p> <code>dtype</code> <p>The dtype to convert to</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Same shape tensor with the values normalized to the unit interval [0, 1].</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>def to_unit(self, x: Tensor, *, dtype: torch.dtype | None = None) -&gt; Tensor:\n    \"\"\"Transform a tensor of values from this domain to the unit interval [0, 1].\n\n    Args:\n        x: Tensor of values in this domain to convert.\n        dtype: The dtype to convert to\n\n    Returns:\n        Same shape tensor with the values normalized to the unit interval [0, 1].\n    \"\"\"\n    if dtype is None:\n        dtype = torch.float64\n    elif not dtype.is_floating_point:\n        raise ValueError(f\"Unit interval only allows floating dtypes, got {dtype}.\")\n\n    q = self.cardinality\n    if self.is_unit_float and q is None:\n        return x.to(dtype)\n\n    if self.log_bounds is not None:\n        x = torch.log(x)\n        lower, upper = self.log_bounds\n    else:\n        lower, upper = self.lower, self.upper\n\n    x = (x - lower) / (upper - lower)\n\n    if q is not None:\n        quantization_levels = torch.floor(x * q).clip(0, q - 1)\n        x = quantization_levels / (q - 1)\n\n    return x.type(dtype)\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.to_unit_one","title":"to_unit_one","text":"<pre><code>to_unit_one(x: float | int) -&gt; float\n</code></pre> <p>Transform a single value from this domain to the unit interval [0, 1].</p> PARAMETER DESCRIPTION <code>x</code> <p>Value in this domain to convert.</p> <p> TYPE: <code>float | int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Value normalized to the unit interval [0, 1].</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>def to_unit_one(self, x: float | int) -&gt; float:\n    \"\"\"Transform a single value from this domain to the unit interval [0, 1].\n\n    Args:\n        x: Value in this domain to convert.\n\n    Returns:\n        Value normalized to the unit interval [0, 1].\n    \"\"\"\n    return self.to_unit(torch.tensor(x)).item()\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.translate","title":"translate  <code>classmethod</code>","text":"<pre><code>translate(\n    x: Tensor,\n    frm: Domain | Iterable[Domain] | ConfigEncoder,\n    to: Domain | Iterable[Domain] | ConfigEncoder,\n    *,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Cast a tensor of mixed domains to a new set of mixed domains.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor of shape (..., n_dims) with each dim <code>i</code> corresponding to the domain <code>frm[i]</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>frm</code> <p>List of domains to cast from. If list, must be length of <code>n_dims</code>, otherwise we assume the single domain provided is the one to be used across all dimensions.</p> <p> TYPE: <code>Domain | Iterable[Domain] | ConfigEncoder</code> </p> <code>to</code> <p>List of domains to cast to. If list, must be length as <code>n_dims</code>, otherwise we assume the single domain provided is the one to be used across all dimensions.</p> <p> TYPE: <code>Domain | Iterable[Domain] | ConfigEncoder</code> </p> <code>dtype</code> <p>The dtype of the converted tensor</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of the same shape as <code>x</code> with the last dimension casted     from the domain <code>frm[i]</code> to the domain <code>to[i]</code>.</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>@classmethod\ndef translate(\n    cls,\n    x: Tensor,\n    frm: Domain | Iterable[Domain] | ConfigEncoder,\n    to: Domain | Iterable[Domain] | ConfigEncoder,\n    *,\n    dtype: torch.dtype | None = None,\n) -&gt; Tensor:\n    \"\"\"Cast a tensor of mixed domains to a new set of mixed domains.\n\n    Args:\n        x: Tensor of shape (..., n_dims) with each dim `i` corresponding\n            to the domain `frm[i]`.\n        frm: List of domains to cast from. If list, must be length of `n_dims`,\n            otherwise we assume the single domain provided is the one to be used\n            across all dimensions.\n        to: List of domains to cast to. If list, must be length as `n_dims`,\n            otherwise we assume the single domain provided is the one to be used\n            across all dimensions.\n        dtype: The dtype of the converted tensor\n\n    Returns:\n        Tensor of the same shape as `x` with the last dimension casted\n            from the domain `frm[i]` to the domain `to[i]`.\n    \"\"\"\n    if x.ndim == 0:\n        raise ValueError(\"Expected a tensor with at least one dimension.\")\n\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n\n    ndims = x.shape[-1]\n\n    # If both are not a list, we can just cast the whole tensor\n    if isinstance(frm, Domain) and isinstance(to, Domain):\n        return to.cast(x, frm=frm, dtype=dtype)\n\n    from neps.search_spaces.encoding import ConfigEncoder\n\n    frm = (\n        [frm] * ndims\n        if isinstance(frm, Domain)\n        else (frm.domains if isinstance(frm, ConfigEncoder) else list(frm))\n    )\n    to = (\n        [to] * ndims\n        if isinstance(to, Domain)\n        else (to.domains if isinstance(to, ConfigEncoder) else list(to))\n    )\n\n    if len(frm) != ndims:\n        raise ValueError(\n            \"The number of domains in `frm` must match the number of tensors\"\n            \" if provided as a list.\"\n            f\" Expected {ndims} from last dimension of {x.shape}, got {len(frm)}.\"\n        )\n\n    if len(to) != ndims:\n        raise ValueError(\n            \"The number of domains in `to` must match the number of tensors\"\n            \" if provided as a list.\"\n            f\" Expected {ndims} from last dimension of {x.shape=}, got {len(to)}.\"\n        )\n\n    out = torch.empty_like(x, dtype=dtype)\n    for i, (f, t) in enumerate(zip(frm, to, strict=False)):\n        out[..., i] = t.cast(x[..., i], frm=f, dtype=dtype)\n\n    return out\n</code></pre>"},{"location":"api/neps/search_spaces/domain/#neps.search_spaces.domain.Domain.unit_float","title":"unit_float  <code>classmethod</code>","text":"<pre><code>unit_float() -&gt; Domain[float]\n</code></pre> <p>Get a domain for the unit interval [0, 1].</p> Source code in <code>neps/search_spaces/domain.py</code> <pre><code>@classmethod\ndef unit_float(cls) -&gt; Domain[float]:\n    \"\"\"Get a domain for the unit interval [0, 1].\"\"\"\n    return UNIT_FLOAT_DOMAIN\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/","title":"Encoding","text":""},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding","title":"neps.search_spaces.encoding","text":"<p>Encoding of hyperparameter configurations into tensors.</p> <p>For the most part, you can just use <code>ConfigEncoder.from_space()</code> to create an encoder over a list of hyperparameters, along with any constants you want to include when decoding configurations.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.CategoricalToIntegerTransformer","title":"CategoricalToIntegerTransformer  <code>dataclass</code>","text":"<pre><code>CategoricalToIntegerTransformer(choices: Sequence[Any])\n</code></pre> <p>               Bases: <code>TensorTransformer</code></p> <p>A transformer that encodes categorical values into integers.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.CategoricalToUnitNorm","title":"CategoricalToUnitNorm  <code>dataclass</code>","text":"<pre><code>CategoricalToUnitNorm(choices: Sequence[Any])\n</code></pre> <p>               Bases: <code>TensorTransformer</code></p> <p>A transformer that encodes categorical values into a unit normalized tensor.</p> <p>If there are <code>n</code> choices, the tensor will have <code>n</code> bins between <code>0</code> and <code>1</code>.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder","title":"ConfigEncoder  <code>dataclass</code>","text":"<pre><code>ConfigEncoder(\n    transformers: dict[str, TensorTransformer],\n    constants: Mapping[str, Any] = dict(),\n)\n</code></pre> <p>An encoder for hyperparameter configurations.</p> <p>This class is used to encode and decode hyperparameter configurations into tensors and back. It's main uses currently are to support surrogate models that require tensors.</p> <p>The primary methods/properties to be aware of are: * <code>from_space()</code>: The     [<code>Domain</code>][neps.search_spaces.domain.Domain] that each hyperparameter is encoded     into. This is useful in combination with classes like     <code>Sampler</code>,     <code>Prior</code>, and     <code>TorchDistributionWithDomain</code>,     which require knowledge of the     domains of each column for the tensor, for example, to sample values directly     into the encoded space, getting log probabilities of the encoded values. * <code>ncols</code>: The number of columns     in the encoded tensor, useful for initializing some <code>Sampler</code>s.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.domains","title":"domains  <code>property</code>","text":"<pre><code>domains: list[Domain]\n</code></pre> <p>The domains of the encoded hyperparameters.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.ncols","title":"ncols  <code>property</code>","text":"<pre><code>ncols: int\n</code></pre> <p>The number of columns in the encoded tensor.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.decode","title":"decode","text":"<pre><code>decode(x: Tensor) -&gt; list[dict[str, Any]]\n</code></pre> <p>Decode a tensor of hyperparameter configurations into a list of configurations.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of shape <code>(N, ncols)</code> containing the encoded configurations.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>A list of <code>N</code> configurations, including any constants that were included when creating the encoder.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; list[dict[str, Any]]:\n    \"\"\"Decode a tensor of hyperparameter configurations into a list of configurations.\n\n    Args:\n        x: A tensor of shape `(N, ncols)` containing the encoded configurations.\n\n    Returns:\n        A list of `N` configurations, including any constants that were included\n        when creating the encoder.\n    \"\"\"\n    values: dict[str, list[Any]] = {}\n    N = len(x)\n    for hp_name, transformer in self.transformers.items():\n        lookup = self.index_of[hp_name]\n        tensor = x[:, lookup]\n        values[hp_name] = transformer.decode(tensor)\n\n    constants = {name: [v] * N for name, v in self.constants.items()}\n    values.update(constants)\n\n    keys = list(values.keys())\n    return [\n        dict(zip(keys, vals, strict=False))\n        for vals in zip(*values.values(), strict=False)\n    ]\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.decode_one","title":"decode_one","text":"<pre><code>decode_one(x: Tensor) -&gt; dict[str, Any]\n</code></pre> <p>Decode a tensor representing one configuration into a dict.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def decode_one(self, x: torch.Tensor) -&gt; dict[str, Any]:\n    \"\"\"Decode a tensor representing one configuration into a dict.\"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n    return self.decode(x)[0]\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.encode","title":"encode","text":"<pre><code>encode(\n    x: Sequence[Mapping[str, Any]],\n    *,\n    device: device | None = None,\n    dtype: dtype | None = None\n) -&gt; Tensor\n</code></pre> <p>Encode a list of hyperparameter configurations into a tensor.</p> <p>Constants</p> <p>Constants included in configurations will not be encoded into the tensor, but are included when decoding.</p> <p>Parameters with no transformers</p> <p>Any parameters in the configurations, whos key is not in <code>self.transformers</code>, will be ignored.</p> PARAMETER DESCRIPTION <code>x</code> <p>A list of hyperparameter configurations.</p> <p> TYPE: <code>Sequence[Mapping[str, Any]]</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor of shape <code>(len(x), ncols)</code> containing the encoded configurations.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def encode(\n    self,\n    x: Sequence[Mapping[str, Any]],\n    *,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Encode a list of hyperparameter configurations into a tensor.\n\n    !!! warning \"Constants\"\n\n        Constants included in configurations will not be encoded into the tensor,\n        but are included when decoding.\n\n    !!! warning \"Parameters with no transformers\"\n\n        Any parameters in the configurations, whos key is not in\n        `self.transformers`, will be ignored.\n\n    Args:\n        x: A list of hyperparameter configurations.\n        device: The device of the tensor.\n        dtype: The dtype of the tensor.\n\n    Returns:\n        A tensor of shape `(len(x), ncols)` containing the encoded configurations.\n    \"\"\"\n    dtype = torch.float64 if dtype is None else dtype\n    width = len(self.transformers)\n    buffer = torch.empty((len(x), width), dtype=dtype, device=device)\n\n    for hp_name, transformer in self.transformers.items():\n        values = [conf[hp_name] for conf in x]\n        lookup = self.index_of[hp_name]\n\n        # Encode directly into buffer\n        transformer.encode(\n            values,\n            out=buffer[:, lookup],\n            dtype=dtype,\n            device=device,\n        )\n\n    return buffer\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.from_parameters","title":"from_parameters  <code>classmethod</code>","text":"<pre><code>from_parameters(\n    parameters: Mapping[str, Parameter],\n    constants: Mapping[str, Any] | None = None,\n    *,\n    custom_transformers: (\n        dict[str, TensorTransformer] | None\n    ) = None\n) -&gt; ConfigEncoder\n</code></pre> <p>Create a default encoder over a list of hyperparameters.</p> <p>This method creates a default encoder over a list of hyperparameters. It automatically creates transformers for each hyperparameter based on its type. The transformers are as follows:</p> <ul> <li><code>Float</code> and <code>Integer</code> are normalized to the unit interval.</li> <li><code>Categorical</code> is transformed into an integer.</li> </ul> PARAMETER DESCRIPTION <code>parameters</code> <p>A mapping of hyperparameter names to hyperparameters.</p> <p> TYPE: <code>Mapping[str, Parameter]</code> </p> <code>constants</code> <p>A mapping of constant hyperparameters to include when decoding.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>custom_transformers</code> <p>A mapping of hyperparameter names to custom transformers.</p> <p> TYPE: <code>dict[str, TensorTransformer] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ConfigEncoder</code> <p>A <code>ConfigEncoder</code> instance</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>@classmethod\ndef from_parameters(\n    cls,\n    parameters: Mapping[str, Parameter],\n    constants: Mapping[str, Any] | None = None,\n    *,\n    custom_transformers: dict[str, TensorTransformer] | None = None,\n) -&gt; ConfigEncoder:\n    \"\"\"Create a default encoder over a list of hyperparameters.\n\n    This method creates a default encoder over a list of hyperparameters. It\n    automatically creates transformers for each hyperparameter based on its type.\n    The transformers are as follows:\n\n    * `Float` and `Integer` are normalized to the unit interval.\n    * `Categorical` is transformed into an integer.\n\n    Args:\n        parameters: A mapping of hyperparameter names to hyperparameters.\n        constants: A mapping of constant hyperparameters to include when decoding.\n        custom_transformers: A mapping of hyperparameter names to custom transformers.\n\n    Returns:\n        A `ConfigEncoder` instance\n    \"\"\"\n    if constants is not None:\n        overlap = set(parameters) &amp; set(constants)\n        if any(overlap):\n            raise ValueError(\n                \"`constants=` and `parameters=` cannot have overlapping\"\n                f\" keys: {overlap=}\"\n            )\n        if custom_transformers is not None:\n            overlap = set(custom_transformers) &amp; set(constants)\n            if any(overlap):\n                raise ValueError(\n                    f\"Can not apply `custom_transformers=`\"\n                    f\" to `constants=`: {overlap=}\"\n                )\n    else:\n        constants = {}\n\n    custom = custom_transformers or {}\n    transformers: dict[str, TensorTransformer] = {}\n    for name, hp in parameters.items():\n        if name in custom:\n            transformers[name] = custom[name]\n            continue\n\n        match hp:\n            case Float() | Integer():\n                transformers[name] = MinMaxNormalizer(hp.domain)  # type: ignore\n            case Categorical():\n                transformers[name] = CategoricalToIntegerTransformer(hp.choices)\n            case _:\n                raise ValueError(\n                    f\"Unsupported parameter type: {type(hp)}. If hp is a constant, \"\n                    \" please provide it as `constants=`.\"\n                )\n\n    return cls(transformers, constants=constants)\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.from_space","title":"from_space  <code>classmethod</code>","text":"<pre><code>from_space(\n    space: SearchSpace,\n    *,\n    include_fidelity: bool = False,\n    include_constants_when_decoding: bool = True,\n    custom_transformers: (\n        dict[str, TensorTransformer] | None\n    ) = None\n) -&gt; ConfigEncoder\n</code></pre> <p>Create a default encoder over a list of hyperparameters.</p> <p>This method creates a default encoder over a list of hyperparameters. It automatically creates transformers for each hyperparameter based on its type. The transformers are as follows:</p> <ul> <li><code>Float</code> and <code>Integer</code> are normalized to the unit interval.</li> <li><code>Categorical</code> is transformed into an integer.</li> </ul> PARAMETER DESCRIPTION <code>space</code> <p>The search space to build an encoder for</p> <p> TYPE: <code>SearchSpace</code> </p> <code>include_constants_when_decoding</code> <p>Whether to include constants in the encoder. These will not be present in the encoded tensors obtained in <code>encode()</code> but will present when using <code>decode()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>include_fidelity</code> <p>Whether to include fidelities in the encoding</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_transformers</code> <p>A mapping of hyperparameter names to custom transformers to use</p> <p> TYPE: <code>dict[str, TensorTransformer] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ConfigEncoder</code> <p>A <code>ConfigEncoder</code> instance</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>@classmethod\ndef from_space(\n    cls,\n    space: SearchSpace,\n    *,\n    include_fidelity: bool = False,\n    include_constants_when_decoding: bool = True,\n    custom_transformers: dict[str, TensorTransformer] | None = None,\n) -&gt; ConfigEncoder:\n    \"\"\"Create a default encoder over a list of hyperparameters.\n\n    This method creates a default encoder over a list of hyperparameters. It\n    automatically creates transformers for each hyperparameter based on its type.\n    The transformers are as follows:\n\n    * `Float` and `Integer` are normalized to the unit interval.\n    * `Categorical` is transformed into an integer.\n\n    Args:\n        space: The search space to build an encoder for\n        include_constants_when_decoding: Whether to include constants in the encoder.\n            These will not be present in the encoded tensors obtained in\n            [`encode()`][neps.search_spaces.encoding.ConfigEncoder.encode]\n            but will present when using\n            [`decode()`][neps.search_spaces.encoding.ConfigEncoder.decode].\n        include_fidelity: Whether to include fidelities in the encoding\n        custom_transformers: A mapping of hyperparameter names\n            to custom transformers to use\n\n    Returns:\n        A `ConfigEncoder` instance\n    \"\"\"\n    parameters = {**space.numerical, **space.categoricals}\n    if include_fidelity:\n        parameters.update(space.fidelities)\n\n    return ConfigEncoder.from_parameters(\n        parameters=parameters,\n        constants=space.constants if include_constants_when_decoding else None,\n        custom_transformers=custom_transformers,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.pdist","title":"pdist","text":"<pre><code>pdist(\n    x: Tensor,\n    *,\n    numerical_ord: int = 2,\n    categorical_ord: int = 0,\n    dtype: dtype = float64,\n    square_form: bool = False\n) -&gt; Tensor\n</code></pre> <p>Compute the pairwise distance between rows of a tensor.</p> <p>Will sum the results of the numerical and categorical distances. The encoding will be normalized such that all numericals lie within the unit cube, and categoricals will by default, have a <code>p=0</code> norm, which is equivalent to the Hamming distance.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of shape <code>(N, ncols)</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>numerical_ord</code> <p>The order of the norm to use for the numerical columns.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>categorical_ord</code> <p>The order of the norm to use for the categorical columns.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>dtype</code> <p>The dtype of the output tensor.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float64</code> </p> <code>square_form</code> <p>If <code>True</code>, the output will be a square matrix of shape <code>(N, N)</code>. If <code>False</code>, the output will be a single dim tensor of shape <code>1/2 * N * (N - 1)</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The distances, shaped according to <code>square_form</code>.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def pdist(\n    self,\n    x: torch.Tensor,\n    *,\n    numerical_ord: int = 2,\n    categorical_ord: int = 0,\n    dtype: torch.dtype = torch.float64,\n    square_form: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pairwise distance between rows of a tensor.\n\n    Will sum the results of the numerical and categorical distances.\n    The encoding will be normalized such that all numericals lie within the unit\n    cube, and categoricals will by default, have a `p=0` norm, which is equivalent\n    to the Hamming distance.\n\n    Args:\n        x: A tensor of shape `(N, ncols)`.\n        numerical_ord: The order of the norm to use for the numerical columns.\n        categorical_ord: The order of the norm to use for the categorical columns.\n        dtype: The dtype of the output tensor.\n        square_form: If `True`, the output will be a square matrix of shape\n            `(N, N)`. If `False`, the output will be a single dim tensor of shape\n            `1/2 * N * (N - 1)`.\n\n    Returns:\n        The distances, shaped according to `square_form`.\n    \"\"\"\n    categoricals = self.select_categorical(x)\n    numericals = self.select_numerical(x)\n\n    dists: torch.Tensor | None = None\n    if numericals is not None:\n        # Ensure they are all within the unit cube\n        numericals = Domain.translate(\n            numericals,\n            frm=self.numerical_domains,\n            to=UNIT_FLOAT_DOMAIN,\n        )\n\n        dists = torch.nn.functional.pdist(numericals, p=numerical_ord)\n\n    if categoricals is not None:\n        cat_dists = torch.nn.functional.pdist(categoricals, p=categorical_ord)\n        if dists is None:\n            dists = cat_dists\n        else:\n            dists += cat_dists\n\n    if dists is None:\n        raise ValueError(\"No columns to compute distances on.\")\n\n    if not square_form:\n        return dists\n\n    # Turn the single dimensional vector into a square matrix\n    N = len(x)\n    sq = torch.zeros((N, N), dtype=dtype)\n    row_ix, col_ix = torch.triu_indices(N, N, offset=1)\n    sq[row_ix, col_ix] = dists\n    sq[col_ix, row_ix] = dists\n    return sq\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.select_categorical","title":"select_categorical","text":"<pre><code>select_categorical(x: Tensor) -&gt; Tensor | None\n</code></pre> <p>Select the categorical columns from a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of shape <code>(N, ncols)</code>.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor | None</code> <p>A tensor of shape <code>(N, n_categorical)</code>.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def select_categorical(self, x: torch.Tensor) -&gt; torch.Tensor | None:\n    \"\"\"Select the categorical columns from a tensor.\n\n    Args:\n        x: A tensor of shape `(N, ncols)`.\n\n    Returns:\n        A tensor of shape `(N, n_categorical)`.\n    \"\"\"\n    if self.categorical_slice is None:\n        return None\n\n    return x[:, self.categorical_slice]\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.ConfigEncoder.select_numerical","title":"select_numerical","text":"<pre><code>select_numerical(x: Tensor) -&gt; Tensor | None\n</code></pre> <p>Select the numerical columns from a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of shape <code>(N, ncols)</code>.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor | None</code> <p>A tensor of shape <code>(N, n_numerical)</code>.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def select_numerical(self, x: torch.Tensor) -&gt; torch.Tensor | None:\n    \"\"\"Select the numerical columns from a tensor.\n\n    Args:\n        x: A tensor of shape `(N, ncols)`.\n\n    Returns:\n        A tensor of shape `(N, n_numerical)`.\n    \"\"\"\n    if self.numerical_slice is None:\n        return None\n\n    return x[:, self.numerical_slice]\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.MinMaxNormalizer","title":"MinMaxNormalizer  <code>dataclass</code>","text":"<pre><code>MinMaxNormalizer(\n    original_domain: Domain[V], bins: int | None = None\n)\n</code></pre> <p>               Bases: <code>TensorTransformer</code>, <code>Generic[V]</code></p> <p>A transformer that normalizes values to the unit interval.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.TensorTransformer","title":"TensorTransformer","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for encoding and decoding hyperparameter values into tensors.</p>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.TensorTransformer.decode","title":"decode","text":"<pre><code>decode(x: Tensor) -&gt; list[Any]\n</code></pre> <p>Decode a tensor of hyperparameter values into a sequence of values.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of hyperparameter values.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>list[Any]</code> <p>A sequence of hyperparameter values.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; list[Any]:\n    \"\"\"Decode a tensor of hyperparameter values into a sequence of values.\n\n    Args:\n        x: A tensor of hyperparameter values.\n\n    Returns:\n        A sequence of hyperparameter values.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/search_spaces/encoding/#neps.search_spaces.encoding.TensorTransformer.encode","title":"encode","text":"<pre><code>encode(\n    x: Sequence[Any],\n    *,\n    out: Tensor | None = None,\n    dtype: dtype | None = None,\n    device: device | None = None\n) -&gt; Tensor\n</code></pre> <p>Encode a sequence of hyperparameter values into a tensor.</p> PARAMETER DESCRIPTION <code>x</code> <p>A sequence of hyperparameter values.</p> <p> TYPE: <code>Sequence[Any]</code> </p> <code>out</code> <p>An optional tensor to write the encoded values to.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype of the tensor.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>The device of the tensor.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The encoded tensor.</p> Source code in <code>neps/search_spaces/encoding.py</code> <pre><code>def encode(\n    self,\n    x: Sequence[Any],\n    *,\n    out: torch.Tensor | None = None,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Encode a sequence of hyperparameter values into a tensor.\n\n    Args:\n        x: A sequence of hyperparameter values.\n        out: An optional tensor to write the encoded values to.\n        dtype: The dtype of the tensor.\n        device: The device of the tensor.\n\n    Returns:\n        The encoded tensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/search_spaces/functions/","title":"Functions","text":""},{"location":"api/neps/search_spaces/functions/#neps.search_spaces.functions","title":"neps.search_spaces.functions","text":"<p>Functions for working with search spaces.</p>"},{"location":"api/neps/search_spaces/functions/#neps.search_spaces.functions.pairwise_dist","title":"pairwise_dist","text":"<pre><code>pairwise_dist(\n    x: Tensor,\n    encoder: ConfigEncoder,\n    *,\n    numerical_ord: int = 2,\n    categorical_ord: int = 0,\n    dtype: dtype = float64,\n    square_form: bool = False\n) -&gt; Tensor\n</code></pre> <p>Compute the pairwise distance between rows of a tensor.</p> <p>Will sum the results of the numerical and categorical distances. The encoding will be normalized such that all numericals lie within the unit cube, and categoricals will by default, have a <code>p=0</code> norm, which is equivalent to the Hamming distance.</p> PARAMETER DESCRIPTION <code>x</code> <p>A tensor of shape <code>(N, ncols)</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>encoder</code> <p>The encoder used to encode the configs into the tensor.</p> <p> TYPE: <code>ConfigEncoder</code> </p> <code>numerical_ord</code> <p>The order of the norm to use for the numerical columns.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>categorical_ord</code> <p>The order of the norm to use for the categorical columns.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>dtype</code> <p>The dtype of the output tensor.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float64</code> </p> <code>square_form</code> <p>If <code>True</code>, the output will be a square matrix of shape <code>(N, N)</code>. If <code>False</code>, the output will be a single dim tensor of shape <code>1/2 * N * (N - 1)</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The distances, shaped according to <code>square_form</code>.</p> Source code in <code>neps/search_spaces/functions.py</code> <pre><code>def pairwise_dist(\n    x: torch.Tensor,\n    encoder: ConfigEncoder,\n    *,\n    numerical_ord: int = 2,\n    categorical_ord: int = 0,\n    dtype: torch.dtype = torch.float64,\n    square_form: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the pairwise distance between rows of a tensor.\n\n    Will sum the results of the numerical and categorical distances.\n    The encoding will be normalized such that all numericals lie within the unit\n    cube, and categoricals will by default, have a `p=0` norm, which is equivalent\n    to the Hamming distance.\n\n    Args:\n        x: A tensor of shape `(N, ncols)`.\n        encoder: The encoder used to encode the configs into the tensor.\n        numerical_ord: The order of the norm to use for the numerical columns.\n        categorical_ord: The order of the norm to use for the categorical columns.\n        dtype: The dtype of the output tensor.\n        square_form: If `True`, the output will be a square matrix of shape\n            `(N, N)`. If `False`, the output will be a single dim tensor of shape\n            `1/2 * N * (N - 1)`.\n\n    Returns:\n        The distances, shaped according to `square_form`.\n    \"\"\"\n    categoricals = encoder.select_categorical(x)\n    numericals = encoder.select_numerical(x)\n\n    dists: torch.Tensor | None = None\n    if numericals is not None:\n        # Ensure they are all within the unit cube\n        numericals = Domain.translate(\n            numericals,\n            frm=encoder.numerical_domains,\n            to=UNIT_FLOAT_DOMAIN,\n        )\n\n        dists = torch.nn.functional.pdist(numericals, p=numerical_ord)\n\n    if categoricals is not None:\n        # Does Hamming distance\n        cat_dists = torch.nn.functional.pdist(categoricals, p=categorical_ord)\n        if dists is None:\n            dists = cat_dists\n        else:\n            dists += cat_dists\n\n    if dists is None:\n        raise ValueError(\"No columns to compute distances on.\")\n\n    if not square_form:\n        return dists\n\n    # Turn the single dimensional vector into a square matrix\n    N = len(x)\n    sq = torch.zeros((N, N), dtype=dtype)\n    row_ix, col_ix = torch.triu_indices(N, N, offset=1)\n    sq[row_ix, col_ix] = dists\n    sq[col_ix, row_ix] = dists\n    return sq\n</code></pre>"},{"location":"api/neps/search_spaces/functions/#neps.search_spaces.functions.sample_one_old","title":"sample_one_old","text":"<pre><code>sample_one_old(\n    space: SearchSpace,\n    *,\n    user_priors: bool = False,\n    patience: int = 1,\n    ignore_fidelity: bool = True\n) -&gt; SearchSpace\n</code></pre> <p>Sample a configuration from the search space.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to sample from.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>user_priors</code> <p>Whether to use user priors when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patience</code> <p>The number of times to try to sample a valid value for a hyperparameter.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>ignore_fidelity</code> <p>Whether to ignore the fidelity parameter when sampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>SearchSpace</code> <p>A sampled configuration from the search space.</p> Source code in <code>neps/search_spaces/functions.py</code> <pre><code>def sample_one_old(\n    space: SearchSpace,\n    *,\n    user_priors: bool = False,\n    patience: int = 1,\n    ignore_fidelity: bool = True,\n) -&gt; SearchSpace:\n    \"\"\"Sample a configuration from the search space.\n\n    Args:\n        space: The search space to sample from.\n        user_priors: Whether to use user priors when sampling.\n        patience: The number of times to try to sample a valid value for a\n            hyperparameter.\n        ignore_fidelity: Whether to ignore the fidelity parameter when sampling.\n\n    Returns:\n        A sampled configuration from the search space.\n    \"\"\"\n    sampled_hps: dict[str, Parameter] = {}\n\n    for name, hp in space.hyperparameters.items():\n        if hp.is_fidelity and ignore_fidelity:\n            sampled_hps[name] = hp.clone()\n            continue\n\n        for attempt in range(patience):\n            try:\n                if user_priors and isinstance(hp, ParameterWithPrior):\n                    sampled_hps[name] = hp.sample(user_priors=user_priors)\n                else:\n                    sampled_hps[name] = hp.sample()\n                break\n            except Exception as e:  # noqa: BLE001\n                logger.warning(\n                    f\"Attempt {attempt + 1}/{patience} failed for\"\n                    f\" sampling {name}: {e!s}\"\n                )\n        else:\n            logger.error(\n                f\"Failed to sample valid value for {name} after {patience} attempts\"\n            )\n            raise ValueError(\n                f\"Could not sample valid value for hyperparameter {name}\"\n                f\" in {patience} tries!\"\n            )\n\n    return SearchSpace(**sampled_hps)\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/","title":"Parameter","text":""},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter","title":"neps.search_spaces.parameter","text":"<p>The base <code>Parameter</code> class.</p> <p>The <code>Parameter</code> refers to both the hyperparameter definition but also holds a <code>.value</code> which can be set or empty, in which case it is <code>None</code>.</p> <p>Tip</p> <p>A <code>Parameter</code> which allows for defining a <code>.default</code> and some prior, i.e. some default value along with a confidence that this is a good setting, should implement the <code>ParameterWithPrior</code> class.</p> <p>This is utilized by certain optimization routines to inform the search process.</p>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter","title":"Parameter","text":"<pre><code>Parameter(\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool\n)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[ValueT, SerializedT]</code></p> <p>A base class for hyperparameters.</p> ATTRIBUTE DESCRIPTION <code>default</code> <p>default value for the hyperparameter. This value is used as a prior to inform algorithms about a decent default value for the hyperparameter, as well as use attributes from <code>ParameterWithPrior</code>, to aid in optimization.</p> <p> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> </p> <code>value</code> <p>value for the hyperparameter, if any.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>normalized_value</code> <p>normalized value for the hyperparameter.</p> <p> </p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool,\n):\n    \"\"\"Create a new `Parameter`.\n\n    Args:\n        value: value for the hyperparameter.\n        default: default value for the hyperparameter.\n        is_fidelity: whether the hyperparameter is fidelity.\n    \"\"\"\n    self.default = default\n    self.is_fidelity = is_fidelity\n\n    # TODO(eddiebergman): The reason to have this not as a straight alone\n    # attribute is that the graph parameters currently expose there own\n    # way of calculating a value on demand.\n    # To fix this would mean to essentially decouple GraphParameter entirely\n    # from Parameter as it's less of a heirarchy and more of just a small overlap\n    # of functionality.\n    self._value = value\n    self.normalized_value = (\n        self.value_to_normalized(value) if value is not None else None\n    )\n\n    # TODO: Pass in through subclasses\n    self.default_confidence_score: float\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.clone","title":"clone  <code>abstractmethod</code>","text":"<pre><code>clone() -&gt; Self\n</code></pre> <p>Create a copy of the <code>Parameter</code>.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef clone(self) -&gt; Self:\n    \"\"\"Create a copy of the `Parameter`.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.normalized_to_value","title":"normalized_to_value  <code>abstractmethod</code>","text":"<pre><code>normalized_to_value(normalized_value: float) -&gt; ValueT\n</code></pre> <p>Convert a normalized value back to value in the defined hyperparameter range.</p> PARAMETER DESCRIPTION <code>normalized_value</code> <p>normalized value to convert.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef normalized_to_value(self, normalized_value: float) -&gt; ValueT:\n    \"\"\"Convert a normalized value back to value in the defined hyperparameter range.\n\n    Args:\n        normalized_value: normalized value to convert.\n\n    Returns:\n        The value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.sample","title":"sample","text":"<pre><code>sample() -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Will set the <code>.value</code> to the sampled value.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Will set the [`.value`][neps.search_spaces.Parameter.value] to the\n    sampled value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value()\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.sample_value","title":"sample_value  <code>abstractmethod</code>","text":"<pre><code>sample_value() -&gt; ValueT\n</code></pre> <p>Sample a new value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef sample_value(self) -&gt; ValueT:\n    \"\"\"Sample a new value.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.set_value","title":"set_value  <code>abstractmethod</code>","text":"<pre><code>set_value(value: ValueT | None) -&gt; None\n</code></pre> <p>Set the value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_value(self, value: ValueT | None) -&gt; None:\n    \"\"\"Set the value for the hyperparameter.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.Parameter.value_to_normalized","title":"value_to_normalized  <code>abstractmethod</code>","text":"<pre><code>value_to_normalized(value: ValueT) -&gt; float\n</code></pre> <p>Convert a value to a normalized value.</p> <p>Normalization is different per hyperparameter type, but roughly refers to numeric values.</p> <ul> <li><code>(0, 1)</code> scaling in the case of     a <code>Numerical</code>,</li> <li><code>{0.0, 1.0}</code> for a <code>Constant</code>,</li> <li><code>[0, 1, ..., n]</code> for a     <code>Categorical</code>.</li> </ul> PARAMETER DESCRIPTION <code>value</code> <p>value to convert.</p> <p> TYPE: <code>ValueT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The normalized value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef value_to_normalized(self, value: ValueT) -&gt; float:\n    \"\"\"Convert a value to a normalized value.\n\n    Normalization is different per hyperparameter type,\n    but roughly refers to numeric values.\n\n    * `(0, 1)` scaling in the case of\n        a [`Numerical`][neps.search_spaces.Numerical],\n    * `{0.0, 1.0}` for a [`Constant`][neps.search_spaces.Constant],\n    * `[0, 1, ..., n]` for a\n        [`Categorical`][neps.search_spaces.Categorical].\n\n    Args:\n        value: value to convert.\n\n    Returns:\n        The normalized value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior","title":"ParameterWithPrior","text":"<pre><code>ParameterWithPrior(\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool\n)\n</code></pre> <p>               Bases: <code>Parameter[ValueT, SerializedT]</code></p> <p>A base class for hyperparameters with priors.</p> ATTRIBUTE DESCRIPTION <code>default_confidence_choice</code> <p>The choice of how confident any algorithm should be in the default value being a good value.</p> <p> TYPE: <code>str</code> </p> <code>default_confidence_score</code> <p>A score used by algorithms to utilize the default value.</p> <p> TYPE: <code>float</code> </p> <code>has_prior</code> <p>whether the hyperparameter has a prior that can be used by an algorithm. In many cases, this refers to having a default value.</p> <p> TYPE: <code>bool</code> </p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    value: ValueT | None,\n    default: ValueT | None,\n    is_fidelity: bool,\n):\n    \"\"\"Create a new `Parameter`.\n\n    Args:\n        value: value for the hyperparameter.\n        default: default value for the hyperparameter.\n        is_fidelity: whether the hyperparameter is fidelity.\n    \"\"\"\n    self.default = default\n    self.is_fidelity = is_fidelity\n\n    # TODO(eddiebergman): The reason to have this not as a straight alone\n    # attribute is that the graph parameters currently expose there own\n    # way of calculating a value on demand.\n    # To fix this would mean to essentially decouple GraphParameter entirely\n    # from Parameter as it's less of a heirarchy and more of just a small overlap\n    # of functionality.\n    self._value = value\n    self.normalized_value = (\n        self.value_to_normalized(value) if value is not None else None\n    )\n\n    # TODO: Pass in through subclasses\n    self.default_confidence_score: float\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.clone","title":"clone  <code>abstractmethod</code>","text":"<pre><code>clone() -&gt; Self\n</code></pre> <p>Create a copy of the <code>Parameter</code>.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef clone(self) -&gt; Self:\n    \"\"\"Create a copy of the `Parameter`.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.normalized_to_value","title":"normalized_to_value  <code>abstractmethod</code>","text":"<pre><code>normalized_to_value(normalized_value: float) -&gt; ValueT\n</code></pre> <p>Convert a normalized value back to value in the defined hyperparameter range.</p> PARAMETER DESCRIPTION <code>normalized_value</code> <p>normalized value to convert.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef normalized_to_value(self, normalized_value: float) -&gt; ValueT:\n    \"\"\"Convert a normalized value back to value in the defined hyperparameter range.\n\n    Args:\n        normalized_value: normalized value to convert.\n\n    Returns:\n        The value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.sample_value","title":"sample_value  <code>abstractmethod</code>","text":"<pre><code>sample_value(*, user_priors: bool = False) -&gt; ValueT\n</code></pre> <p>Sample a new value.</p> <p>Similar to <code>Parameter.sample_value()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef sample_value(self, *, user_priors: bool = False) -&gt; ValueT:\n    \"\"\"Sample a new value.\n\n    Similar to\n    [`Parameter.sample_value()`][neps.search_spaces.Parameter.sample_value],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        The sampled value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.set_value","title":"set_value  <code>abstractmethod</code>","text":"<pre><code>set_value(value: ValueT | None) -&gt; None\n</code></pre> <p>Set the value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_value(self, value: ValueT | None) -&gt; None:\n    \"\"\"Set the value for the hyperparameter.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/parameter/#neps.search_spaces.parameter.ParameterWithPrior.value_to_normalized","title":"value_to_normalized  <code>abstractmethod</code>","text":"<pre><code>value_to_normalized(value: ValueT) -&gt; float\n</code></pre> <p>Convert a value to a normalized value.</p> <p>Normalization is different per hyperparameter type, but roughly refers to numeric values.</p> <ul> <li><code>(0, 1)</code> scaling in the case of     a <code>Numerical</code>,</li> <li><code>{0.0, 1.0}</code> for a <code>Constant</code>,</li> <li><code>[0, 1, ..., n]</code> for a     <code>Categorical</code>.</li> </ul> PARAMETER DESCRIPTION <code>value</code> <p>value to convert.</p> <p> TYPE: <code>ValueT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The normalized value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef value_to_normalized(self, value: ValueT) -&gt; float:\n    \"\"\"Convert a value to a normalized value.\n\n    Normalization is different per hyperparameter type,\n    but roughly refers to numeric values.\n\n    * `(0, 1)` scaling in the case of\n        a [`Numerical`][neps.search_spaces.Numerical],\n    * `{0.0, 1.0}` for a [`Constant`][neps.search_spaces.Constant],\n    * `[0, 1, ..., n]` for a\n        [`Categorical`][neps.search_spaces.Categorical].\n\n    Args:\n        value: value to convert.\n\n    Returns:\n        The normalized value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/","title":"Search space","text":""},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space","title":"neps.search_spaces.search_space","text":"<p>Contains the <code>SearchSpace</code> class which is a container for hyperparameters that can be sampled, mutated, and crossed over.</p>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace","title":"SearchSpace","text":"<pre><code>SearchSpace(**hyperparameters: Parameter)\n</code></pre> <p>               Bases: <code>Mapping[str, Any]</code></p> <p>A container for hyperparameters that can be sampled, mutated, and crossed over.</p> <p>Provides operations for operating on and generating new configurations from the hyperparameters.</p> <p>Note</p> <p>The <code>SearchSpace</code> class is both the definition of the search space and also a configuration at the same time.</p> <p>When refering to the <code>SearchSpace</code> as a configuration, the documentation will refer to it as a <code>configuration</code> or <code>config</code>. Otherwise, it will be referred to as a <code>search space</code>.</p> <p>TODO</p> <p>This documentation is WIP. If you have any questions, please reach out so we can know better what to document.</p> PARAMETER DESCRIPTION <code>**hyperparameters</code> <p>The hyperparameters that define the search space.</p> <p> TYPE: <code>Parameter</code> DEFAULT: <code>{}</code> </p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def __init__(self, **hyperparameters: Parameter):  # noqa: C901, PLR0912\n    \"\"\"Initialize the SearchSpace with hyperparameters.\n\n    Args:\n        **hyperparameters: The hyperparameters that define the search space.\n    \"\"\"\n    # Ensure a consistent ordering for uses throughout the lib\n    _hyperparameters = sorted(hyperparameters.items(), key=lambda x: x[0])\n    _fidelity_param: Numerical | None = None\n    _fidelity_name: str | None = None\n    _has_prior: bool = False\n\n    for name, hp in _hyperparameters:\n        if hp.is_fidelity:\n            if _fidelity_param is not None:\n                raise ValueError(\n                    \"neps only supports one fidelity parameter in the pipeline space,\"\n                    \" but multiple were given. (Hint: check you pipeline space for \"\n                    \"multiple is_fidelity=True)\"\n                )\n\n            if not isinstance(hp, Numerical):\n                raise ValueError(\n                    f\"Only float and integer fidelities supported, got {hp}\"\n                )\n\n            _fidelity_param = hp\n            _fidelity_name = name\n\n        if isinstance(hp, ParameterWithPrior) and hp.has_prior:\n            _has_prior = True\n\n    self.hyperparameters: dict[str, Parameter] = dict(_hyperparameters)\n    self.fidelity: Numerical | None = _fidelity_param\n    self.fidelity_name: str | None = _fidelity_name\n    self.has_prior: bool = _has_prior\n\n    self.default_config = {}\n    for name, hp in _hyperparameters:\n        if hp.default is not None:\n            self.default_config[name] = hp.default\n            continue\n\n        match hp:\n            case Categorical():\n                first_choice = hp.choices[0]\n                self.default_config[name] = first_choice\n            case Integer() | Float():\n                if hp.is_fidelity:\n                    self.default_config[name] = hp.upper\n                    continue\n\n                midpoint = hp.domain.cast_one(0.5, frm=UNIT_FLOAT_DOMAIN)\n                self.default_config[name] = midpoint\n            case Constant():\n                self.default_config[name] = hp.value\n            case GraphParameter():\n                self.default_config[name] = hp.default\n            case _:\n                raise TypeError(f\"Unknown hyperparameter type {hp}\")\n\n    self.categoricals: Mapping[str, Categorical] = {\n        k: hp for k, hp in _hyperparameters if isinstance(hp, Categorical)\n    }\n    self.numerical: Mapping[str, Integer | Float] = {\n        k: hp\n        for k, hp in _hyperparameters\n        if isinstance(hp, Integer | Float) and not hp.is_fidelity\n    }\n    self.graphs: Mapping[str, GraphParameter] = {\n        k: hp for k, hp in _hyperparameters if isinstance(hp, GraphParameter)\n    }\n    self.constants: Mapping[str, Any] = {\n        k: hp.value for k, hp in _hyperparameters if isinstance(hp, Constant)\n    }\n    # NOTE: For future of multiple fidelities\n    self.fidelities: Mapping[str, Integer | Float] = {}\n    if _fidelity_param is not None and _fidelity_name is not None:\n        assert isinstance(_fidelity_param, Integer | Float)\n        self.fidelities = {_fidelity_name: _fidelity_param}\n\n    # TODO: Deprecate out, ideally configs are just dictionaries,\n    # not attached to this space object\n    self._values = {\n        hp_name: hp if isinstance(hp, GraphParameter) else hp.value\n        for hp_name, hp in self.hyperparameters.items()\n    }\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.clone","title":"clone","text":"<pre><code>clone() -&gt; SearchSpace\n</code></pre> <p>Create a copy of the search space.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def clone(self) -&gt; SearchSpace:\n    \"\"\"Create a copy of the search space.\"\"\"\n    return self.__class__(**{k: v.clone() for k, v in self.hyperparameters.items()})\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.SearchSpace.from_dict","title":"from_dict","text":"<pre><code>from_dict(\n    config: Mapping[str, Any | GraphParameter]\n) -&gt; SearchSpace\n</code></pre> <p>Create a new instance of this search space with parameters set from the config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The dictionary of hyperparameters to set with values.</p> <p> TYPE: <code>Mapping[str, Any | GraphParameter]</code> </p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def from_dict(self, config: Mapping[str, Any | GraphParameter]) -&gt; SearchSpace:\n    \"\"\"Create a new instance of this search space with parameters set from the config.\n\n    Args:\n        config: The dictionary of hyperparameters to set with values.\n    \"\"\"\n    new = self.clone()\n    for name, val in config.items():\n        new.hyperparameters[name].load_from(val)\n        new._values[name] = new.hyperparameters[name].value\n\n    return new\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.pipeline_space_from_configspace","title":"pipeline_space_from_configspace","text":"<pre><code>pipeline_space_from_configspace(\n    configspace: ConfigurationSpace,\n) -&gt; dict[str, Parameter]\n</code></pre> <p>Constructs the <code>Parameter</code> objects from a <code>ConfigurationSpace</code>.</p> PARAMETER DESCRIPTION <code>configspace</code> <p>The configuration space to construct the pipeline space from.</p> <p> TYPE: <code>ConfigurationSpace</code> </p> RETURNS DESCRIPTION <code>dict[str, Parameter]</code> <p>A dictionary where keys are parameter names and values are parameter objects.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def pipeline_space_from_configspace(\n    configspace: CS.ConfigurationSpace,\n) -&gt; dict[str, Parameter]:\n    \"\"\"Constructs the [`Parameter`][neps.search_spaces.parameter.Parameter] objects\n    from a [`ConfigurationSpace`][ConfigSpace.configuration_space.ConfigurationSpace].\n\n    Args:\n        configspace: The configuration space to construct the pipeline space from.\n\n    Returns:\n        A dictionary where keys are parameter names and values are parameter objects.\n    \"\"\"\n    pipeline_space = {}\n    parameter: Parameter\n    if any(configspace.get_conditions()) or any(configspace.get_forbiddens()):\n        raise NotImplementedError(\n            \"The ConfigurationSpace has conditions or forbidden clauses, \"\n            \"which are not supported by neps.\"\n        )\n\n    for hyperparameter in configspace.get_hyperparameters():\n        if isinstance(hyperparameter, CS.Constant):\n            parameter = Constant(value=hyperparameter.value)\n        elif isinstance(hyperparameter, CS.CategoricalHyperparameter):\n            parameter = Categorical(\n                hyperparameter.choices,\n                default=hyperparameter.default_value,\n            )\n        elif isinstance(hyperparameter, CS.OrdinalHyperparameter):\n            parameter = Categorical(\n                hyperparameter.sequence,\n                default=hyperparameter.default_value,\n            )\n        elif isinstance(hyperparameter, CS.UniformIntegerHyperparameter):\n            parameter = Integer(\n                lower=hyperparameter.lower,\n                upper=hyperparameter.upper,\n                log=hyperparameter.log,\n                default=hyperparameter.default_value,\n            )\n        elif isinstance(hyperparameter, CS.UniformFloatHyperparameter):\n            parameter = Float(\n                lower=hyperparameter.lower,\n                upper=hyperparameter.upper,\n                log=hyperparameter.log,\n                default=hyperparameter.default_value,\n            )\n        else:\n            raise ValueError(f\"Unknown hyperparameter type {hyperparameter}\")\n        pipeline_space[hyperparameter.name] = parameter\n    return pipeline_space\n</code></pre>"},{"location":"api/neps/search_spaces/search_space/#neps.search_spaces.search_space.pipeline_space_from_yaml","title":"pipeline_space_from_yaml","text":"<pre><code>pipeline_space_from_yaml(\n    config: str | Path | dict,\n) -&gt; dict[str, Parameter]\n</code></pre> <p>Reads configuration details from a YAML file or a dictionary and constructs a pipeline space dictionary.</p> PARAMETER DESCRIPTION <code>config</code> <p>Path to the YAML file or a dictionary containing parameter configurations.</p> <p> TYPE: <code>str | Path | dict</code> </p> RETURNS DESCRIPTION <code>dict[str, Parameter]</code> <p>A dictionary where keys are parameter names and values are parameter objects.</p> RAISES DESCRIPTION <code>SearchSpaceFromYamlFileError</code> <p>Raised if there are issues with the YAML file's format, contents, or if the dictionary is invalid.</p> Source code in <code>neps/search_spaces/search_space.py</code> <pre><code>def pipeline_space_from_yaml(  # noqa: C901\n    config: str | Path | dict,\n) -&gt; dict[str, Parameter]:\n    \"\"\"Reads configuration details from a YAML file or a dictionary and constructs a\n    pipeline space dictionary.\n\n    Args:\n        config: Path to the YAML file or a dictionary containing parameter configurations.\n\n    Returns:\n        A dictionary where keys are parameter names and values are parameter objects.\n\n    Raises:\n        SearchSpaceFromYamlFileError: Raised if there are issues with the YAML file's\n            format, contents, or if the dictionary is invalid.\n    \"\"\"\n    try:\n        if isinstance(config, str | Path):\n            # try to load the YAML file\n            try:\n                yaml_file_path = Path(config)\n                with yaml_file_path.open(\"r\") as file:\n                    config = yaml.safe_load(file)\n                if not isinstance(config, dict):\n                    raise ValueError(\n                        \"The loaded pipeline_space is not a valid dictionary. Please \"\n                        \"ensure that you use a proper structure. See the documentation \"\n                        \"for more details.\"\n                    )\n            except FileNotFoundError as e:\n                raise FileNotFoundError(\n                    f\"Unable to find the specified file for 'pipeline_space' at \"\n                    f\"'{config}'. Please verify the path specified in the \"\n                    f\"'pipeline_space' argument and try again.\"\n                ) from e\n            except yaml.YAMLError as e:\n                raise ValueError(f\"The file at {config} is not a valid YAML file.\") from e\n\n        pipeline_space: dict[str, Parameter] = {}\n\n        if len(config) == 1 and \"pipeline_space\" in config:\n            config = config[\"pipeline_space\"]\n        for name, details in config.items():  # type: ignore\n            param_type = deduce_type(name, details)\n\n            if param_type in (\"int\", \"integer\"):\n                formatted_details = formatting_int(name, details)\n                pipeline_space[name] = Integer(**formatted_details)\n            elif param_type == \"float\":\n                formatted_details = formatting_float(name, details)\n                pipeline_space[name] = Float(**formatted_details)\n            elif param_type in (\"cat\", \"categorical\"):\n                formatted_details = formatting_cat(name, details)\n                pipeline_space[name] = Categorical(**formatted_details)\n            elif param_type == \"const\":\n                const_details = formatting_const(details)\n                pipeline_space[name] = Constant(const_details)  # type: ignore\n            else:\n                # Handle unknown parameter type\n                raise TypeError(\n                    f\"Unsupported parameter with details: {details} for '{name}'.\\n\"\n                    f\"Supported Types for argument type are:\\n\"\n                    \"For integer parameter: int, integer\\n\"\n                    \"For float parameter: float\\n\"\n                    \"For categorical parameter: cat, categorical\\n\"\n                    \"Constant parameter was not detect\\n\"\n                )\n    except (KeyError, TypeError, ValueError, FileNotFoundError) as e:\n        raise SearchSpaceFromYamlFileError(e) from e\n\n    return pipeline_space\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/","title":"Yaml search space utils","text":""},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils","title":"neps.search_spaces.yaml_search_space_utils","text":""},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.SearchSpaceFromYamlFileError","title":"SearchSpaceFromYamlFileError","text":"<pre><code>SearchSpaceFromYamlFileError(exception: Exception)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised for errors occurring during the initialization of the search space from a YAML file.</p> ATTRIBUTE DESCRIPTION <code>exception_type</code> <p>The type of the original exception.</p> <p> TYPE: <code>str</code> </p> <code>message</code> <p>A detailed message that includes the type of the original exception            and the error description.</p> <p> TYPE: <code>str</code> </p> PARAMETER DESCRIPTION <code>exception</code> <p>The original exception that was raised during the                     initialization of the search space from the YAML file.</p> <p> TYPE: <code>Exception</code> </p> Example Usage <p>try:     # Code to initialize search space from YAML file except (KeyError, TypeError, ValueError) as e:     raise SearchSpaceFromYamlFileError(e)</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def __init__(self, exception: Exception) -&gt; None:\n    self.exception_type = type(exception).__name__\n    self.message = (\n        f\"Error occurred during initialization of search space from \"\n        f\"YAML file.\\n {self.exception_type}: {exception}\"\n    )\n    super().__init__(self.message)\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.convert_scientific_notation","title":"convert_scientific_notation","text":"<pre><code>convert_scientific_notation(\n    value: str | int | float,\n    show_usage_flag: Literal[False] = False,\n) -&gt; float\n</code></pre><pre><code>convert_scientific_notation(\n    value: str | int | float, show_usage_flag: Literal[True]\n) -&gt; tuple[float, bool]\n</code></pre> <pre><code>convert_scientific_notation(\n    value: str | int | float, show_usage_flag: bool = False\n) -&gt; float | tuple[float, bool]\n</code></pre> <p>Convert a given value to a float if it's a string that matches scientific e notation. This is especially useful for numbers like \"3.3e-5\" which YAML parsers may not directly interpret as floats.</p> <p>If the 'show_usage_flag' is set to True, the function returns a tuple of the float conversion and a boolean flag indicating whether scientific notation was detected.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to convert. Can be an integer, float,                        or a string representing a number, possibly in                        scientific notation.</p> <p> TYPE: <code>str | int | float</code> </p> <code>show_usage_flag</code> <p>Optional; defaults to False. If True, the function                     also returns a flag indicating whether scientific                     notation was detected in the string.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The value converted to float if 'show_usage_flag' is False. (float, bool): A tuple containing the value converted to float and a flag                indicating scientific notation detection if 'show_usage_flag'                is True.</p> <p> TYPE: <code>float | tuple[float, bool]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the value is a string and does not represent a valid number.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def convert_scientific_notation(\n    value: str | int | float, show_usage_flag: bool = False\n) -&gt; float | tuple[float, bool]:\n    \"\"\"\n    Convert a given value to a float if it's a string that matches scientific e notation.\n    This is especially useful for numbers like \"3.3e-5\" which YAML parsers may not\n    directly interpret as floats.\n\n    If the 'show_usage_flag' is set to True, the function returns a tuple of the float\n    conversion and a boolean flag indicating whether scientific notation was detected.\n\n    Args:\n        value (str | int | float): The value to convert. Can be an integer, float,\n                                   or a string representing a number, possibly in\n                                   scientific notation.\n        show_usage_flag (bool): Optional; defaults to False. If True, the function\n                                also returns a flag indicating whether scientific\n                                notation was detected in the string.\n\n    Returns:\n        float: The value converted to float if 'show_usage_flag' is False.\n        (float, bool): A tuple containing the value converted to float and a flag\n                       indicating scientific notation detection if 'show_usage_flag'\n                       is True.\n\n    Raises:\n        ValueError: If the value is a string and does not represent a valid number.\n    \"\"\"\n\n    e_notation_pattern = r\"^-?\\d+(\\.\\d+)?[eE]-?\\d+$\"\n\n    flag = False  # Flag if e notation was detected\n\n    if isinstance(value, str):\n        # Remove all whitespace from the string\n        value_no_space = value.replace(\" \", \"\")\n\n        # check for e notation\n        if re.match(e_notation_pattern, value_no_space):\n            flag = True\n\n    if show_usage_flag is True:\n        return float(value), flag\n    else:\n        return float(value)\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.deduce_param_type","title":"deduce_param_type","text":"<pre><code>deduce_param_type(\n    name: str, details: dict[str, int | str | float]\n) -&gt; str\n</code></pre> <p>Deduces the parameter type based on the provided details.</p> <p>The function interprets the 'details' dictionary to determine the parameter type. The dictionary should include key-value pairs that describe the parameter's characteristics, such as lower, upper and choices.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing parameter</p> <p> TYPE: <code>dict[str, int | str | float]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The deduced parameter type ('int', 'float' or 'categorical').</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the parameter type cannot be deduced from the details, or if the</p> Example <p>param_type = deduce_param_type('example_param', {'lower': 0, 'upper': 10})</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def deduce_param_type(name: str, details: dict[str, int | str | float]) -&gt; str:\n    \"\"\"Deduces the parameter type based on the provided details.\n\n    The function interprets the 'details' dictionary to determine the parameter type.\n    The dictionary should include key-value pairs that describe the parameter's\n    characteristics, such as lower, upper and choices.\n\n\n    Args:\n        name (str): The name of the parameter.\n        details ((dict[str, int | str | float])): A dictionary containing parameter\n        specifications.\n\n    Returns:\n        str: The deduced parameter type ('int', 'float' or 'categorical').\n\n    Raises:\n        TypeError: If the parameter type cannot be deduced from the details, or if the\n        provided details have inconsistent types for expected keys.\n\n    Example:\n        param_type = deduce_param_type('example_param', {'lower': 0, 'upper': 10})\"\"\"\n    # Logic to deduce type from details\n\n    # check for int and float conditions\n    if \"lower\" in details and \"upper\" in details:\n        # Determine if it's an integer or float range parameter\n        if isinstance(details[\"lower\"], int) and isinstance(details[\"upper\"], int):\n            param_type = \"int\"\n        elif isinstance(details[\"lower\"], float) and isinstance(details[\"upper\"], float):\n            param_type = \"float\"\n        else:\n            try:\n                details[\"lower\"], flag_lower = convert_scientific_notation(\n                    details[\"lower\"], show_usage_flag=True\n                )\n                details[\"upper\"], flag_upper = convert_scientific_notation(\n                    details[\"upper\"], show_usage_flag=True\n                )\n            except ValueError as e:\n                raise TypeError(\n                    f\"Inconsistent types for 'lower' and 'upper' in '{name}'. \"\n                    f\"Both must be either integers or floats.\"\n                ) from e\n\n            # check if one value is e notation and if so convert it to float\n            if flag_lower or flag_upper:\n                logger.info(\n                    f\"Because of e notation, Parameter {name} gets \"\n                    f\"interpreted as float\"\n                )\n                param_type = \"float\"\n            else:\n                raise TypeError(\n                    f\"Inconsistent types for 'lower' and 'upper' in '{name}'. \"\n                    f\"Both must be either integers or floats.\"\n                )\n    # check for categorical condition\n    elif \"choices\" in details:\n        param_type = \"categorical\"\n    else:\n        raise KeyError(\n            f\"Unable to deduce parameter type from {name} \"\n            f\"with details {details}\\n\"\n            \"Supported parameters:\\n\"\n            \"Float and Integer: Expected keys: 'lower', 'upper'\\n\"\n            \"Categorical: Expected keys: 'choices'\\n\"\n        )\n    return param_type\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.deduce_type","title":"deduce_type","text":"<pre><code>deduce_type(\n    name: str,\n    details: (\n        dict[str, str | int | float] | str | int | float\n    ),\n) -&gt; str\n</code></pre> <p>Deduces the parameter type from details.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing parameter specifications or a direct value (string, integer, or float).</p> <p> TYPE: <code>dict[str, str | int | float] | str | int | float</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The deduced parameter type ('int', 'float', 'categorical', or 'constant').</p> RAISES DESCRIPTION <code>TypeError</code> <p>If the type cannot be deduced or the details don't align with expected     constraints.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def deduce_type(\n    name: str, details: dict[str, str | int | float] | str | int | float\n) -&gt; str:\n    \"\"\"Deduces the parameter type from details.\n\n    Args:\n        name: The name of the parameter.\n        details: A dictionary containing parameter specifications or\n            a direct value (string, integer, or float).\n\n    Returns:\n        The deduced parameter type ('int', 'float', 'categorical', or 'constant').\n\n    Raises:\n        TypeError: If the type cannot be deduced or the details don't align with expected\n                constraints.\n    \"\"\"\n    if isinstance(details, (str, int, float)):\n        return \"const\"\n\n    if isinstance(details, dict):\n        if \"type\" in details:\n            param_type = details.pop(\"type\")\n            assert isinstance(param_type, str)\n            return param_type.lower()\n\n        return deduce_param_type(name, details)\n\n    raise TypeError(\n        f\"Unable to deduce parameter type for '{name}' with details '{details}'.\"\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_cat","title":"formatting_cat","text":"<pre><code>formatting_cat(\n    name: str, details: dict[str, list | str | int | float]\n) -&gt; dict\n</code></pre> <p>This function ensures that the 'choices' key in the details is a list and attempts to convert any elements expressed in scientific notation to floats. It also handles the 'default' value, converting it from scientific notation if necessary.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the categorical parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing the parameter's specifications. The required key      is 'choices', which must be a list. The 'default' key is optional.</p> <p> TYPE: <code>dict[str, list | str | int | float]</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If 'choices' is not a list.</p> RETURNS DESCRIPTION <code>dict</code> <p>The validated and possibly converted categorical parameter details.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_cat(name: str, details: dict[str, list | str | int | float]) -&gt; dict:\n    \"\"\"\n    This function ensures that the 'choices' key in the details is a list and attempts\n    to convert any elements expressed in scientific notation to floats. It also handles\n    the 'default' value, converting it from scientific notation if necessary.\n\n    Args:\n        name: The name of the categorical parameter.\n        details: A dictionary containing the parameter's specifications. The required key\n                 is 'choices', which must be a list. The 'default' key is optional.\n\n    Raises:\n        TypeError: If 'choices' is not a list.\n\n    Returns:\n        The validated and possibly converted categorical parameter details.\n    \"\"\"\n    if not isinstance(details[\"choices\"], list):\n        raise TypeError(f\"The 'choices' for '{name}' must be a list.\")\n\n    for i, element in enumerate(details[\"choices\"]):\n        try:\n            converted_value, e_flag = convert_scientific_notation(\n                element, show_usage_flag=True\n            )\n\n            if e_flag:\n                # Replace the element at the same position\n                details[\"choices\"][i] = converted_value\n        except ValueError:\n            pass  # If a ValueError occurs, simply continue to the next element\n\n    if \"default\" in details:\n        e_flag = False\n        extracted_default = details[\"default\"]\n        if not isinstance(extracted_default, (str, int, float)):\n            raise TypeError(\n                f\"The 'default' value for '{name}' must be a string, integer, or float.\"\n                f\" Got {type(extracted_default).__name__}.\"\n            )\n\n        try:\n            # check if e notation, if then convert to number\n            default, e_flag = convert_scientific_notation(\n                extracted_default, show_usage_flag=True\n            )\n        except ValueError:\n            pass  # if default value is not in a numeric format, Value Error occurs\n\n        if e_flag is True:\n            details[\"default\"] = default\n\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_const","title":"formatting_const","text":"<pre><code>formatting_const(\n    details: str | int | float,\n) -&gt; str | int | float\n</code></pre> <p>Validates and converts a constant parameter.</p> <p>This function checks if the 'details' parameter contains a value expressed in scientific notation and converts it to a float. It ensures that the input is appropriately formatted, either as a string, integer, or float.</p> PARAMETER DESCRIPTION <code>details</code> <p>A constant parameter that can be a string, integer, or float.      If the value is in scientific notation, it will be converted to a float.</p> <p> TYPE: <code>str | int | float</code> </p> RETURNS DESCRIPTION <code>str | int | float</code> <p>The validated and possibly converted constant parameter.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_const(details: str | int | float) -&gt; str | int | float:\n    \"\"\"Validates and converts a constant parameter.\n\n    This function checks if the 'details' parameter contains a value expressed in\n    scientific notation and converts it to a float. It ensures that the input\n    is appropriately formatted, either as a string, integer, or float.\n\n    Args:\n        details: A constant parameter that can be a string, integer, or float.\n                 If the value is in scientific notation, it will be converted to a float.\n\n    Returns:\n        The validated and possibly converted constant parameter.\n    \"\"\"\n\n    # check for e notation and convert it to float\n    e_flag = False\n    try:\n        converted_value, e_flag = convert_scientific_notation(\n            details, show_usage_flag=True\n        )\n    except ValueError:\n        # if the value is not able to convert to float a ValueError get raised by\n        # convert_scientific_notation function\n        pass\n\n    if e_flag:\n        details = converted_value\n\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_float","title":"formatting_float","text":"<pre><code>formatting_float(\n    name: str, details: dict[str, str | int | float]\n) -&gt; dict\n</code></pre> <p>Converts scientific notation values to floats.</p> <p>This function converts the 'lower' and 'upper' bounds, as well as the 'default' value (if present), from scientific notation to floats.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the float parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing the parameter's specifications. Expected keys      include 'lower', 'upper', and optionally 'default'.</p> <p> TYPE: <code>dict[str, str | int | float]</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If 'lower', 'upper', or 'default' cannot be converted from scientific        notation to floats.</p> RETURNS DESCRIPTION <code>dict</code> <p>The dictionary with the converted float parameter details.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_float(name: str, details: dict[str, str | int | float]) -&gt; dict:\n    \"\"\"\n    Converts scientific notation values to floats.\n\n    This function converts the 'lower' and 'upper' bounds, as well as the 'default'\n    value (if present), from scientific notation to floats.\n\n    Args:\n        name: The name of the float parameter.\n        details: A dictionary containing the parameter's specifications. Expected keys\n                 include 'lower', 'upper', and optionally 'default'.\n\n    Raises:\n        TypeError: If 'lower', 'upper', or 'default' cannot be converted from scientific\n                   notation to floats.\n\n    Returns:\n        The dictionary with the converted float parameter details.\n    \"\"\"\n\n    if not isinstance(details[\"lower\"], float) or not isinstance(details[\"upper\"], float):\n        try:\n            # for numbers like 1e-5 and 10^\n            details[\"lower\"] = convert_scientific_notation(details[\"lower\"])\n            details[\"upper\"] = convert_scientific_notation(details[\"upper\"])\n        except ValueError as e:\n            raise TypeError(\n                f\"'lower' and 'upper' must be float for \" f\"float parameter '{name}'.\"\n            ) from e\n    if \"default\" in details:\n        if not isinstance(details[\"default\"], float):\n            try:\n                details[\"default\"] = convert_scientific_notation(details[\"default\"])\n            except ValueError as e:\n                raise TypeError(\n                    f\" default'{details['default']}' must be float for float \"\n                    f\"parameter {name} \"\n                ) from e\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/yaml_search_space_utils/#neps.search_spaces.yaml_search_space_utils.formatting_int","title":"formatting_int","text":"<pre><code>formatting_int(\n    name: str, details: dict[str, str | int | float]\n) -&gt; dict\n</code></pre> <p>Converts scientific notation values to integers.</p> <p>This function converts the 'lower' and 'upper' bounds, as well as the 'default' value (if present), from scientific notation to integers.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the integer parameter.</p> <p> TYPE: <code>str</code> </p> <code>details</code> <p>A dictionary containing the parameter's                                     specifications. Expected keys include                                     'lower', 'upper', and optionally 'default'.</p> <p> TYPE: <code>dict[str, str | int | float]</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If 'lower', 'upper', or 'default' cannot be converted from scientific        notation to integers.</p> RETURNS DESCRIPTION <code>dict</code> <p>The dictionary with the converted integer parameter details.</p> Source code in <code>neps/search_spaces/yaml_search_space_utils.py</code> <pre><code>def formatting_int(name: str, details: dict[str, str | int | float]) -&gt; dict:\n    \"\"\"\n     Converts scientific notation values to integers.\n\n    This function converts the 'lower' and 'upper' bounds, as well as the 'default'\n    value (if present), from scientific notation to integers.\n\n    Args:\n        name (str): The name of the integer parameter.\n        details (dict[str, str | int | float]): A dictionary containing the parameter's\n                                                specifications. Expected keys include\n                                                'lower', 'upper', and optionally 'default'.\n\n    Raises:\n        TypeError: If 'lower', 'upper', or 'default' cannot be converted from scientific\n                   notation to integers.\n\n    Returns:\n        The dictionary with the converted integer parameter details.\n    \"\"\"\n    if not isinstance(details[\"lower\"], int) or not isinstance(details[\"upper\"], int):\n        try:\n            # for numbers like 1e2 and 10^\n            lower, flag_lower = convert_scientific_notation(\n                details[\"lower\"], show_usage_flag=True\n            )\n            upper, flag_upper = convert_scientific_notation(\n                details[\"upper\"], show_usage_flag=True\n            )\n            # check if one value format is e notation and if it's an integer\n            if flag_lower or flag_upper:\n                if lower == int(lower) and upper == int(upper):\n                    details[\"lower\"] = int(lower)\n                    details[\"upper\"] = int(upper)\n                else:\n                    raise TypeError()\n            else:\n                raise TypeError()\n        except (ValueError, TypeError) as e:\n            raise TypeError(\n                f\"'lower' and 'upper' must be integer for \" f\"integer parameter '{name}'.\"\n            ) from e\n    if \"default\" in details:\n        if not isinstance(details[\"default\"], int):\n            try:\n                # convert value can raise ValueError\n                default = convert_scientific_notation(details[\"default\"])\n                if default == int(default):\n                    details[\"default\"] = int(default)\n                else:\n                    raise TypeError()  # type of value is not int\n            except (ValueError, TypeError) as e:\n                raise TypeError(\n                    f\"default value {details['default']} \"\n                    f\"must be integer for integer parameter {name}\"\n                ) from e\n    return details\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/api/","title":"Api","text":""},{"location":"api/neps/search_spaces/architecture/api/#neps.search_spaces.architecture.api","title":"neps.search_spaces.architecture.api","text":""},{"location":"api/neps/search_spaces/architecture/api/#neps.search_spaces.architecture.api.Architecture","title":"Architecture","text":"<pre><code>Architecture(**kwargs)\n</code></pre> <p>Factory function.</p> Source code in <code>neps/search_spaces/architecture/api.py</code> <pre><code>def Architecture(**kwargs):\n    \"\"\"Factory function.\"\"\"\n    if \"structure\" not in kwargs:\n        raise ValueError(\"Factory function requires structure\")\n    if not isinstance(kwargs[\"structure\"], list) or len(kwargs[\"structure\"]) == 1:\n        base = GraphGrammar\n\n    class _FunctionParameter(base):\n        def __init__(\n            self,\n            structure: Grammar\n            | list[Grammar]\n            | ConstrainedGrammar\n            | list[ConstrainedGrammar]\n            | str\n            | list[str]\n            | dict\n            | list[dict],\n            primitives: dict,\n            # TODO: Follow this rabbit hole for `constraint_kwargs`,\n            # it can all be deleted my friend\n            constraint_kwargs: dict | None = None,\n            name: str = \"ArchitectureParameter\",\n            set_recursive_attribute: Callable | None = None,\n            **kwargs,\n        ):\n            local_vars = locals()\n            self.input_kwargs = {\n                args: local_vars[args]\n                for args in inspect.getfullargspec(self.__init__).args  # type: ignore[misc]\n                if args != \"self\"\n            }\n            self.input_kwargs.update(**kwargs)\n\n            if isinstance(structure, list):\n                structures = [\n                    _dict_structure_to_str(\n                        st,\n                        primitives,\n                        repetitive_mapping=kwargs.get(\n                            \"terminal_to_sublanguage_map\", None\n                        ),\n                    )\n                    if isinstance(st, dict)\n                    else st\n                    for st in structure\n                ]\n                _structures = []\n                for st in structures:\n                    if isinstance(st, str):\n                        if constraint_kwargs is None:\n                            _st = Grammar.fromstring(st)\n                        else:\n                            _st = ConstrainedGrammar.fromstring(st)\n                            _st.set_constraints(**constraint_kwargs)\n                    _structures.append(_st)  # type: ignore[has-type]\n                structures = _structures\n\n                super().__init__(\n                    grammars=structures,\n                    terminal_to_op_names=primitives,\n                    edge_attr=False,\n                    **kwargs,\n                )\n            else:\n                if isinstance(structure, dict):\n                    structure = _dict_structure_to_str(structure, primitives)\n\n                if isinstance(structure, str):\n                    if constraint_kwargs is None:\n                        structure = Grammar.fromstring(structure)\n                    else:\n                        structure = ConstrainedGrammar.fromstring(structure)\n                        structure.set_constraints(**constraint_kwargs)  # type: ignore[union-attr]\n\n                super().__init__(\n                    grammar=structure,  # type: ignore[arg-type]\n                    terminal_to_op_names=primitives,\n                    edge_attr=False,\n                    **kwargs,\n                )\n\n            self._set_recursive_attribute = set_recursive_attribute\n            self.name: str = name\n\n        def to_pytorch(self) -&gt; nn.Module:\n            self.clear_graph()\n            if len(self.nodes()) == 0:\n                composed_function = self.compose_functions()\n                # part below is required since PyTorch has no standard functional API\n                self.graph_to_self(composed_function)\n                self.prune_graph()\n\n                if self._set_recursive_attribute:\n                    m = _build(self, self._set_recursive_attribute)\n\n                if m is not None:\n                    return m\n\n                self.compile()\n                self.update_op_names()\n            return super().to_pytorch()  # create PyTorch model\n\n        def create_new_instance_from_id(self, identifier: str):\n            g = Architecture(**self.input_kwargs)  # type: ignore[arg-type]\n            g.load_from(identifier)\n            return g\n\n    return _FunctionParameter(**kwargs)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/api/#neps.search_spaces.architecture.api.ArchitectureParameter","title":"ArchitectureParameter","text":"<pre><code>ArchitectureParameter(**kwargs)\n</code></pre> <p>Deprecated: Use <code>Architecture</code> instead of <code>ArchitectureParameter</code>.</p> <p>This function remains for backward compatibility and will raise a deprecation warning if used.</p> Source code in <code>neps/search_spaces/architecture/api.py</code> <pre><code>def ArchitectureParameter(**kwargs):\n    \"\"\"Deprecated: Use `Architecture` instead of `ArchitectureParameter`.\n\n    This function remains for backward compatibility and will raise a deprecation\n    warning if used.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        (\n            \"Usage of 'neps.ArchitectureParameter' is deprecated and will be removed in\"\n            \" future releases. Please use 'neps.Architecture' instead.\"\n        ),\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    return Architecture(**kwargs)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/api/#neps.search_spaces.architecture.api.FunctionParameter","title":"FunctionParameter","text":"<pre><code>FunctionParameter(**kwargs)\n</code></pre> <p>Deprecated: Use <code>Function</code> instead of <code>FunctionParameter</code>.</p> <p>This function remains for backward compatibility and will raise a deprecation warning if used.</p> Source code in <code>neps/search_spaces/architecture/api.py</code> <pre><code>def FunctionParameter(**kwargs):\n    \"\"\"Deprecated: Use `Function` instead of `FunctionParameter`.\n\n    This function remains for backward compatibility and will raise a deprecation\n    warning if used.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        (\n            \"Usage of 'neps.FunctionParameter' is deprecated and will be removed in\"\n            \" future releases. Please use 'neps.Function' instead.\"\n        ),\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    return Function(**kwargs)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/","title":"Cfg","text":""},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg","title":"neps.search_spaces.architecture.cfg","text":""},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar","title":"Grammar","text":"<pre><code>Grammar(*args, **kwargs)\n</code></pre> <p>               Bases: <code>CFG</code></p> <p>Extended context free grammar (CFG) class from the NLTK python package We have provided functionality to sample from the CFG. We have included generation capability within the class (before it was an external function) Also allow sampling to return whole trees (not just the string of terminals).</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    # store some extra quantities needed later\n    non_unique_nonterminals = [str(prod.lhs()) for prod in self.productions()]\n    self.nonterminals = list(set(non_unique_nonterminals))\n    self.terminals = list(\n        {str(individual) for prod in self.productions() for individual in prod.rhs()}\n        - set(self.nonterminals)\n    )\n    # collect nonterminals that are worth swapping when doing genetic operations (i.e not those with a single production that leads to a terminal)\n    self.swappable_nonterminals = list(\n        {i for i in non_unique_nonterminals if non_unique_nonterminals.count(i) &gt; 1}\n    )\n\n    self._prior = None\n\n    if len(set(self.terminals).intersection(set(self.nonterminals))) &gt; 0:\n        raise Exception(\n            f\"Same terminal and nonterminal symbol: {set(self.terminals).intersection(set(self.nonterminals))}!\"\n        )\n    for nt in self.nonterminals:\n        if len(self.productions(Nonterminal(nt))) == 0:\n            raise Exception(f\"There is no production for nonterminal {nt}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar.mutate","title":"mutate","text":"<pre><code>mutate(\n    parent: str,\n    subtree_index: int,\n    subtree_node: str,\n    patience: int = 50,\n) -&gt; str\n</code></pre> <p>Grammar-based mutation, i.e., we sample a new subtree from a nonterminal node in the parse tree.</p> PARAMETER DESCRIPTION <code>parent</code> <p>parent of the mutation.</p> <p> TYPE: <code>str</code> </p> <code>subtree_index</code> <p>index pointing to the node that is root of the subtree.</p> <p> TYPE: <code>int</code> </p> <code>subtree_node</code> <p>nonterminal symbol of the node.</p> <p> TYPE: <code>str</code> </p> <code>patience</code> <p>Number of tries. Defaults to 50.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> RETURNS DESCRIPTION <code>str</code> <p>mutated child from parent.</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>def mutate(\n    self, parent: str, subtree_index: int, subtree_node: str, patience: int = 50\n) -&gt; str:\n    \"\"\"Grammar-based mutation, i.e., we sample a new subtree from a nonterminal\n    node in the parse tree.\n\n    Args:\n        parent (str): parent of the mutation.\n        subtree_index (int): index pointing to the node that is root of the subtree.\n        subtree_node (str): nonterminal symbol of the node.\n        patience (int, optional): Number of tries. Defaults to 50.\n\n    Returns:\n        str: mutated child from parent.\n    \"\"\"\n    # chop out subtree\n    pre, _, post = self.remove_subtree(parent, subtree_index)\n    _patience = patience\n    while _patience &gt; 0:\n        # only sample subtree -&gt; avoids full sampling of large parse trees\n        new_subtree = self.sampler(1, start_symbol=subtree_node)[0]\n        child = pre + new_subtree + post\n        if parent != child:  # ensure that parent is really mutated\n            break\n        _patience -= 1\n\n    return child.strip()\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar.rand_subtree","title":"rand_subtree","text":"<pre><code>rand_subtree(tree: str) -&gt; tuple[str, int]\n</code></pre> <p>Helper function to choose a random subtree in a given parse tree. Runs a single pass through the tree (stored as string) to look for the location of swappable nonterminal symbols.</p> PARAMETER DESCRIPTION <code>tree</code> <p>parse tree.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple[str, int]</code> <p>Tuple[str, int]: return the parent node of the subtree and its index.</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>def rand_subtree(self, tree: str) -&gt; tuple[str, int]:\n    \"\"\"Helper function to choose a random subtree in a given parse tree.\n    Runs a single pass through the tree (stored as string) to look for\n    the location of swappable nonterminal symbols.\n\n    Args:\n        tree (str): parse tree.\n\n    Returns:\n        Tuple[str, int]: return the parent node of the subtree and its index.\n    \"\"\"\n    split_tree = tree.split(\" \")\n    swappable_indices = [\n        i\n        for i in range(len(split_tree))\n        if split_tree[i][1:] in self.swappable_nonterminals\n    ]\n    r = np.random.randint(1, len(swappable_indices))\n    chosen_non_terminal = split_tree[swappable_indices[r]][1:]\n    chosen_non_terminal_index = swappable_indices[r]\n    return chosen_non_terminal, chosen_non_terminal_index\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/cfg/#neps.search_spaces.architecture.cfg.Grammar.remove_subtree","title":"remove_subtree  <code>staticmethod</code>","text":"<pre><code>remove_subtree(\n    tree: str, index: int\n) -&gt; tuple[str, str, str]\n</code></pre> <p>Helper functioon to remove a subtree from a parse tree given its index. E.g. '(S (S (T 2)) (ADD +) (T 1))' becomes '(S (S (T 2)) ', '(T 1))'  after removing (ADD +).</p> PARAMETER DESCRIPTION <code>tree</code> <p>parse tree</p> <p> TYPE: <code>str</code> </p> <code>index</code> <p>index of the subtree root node</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tuple[str, str, str]</code> <p>Tuple[str, str, str]: part before the subtree, subtree, part past subtree</p> Source code in <code>neps/search_spaces/architecture/cfg.py</code> <pre><code>@staticmethod\ndef remove_subtree(tree: str, index: int) -&gt; tuple[str, str, str]:\n    \"\"\"Helper functioon to remove a subtree from a parse tree\n    given its index.\n    E.g. '(S (S (T 2)) (ADD +) (T 1))'\n    becomes '(S (S (T 2)) ', '(T 1))'  after removing (ADD +).\n\n    Args:\n        tree (str): parse tree\n        index (int): index of the subtree root node\n\n    Returns:\n        Tuple[str, str, str]: part before the subtree, subtree, part past subtree\n    \"\"\"\n    split_tree = tree.split(\" \")\n    pre_subtree = \" \".join(split_tree[:index]) + \" \"\n    #  get chars to the right of split\n    right = \" \".join(split_tree[index + 1 :])\n    # remove chosen subtree\n    # single pass to find the bracket matching the start of the split\n    counter, current_index = 1, 0\n    for char in right:\n        if char == \"(\":\n            counter += 1\n        elif char == \")\":\n            counter -= 1\n        if counter == 0:\n            break\n        current_index += 1\n    post_subtree = right[current_index + 1 :]\n    removed = \"\".join(split_tree[index]) + \" \" + right[: current_index + 1]\n    return (pre_subtree, removed, post_subtree)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/","title":"Core graph grammar","text":""},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar","title":"neps.search_spaces.architecture.core_graph_grammar","text":""},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar","title":"CoreGraphGrammar","text":"<pre><code>CoreGraphGrammar(\n    grammars: list[Grammar] | Grammar,\n    terminal_to_op_names: dict,\n    terminal_to_graph_edges: dict | None = None,\n    edge_attr: bool = True,\n    edge_label: str = \"op_name\",\n    zero_op: list | None = None,\n    identity_op: list | None = None,\n    name: str | None = None,\n    scope: str | None = None,\n    return_all_subgraphs: bool = False,\n    return_graph_per_hierarchy: bool = False,\n)\n</code></pre> <p>               Bases: <code>Graph</code></p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def __init__(\n    self,\n    grammars: list[Grammar] | Grammar,\n    terminal_to_op_names: dict,\n    terminal_to_graph_edges: dict | None = None,\n    edge_attr: bool = True,\n    edge_label: str = \"op_name\",\n    zero_op: list | None = None,\n    identity_op: list | None = None,\n    name: str | None = None,\n    scope: str | None = None,\n    return_all_subgraphs: bool = False,\n    return_graph_per_hierarchy: bool = False,\n):\n    super().__init__(name, scope)\n\n    self.grammars = [grammars] if isinstance(grammars, Grammar) else grammars\n\n    self.terminal_to_op_names = terminal_to_op_names\n\n    grammar_terminals = {\n        terminal for grammar in self.grammars for terminal in grammar.terminals\n    }\n    diff_terminals = grammar_terminals - set(self.terminal_to_op_names.keys())\n    if len(diff_terminals) != 0:\n        raise Exception(\n            f\"Terminals {diff_terminals} not defined in primitive mapping!\"\n        )\n\n    if terminal_to_graph_edges is None:  # only compute it once -&gt; more efficient\n        self.terminal_to_graph_edges = get_edge_lists_of_topologies(\n            self.terminal_to_op_names\n        )\n    else:\n        self.terminal_to_graph_edges = terminal_to_graph_edges\n    self.edge_attr = edge_attr\n    self.edge_label = edge_label\n\n    self.zero_op = zero_op if zero_op is not None else []\n    self.identity_op = identity_op if identity_op is not None else []\n\n    self.terminal_to_graph_nodes: dict = {}\n\n    self.return_all_subgraphs = return_all_subgraphs\n    self.return_graph_per_hierarchy = return_graph_per_hierarchy\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.OPTIMIZER_SCOPE","title":"OPTIMIZER_SCOPE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPTIMIZER_SCOPE = 'all'\n</code></pre> <p>Whether the search space has an interface to one of the tabular benchmarks which can then be used to query architecture performances.</p> <p>If this is set to true then <code>query()</code> should be implemented.</p>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>As it is very complicated to compare graphs (i.e. check all edge attributes, do the have shared attributes, ...) use just the name for comparison.</p> <p>This is used when determining whether two instances are copies.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __hash__(self):\n    \"\"\"As it is very complicated to compare graphs (i.e. check all edge\n    attributes, do the have shared attributes, ...) use just the name\n    for comparison.\n\n    This is used when determining whether two instances are copies.\n    \"\"\"\n    h = 0\n    h += hash(self.name)\n    h += hash(self.scope) if self.scope else 0\n    h += hash(self._id)\n    return h\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.add_node","title":"add_node","text":"<pre><code>add_node(node_index, **attr)\n</code></pre> <p>Adds a node to the graph.</p> <p>Note that adding a node using an index that has been used already will override its attributes.</p> PARAMETER DESCRIPTION <code>node_index</code> <p>The index for the node. Expect to be &gt;= 1.</p> <p> TYPE: <code>int</code> </p> <code>**attr</code> <p>The attributes which can be added in a dict like form.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def add_node(self, node_index, **attr):\n    \"\"\"Adds a node to the graph.\n\n    Note that adding a node using an index that has been used already\n    will override its attributes.\n\n    Args:\n        node_index (int): The index for the node. Expect to be &gt;= 1.\n        **attr: The attributes which can be added in a dict like form.\n    \"\"\"\n    assert node_index &gt;= 1, \"Expecting the node index to be greater or equal 1\"\n    nx.DiGraph.add_node(self, node_index, **attr)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Deep copy of the current graph.</p> RETURNS DESCRIPTION <code>Graph</code> <p>Deep copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def clone(self):\n    \"\"\"Deep copy of the current graph.\n\n    Returns:\n        Graph: Deep copy of the graph.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.compile","title":"compile","text":"<pre><code>compile()\n</code></pre> <p>Instanciates the ops at the edges using the arguments specified at the edges.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def compile(self):\n    \"\"\"Instanciates the ops at the edges using the arguments specified at the edges.\"\"\"\n    for graph in [*self._get_child_graphs(single_instances=False), self]:\n        logger.debug(f\"Compiling graph {graph.name}\")\n        for _, v, edge_data in graph.edges.data():\n            if not edge_data.is_final():\n                attr = edge_data.to_dict()\n                op = attr.pop(\"op\")\n\n                if isinstance(op, list):\n                    compiled_ops = []\n                    for i, o in enumerate(op):\n                        if inspect.isclass(o):\n                            # get the relevant parameter if there are more.\n                            a = {\n                                k: v[i] if isinstance(v, list) else v\n                                for k, v in attr.items()\n                            }\n                            compiled_ops.append(o(**a))\n                        else:\n                            logger.debug(f\"op {o} already compiled. Skipping\")\n                    edge_data.set(\"op\", compiled_ops)\n                elif isinstance(op, AbstractPrimitive):\n                    logger.debug(f\"op {op} already compiled. Skipping\")\n                elif inspect.isclass(op) and issubclass(op, AbstractPrimitive):\n                    # Init the class\n                    if \"op_name\" in attr:\n                        del attr[\"op_name\"]\n                    edge_data.set(\"op\", op(**attr))\n                elif isinstance(op, Graph):\n                    pass  # This is already covered by _get_child_graphs\n                else:\n                    raise ValueError(f\"Unkown format of op: {op}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy as defined in networkx, i.e. a shallow copy.</p> <p>Just handling recursively nested graphs seperately.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def copy(self):\n    \"\"\"Copy as defined in networkx, i.e. a shallow copy.\n\n    Just handling recursively nested graphs seperately.\n    \"\"\"\n\n    def copy_dict(d):\n        copied_dict = d.copy()\n        for k, v in d.items():\n            if isinstance(v, Graph):\n                copied_dict[k] = v.copy()\n            elif isinstance(v, list):\n                copied_dict[k] = [i.copy() if isinstance(i, Graph) else i for i in v]\n            elif isinstance(v, (AbstractPrimitive, torch.nn.Module)):\n                copied_dict[k] = copy.deepcopy(v)\n        return copied_dict\n\n    G = self.__class__()\n    G.graph.update(self.graph)\n    G.add_nodes_from((n, copy_dict(d)) for n, d in self._node.items())\n    G.add_edges_from(\n        (u, v, datadict.copy())\n        for u, nbrs in self._adj.items()\n        for v, datadict in nbrs.items()\n    )\n    G.scope = self.scope\n    G.name = self.name\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.from_stringTree_to_graph_repr","title":"from_stringTree_to_graph_repr","text":"<pre><code>from_stringTree_to_graph_repr(\n    string_tree: str,\n    grammar: Grammar,\n    valid_terminals: KeysView,\n    edge_attr: bool = True,\n    sym_name: str = \"op_name\",\n    prune: bool = True,\n    add_subtree_map: bool = False,\n    return_all_subgraphs: bool | None = None,\n    return_graph_per_hierarchy: bool | None = None,\n) -&gt; DiGraph | tuple[DiGraph, OrderedDict]\n</code></pre> <p>Generates graph from parse tree in string representation. Note that we ignore primitive HPs!</p> PARAMETER DESCRIPTION <code>string_tree</code> <p>parse tree.</p> <p> TYPE: <code>str</code> </p> <code>grammar</code> <p>underlying grammar.</p> <p> TYPE: <code>Grammar</code> </p> <code>valid_terminals</code> <p>list of keys.</p> <p> TYPE: <code>list</code> </p> <code>edge_attr</code> <p>Shoud graph be edge attributed (True) or node attributed (False). Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sym_name</code> <p>Attribute name of operation. Defaults to \"op_name\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'op_name'</code> </p> <code>prune</code> <p>Prune graph, e.g., None operations etc. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>add_subtree_map</code> <p>Add attribute indicating to which subtrees of the parse tree the specific part belongs to. Can only be true if you set prune=False! TODO: Check if we really need this constraint or can also allow pruning. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>return_all_subgraphs</code> <p>Additionally returns an hierarchical dictionary containing all subgraphs. Defaults to False. TODO: check if edge attr also works.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>return_graph_per_hierarchy</code> <p>Additionally returns a graph from each each hierarchy.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DiGraph | tuple[DiGraph, OrderedDict]</code> <p>nx.DiGraph: [description]</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def from_stringTree_to_graph_repr(\n    self,\n    string_tree: str,\n    grammar: Grammar,\n    valid_terminals: collections.abc.KeysView,\n    edge_attr: bool = True,\n    sym_name: str = \"op_name\",\n    prune: bool = True,\n    add_subtree_map: bool = False,\n    return_all_subgraphs: bool | None = None,\n    return_graph_per_hierarchy: bool | None = None,\n) -&gt; nx.DiGraph | tuple[nx.DiGraph, collections.OrderedDict]:\n    \"\"\"Generates graph from parse tree in string representation.\n    Note that we ignore primitive HPs!\n\n    Args:\n        string_tree (str): parse tree.\n        grammar (Grammar): underlying grammar.\n        valid_terminals (list): list of keys.\n        edge_attr (bool, optional): Shoud graph be edge attributed (True) or node attributed (False). Defaults to True.\n        sym_name (str, optional): Attribute name of operation. Defaults to \"op_name\".\n        prune (bool, optional): Prune graph, e.g., None operations etc. Defaults to True.\n        add_subtree_map (bool, optional): Add attribute indicating to which subtrees of\n            the parse tree the specific part belongs to. Can only be true if you set prune=False!\n            TODO: Check if we really need this constraint or can also allow pruning. Defaults to False.\n        return_all_subgraphs (bool, optional): Additionally returns an hierarchical dictionary\n            containing all subgraphs. Defaults to False.\n            TODO: check if edge attr also works.\n        return_graph_per_hierarchy (bool, optional): Additionally returns a graph from each\n            each hierarchy.\n\n    Returns:\n        nx.DiGraph: [description]\n    \"\"\"\n\n    def get_node_labels(graph: nx.DiGraph):\n        return [\n            (n, d[sym_name])\n            for n, d in graph.nodes(data=True)\n            if d[sym_name] != \"input\" and d[sym_name] != \"output\"\n        ]\n\n    def get_hierarchicy_dict(\n        string_tree: str,\n        subgraphs: dict,\n        hierarchy_dict: dict | None = None,\n        hierarchy_level_counter: int = 0,\n    ):\n        if hierarchy_dict is None:\n            hierarchy_dict = {}\n        if hierarchy_level_counter not in hierarchy_dict:\n            hierarchy_dict[hierarchy_level_counter] = []\n        hierarchy_dict[hierarchy_level_counter].append(string_tree)\n        node_labels = get_node_labels(subgraphs[string_tree])\n        for _, node_label in node_labels:\n            if node_label in subgraphs:\n                hierarchy_dict = get_hierarchicy_dict(\n                    node_label, subgraphs, hierarchy_dict, hierarchy_level_counter + 1\n                )\n        return hierarchy_dict\n\n    def get_graph_per_hierarchy(string_tree: str, subgraphs: dict):\n        hierarchy_dict = get_hierarchicy_dict(\n            string_tree=string_tree, subgraphs=subgraphs\n        )\n\n        graph_per_hierarchy = collections.OrderedDict()\n        for k, v in hierarchy_dict.items():\n            if k == 0:\n                graph_per_hierarchy[k] = subgraphs[v[0]]\n            else:\n                subgraph_ = graph_per_hierarchy[k - 1].copy()\n                node_labels = get_node_labels(subgraph_)\n                for node, node_label in node_labels:\n                    if node_label in list(subgraphs.keys()):\n                        in_nodes = list(subgraph_.predecessors(node))\n                        out_nodes = list(subgraph_.successors(node))\n                        node_offset = max(subgraph_.nodes) + 1\n\n                        new_subgraph = nx.relabel.relabel_nodes(\n                            subgraphs[node_label],\n                            mapping={\n                                n: n + node_offset\n                                for n in subgraphs[node_label].nodes\n                            },\n                            copy=True,\n                        )\n                        first_nodes = {e[0] for e in new_subgraph.edges}\n                        second_nodes = {e[1] for e in new_subgraph.edges}\n                        (begin_node,) = first_nodes - second_nodes\n                        (end_node,) = second_nodes - first_nodes\n                        successors = list(new_subgraph.successors(begin_node))\n                        predecessors = list(new_subgraph.predecessors(end_node))\n                        new_subgraph.remove_nodes_from([begin_node, end_node])\n                        edges = []\n                        added_identities = False\n                        for in_node in in_nodes:\n                            for succ in successors:\n                                if succ == end_node:\n                                    if not added_identities:\n                                        edges.extend(\n                                            [\n                                                (inn, onn)\n                                                for inn in in_nodes\n                                                for onn in out_nodes\n                                            ]\n                                        )\n                                    added_identities = True\n                                else:\n                                    edges.append((in_node, succ))\n                        for out_node in out_nodes:\n                            for pred in predecessors:\n                                if pred != begin_node:\n                                    edges.append((pred, out_node))\n\n                        subgraph_ = nx.compose(new_subgraph, subgraph_)\n                        subgraph_.add_edges_from(edges)\n                        subgraph_.remove_node(node)\n\n                graph_per_hierarchy[k] = subgraph_\n        return graph_per_hierarchy\n\n    def to_node_attributed_edge_list(\n        edge_list: list[tuple],\n    ) -&gt; tuple[list[tuple[int, int]], dict]:\n        node_offset = 2\n        edge_to_node_map = {e: i + node_offset for i, e in enumerate(edge_list)}\n        first_nodes = {e[0] for e in edge_list}\n        second_nodes = {e[1] for e in edge_list}\n        (src,) = first_nodes - second_nodes\n        (tgt,) = second_nodes - first_nodes\n        node_list = []\n        for e in edge_list:\n            ni = edge_to_node_map[e]\n            u, v = e\n            if u == src:\n                node_list.append((0, ni))\n            if v == tgt:\n                node_list.append((ni, 1))\n\n            for e_ in filter(lambda e: (e[1] == u), edge_list):\n                node_list.append((edge_to_node_map[e_], ni))\n\n        return node_list, edge_to_node_map\n\n    def skip_char(char: str) -&gt; bool:\n        return char in [\" \", \"\\t\", \"\\n\", \"[\", \"]\"]\n\n    if prune:\n        add_subtree_map = False\n\n    if return_all_subgraphs is None:\n        return_all_subgraphs = self.return_all_subgraphs\n    if return_graph_per_hierarchy is None:\n        return_graph_per_hierarchy = self.return_graph_per_hierarchy\n    compute_subgraphs = return_all_subgraphs or return_graph_per_hierarchy\n\n    G = nx.DiGraph()\n    if add_subtree_map:\n        q_nonterminals: collections.deque = collections.deque()\n    if compute_subgraphs:\n        q_subtrees: collections.deque = collections.deque()\n        q_subgraphs: collections.deque = collections.deque()\n        subgraphs_dict = collections.OrderedDict()\n    if edge_attr:\n        node_offset = 0\n        q_el: collections.deque = collections.deque()  # edge-attr\n        terminal_to_graph = self.terminal_to_graph_edges\n    else:  # node-attributed\n        G.add_node(0, **{sym_name: \"input\"})\n        G.add_node(1, **{sym_name: \"output\"})\n        node_offset = 2\n        if bool(self.terminal_to_graph_nodes):\n            terminal_to_graph_nodes = self.terminal_to_graph_nodes\n        else:\n            terminal_to_graph_nodes = {\n                k: to_node_attributed_edge_list(edge_list) if edge_list else []\n                for k, edge_list in self.terminal_to_graph_edges.items()\n            }\n            self.terminal_to_graph_nodes = terminal_to_graph_nodes\n        terminal_to_graph = {\n            k: v[0] if v else [] for k, v in terminal_to_graph_nodes.items()\n        }\n        q_el = collections.deque()  # node-attr\n\n    # pre-compute stuff\n    begin_end_nodes = {}\n    for sym, g in terminal_to_graph.items():\n        if g:\n            first_nodes = {e[0] for e in g}\n            second_nodes = {e[1] for e in g}\n            (begin_node,) = first_nodes - second_nodes\n            (end_node,) = second_nodes - first_nodes\n            begin_end_nodes[sym] = (begin_node, end_node)\n        else:\n            begin_end_nodes[sym] = (None, None)\n\n    for split_idx, sym in enumerate(string_tree.split(\" \")):\n        is_nonterminal = False\n        if sym == \"\":\n            continue\n        if compute_subgraphs:\n            new_sym = True\n            sym_copy = sym[:]\n        if sym[0] == \"(\":\n            sym = sym[1:]\n            is_nonterminal = True\n        if sym[-1] == \")\":\n            if add_subtree_map:\n                for _ in range(sym.count(\")\")):\n                    q_nonterminals.pop()\n            if compute_subgraphs:\n                new_sym = False\n            while sym[-1] == \")\" and sym not in valid_terminals:\n                sym = sym[:-1]\n\n        if compute_subgraphs and new_sym:\n            if sym in grammar.nonterminals:\n                # need dict as a graph can have multiple subgraphs\n                q_subtrees.append(sym_copy[:])\n            else:\n                q_subtrees[-1] += f\" {sym_copy}\"\n\n        if len(sym) == 1 and skip_char(sym[0]):\n            continue\n\n        if add_subtree_map and sym in grammar.nonterminals:\n            q_nonterminals.append((sym, split_idx))\n        elif sym in valid_terminals and not is_nonterminal:  # terminal symbol\n            if sym in self.terminal_to_graph_edges:\n                if len(q_el) == 0:\n                    if edge_attr:\n                        edges = [\n                            tuple(t + node_offset for t in e)\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n                    else:  # node-attr\n                        edges = [\n                            tuple(t for t in e)\n                            for e in terminal_to_graph_nodes[sym][0]\n                        ]\n                        nodes = [\n                            terminal_to_graph_nodes[sym][1][e]\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n                    if add_subtree_map:\n                        subtrees = []\n                    first_nodes = {e[0] for e in edges}\n                    second_nodes = {e[1] for e in edges}\n                    (src_node,) = first_nodes - second_nodes\n                    (sink_node,) = second_nodes - first_nodes\n                else:\n                    begin_node, end_node = begin_end_nodes[sym]\n                    el = q_el.pop()\n                    if edge_attr:\n                        u, v = el\n                        if add_subtree_map:\n                            subtrees = G[u][v][\"subtrees\"]\n                        G.remove_edge(u, v)\n                        edges = [\n                            tuple(\n                                u\n                                if t == begin_node\n                                else v\n                                if t == end_node\n                                else t + node_offset\n                                for t in e\n                            )\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n                    else:  # node-attr\n                        n = el\n                        if add_subtree_map:\n                            subtrees = G.nodes[n][\"subtrees\"]\n                        in_nodes = list(G.predecessors(n))\n                        out_nodes = list(G.successors(n))\n                        G.remove_node(n)\n                        edges = []\n                        for e in terminal_to_graph_nodes[sym][0]:\n                            if not (e[0] == begin_node or e[1] == end_node):\n                                edges.append((e[0] + node_offset, e[1] + node_offset))\n                            elif e[0] == begin_node:\n                                for nin in in_nodes:\n                                    edges.append((nin, e[1] + node_offset))\n                            elif e[1] == end_node:\n                                for nout in out_nodes:\n                                    edges.append((e[0] + node_offset, nout))\n                        nodes = [\n                            terminal_to_graph_nodes[sym][1][e] + node_offset\n                            for e in self.terminal_to_graph_edges[sym]\n                        ]\n\n                G.add_edges_from(edges)\n\n                if compute_subgraphs:\n                    subgraph = nx.DiGraph()\n                    subgraph.add_edges_from(edges)\n                    q_subgraphs.append(\n                        {\n                            \"graph\": subgraph,\n                            \"atoms\": collections.OrderedDict(\n                                (atom, None)\n                                for atom in (edges if edge_attr else nodes)\n                            ),\n                        }\n                    )\n\n                if add_subtree_map:\n                    if edge_attr:\n                        subtrees.append(q_nonterminals[-1])\n                        for u, v in edges:\n                            G[u][v][\"subtrees\"] = subtrees.copy()\n                    else:  # node-attr\n                        subtrees.append(q_nonterminals[-1])\n                        for n in nodes:\n                            G.nodes[n][\"subtrees\"] = subtrees.copy()\n\n                q_el.extend(reversed(edges if edge_attr else nodes))\n                if edge_attr:\n                    node_offset += max(max(self.terminal_to_graph_edges[sym]))\n                else:\n                    node_offset += max(terminal_to_graph_nodes[sym][1].values())\n            else:  # primitive operations\n                el = q_el.pop()\n                if edge_attr:\n                    u, v = el\n                    if prune and sym in self.zero_op:\n                        G.remove_edge(u, v)\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].remove_edge(u, v)\n                            del q_subgraphs[-1][\"atoms\"][(u, v)]\n                    else:\n                        G[u][v][sym_name] = sym\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"][u][v][sym_name] = sym\n                        if add_subtree_map:\n                            G[u][v][\"subtrees\"].append(q_nonterminals[-1])\n                            q_nonterminals.pop()\n                else:  # node-attr\n                    n = el\n                    if prune and sym in self.zero_op:\n                        G.remove_node(n)\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].remove_node(n)\n                            del q_subgraphs[-1][\"atoms\"][n]\n                    elif prune and sym in self.identity_op:\n                        G.add_edges_from(\n                            [\n                                (n_in, n_out)\n                                for n_in in G.predecessors(n)\n                                for n_out in G.successors(n)\n                            ]\n                        )\n                        G.remove_node(n)\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].add_edges_from(\n                                [\n                                    (n_in, n_out)\n                                    for n_in in q_subgraphs[-1][\"graph\"].predecessors(\n                                        n\n                                    )\n                                    for n_out in q_subgraphs[-1][\"graph\"].successors(\n                                        n\n                                    )\n                                ]\n                            )\n                            q_subgraphs[-1][\"graph\"].remove_node(n)\n                            del q_subgraphs[-1][\"atoms\"][n]\n                    else:\n                        G.nodes[n][sym_name] = sym\n                        if compute_subgraphs:\n                            q_subgraphs[-1][\"graph\"].nodes[n][sym_name] = sym\n                            q_subgraphs[-1][\"atoms\"][\n                                next(\n                                    filter(\n                                        lambda x: x[1] is None,\n                                        q_subgraphs[-1][\"atoms\"].items(),\n                                    )\n                                )[0]\n                            ] = sym\n                        if add_subtree_map:\n                            G.nodes[n][\"subtrees\"].append(q_nonterminals[-1])\n                            q_nonterminals.pop()\n        if compute_subgraphs and sym_copy[-1] == \")\":\n            q_subtrees[-1] += f\" {sym_copy}\"\n            for _ in range(sym_copy.count(\")\")):\n                subtree_identifier = q_subtrees.pop()\n                if len(q_subtrees) &gt; 0:\n                    q_subtrees[-1] += f\" {subtree_identifier}\"\n                if len(q_subtrees) == len(q_subgraphs) - 1:\n                    difference = subtree_identifier.count(\n                        \"(\"\n                    ) - subtree_identifier.count(\")\")\n                    if difference &lt; 0:\n                        subtree_identifier = subtree_identifier[:difference]\n                    subgraph_dict = q_subgraphs.pop()\n                    subgraph = subgraph_dict[\"graph\"]\n                    atoms = subgraph_dict[\"atoms\"]\n                    if len(q_subtrees) &gt; 0:\n                        # subtree_identifier is subgraph graph at [-1]\n                        # (and sub-...-subgraph currently in q_subgraphs)\n                        q_subgraphs[-1][\"atoms\"][\n                            next(\n                                filter(\n                                    lambda x: x[1] is None,\n                                    q_subgraphs[-1][\"atoms\"].items(),\n                                )\n                            )[0]\n                        ] = subtree_identifier\n\n                    for atom in filter(lambda x: x[1] is not None, atoms.items()):\n                        if edge_attr:\n                            subgraph[atom[0][0]][atom[0][1]][sym_name] = atom[1]\n                        else:  # node-attr\n                            subgraph.nodes[atom[0]][sym_name] = atom[1]\n\n                    if not edge_attr:  # node-attr\n                        # ensure there is actually one input and output node\n                        first_nodes = {e[0] for e in subgraph.edges}\n                        second_nodes = {e[1] for e in subgraph.edges}\n                        new_src_node = max(subgraph.nodes) + 1\n                        src_nodes = first_nodes - second_nodes\n                        subgraph.add_edges_from(\n                            [\n                                (new_src_node, successor)\n                                for src_node in src_nodes\n                                for successor in subgraph.successors(src_node)\n                            ]\n                        )\n                        subgraph.add_node(new_src_node, **{sym_name: \"input\"})\n                        subgraph.remove_nodes_from(src_nodes)\n                        new_sink_node = max(subgraph.nodes) + 1\n                        sink_nodes = second_nodes - first_nodes\n                        subgraph.add_edges_from(\n                            [\n                                (predecessor, new_sink_node)\n                                for sink_node in sink_nodes\n                                for predecessor in subgraph.predecessors(sink_node)\n                            ]\n                        )\n                        subgraph.add_node(new_sink_node, **{sym_name: \"output\"})\n                        subgraph.remove_nodes_from(sink_nodes)\n                    subgraphs_dict[subtree_identifier] = subgraph\n\n    if len(q_el) != 0:\n        raise Exception(\"Invalid string_tree\")\n\n    if prune:\n        G = self.prune_unconnected_parts(G, src_node, sink_node)\n    self._check_graph(G)\n\n    if return_all_subgraphs or return_graph_per_hierarchy:\n        return_val = [G]\n        subgraphs_dict = collections.OrderedDict(\n            reversed(list(subgraphs_dict.items()))\n        )\n        if prune:\n            for v in subgraphs_dict.values():\n                first_nodes = {e[0] for e in v.edges}\n                second_nodes = {e[1] for e in v.edges}\n                (vG_src_node,) = first_nodes - second_nodes\n                (vG_sink_node,) = second_nodes - first_nodes\n                v = self.prune_unconnected_parts(v, vG_src_node, vG_sink_node)\n                self._check_graph(v)\n        if return_all_subgraphs:\n            return_val.append(subgraphs_dict)\n        if return_graph_per_hierarchy:\n            graph_per_hierarchy = get_graph_per_hierarchy(string_tree, subgraphs_dict)\n            _ = (\n                graph_per_hierarchy.popitem()\n            )  # remove last graph since it is equal to full graph\n            return_val.append(graph_per_hierarchy)\n        return return_val\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.get_graph_representation","title":"get_graph_representation","text":"<pre><code>get_graph_representation(\n    identifier: str, grammar: Grammar, edge_attr: bool\n) -&gt; DiGraph\n</code></pre> <p>This functions takes an identifier and constructs the (multi-variate) composition of the functions it describes.</p> PARAMETER DESCRIPTION <code>identifier</code> <p>identifier</p> <p> TYPE: <code>str</code> </p> <code>grammar</code> <p>grammar</p> <p> TYPE: <code>Grammar</code> </p> <code>flatten_graph</code> <p>Whether to flatten the graph. Defaults to True.</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>DiGraph</code> <p>nx.DiGraph: (multi-variate) composition of functions.</p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def get_graph_representation(\n    self,\n    identifier: str,\n    grammar: Grammar,\n    edge_attr: bool,\n) -&gt; nx.DiGraph:\n    \"\"\"This functions takes an identifier and constructs the\n    (multi-variate) composition of the functions it describes.\n\n    Args:\n        identifier (str): identifier\n        grammar (Grammar): grammar\n        flatten_graph (bool, optional): Whether to flatten the graph. Defaults to True.\n\n    Returns:\n        nx.DiGraph: (multi-variate) composition of functions.\n    \"\"\"\n\n    def _skip_char(char: str) -&gt; bool:\n        return char in [\" \", \"\\t\", \"\\n\", \"[\", \"]\"]\n\n    def _get_sym_from_split(split: str) -&gt; str:\n        start_idx, end_idx = 0, len(split)\n        while start_idx &lt; end_idx and split[start_idx] == \"(\":\n            start_idx += 1\n        while start_idx &lt; end_idx and split[end_idx - 1] == \")\":\n            end_idx -= 1\n        return split[start_idx:end_idx]\n\n    def to_node_attributed_edge_list(\n        edge_list: list[tuple],\n    ) -&gt; tuple[list[tuple[int, int]], dict]:\n        first_nodes = {e[0] for e in edge_list}\n        second_nodes = {e[1] for e in edge_list}\n        src = first_nodes - second_nodes\n        tgt = second_nodes - first_nodes\n        node_offset = len(src)\n        edge_to_node_map = {e: i + node_offset for i, e in enumerate(edge_list)}\n        node_list = []\n        for e in edge_list:\n            ni = edge_to_node_map[e]\n            u, v = e\n            if u in src:\n                node_list.append((u, ni))\n            if v in tgt:\n                node_list.append((ni, v))\n\n            for e_ in filter(lambda e: (e[1] == u), edge_list):\n                node_list.append((edge_to_node_map[e_], ni))\n\n        return node_list, edge_to_node_map\n\n    descriptor = self.id_to_string_tree(identifier)\n\n    if edge_attr:\n        terminal_to_graph = self.terminal_to_graph_edges\n    else:  # node-attr\n        terminal_to_graph_nodes = {\n            k: to_node_attributed_edge_list(edge_list) if edge_list else (None, None)\n            for k, edge_list in self.terminal_to_graph_edges.items()\n        }\n        terminal_to_graph = {k: v[0] for k, v in terminal_to_graph_nodes.items()}\n        # edge_to_node_map = {k: v[1] for k, v in terminal_to_graph_nodes.items()}\n\n    q_nonterminals: queue.LifoQueue = queue.LifoQueue()\n    q_topologies: queue.LifoQueue = queue.LifoQueue()\n    q_primitives: queue.LifoQueue = queue.LifoQueue()\n\n    G = nx.DiGraph()\n    for _, split in enumerate(descriptor.split(\" \")):\n        if _skip_char(split):\n            continue\n        sym = _get_sym_from_split(split)\n\n        if sym in grammar.terminals:\n            is_topology = False\n            if (\n                inspect.isclass(self.terminal_to_op_names[sym])\n                and issubclass(self.terminal_to_op_names[sym], AbstractTopology)\n                or isinstance(self.terminal_to_op_names[sym], partial)\n                and issubclass(self.terminal_to_op_names[sym].func, AbstractTopology)\n            ):\n                is_topology = True\n\n            if is_topology:\n                q_topologies.put([self.terminal_to_op_names[sym], 0])\n            else:  # is primitive operation\n                q_primitives.put(self.terminal_to_op_names[sym])\n                q_topologies.queue[-1][1] += 1  # count number of primitives\n        elif sym in grammar.nonterminals:\n            q_nonterminals.put(sym)\n        else:\n            raise Exception(f\"Unknown symbol {sym}\")\n\n        if \")\" in split:\n            # closing symbol of production\n            while \")\" in split:\n                if q_nonterminals.qsize() == q_topologies.qsize():\n                    topology, number_of_primitives = q_topologies.get(block=False)\n                    primitives = [\n                        q_primitives.get(block=False)\n                        for _ in range(number_of_primitives)\n                    ][::-1]\n                    if (\n                        topology in terminal_to_graph\n                        and terminal_to_graph[topology] is not None\n                    ) or isinstance(topology, partial):\n                        raise NotImplementedError\n                    else:\n                        composed_function = topology(*primitives)\n                        node_attr_dag = composed_function.get_node_list_and_ops()\n                        G = node_attr_dag  # TODO only works for DARTS for now\n\n                    if not q_topologies.empty():\n                        q_primitives.put(composed_function)\n                        q_topologies.queue[-1][1] += 1\n\n                _ = q_nonterminals.get(block=False)\n                split = split[:-1]\n\n    if not q_topologies.empty():\n        raise Exception(\"Invalid descriptor\")\n\n    # G = self.prune_unconnected_parts(G, src_node, sink_node)\n    # self._check_graph(G)\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.graph_to_self","title":"graph_to_self","text":"<pre><code>graph_to_self(\n    graph: DiGraph, clear_self: bool = True\n) -&gt; None\n</code></pre> <p>Copies graph to self.</p> PARAMETER DESCRIPTION <code>graph</code> <p>graph</p> <p> TYPE: <code>DiGraph</code> </p> Source code in <code>neps/search_spaces/architecture/core_graph_grammar.py</code> <pre><code>def graph_to_self(self, graph: nx.DiGraph, clear_self: bool = True) -&gt; None:\n    \"\"\"Copies graph to self.\n\n    Args:\n        graph (nx.DiGraph): graph\n    \"\"\"\n    if clear_self:\n        self.clear()\n    for u, v, data in graph.edges(data=True):\n        self.add_edge(u, v)  # type: ignore[union-attr]\n        self.edges[u, v].update(data)  # type: ignore[union-attr]\n    for n, data in graph.nodes(data=True):\n        self.nodes[n].update(**data)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.parse","title":"parse","text":"<pre><code>parse()\n</code></pre> <p>Convert the graph into a neural network which can then be optimized by pytorch.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def parse(self):\n    \"\"\"Convert the graph into a neural network which can then\n    be optimized by pytorch.\n    \"\"\"\n    for node_idx in lexicographical_topological_sort(self):\n        if \"subgraph\" in self.nodes[node_idx]:\n            self.nodes[node_idx][\"subgraph\"].parse()\n            self.add_module(\n                f\"{self.name}-subgraph_at({node_idx})\",\n                self.nodes[node_idx][\"subgraph\"],\n            )\n        elif isinstance(self.nodes[node_idx][\"comb_op\"], torch.nn.Module):\n            self.add_module(\n                f\"{self.name}-comb_op_at({node_idx})\",\n                self.nodes[node_idx][\"comb_op\"],\n            )\n\n        for neigbor_idx in self.neighbors(node_idx):\n            edge_data = self.get_edge_data(node_idx, neigbor_idx)\n            if isinstance(edge_data.op, Graph):\n                edge_data.op.parse()\n            elif edge_data.op.get_embedded_ops():\n                for primitive in edge_data.op.get_embedded_ops():\n                    if isinstance(primitive, Graph):\n                        primitive.parse()\n\n            self.add_module(\n                f\"{self.name}-edge({node_idx},{neigbor_idx})\",\n                edge_data.op,\n            )\n    self.is_parsed = True\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.set_scope","title":"set_scope","text":"<pre><code>set_scope(scope: str, recursively=True)\n</code></pre> <p>Sets the scope of this instance of the graph.</p> <p>The function should be used in a builder-like pattern <code>'subgraph'=Graph().set_scope(\"scope\")</code>.</p> PARAMETER DESCRIPTION <code>scope</code> <p>the scope</p> <p> TYPE: <code>str</code> </p> <code>recursively</code> <p>Also set the scope for all child graphs. default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Graph</code> <p>self with the setted scope.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_scope(self, scope: str, recursively=True):\n    \"\"\"Sets the scope of this instance of the graph.\n\n    The function should be used in a builder-like pattern\n    `'subgraph'=Graph().set_scope(\"scope\")`.\n\n    Args:\n        scope (str): the scope\n        recursively (bool): Also set the scope for all child graphs.\n            default True\n\n    Returns:\n        Graph: self with the setted scope.\n    \"\"\"\n    self.scope = scope\n    if recursively:\n        for g in self._get_child_graphs(single_instances=False):\n            g.scope = scope\n    return self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/core_graph_grammar/#neps.search_spaces.architecture.core_graph_grammar.CoreGraphGrammar.unparse","title":"unparse","text":"<pre><code>unparse()\n</code></pre> <p>Undo the pytorch parsing by reconstructing the graph uusing the networkx data structures.</p> <p>This is done recursively also for child graphs.</p> RETURNS DESCRIPTION <code>Graph</code> <p>An unparsed shallow copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def unparse(self):\n    \"\"\"Undo the pytorch parsing by reconstructing the graph uusing the\n    networkx data structures.\n\n    This is done recursively also for child graphs.\n\n    Returns:\n        Graph: An unparsed shallow copy of the graph.\n    \"\"\"\n    g = self.__class__()\n    g.clear()\n\n    graph_nodes = self.nodes\n    graph_edges = self.edges\n\n    # unparse possible child graphs\n    # be careful with copying/deepcopying here cause of shared edge data\n    for _, data in graph_nodes.data():\n        if \"subgraph\" in data:\n            data[\"subgraph\"] = data[\"subgraph\"].unparse()\n    for _, _, data in graph_edges.data():\n        if isinstance(data.op, Graph):\n            data.set(\"op\", data.op.unparse())\n\n    # create the new graph\n    # Remember to add all members here to update. I know it is ugly but don't know better\n    g.add_nodes_from(graph_nodes.data())\n    g.add_edges_from(graph_edges.data())\n    g.graph.update(self.graph)\n    g.name = self.name\n    g.input_node_idxs = self.input_node_idxs\n    g.scope = self.scope\n    g.is_parsed = False\n    g._id = self._id\n    g.OPTIMIZER_SCOPE = self.OPTIMIZER_SCOPE\n    g.QUERYABLE = self.QUERYABLE\n\n    return g\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/","title":"Graph","text":""},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph","title":"neps.search_spaces.architecture.graph","text":""},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData","title":"EdgeData","text":"<pre><code>EdgeData(data: dict | None = None)\n</code></pre> <p>Class that holds data for each edge. Data can be shared between instances of the graph where the edges lives in.</p> <p>Also defines the default key 'op', which is <code>Identity()</code>. It must be private always.</p> <p>Items can be accessed directly as attributes with <code>.key</code> or in a dict-like fashion with <code>[key]</code>. To set a new item use <code>.set()</code>.</p> PARAMETER DESCRIPTION <code>data</code> <p>Inject some initial data. Will be always private.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __init__(self, data: dict | None = None):\n    \"\"\"Initializes a new EdgeData object.\n    'op' is set as Identity() and private by default.\n\n    Args:\n        data (dict): Inject some initial data. Will be always private.\n    \"\"\"\n    if data is None:\n        data = {}\n    self._private = {}\n    self._shared = {}\n\n    # set internal attributes\n    self._shared[\"_deleted\"] = False\n    self._private[\"_final\"] = False\n\n    # set defaults and potential input\n    self.set(\"op\", Identity(), shared=False)\n    for k, v in data.items():\n        self.set(k, v, shared=False)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Return a true deep copy of EdgeData. Even shared items are not shared anymore.</p> RETURNS DESCRIPTION <code>EdgeData</code> <p>New independent instance.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def clone(self):\n    \"\"\"Return a true deep copy of EdgeData. Even shared\n    items are not shared anymore.\n\n    Returns:\n        EdgeData: New independent instance.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>When a graph is copied to get multiple instances (e.g. when reusing subgraphs at more than one location) then this function will be called for all edges.</p> <p>It will create a deep copy for the private entries but only a shallow copy for the shared entries. E.g. architectural weights should be shared, but parameters of a 3x3 convolution not.</p> <p>Therefore 'op' must be always private.</p> RETURNS DESCRIPTION <code>EdgeData</code> <p>A new EdgeData object with independent private     items, but shallow shared items.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def copy(self):\n    \"\"\"When a graph is copied to get multiple instances (e.g. when\n    reusing subgraphs at more than one location) then\n    this function will be called for all edges.\n\n    It will create a deep copy for the private entries but\n    only a shallow copy for the shared entries. E.g. architectural\n    weights should be shared, but parameters of a 3x3 convolution not.\n\n    Therefore 'op' must be always private.\n\n    Returns:\n        EdgeData: A new EdgeData object with independent private\n            items, but shallow shared items.\n    \"\"\"\n    new_self = EdgeData()\n    new_self._private = copy.deepcopy(self._private)\n    new_self._shared = self._shared\n\n    # we need to handle copy of graphs seperately\n    for k, v in self._private.items():\n        if isinstance(v, Graph):\n            new_self._private[k] = v.copy()\n        elif isinstance(v, list):\n            new_self._private[k] = [\n                i.copy() if isinstance(i, Graph) else i for i in v\n            ]\n\n    return new_self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.is_final","title":"is_final","text":"<pre><code>is_final()\n</code></pre> <p>Returns: bool: True if the edge was finalized, False else.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def is_final(self):\n    \"\"\"Returns:\n    bool: True if the edge was finalized, False else.\n    \"\"\"\n    return self._private[\"_final\"]\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.remove","title":"remove","text":"<pre><code>remove(key: str)\n</code></pre> <p>Removes an item from the EdgeData.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key for the item to be removed.</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def remove(self, key: str):\n    \"\"\"Removes an item from the EdgeData.\n\n    Args:\n        key (str): The key for the item to be removed.\n    \"\"\"\n    if key in self._private:\n        del self._private[key]\n    elif key in self._shared:\n        del self._shared[key]\n    else:\n        raise KeyError(f\"Tried to delete unkown key {key}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.set","title":"set","text":"<pre><code>set(key: str, value, shared=False)\n</code></pre> <p>Used to assign a new item to the EdgeData object.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The value to store</p> <p> TYPE: <code>object</code> </p> <code>shared</code> <p>Default: False. Whether the item should be a shallow copy between different instances of EdgeData (and consequently between different instances of Graph).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set(self, key: str, value, shared=False):\n    \"\"\"Used to assign a new item to the EdgeData object.\n\n    Args:\n        key (str): The key.\n        value (object): The value to store\n        shared (bool): Default: False. Whether the item should\n            be a shallow copy between different instances of EdgeData\n            (and consequently between different instances of Graph).\n    \"\"\"\n    assert isinstance(key, str), f\"Accepting only string keys, got {type(key)}\"\n    assert not key.startswith(\"_\"), \"Access to private keys not allowed!\"\n    assert not self.is_final(), \"Trying to change finalized edge!\"\n    if shared:\n        if key in self._private:\n            raise ValueError(\"Key {} alredy defined as non-shared\")\n        else:\n            self._shared[key] = value\n    elif key in self._shared:\n        raise ValueError(f\"Key {key} alredy defined as shared\")\n    else:\n        self._private[key] = value\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.EdgeData.update","title":"update","text":"<pre><code>update(data)\n</code></pre> <p>Update the data in here. If the data is added as dict, then all variables will be handled as private.</p> PARAMETER DESCRIPTION <code>data</code> <p>If dict, then values will be set as private. If EdgeData then all entries will be replaced.</p> <p> TYPE: <code>EdgeData or dict</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def update(self, data):\n    \"\"\"Update the data in here. If the data is added as dict,\n    then all variables will be handled as private.\n\n    Args:\n        data (EdgeData or dict): If dict, then values will be set as\n            private. If EdgeData then all entries will be replaced.\n    \"\"\"\n    if isinstance(data, dict):\n        for k, v in data.items():\n            self.set(k, v)\n    elif isinstance(data, EdgeData):\n        # TODO: do update and not replace!\n        self.__dict__.update(data.__dict__)\n    else:\n        raise ValueError(f\"Unsupported type {data}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph","title":"Graph","text":"<pre><code>Graph(name: str | None = None, scope: str | None = None)\n</code></pre> <p>               Bases: <code>Module</code>, <code>DiGraph</code></p> <p>Base class for defining a search space. Add nodes and edges as for a directed acyclic graph in <code>networkx</code>. Nodes can contain graphs as children, also edges can contain graphs as operations.</p> <p>Note, if a graph is copied, the shared attributes of its edges are shallow copies whereas the private attributes are deep copies.</p> <p>To differentiate copies of the same graph you can define a <code>scope</code> with <code>set_scope()</code>.</p> <p>Graph at nodes:</p> <p>graph = Graph() graph.add_node(1, subgraph=Graph())</p> <p>If the node has more than one input use <code>set_input()</code> to define the routing to the input nodes of the subgraph.</p> <p>Graph at edges:</p> <p>graph = Graph() graph.add_nodes_from([1, 2]) graph.add_edge(1, 2, EdgeData({'op': Graph()}))</p> <p>Modify the graph after definition</p> <p>If you want to modify the graph e.g. in an optimizer once it has been defined already use the function <code>update_edges()</code> or <code>update_nodes()</code>.</p> <p>Use as pytorch module If you want to learn the weights of the operations or any other parameters of the graph you have to parse it first.</p> <p>graph = getFancySearchSpace() graph.parse() logits = graph(data) optimizer.min(loss(logits, target))</p> <p>To update the pytorch module representation (e.g. after removing or adding some new edges), you have to unparse. Beware that this is not fast, so it should not be done on each batch or epoch, rather once after discretizising. If you want to change the representation of the graph use rather some shared operation indexing at the edges.</p> <p>graph.update(remove_random_edges) graph.unparse() graph.parse() logits = graph(data)</p> <p>is set as sum.</p> Note <p>When inheriting form <code>Graph</code> note that <code>__init__()</code> cannot take any parameters. This is due to the way how networkx is implemented, i.e. graphs are reconstructed internally and no parameters for init are considered.</p> <p>Our recommended solution is to create static attributes before initialization and then load them dynamically in <code>__init__()</code>.</p> <p>def init(self):     num_classes = self.NUM_CLASSES MyGraph.NUM_CLASSES = 42 my_graph_42_classes = MyGraph()</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __init__(self, name: str | None = None, scope: str | None = None):\n    \"\"\"Initialise a graph. The edges are automatically filled with an EdgeData object\n    which defines the default operation as Identity. The default combination operation\n    is set as sum.\n\n    Note:\n        When inheriting form `Graph` note that `__init__()` cannot take any\n        parameters. This is due to the way how networkx is implemented, i.e. graphs\n        are reconstructed internally and no parameters for init are considered.\n\n        Our recommended solution is to create static attributes before initialization\n        and then load them dynamically in `__init__()`.\n\n        &gt;&gt;&gt; def __init__(self):\n        &gt;&gt;&gt;     num_classes = self.NUM_CLASSES\n        &gt;&gt;&gt; MyGraph.NUM_CLASSES = 42\n        &gt;&gt;&gt; my_graph_42_classes = MyGraph()\n\n    \"\"\"\n    # super().__init__()\n    nx.DiGraph.__init__(self)\n    torch.nn.Module.__init__(self)\n\n    # Make DiGraph a member and not inherit. This is because when inheriting from\n    # `Graph` note that `__init__()` cannot take any parameters. This is due to\n    # the way how networkx is implemented, i.e. graphs are reconstructed internally\n    # and no parameters for init are considered.\n    # Therefore __getattr__ and __iter__ forward the DiGraph methods for straight-forward\n    # usage as if we would inherit.\n\n    # self._nxgraph = nx.DiGraph()\n\n    # Replace the default dicts at the edges with `EdgeData` objects\n    # `EdgeData` can be easily customized and allow shared parameters\n    # across different Graph instances.\n\n    # self._nxgraph.edge_attr_dict_factory = lambda: EdgeData()\n    self.edge_attr_dict_factory = lambda: EdgeData()\n\n    # Replace the default dicts at the nodes to include `input` from the beginning.\n    # `input` is required for storing the results of incoming edges.\n\n    # self._nxgraph.node_attr_dict_factory = lambda: dict({'input': {}, 'comb_op': sum})\n    self.node_attr_dict_factory = lambda: {\"input\": {}, \"comb_op\": sum}\n\n    # remember to add all members also in `unparse()`\n    self.name = name\n    self.scope = scope\n    self.input_node_idxs = None\n    self.is_parsed = False\n    self._id = random.random()  # pytorch expects unique modules in `add_module()`\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.OPTIMIZER_SCOPE","title":"OPTIMIZER_SCOPE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPTIMIZER_SCOPE = 'all'\n</code></pre> <p>Whether the search space has an interface to one of the tabular benchmarks which can then be used to query architecture performances.</p> <p>If this is set to true then <code>query()</code> should be implemented.</p>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>As it is very complicated to compare graphs (i.e. check all edge attributes, do the have shared attributes, ...) use just the name for comparison.</p> <p>This is used when determining whether two instances are copies.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def __hash__(self):\n    \"\"\"As it is very complicated to compare graphs (i.e. check all edge\n    attributes, do the have shared attributes, ...) use just the name\n    for comparison.\n\n    This is used when determining whether two instances are copies.\n    \"\"\"\n    h = 0\n    h += hash(self.name)\n    h += hash(self.scope) if self.scope else 0\n    h += hash(self._id)\n    return h\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.add_node","title":"add_node","text":"<pre><code>add_node(node_index, **attr)\n</code></pre> <p>Adds a node to the graph.</p> <p>Note that adding a node using an index that has been used already will override its attributes.</p> PARAMETER DESCRIPTION <code>node_index</code> <p>The index for the node. Expect to be &gt;= 1.</p> <p> TYPE: <code>int</code> </p> <code>**attr</code> <p>The attributes which can be added in a dict like form.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def add_node(self, node_index, **attr):\n    \"\"\"Adds a node to the graph.\n\n    Note that adding a node using an index that has been used already\n    will override its attributes.\n\n    Args:\n        node_index (int): The index for the node. Expect to be &gt;= 1.\n        **attr: The attributes which can be added in a dict like form.\n    \"\"\"\n    assert node_index &gt;= 1, \"Expecting the node index to be greater or equal 1\"\n    nx.DiGraph.add_node(self, node_index, **attr)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Deep copy of the current graph.</p> RETURNS DESCRIPTION <code>Graph</code> <p>Deep copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def clone(self):\n    \"\"\"Deep copy of the current graph.\n\n    Returns:\n        Graph: Deep copy of the graph.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.compile","title":"compile","text":"<pre><code>compile()\n</code></pre> <p>Instanciates the ops at the edges using the arguments specified at the edges.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def compile(self):\n    \"\"\"Instanciates the ops at the edges using the arguments specified at the edges.\"\"\"\n    for graph in [*self._get_child_graphs(single_instances=False), self]:\n        logger.debug(f\"Compiling graph {graph.name}\")\n        for _, v, edge_data in graph.edges.data():\n            if not edge_data.is_final():\n                attr = edge_data.to_dict()\n                op = attr.pop(\"op\")\n\n                if isinstance(op, list):\n                    compiled_ops = []\n                    for i, o in enumerate(op):\n                        if inspect.isclass(o):\n                            # get the relevant parameter if there are more.\n                            a = {\n                                k: v[i] if isinstance(v, list) else v\n                                for k, v in attr.items()\n                            }\n                            compiled_ops.append(o(**a))\n                        else:\n                            logger.debug(f\"op {o} already compiled. Skipping\")\n                    edge_data.set(\"op\", compiled_ops)\n                elif isinstance(op, AbstractPrimitive):\n                    logger.debug(f\"op {op} already compiled. Skipping\")\n                elif inspect.isclass(op) and issubclass(op, AbstractPrimitive):\n                    # Init the class\n                    if \"op_name\" in attr:\n                        del attr[\"op_name\"]\n                    edge_data.set(\"op\", op(**attr))\n                elif isinstance(op, Graph):\n                    pass  # This is already covered by _get_child_graphs\n                else:\n                    raise ValueError(f\"Unkown format of op: {op}\")\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy as defined in networkx, i.e. a shallow copy.</p> <p>Just handling recursively nested graphs seperately.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def copy(self):\n    \"\"\"Copy as defined in networkx, i.e. a shallow copy.\n\n    Just handling recursively nested graphs seperately.\n    \"\"\"\n\n    def copy_dict(d):\n        copied_dict = d.copy()\n        for k, v in d.items():\n            if isinstance(v, Graph):\n                copied_dict[k] = v.copy()\n            elif isinstance(v, list):\n                copied_dict[k] = [i.copy() if isinstance(i, Graph) else i for i in v]\n            elif isinstance(v, (AbstractPrimitive, torch.nn.Module)):\n                copied_dict[k] = copy.deepcopy(v)\n        return copied_dict\n\n    G = self.__class__()\n    G.graph.update(self.graph)\n    G.add_nodes_from((n, copy_dict(d)) for n, d in self._node.items())\n    G.add_edges_from(\n        (u, v, datadict.copy())\n        for u, nbrs in self._adj.items()\n        for v, datadict in nbrs.items()\n    )\n    G.scope = self.scope\n    G.name = self.name\n    return G\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.parse","title":"parse","text":"<pre><code>parse()\n</code></pre> <p>Convert the graph into a neural network which can then be optimized by pytorch.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def parse(self):\n    \"\"\"Convert the graph into a neural network which can then\n    be optimized by pytorch.\n    \"\"\"\n    for node_idx in lexicographical_topological_sort(self):\n        if \"subgraph\" in self.nodes[node_idx]:\n            self.nodes[node_idx][\"subgraph\"].parse()\n            self.add_module(\n                f\"{self.name}-subgraph_at({node_idx})\",\n                self.nodes[node_idx][\"subgraph\"],\n            )\n        elif isinstance(self.nodes[node_idx][\"comb_op\"], torch.nn.Module):\n            self.add_module(\n                f\"{self.name}-comb_op_at({node_idx})\",\n                self.nodes[node_idx][\"comb_op\"],\n            )\n\n        for neigbor_idx in self.neighbors(node_idx):\n            edge_data = self.get_edge_data(node_idx, neigbor_idx)\n            if isinstance(edge_data.op, Graph):\n                edge_data.op.parse()\n            elif edge_data.op.get_embedded_ops():\n                for primitive in edge_data.op.get_embedded_ops():\n                    if isinstance(primitive, Graph):\n                        primitive.parse()\n\n            self.add_module(\n                f\"{self.name}-edge({node_idx},{neigbor_idx})\",\n                edge_data.op,\n            )\n    self.is_parsed = True\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.set_scope","title":"set_scope","text":"<pre><code>set_scope(scope: str, recursively=True)\n</code></pre> <p>Sets the scope of this instance of the graph.</p> <p>The function should be used in a builder-like pattern <code>'subgraph'=Graph().set_scope(\"scope\")</code>.</p> PARAMETER DESCRIPTION <code>scope</code> <p>the scope</p> <p> TYPE: <code>str</code> </p> <code>recursively</code> <p>Also set the scope for all child graphs. default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Graph</code> <p>self with the setted scope.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def set_scope(self, scope: str, recursively=True):\n    \"\"\"Sets the scope of this instance of the graph.\n\n    The function should be used in a builder-like pattern\n    `'subgraph'=Graph().set_scope(\"scope\")`.\n\n    Args:\n        scope (str): the scope\n        recursively (bool): Also set the scope for all child graphs.\n            default True\n\n    Returns:\n        Graph: self with the setted scope.\n    \"\"\"\n    self.scope = scope\n    if recursively:\n        for g in self._get_child_graphs(single_instances=False):\n            g.scope = scope\n    return self\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph/#neps.search_spaces.architecture.graph.Graph.unparse","title":"unparse","text":"<pre><code>unparse()\n</code></pre> <p>Undo the pytorch parsing by reconstructing the graph uusing the networkx data structures.</p> <p>This is done recursively also for child graphs.</p> RETURNS DESCRIPTION <code>Graph</code> <p>An unparsed shallow copy of the graph.</p> Source code in <code>neps/search_spaces/architecture/graph.py</code> <pre><code>def unparse(self):\n    \"\"\"Undo the pytorch parsing by reconstructing the graph uusing the\n    networkx data structures.\n\n    This is done recursively also for child graphs.\n\n    Returns:\n        Graph: An unparsed shallow copy of the graph.\n    \"\"\"\n    g = self.__class__()\n    g.clear()\n\n    graph_nodes = self.nodes\n    graph_edges = self.edges\n\n    # unparse possible child graphs\n    # be careful with copying/deepcopying here cause of shared edge data\n    for _, data in graph_nodes.data():\n        if \"subgraph\" in data:\n            data[\"subgraph\"] = data[\"subgraph\"].unparse()\n    for _, _, data in graph_edges.data():\n        if isinstance(data.op, Graph):\n            data.set(\"op\", data.op.unparse())\n\n    # create the new graph\n    # Remember to add all members here to update. I know it is ugly but don't know better\n    g.add_nodes_from(graph_nodes.data())\n    g.add_edges_from(graph_edges.data())\n    g.graph.update(self.graph)\n    g.name = self.name\n    g.input_node_idxs = self.input_node_idxs\n    g.scope = self.scope\n    g.is_parsed = False\n    g._id = self._id\n    g.OPTIMIZER_SCOPE = self.OPTIMIZER_SCOPE\n    g.QUERYABLE = self.QUERYABLE\n\n    return g\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/graph_grammar/","title":"Graph grammar","text":""},{"location":"api/neps/search_spaces/architecture/graph_grammar/#neps.search_spaces.architecture.graph_grammar","title":"neps.search_spaces.architecture.graph_grammar","text":""},{"location":"api/neps/search_spaces/architecture/mutations/","title":"Mutations","text":""},{"location":"api/neps/search_spaces/architecture/mutations/#neps.search_spaces.architecture.mutations","title":"neps.search_spaces.architecture.mutations","text":""},{"location":"api/neps/search_spaces/architecture/primitives/","title":"Primitives","text":""},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives","title":"neps.search_spaces.architecture.primitives","text":""},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.AvgPool","title":"AvgPool","text":"<pre><code>AvgPool(kernel_size: int, stride: int, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of Avergae Pooling.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, kernel_size: int, stride: int, **kwargs):  # noqa: D107\n    stride = int(stride)\n    super().__init__(locals())\n    self.avgpool = nn.AvgPool2d(\n        kernel_size=3, stride=stride, padding=1, count_include_pad=False\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.AvgPool1x1","title":"AvgPool1x1","text":"<pre><code>AvgPool1x1(\n    kernel_size: int,\n    stride: int,\n    c_in: int,\n    c_out: int,\n    affine: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of Avergae Pooling with an optional 1x1 convolution afterwards. The convolution is required to increase the number of channels if stride &gt; 1.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    kernel_size: int,\n    stride: int,\n    c_in: int,\n    c_out: int,\n    affine: bool = True,  # noqa: FBT001, FBT002\n    **kwargs,\n):\n    super().__init__(locals())\n    stride = int(stride)\n    self.stride = int(stride)\n    self.avgpool = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)\n    if stride &gt; 1:\n        assert c_in is not None\n        assert c_out is not None\n        self.conv = nn.Conv2d(c_in, c_out, 1, stride=1, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(c_out, affine=affine)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Concat1x1","title":"Concat1x1","text":"<pre><code>Concat1x1(\n    num_in_edges: int,\n    c_out: int,\n    affine: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of the channel-wise concatination followed by a 1x1 convolution to retain the channel dimension.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    num_in_edges: int,\n    c_out: int,\n    affine: bool = True,  # noqa: FBT001, FBT002\n    **kwargs,\n):\n    super().__init__(locals())\n    self.conv = nn.Conv2d(\n        num_in_edges * c_out, c_out, kernel_size=1, stride=1, padding=0, bias=False\n    )\n    self.bn = nn.BatchNorm2d(c_out, affine=affine)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Concat1x1.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Expecting a list of input tensors. Stacking them channel-wise and applying 1x1 conv.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def forward(self, x):\n    \"\"\"Expecting a list of input tensors. Stacking them channel-wise\n    and applying 1x1 conv.\n    \"\"\"\n    x = torch.cat(x, dim=1)\n    x = self.conv(x)\n    return self.bn(x)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.ConvBn","title":"ConvBn","text":"<pre><code>ConvBn(\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride=1,\n    affine: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of 2d convolution, followed by 2d batch normalization and ReLU activation.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride=1,\n    affine: bool = True,  # noqa: FBT001, FBT002\n    **kwargs,\n):\n    super().__init__(locals())\n    self.kernel_size = kernel_size\n    pad = 0 if stride == 1 and kernel_size == 1 else 1\n    self.op = nn.Sequential(\n        nn.Conv2d(c_in, c_out, kernel_size, stride=stride, padding=pad, bias=False),\n        nn.BatchNorm2d(c_out, affine=affine),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.ConvBnReLU","title":"ConvBnReLU","text":"<pre><code>ConvBnReLU(\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride: int = 1,\n    affine: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of 2d convolution, followed by 2d batch normalization and ReLU activation.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride: int = 1,\n    affine: bool = True,  # noqa: FBT001, FBT002\n    **kwargs,\n):\n    super().__init__(locals())\n    self.kernel_size = kernel_size\n    pad = 0 if stride == 1 and kernel_size == 1 else 1\n    self.op = nn.Sequential(\n        nn.Conv2d(c_in, c_out, kernel_size, stride=stride, padding=pad, bias=False),\n        nn.BatchNorm2d(c_out, affine=affine),\n        nn.ReLU(inplace=False),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.DilConv","title":"DilConv","text":"<pre><code>DilConv(\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride: int,\n    padding: int,\n    dilation: int,\n    affine: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of a dilated separable convolution as used in the DARTS paper.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride: int,\n    padding: int,\n    dilation: int,\n    affine: bool = True,  # noqa: FBT001, FBT002\n    **kwargs,\n):\n    super().__init__(locals())\n\n    c_in = int(c_in)\n    c_out = int(c_out)\n    kernel_size = int(kernel_size)\n    stride = int(stride)\n    padding = int(padding)\n    dilation = int(dilation)\n    affine = bool(affine)\n\n    self.kernel_size = kernel_size\n    self.op = nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(\n            c_in,\n            c_in,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=c_in,\n            bias=False,\n        ),\n        nn.Conv2d(c_in, c_out, kernel_size=1, padding=0, bias=False),\n        nn.BatchNorm2d(c_out, affine=affine),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Identity","title":"Identity","text":"<pre><code>Identity(**kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>An implementation of the Identity operation.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, **kwargs):  # noqa: D107\n    super().__init__(locals())\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.MaxPool1x1","title":"MaxPool1x1","text":"<pre><code>MaxPool1x1(\n    kernel_size: int,\n    stride: int,\n    c_in: int,\n    c_out: int,\n    affine: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of MaxPool with an optional 1x1 convolution in case stride &gt; 1. The 1x1 convolution is required to increase the number of channels.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    kernel_size: int,\n    stride: int,\n    c_in: int,\n    c_out: int,\n    affine: bool = True,  # noqa: FBT001, FBT002\n    **kwargs,\n):\n    super().__init__(locals())\n\n    kernel_size = int(kernel_size)\n    stride = int(stride)\n    c_in = int(c_in)\n    c_out = int(c_out)\n    affine = bool(affine)\n\n    self.stride = stride\n    self.maxpool = nn.MaxPool2d(kernel_size, stride=stride, padding=1)\n    if stride &gt; 1:\n        assert c_in is not None\n        assert c_out is not None\n        self.conv = nn.Conv2d(c_in, c_out, 1, stride=1, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(c_out, affine=affine)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.SepConv","title":"SepConv","text":"<pre><code>SepConv(\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride: int,\n    padding: int,\n    affine: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of Separable convolution operation as in the DARTS paper, i.e. 2 sepconv directly after another.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    c_in: int,\n    c_out: int,\n    kernel_size: int,\n    stride: int,\n    padding: int,\n    affine: bool = True,  # noqa: FBT001, FBT002\n    **kwargs,\n):\n    super().__init__(locals())\n\n    c_in = int(c_in)\n    c_out = int(c_out)\n    kernel_size = int(kernel_size)\n    stride = int(stride)\n    padding = int(padding)\n    affine = bool(affine)\n\n    self.kernel_size = kernel_size\n    self.op = nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(\n            c_in,\n            c_in,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=c_in,\n            bias=False,\n        ),\n        nn.Conv2d(c_in, c_in, kernel_size=1, padding=0, bias=False),\n        nn.BatchNorm2d(c_in, affine=affine),\n        nn.ReLU(inplace=False),\n        nn.Conv2d(\n            c_in,\n            c_in,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=padding,\n            groups=c_in,\n            bias=False,\n        ),\n        nn.Conv2d(c_in, c_out, kernel_size=1, padding=0, bias=False),\n        nn.BatchNorm2d(c_out, affine=affine),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Sequential","title":"Sequential","text":"<pre><code>Sequential(*args, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of <code>torch.nn.Sequential</code> to be used as op on edges.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, *args, **kwargs):  # noqa: D107\n    super().__init__(locals())\n    self.primitives = args\n    self.op = nn.Sequential(*args)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Stem","title":"Stem","text":"<pre><code>Stem(c_out: int, c_in: int = 3, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>This is used as an initial layer directly after the image input.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, c_out: int, c_in: int = 3, **kwargs):  # noqa: D107\n    super().__init__(locals())\n\n    c_out = int(c_out)\n\n    self.seq = nn.Sequential(\n        nn.Conv2d(c_in, c_out, 3, padding=1, bias=False), nn.BatchNorm2d(c_out)\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Zero","title":"Zero","text":"<pre><code>Zero(stride, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of the zero operation. It removes the connection by multiplying its input with zero.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, stride, **kwargs):\n    \"\"\"When setting stride &gt; 1 then it is assumed that the\n    channels must be doubled.\n    \"\"\"\n    super().__init__(locals())\n    self.stride = int(stride)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/primitives/#neps.search_spaces.architecture.primitives.Zero1x1","title":"Zero1x1","text":"<pre><code>Zero1x1(stride, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractPrimitive</code></p> <p>Implementation of the zero operation. It removes the connection by multiplying its input with zero.</p> Source code in <code>neps/search_spaces/architecture/primitives.py</code> <pre><code>def __init__(self, stride, **kwargs):\n    \"\"\"When setting stride &gt; 1 then it is assumed that the\n    channels must be doubled.\n    \"\"\"\n    super().__init__(locals())\n    self.stride = int(stride)\n</code></pre>"},{"location":"api/neps/search_spaces/architecture/topologies/","title":"Topologies","text":""},{"location":"api/neps/search_spaces/architecture/topologies/#neps.search_spaces.architecture.topologies","title":"neps.search_spaces.architecture.topologies","text":""},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/","title":"Constrained cfg","text":""},{"location":"api/neps/search_spaces/architecture/cfg_variants/constrained_cfg/#neps.search_spaces.architecture.cfg_variants.constrained_cfg","title":"neps.search_spaces.architecture.cfg_variants.constrained_cfg","text":""},{"location":"api/neps/search_spaces/hyperparameters/categorical/","title":"Categorical","text":""},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical","title":"neps.search_spaces.hyperparameters.categorical","text":"<p>Categorical hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.Categorical","title":"Categorical","text":"<pre><code>Categorical(\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>ParameterWithPrior[CategoricalTypes, CategoricalTypes]</code></p> <p>A list of unordered choices for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters that can take on a discrete set of unordered values. For example, the <code>optimizer</code> hyperparameter in a neural network search space can be a <code>Categorical</code> with choices like <code>[\"adam\", \"sgd\", \"rmsprop\"]</code>.</p> <pre><code>import neps\n\noptimizer_choice = neps.Categorical(\n    [\"adam\", \"sgd\", \"rmsprop\"],\n    default=\"adam\"\n)\n</code></pre> <p>Please see the <code>Parameter</code>, <code>ParameterWithPrior</code>, for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>choices</code> <p>choices for the hyperparameter.</p> <p> TYPE: <code>Iterable[float | int | str]</code> </p> <code>default</code> <p>default value for the hyperparameter, must be in <code>choices=</code> if provided.</p> <p> TYPE: <code>float | int | str | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/categorical.py</code> <pre><code>def __init__(\n    self,\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `Categorical`.\n\n    Args:\n        choices: choices for the hyperparameter.\n        default: default value for the hyperparameter, must be in `choices=`\n            if provided.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    choices = list(choices)\n    if len(choices) &lt;= 1:\n        raise ValueError(\"Categorical choices must have more than one value.\")\n\n    super().__init__(value=None, is_fidelity=False, default=default)\n\n    for choice in choices:\n        if not isinstance(choice, float | int | str):\n            raise TypeError(\n                f'Choice \"{choice}\" is not of a valid type (float, int, str)'\n            )\n\n    if not all_unique(choices):\n        raise ValueError(f\"Choices must be unique but got duplicates.\\n{choices}\")\n\n    if default is not None and default not in choices:\n        raise ValueError(\n            f\"Default value {default} is not in the provided choices {choices}\"\n        )\n\n    self.choices = list(choices)\n\n    # NOTE(eddiebergman): If there's ever a very large categorical,\n    # then it would be beneficial to have a lookup table for indices as\n    # currently we do a list.index() operation which is O(n).\n    # However for small sized categoricals this is likely faster than\n    # a lookup table.\n    # For now we can just cache the index of the value and default.\n    self._value_index: int | None = None\n\n    self.default_confidence_choice = default_confidence\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n    self.has_prior = self.default is not None\n    self._default_index: int | None = (\n        self.choices.index(default) if default is not None else None\n    )\n    self.domain = Domain.indices(len(self.choices))\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.Categorical.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.Categorical.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.Categorical.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter","title":"CategoricalParameter","text":"<pre><code>CategoricalParameter(\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Categorical</code></p> <p>Deprecated: Use <code>Categorical</code> instead of <code>CategoricalParameter</code>.</p> <p>This class remains for backward compatibility and will raise a deprecation warning if used.</p> PARAMETER DESCRIPTION <code>choices</code> <p>choices for the hyperparameter.</p> <p> TYPE: <code>Iterable[float | int | str]</code> </p> <code>default</code> <p>default value for the hyperparameter, must be in <code>choices=</code> if provided.</p> <p> TYPE: <code>float | int | str | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> RAISES DESCRIPTION <code>DeprecationWarning</code> <p>A warning indicating that <code>neps.CategoricalParameter</code> is</p> Source code in <code>neps/search_spaces/hyperparameters/categorical.py</code> <pre><code>def __init__(\n    self,\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Initialize a deprecated `CategoricalParameter`.\n\n    Args:\n        choices: choices for the hyperparameter.\n        default: default value for the hyperparameter, must be in `choices=`\n            if provided.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n\n    Raises:\n        DeprecationWarning: A warning indicating that `neps.CategoricalParameter` is\n        deprecated and `neps.Categorical` should be used instead.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        (\n            \"Usage of 'neps.CategoricalParameter' is deprecated and will be removed \"\n            \"in future releases. Please use 'neps.Categorical' instead.\"\n        ),\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    super().__init__(\n        choices=choices,\n        default=default,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/categorical/#neps.search_spaces.hyperparameters.categorical.CategoricalParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/","title":"Constant","text":""},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant","title":"neps.search_spaces.hyperparameters.constant","text":"<p>Constant hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.Constant","title":"Constant","text":"<pre><code>Constant(value: T)\n</code></pre> <p>               Bases: <code>Parameter[T, T]</code></p> <p>A constant value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with values that should not change during optimization. For example, the <code>batch_size</code> hyperparameter in a neural network search space can be a <code>Constant</code> with a value of <code>32</code>.</p> <pre><code>import neps\n\nbatch_size = neps.Constant(32)\n</code></pre> <p>Note</p> <p>As the name suggests, the value of a <code>Constant</code> only have one value and so its <code>.default</code> and <code>.value</code> should always be the same.</p> <p>This also implies that the <code>.default</code> can never be <code>None</code>.</p> <p>Please use <code>.set_constant_value()</code> if you need to change the value of the constant parameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>T</code> </p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>def __init__(self, value: T):\n    \"\"\"Create a new `Constant`.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n    super().__init__(value=value, default=value, is_fidelity=False)  # type: ignore\n    self._value: T = value  # type: ignore\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.Constant.value","title":"value  <code>property</code>","text":"<pre><code>value: T\n</code></pre> <p>Get the value of the constant parameter.</p>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.Constant.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.Constant.sample","title":"sample","text":"<pre><code>sample() -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Will set the <code>.value</code> to the sampled value.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Will set the [`.value`][neps.search_spaces.Parameter.value] to the\n    sampled value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value()\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.Constant.set_value","title":"set_value","text":"<pre><code>set_value(value: T | None) -&gt; None\n</code></pre> <p>Set the value of the constant parameter.</p> <p>Note</p> <p>This method is a no-op but will raise a <code>ValueError</code> if the value is different from the current value.</p> <p>Please see <code>.set_constant_value()</code> which can be used to set both the <code>.value</code> and the <code>.default</code> at once</p> PARAMETER DESCRIPTION <code>value</code> <p>value to set the parameter to.</p> <p> TYPE: <code>T | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the value is different from the current value.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>@override\ndef set_value(self, value: T | None) -&gt; None:\n    \"\"\"Set the value of the constant parameter.\n\n    !!! note\n\n        This method is a no-op but will raise a `ValueError` if the value\n        is different from the current value.\n\n        Please see\n        [`.set_constant_value()`][neps.search_spaces.hyperparameters.constant.Constant.set_constant_value]\n        which can be used to set both the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        and the [`.default`][neps.search_spaces.parameter.Parameter.default] at once\n\n    Args:\n        value: value to set the parameter to.\n\n    Raises:\n        ValueError: if the value is different from the current value.\n    \"\"\"\n    if value != self._value:\n        raise ValueError(\n            f\"Constant does not allow chaning the set value. \"\n            f\"Tried to set value to {value}, but it is already {self.value}\"\n        )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter","title":"ConstantParameter","text":"<pre><code>ConstantParameter(value: T)\n</code></pre> <p>               Bases: <code>Constant</code></p> <p>Deprecated: Use <code>Constant</code> instead of <code>ConstantParameter</code>.</p> <p>This class remains for backward compatibility and will raise a deprecation warning if used.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>T</code> </p> RAISES DESCRIPTION <code>DeprecationWarning</code> <p>A warning indicating that <code>neps.ConstantParameter</code> is</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>def __init__(self, value: T):\n    \"\"\"Initialize a deprecated `ConstantParameter`.\n\n    Args:\n        value: value for the hyperparameter.\n\n    Raises:\n        DeprecationWarning: A warning indicating that `neps.ConstantParameter` is\n        deprecated and `neps.Constant` should be used instead.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        (\n            \"Usage of 'neps.ConstantParameter' is deprecated and will be removed in\"\n            \" future releases. Please use 'neps.Constant' instead.\"\n        ),\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    super().__init__(value=value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: T\n</code></pre> <p>Get the value of the constant parameter.</p>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.sample","title":"sample","text":"<pre><code>sample() -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Will set the <code>.value</code> to the sampled value.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Will set the [`.value`][neps.search_spaces.Parameter.value] to the\n    sampled value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value()\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/constant/#neps.search_spaces.hyperparameters.constant.ConstantParameter.set_value","title":"set_value","text":"<pre><code>set_value(value: T | None) -&gt; None\n</code></pre> <p>Set the value of the constant parameter.</p> <p>Note</p> <p>This method is a no-op but will raise a <code>ValueError</code> if the value is different from the current value.</p> <p>Please see <code>.set_constant_value()</code> which can be used to set both the <code>.value</code> and the <code>.default</code> at once</p> PARAMETER DESCRIPTION <code>value</code> <p>value to set the parameter to.</p> <p> TYPE: <code>T | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the value is different from the current value.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>@override\ndef set_value(self, value: T | None) -&gt; None:\n    \"\"\"Set the value of the constant parameter.\n\n    !!! note\n\n        This method is a no-op but will raise a `ValueError` if the value\n        is different from the current value.\n\n        Please see\n        [`.set_constant_value()`][neps.search_spaces.hyperparameters.constant.Constant.set_constant_value]\n        which can be used to set both the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        and the [`.default`][neps.search_spaces.parameter.Parameter.default] at once\n\n    Args:\n        value: value to set the parameter to.\n\n    Raises:\n        ValueError: if the value is different from the current value.\n    \"\"\"\n    if value != self._value:\n        raise ValueError(\n            f\"Constant does not allow chaning the set value. \"\n            f\"Tried to set value to {value}, but it is already {self.value}\"\n        )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/","title":"Float","text":""},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float","title":"neps.search_spaces.hyperparameters.float","text":"<p>Float hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.Float","title":"Float","text":"<pre><code>Float(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Numerical[float]</code></p> <p>A float value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous float values, optionally specifying if it exists on a log scale. For example, <code>l2_norm</code> could be a value in <code>(0.1)</code>, while the <code>learning_rate</code> hyperparameter in a neural network search space can be a <code>Float</code> with a range of <code>(0.0001, 0.1)</code> but on a log scale.</p> <pre><code>import neps\n\nl2_norm = neps.Float(0, 1)\nlearning_rate = neps.Float(1e-4, 1e-1, log=True)\n</code></pre> <p>Please see the <code>Numerical</code> class for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsidering prior based optimization..</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/float.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `Float`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsidering prior based optimization..\n    \"\"\"\n    super().__init__(\n        lower=float(lower),\n        upper=float(upper),\n        log=log,\n        default=float(default) if default is not None else None,\n        default_confidence=default_confidence,\n        is_fidelity=is_fidelity,\n        domain=Domain.floating(lower, upper, log=log),\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.Float.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.Float.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.Float.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter","title":"FloatParameter","text":"<pre><code>FloatParameter(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Float</code></p> <p>Deprecated: Use <code>Float</code> instead of <code>FloatParameter</code>.</p> <p>This class remains for backward compatibility and will raise a deprecation warning if used.</p> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsidering prior based optimization..</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> RAISES DESCRIPTION <code>DeprecationWarning</code> <p>A warning indicating that <code>neps.FloatParameter</code> is</p> Source code in <code>neps/search_spaces/hyperparameters/float.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Initialize a deprecated `FloatParameter`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsidering prior based optimization..\n\n    Raises:\n        DeprecationWarning: A warning indicating that `neps.FloatParameter` is\n        deprecated and `neps.Float` should be used instead.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        (\n            \"Usage of 'neps.FloatParameter' is deprecated and will be removed in\"\n            \" future releases. Please use 'neps.Float' instead.\"\n        ),\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    super().__init__(\n        lower=lower,\n        upper=upper,\n        log=log,\n        is_fidelity=is_fidelity,\n        default=default,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/float/#neps.search_spaces.hyperparameters.float.FloatParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/","title":"Integer","text":""},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer","title":"neps.search_spaces.hyperparameters.integer","text":"<p>Float hyperparameter for search spaces.</p>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.Integer","title":"Integer","text":"<pre><code>Integer(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Numerical[int]</code></p> <p>An integer value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous integer values, optionally specifying f it exists on a log scale. For example, <code>batch_size</code> could be a value in <code>(32, 128)</code>, while the <code>num_layers</code> hyperparameter in a neural network search space can be a <code>Integer</code> with a range of <code>(1, 1000)</code> but on a log scale.</p> <pre><code>import neps\n\nbatch_size = neps.Integer(32, 128)\nnum_layers = neps.Integer(1, 1000, log=True)\n</code></pre> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/integer.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `Integer`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    lower = int(np.rint(lower))\n    upper = int(np.rint(upper))\n    _size = upper - lower + 1\n    if _size &lt;= 1:\n        raise ValueError(\n            f\"Integer: expected at least 2 possible values in the range,\"\n            f\" got upper={upper}, lower={lower}.\"\n        )\n\n    super().__init__(\n        lower=int(np.rint(lower)),\n        upper=int(np.rint(upper)),\n        log=log,\n        is_fidelity=is_fidelity,\n        default=int(np.rint(default)) if default is not None else None,\n        default_confidence=default_confidence,\n        domain=Domain.integer(lower, upper, log=log),\n    )\n\n    # We subtract/add 0.499999 from lower/upper bounds respectively, such that\n    # sampling in the float space gives equal probability for all integer values,\n    # i.e. [x - 0.499999, x + 0.499999]\n    self.float_hp = Float(\n        lower=self.lower - 0.499999,\n        upper=self.upper + 0.499999,\n        log=self.log,\n        is_fidelity=is_fidelity,\n        default=default,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.Integer.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.Integer.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter","title":"IntegerParameter","text":"<pre><code>IntegerParameter(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Integer</code></p> <p>Deprecated: Use <code>Integer</code> instead of <code>IntegerParameter</code>.</p> <p>This class remains for backward compatibility and will raise a deprecation warning if used.</p> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> RAISES DESCRIPTION <code>DeprecationWarning</code> <p>A warning indicating that <code>neps.IntegerParameter</code> is</p> Source code in <code>neps/search_spaces/hyperparameters/integer.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Initialize a deprecated `IntegerParameter`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n\n    Raises:\n        DeprecationWarning: A warning indicating that `neps.IntegerParameter` is\n        deprecated and `neps.Integer` should be used instead.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        (\n            \"Usage of 'neps.IntegerParameter' is deprecated and will be removed in\"\n            \" future releases. Please use 'neps.Integer' instead.\"\n        ),\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    super().__init__(\n        lower=lower,\n        upper=upper,\n        log=log,\n        is_fidelity=is_fidelity,\n        default=default,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/integer/#neps.search_spaces.hyperparameters.integer.IntegerParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/","title":"Numerical","text":""},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical","title":"neps.search_spaces.hyperparameters.numerical","text":"<p>The <code>Numerical</code> is a <code>Parameter</code> that represents a numerical range.</p> <p>The two primary numerical hyperparameters are:</p> <ul> <li><code>Float</code> for continuous     float values.</li> <li><code>Integer</code> for discrete     integer values.</li> </ul> <p>The <code>Numerical</code> is a base class for both of these hyperparameters, and includes methods from both <code>ParameterWithPrior</code>, allowing you to set a confidence along with a <code>.default</code> that can be used with certain algorithms.</p>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical","title":"Numerical","text":"<pre><code>Numerical(\n    lower: T,\n    upper: T,\n    *,\n    log: bool = False,\n    default: T | None,\n    is_fidelity: bool,\n    domain: Domain[T],\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>ParameterWithPrior[T, T]</code></p> <p>A numerical hyperparameter is bounded by a lower and upper value.</p> ATTRIBUTE DESCRIPTION <code>lower</code> <p>The lower bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>upper</code> <p>The upper bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>log</code> <p>Whether the hyperparameter is in log space.</p> <p> TYPE: <code>bool</code> </p> <code>log_bounds</code> <p>The log bounds of the hyperparameter, if <code>log=True</code>.</p> <p> TYPE: <code>tuple[float, float] | None</code> </p> <code>log_default</code> <p>The log default value of the hyperparameter, if <code>log=True</code> and a <code>default</code> is set.</p> <p> TYPE: <code>float | None</code> </p> <code>default_confidence_choice</code> <p>The default confidence choice.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> </p> <code>default_confidence_score</code> <p>The default confidence score.</p> <p> TYPE: <code>float</code> </p> <code>has_prior</code> <p>Whether the hyperparameter has a prior.</p> <p> TYPE: <code>bool</code> </p> PARAMETER DESCRIPTION <code>lower</code> <p>The lower bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>upper</code> <p>The upper bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>log</code> <p>Whether the hyperparameter is in log space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>The default value of the hyperparameter.</p> <p> TYPE: <code>T | None</code> </p> <code>is_fidelity</code> <p>Whether the hyperparameter is a fidelity parameter.</p> <p> TYPE: <code>bool</code> </p> <code>domain</code> <p>The domain of the hyperparameter.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>default_confidence</code> <p>The default confidence choice.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def __init__(\n    self,\n    lower: T,\n    upper: T,\n    *,\n    log: bool = False,\n    default: T | None,\n    is_fidelity: bool,\n    domain: Domain[T],\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Initialize the numerical hyperparameter.\n\n    Args:\n        lower: The lower bound of the numerical hyperparameter.\n        upper: The upper bound of the numerical hyperparameter.\n        log: Whether the hyperparameter is in log space.\n        default: The default value of the hyperparameter.\n        is_fidelity: Whether the hyperparameter is a fidelity parameter.\n        domain: The domain of the hyperparameter.\n        default_confidence: The default confidence choice.\n    \"\"\"\n    super().__init__(value=None, default=default, is_fidelity=is_fidelity)  # type: ignore\n    _cls_name = self.__class__.__name__\n    if lower &gt;= upper:\n        raise ValueError(\n            f\"{_cls_name} parameter: bounds error (lower &gt;= upper). Actual values: \"\n            f\"lower={lower}, upper={upper}\"\n        )\n\n    if log and (lower &lt;= 0 or upper &lt;= 0):\n        raise ValueError(\n            f\"{_cls_name} parameter: bounds error (log scale cant have bounds &lt;= 0).\"\n            f\" Actual values: lower={lower}, upper={upper}\"\n        )\n\n    if default is not None and not lower &lt;= default &lt;= upper:\n        raise ValueError(\n            f\"Float parameter: default bounds error. Expected lower &lt;= default\"\n            f\" &lt;= upper, but got lower={lower}, default={default},\"\n            f\" upper={upper}\"\n        )\n\n    if default_confidence not in self.DEFAULT_CONFIDENCE_SCORES:\n        raise ValueError(\n            f\"{_cls_name} parameter: default confidence score error. Expected one of \"\n            f\"{list(self.DEFAULT_CONFIDENCE_SCORES.keys())}, but got \"\n            f\"{default_confidence}\"\n        )\n\n    # Validate 'log' and 'is_fidelity' types to prevent configuration errors\n    # from the YAML input\n    for param, value in {\"log\": log, \"is_fidelity\": is_fidelity}.items():\n        if not isinstance(value, bool):\n            raise TypeError(\n                f\"Expected '{param}' to be a boolean, but got type: \"\n                f\"{type(value).__name__}\"\n            )\n\n    self.lower: T = lower\n    self.upper: T = upper\n    self.log: bool = log\n    self.domain: Domain[T] = domain\n    self.log_bounds: tuple[float, float] | None = None\n    self.log_default: float | None = None\n    if self.log:\n        self.log_bounds = (float(np.log(lower)), float(np.log(upper)))\n        self.log_default = (\n            float(np.log(self.default)) if self.default is not None else None\n        )\n\n    self.default_confidence_choice: Literal[\"low\", \"medium\", \"high\"] = (\n        default_confidence\n    )\n\n    self.default_confidence_score: float = self.DEFAULT_CONFIDENCE_SCORES[\n        default_confidence\n    ]\n    self.has_prior: bool = self.default is not None\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.clone","title":"clone  <code>abstractmethod</code>","text":"<pre><code>clone() -&gt; Self\n</code></pre> <p>Create a copy of the <code>Parameter</code>.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef clone(self) -&gt; Self:\n    \"\"\"Create a copy of the `Parameter`.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.normalized_to_value","title":"normalized_to_value  <code>abstractmethod</code>","text":"<pre><code>normalized_to_value(normalized_value: float) -&gt; ValueT\n</code></pre> <p>Convert a normalized value back to value in the defined hyperparameter range.</p> PARAMETER DESCRIPTION <code>normalized_value</code> <p>normalized value to convert.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef normalized_to_value(self, normalized_value: float) -&gt; ValueT:\n    \"\"\"Convert a normalized value back to value in the defined hyperparameter range.\n\n    Args:\n        normalized_value: normalized value to convert.\n\n    Returns:\n        The value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.sample_value","title":"sample_value  <code>abstractmethod</code>","text":"<pre><code>sample_value(*, user_priors: bool = False) -&gt; ValueT\n</code></pre> <p>Sample a new value.</p> <p>Similar to <code>Parameter.sample_value()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef sample_value(self, *, user_priors: bool = False) -&gt; ValueT:\n    \"\"\"Sample a new value.\n\n    Similar to\n    [`Parameter.sample_value()`][neps.search_spaces.Parameter.sample_value],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        The sampled value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.set_value","title":"set_value  <code>abstractmethod</code>","text":"<pre><code>set_value(value: ValueT | None) -&gt; None\n</code></pre> <p>Set the value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_value(self, value: ValueT | None) -&gt; None:\n    \"\"\"Set the value for the hyperparameter.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.Numerical.value_to_normalized","title":"value_to_normalized  <code>abstractmethod</code>","text":"<pre><code>value_to_normalized(value: ValueT) -&gt; float\n</code></pre> <p>Convert a value to a normalized value.</p> <p>Normalization is different per hyperparameter type, but roughly refers to numeric values.</p> <ul> <li><code>(0, 1)</code> scaling in the case of     a <code>Numerical</code>,</li> <li><code>{0.0, 1.0}</code> for a <code>Constant</code>,</li> <li><code>[0, 1, ..., n]</code> for a     <code>Categorical</code>.</li> </ul> PARAMETER DESCRIPTION <code>value</code> <p>value to convert.</p> <p> TYPE: <code>ValueT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The normalized value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef value_to_normalized(self, value: ValueT) -&gt; float:\n    \"\"\"Convert a value to a normalized value.\n\n    Normalization is different per hyperparameter type,\n    but roughly refers to numeric values.\n\n    * `(0, 1)` scaling in the case of\n        a [`Numerical`][neps.search_spaces.Numerical],\n    * `{0.0, 1.0}` for a [`Constant`][neps.search_spaces.Constant],\n    * `[0, 1, ..., n]` for a\n        [`Categorical`][neps.search_spaces.Categorical].\n\n    Args:\n        value: value to convert.\n\n    Returns:\n        The normalized value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter","title":"NumericalParameter","text":"<pre><code>NumericalParameter(\n    lower: T,\n    upper: T,\n    *,\n    log: bool = False,\n    default: T | None,\n    is_fidelity: bool,\n    domain: Domain[T],\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Numerical</code></p> <p>Deprecated: Use <code>Numerical</code> instead of <code>NumericalParameter</code>.</p> <p>This class remains for backward compatibility and will raise a deprecation warning if used.</p> PARAMETER DESCRIPTION <code>lower</code> <p>The lower bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>upper</code> <p>The upper bound of the numerical hyperparameter.</p> <p> TYPE: <code>T</code> </p> <code>log</code> <p>Whether the hyperparameter is in log space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>The default value of the hyperparameter.</p> <p> TYPE: <code>T | None</code> </p> <code>is_fidelity</code> <p>Whether the hyperparameter is a fidelity parameter.</p> <p> TYPE: <code>bool</code> </p> <code>domain</code> <p>The domain of the hyperparameter.</p> <p> TYPE: <code>Domain[T]</code> </p> <code>default_confidence</code> <p>The default confidence choice.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> RAISES DESCRIPTION <code>DeprecationWarning</code> <p>A warning indicating that <code>neps.NumericalParameter</code> is</p> Source code in <code>neps/search_spaces/hyperparameters/numerical.py</code> <pre><code>def __init__(\n    self,\n    lower: T,\n    upper: T,\n    *,\n    log: bool = False,\n    default: T | None,\n    is_fidelity: bool,\n    domain: Domain[T],\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Initialize a deprecated `NumericalParameter`.\n\n    Args:\n        lower: The lower bound of the numerical hyperparameter.\n        upper: The upper bound of the numerical hyperparameter.\n        log: Whether the hyperparameter is in log space.\n        default: The default value of the hyperparameter.\n        is_fidelity: Whether the hyperparameter is a fidelity parameter.\n        domain: The domain of the hyperparameter.\n        default_confidence: The default confidence choice.\n\n    Raises:\n        DeprecationWarning: A warning indicating that `neps.NumericalParameter` is\n        deprecated and `neps.Numerical` should be used instead.\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        (\n            \"Usage of 'neps.NumericalParameter' is deprecated and will be removed in\"\n            \" future releases. Please use 'neps.Numerical' instead.\"\n        ),\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    super().__init__(\n        lower=lower,\n        upper=upper,\n        log=log,\n        default=default,\n        is_fidelity=is_fidelity,\n        domain=domain,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.clone","title":"clone  <code>abstractmethod</code>","text":"<pre><code>clone() -&gt; Self\n</code></pre> <p>Create a copy of the <code>Parameter</code>.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef clone(self) -&gt; Self:\n    \"\"\"Create a copy of the `Parameter`.\"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.normalized_to_value","title":"normalized_to_value  <code>abstractmethod</code>","text":"<pre><code>normalized_to_value(normalized_value: float) -&gt; ValueT\n</code></pre> <p>Convert a normalized value back to value in the defined hyperparameter range.</p> PARAMETER DESCRIPTION <code>normalized_value</code> <p>normalized value to convert.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef normalized_to_value(self, normalized_value: float) -&gt; ValueT:\n    \"\"\"Convert a normalized value back to value in the defined hyperparameter range.\n\n    Args:\n        normalized_value: normalized value to convert.\n\n    Returns:\n        The value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.sample_value","title":"sample_value  <code>abstractmethod</code>","text":"<pre><code>sample_value(*, user_priors: bool = False) -&gt; ValueT\n</code></pre> <p>Sample a new value.</p> <p>Similar to <code>Parameter.sample_value()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef sample_value(self, *, user_priors: bool = False) -&gt; ValueT:\n    \"\"\"Sample a new value.\n\n    Similar to\n    [`Parameter.sample_value()`][neps.search_spaces.Parameter.sample_value],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        The sampled value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.set_value","title":"set_value  <code>abstractmethod</code>","text":"<pre><code>set_value(value: ValueT | None) -&gt; None\n</code></pre> <p>Set the value for the hyperparameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>ValueT | None</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef set_value(self, value: ValueT | None) -&gt; None:\n    \"\"\"Set the value for the hyperparameter.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/search_spaces/hyperparameters/numerical/#neps.search_spaces.hyperparameters.numerical.NumericalParameter.value_to_normalized","title":"value_to_normalized  <code>abstractmethod</code>","text":"<pre><code>value_to_normalized(value: ValueT) -&gt; float\n</code></pre> <p>Convert a value to a normalized value.</p> <p>Normalization is different per hyperparameter type, but roughly refers to numeric values.</p> <ul> <li><code>(0, 1)</code> scaling in the case of     a <code>Numerical</code>,</li> <li><code>{0.0, 1.0}</code> for a <code>Constant</code>,</li> <li><code>[0, 1, ..., n]</code> for a     <code>Categorical</code>.</li> </ul> PARAMETER DESCRIPTION <code>value</code> <p>value to convert.</p> <p> TYPE: <code>ValueT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The normalized value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>@abstractmethod\ndef value_to_normalized(self, value: ValueT) -&gt; float:\n    \"\"\"Convert a value to a normalized value.\n\n    Normalization is different per hyperparameter type,\n    but roughly refers to numeric values.\n\n    * `(0, 1)` scaling in the case of\n        a [`Numerical`][neps.search_spaces.Numerical],\n    * `{0.0, 1.0}` for a [`Constant`][neps.search_spaces.Constant],\n    * `[0, 1, ..., n]` for a\n        [`Categorical`][neps.search_spaces.Categorical].\n\n    Args:\n        value: value to convert.\n\n    Returns:\n        The normalized value.\n    \"\"\"\n</code></pre>"},{"location":"api/neps/state/err_dump/","title":"Err dump","text":""},{"location":"api/neps/state/err_dump/#neps.state.err_dump","title":"neps.state.err_dump","text":"<p>Error dump for serializing errors.</p> <p>This resource is used to store errors that can be serialized and deserialized, such that they can be shared between workers.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump","title":"ErrDump  <code>dataclass</code>","text":"<pre><code>ErrDump(\n    SerializableTrialError: ClassVar = SerializableTrialError,\n    errs: list[SerializableTrialError] = list(),\n)\n</code></pre> <p>A collection of errors that can be serialized and deserialized.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump.append","title":"append","text":"<pre><code>append(err: SerializableTrialError) -&gt; None\n</code></pre> <p>Append the an error to the reported errors.</p> Source code in <code>neps/state/err_dump.py</code> <pre><code>def append(self, err: SerializableTrialError) -&gt; None:\n    \"\"\"Append the an error to the reported errors.\"\"\"\n    return self.errs.append(err)\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump.empty","title":"empty","text":"<pre><code>empty() -&gt; bool\n</code></pre> <p>Check if the queue is empty.</p> Source code in <code>neps/state/err_dump.py</code> <pre><code>def empty(self) -&gt; bool:\n    \"\"\"Check if the queue is empty.\"\"\"\n    return not self.errs\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.ErrDump.latest_err_as_raisable","title":"latest_err_as_raisable","text":"<pre><code>latest_err_as_raisable() -&gt; SerializedError | None\n</code></pre> <p>Get the latest error.</p> Source code in <code>neps/state/err_dump.py</code> <pre><code>def latest_err_as_raisable(self) -&gt; SerializedError | None:\n    \"\"\"Get the latest error.\"\"\"\n    if self.errs:\n        return self.errs[-1].as_raisable()\n    return None\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError","title":"SerializableTrialError  <code>dataclass</code>","text":"<pre><code>SerializableTrialError(\n    trial_id: str,\n    worker_id: str,\n    err_type: str,\n    err: str,\n    tb: str | None,\n)\n</code></pre> <p>Error information for a trial.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.err","title":"err  <code>instance-attribute</code>","text":"<pre><code>err: str\n</code></pre> <p>The error msg.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.err_type","title":"err_type  <code>instance-attribute</code>","text":"<pre><code>err_type: str\n</code></pre> <p>The type of the error.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.tb","title":"tb  <code>instance-attribute</code>","text":"<pre><code>tb: str | None\n</code></pre> <p>The traceback of the error.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.trial_id","title":"trial_id  <code>instance-attribute</code>","text":"<pre><code>trial_id: str\n</code></pre> <p>The ID of the trial.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.worker_id","title":"worker_id  <code>instance-attribute</code>","text":"<pre><code>worker_id: str\n</code></pre> <p>The ID of the worker that evaluated the trial which caused the error.</p>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializableTrialError.as_raisable","title":"as_raisable","text":"<pre><code>as_raisable() -&gt; SerializedError\n</code></pre> <p>Convert the error to a raisable error.</p> Source code in <code>neps/state/err_dump.py</code> <pre><code>def as_raisable(self) -&gt; SerializedError:\n    \"\"\"Convert the error to a raisable error.\"\"\"\n    return SerializedError(\n        f\"An error occurred during the evaluation of a trial '{self.trial_id}' which\"\n        f\" was evaluted by worker '{self.worker_id}'. The original error could not\"\n        \" be deserialized but had the following information:\"\n        \"\\n\"\n        f\"{self.err_type}: {self.err}\"\n        \"\\n\\n\"\n        f\"{self.tb}\"\n    )\n</code></pre>"},{"location":"api/neps/state/err_dump/#neps.state.err_dump.SerializedError","title":"SerializedError","text":"<p>               Bases: <code>NePSError</code></p> <p>An error the is serialized.</p>"},{"location":"api/neps/state/filebased/","title":"Filebased","text":""},{"location":"api/neps/state/filebased/#neps.state.filebased","title":"neps.state.filebased","text":"<p>This module houses the implementation of a NePSState that does everything on the filesystem, i.e. locking, versioning and storing/loading.</p> <p>The main components are: * <code>FileVersioner</code>: A versioner that     stores a version tag on disk, usually for a resource like a Trial. * <code>FileLocker</code>: A locker that uses a file     to lock between processes. * <code>TrialRepoInDirectory</code>: A     repository of Trials that are stored in a directory. * <code>ReaderWriterXXX</code>: Reader/writers for various resources NePSState needs * <code>load_filebased_neps_state</code>:     A function to load a NePSState from a directory. * <code>create_filebased_neps_state</code>:     A function to create a new NePSState in a directory.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.FileLocker","title":"FileLocker  <code>dataclass</code>","text":"<pre><code>FileLocker(\n    lock_path: Path, poll: float, timeout: float | None\n)\n</code></pre> <p>               Bases: <code>Locker</code></p> <p>File-based locker using <code>portalocker</code>.</p> <p><code>FileLocker</code> implements the <code>Locker</code> protocol using <code>portalocker</code> to lock a file between processes with a shared filesystem.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.FileVersioner","title":"FileVersioner  <code>dataclass</code>","text":"<pre><code>FileVersioner(version_file: Path)\n</code></pre> <p>               Bases: <code>Versioner</code></p> <p>A versioner that stores a version tag on disk.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterErrDump","title":"ReaderWriterErrDump  <code>dataclass</code>","text":"<pre><code>ReaderWriterErrDump(name: str)\n</code></pre> <p>               Bases: <code>ReaderWriter[ErrDump, Path]</code></p> <p>ReaderWriter for shared error lists.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterOptimizationState","title":"ReaderWriterOptimizationState  <code>dataclass</code>","text":"<pre><code>ReaderWriterOptimizationState(\n    STATE_FILE_NAME: ClassVar = \"state.yaml\",\n)\n</code></pre> <p>               Bases: <code>ReaderWriter[OptimizationState, Path]</code></p> <p>ReaderWriter for OptimizationState objects.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterOptimizerInfo","title":"ReaderWriterOptimizerInfo  <code>dataclass</code>","text":"<pre><code>ReaderWriterOptimizerInfo(\n    INFO_FILENAME: ClassVar = \"info.yaml\",\n)\n</code></pre> <p>               Bases: <code>ReaderWriter[OptimizerInfo, Path]</code></p> <p>ReaderWriter for OptimizerInfo objects.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterSeedSnapshot","title":"ReaderWriterSeedSnapshot  <code>dataclass</code>","text":"<pre><code>ReaderWriterSeedSnapshot(\n    PY_RNG_STATE_DTYPE: ClassVar = int64,\n    PY_RNG_TUPLE_FILENAME: ClassVar = \"py_rng.npy\",\n    NP_RNG_STATE_FILENAME: ClassVar = \"np_rng_state.npy\",\n    TORCH_RNG_STATE_FILENAME: ClassVar = \"torch_rng_state.pt\",\n    TORCH_CUDA_RNG_STATE_FILENAME: ClassVar = \"torch_cuda_rng_state.pt\",\n    SEED_INFO_FILENAME: ClassVar = \"seed_info.json\",\n)\n</code></pre> <p>               Bases: <code>ReaderWriter[SeedSnapshot, Path]</code></p> <p>ReaderWriter for SeedSnapshot objects.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.ReaderWriterTrial","title":"ReaderWriterTrial  <code>dataclass</code>","text":"<pre><code>ReaderWriterTrial()\n</code></pre> <p>               Bases: <code>ReaderWriter[Trial, Path]</code></p> <p>ReaderWriter for Trial objects.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.TrialRepoInDirectory","title":"TrialRepoInDirectory  <code>dataclass</code>","text":"<pre><code>TrialRepoInDirectory(\n    directory: Path,\n    _cache: dict[str, Synced[Trial, Path]] = dict(),\n)\n</code></pre> <p>               Bases: <code>TrialRepo[Path]</code></p> <p>A repository of Trials that are stored in a directory.</p>"},{"location":"api/neps/state/filebased/#neps.state.filebased.TrialRepoInDirectory.all","title":"all","text":"<pre><code>all() -&gt; dict[str, Synced[Trial, Path]]\n</code></pre> <p>Get a dictionary of all the Trials in the repository.</p> <p>Note</p> <p>See <code>get_by_id()</code> for notes on the trials syncing.</p> Source code in <code>neps/state/filebased.py</code> <pre><code>@override\ndef all(self) -&gt; dict[str, Synced[Trial, Path]]:\n    \"\"\"Get a dictionary of all the Trials in the repository.\n\n    !!! note\n        See [`get_by_id()`][neps.state.filebased.TrialRepoInDirectory.get_by_id]\n        for notes on the trials syncing.\n    \"\"\"\n    return {trial_id: self.get_by_id(trial_id) for trial_id in self.all_trial_ids()}\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.TrialRepoInDirectory.all_trial_ids","title":"all_trial_ids","text":"<pre><code>all_trial_ids() -&gt; set[str]\n</code></pre> <p>List all the trial ids in this trial Repo.</p> Source code in <code>neps/state/filebased.py</code> <pre><code>@override\ndef all_trial_ids(self) -&gt; set[str]:\n    \"\"\"List all the trial ids in this trial Repo.\"\"\"\n    return {\n        config_path.name.replace(\"config_\", \"\")\n        for config_path in self.directory.iterdir()\n        if config_path.name.startswith(\"config_\") and config_path.is_dir()\n    }\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.TrialRepoInDirectory.get_by_id","title":"get_by_id","text":"<pre><code>get_by_id(\n    trial_id: str,\n    *,\n    lock_poll: float = TRIAL_FILELOCK_POLL,\n    lock_timeout: float | None = TRIAL_FILELOCK_TIMEOUT\n) -&gt; Synced[Trial, Path]\n</code></pre> <p>Get a Trial by its ID.</p> <p>Note</p> <p>This will not explicitly sync the trial and it is up to the caller to do so. Most of the time, the caller should be a NePSState object which will do that for you. However if the trial is not in the cache, then it will be loaded from disk which requires syncing.</p> PARAMETER DESCRIPTION <code>trial_id</code> <p>The ID of the trial to get.</p> <p> TYPE: <code>str</code> </p> <code>lock_poll</code> <p>The poll time for the file lock.</p> <p> TYPE: <code>float</code> DEFAULT: <code>TRIAL_FILELOCK_POLL</code> </p> <code>lock_timeout</code> <p>The timeout for the file lock.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>TRIAL_FILELOCK_TIMEOUT</code> </p> RETURNS DESCRIPTION <code>Synced[Trial, Path]</code> <p>The trial with the given ID.</p> Source code in <code>neps/state/filebased.py</code> <pre><code>@override\ndef get_by_id(\n    self,\n    trial_id: str,\n    *,\n    lock_poll: float = TRIAL_FILELOCK_POLL,\n    lock_timeout: float | None = TRIAL_FILELOCK_TIMEOUT,\n) -&gt; Synced[Trial, Path]:\n    \"\"\"Get a Trial by its ID.\n\n    !!! note\n\n        This will **not** explicitly sync the trial and it is up to the caller\n        to do so. Most of the time, the caller should be a NePSState\n        object which will do that for you. However if the trial is not in the\n        cache, then it will be loaded from disk which requires syncing.\n\n    Args:\n        trial_id: The ID of the trial to get.\n        lock_poll: The poll time for the file lock.\n        lock_timeout: The timeout for the file lock.\n\n    Returns:\n        The trial with the given ID.\n    \"\"\"\n    trial = self._cache.get(trial_id)\n    if trial is not None:\n        return trial\n\n    config_path = self.directory / f\"config_{trial_id}\"\n    if not config_path.exists():\n        raise TrialRepo.TrialNotFoundError(trial_id, config_path)\n\n    trial = Synced.load(\n        location=config_path,\n        locker=FileLocker(\n            lock_path=config_path / \".lock\",\n            poll=lock_poll,\n            timeout=lock_timeout,\n        ),\n        versioner=FileVersioner(version_file=config_path / \".version\"),\n        reader_writer=ReaderWriterTrial(),\n    )\n    self._cache[trial_id] = trial\n    return trial\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.TrialRepoInDirectory.put_new","title":"put_new","text":"<pre><code>put_new(\n    trial: Trial,\n    *,\n    lock_poll: float = TRIAL_FILELOCK_POLL,\n    lock_timeout: float | None = TRIAL_FILELOCK_TIMEOUT\n) -&gt; Synced[Trial, Path]\n</code></pre> <p>Put a new Trial into the repository.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial to put.</p> <p> TYPE: <code>Trial</code> </p> <code>lock_poll</code> <p>The poll time for the file lock.</p> <p> TYPE: <code>float</code> DEFAULT: <code>TRIAL_FILELOCK_POLL</code> </p> <code>lock_timeout</code> <p>The timeout for the file lock.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>TRIAL_FILELOCK_TIMEOUT</code> </p> RETURNS DESCRIPTION <code>Synced[Trial, Path]</code> <p>The synced trial.</p> RAISES DESCRIPTION <code>TrialAlreadyExistsError</code> <p>If the trial already exists in the repository.</p> Source code in <code>neps/state/filebased.py</code> <pre><code>@override\ndef put_new(\n    self,\n    trial: Trial,\n    *,\n    lock_poll: float = TRIAL_FILELOCK_POLL,\n    lock_timeout: float | None = TRIAL_FILELOCK_TIMEOUT,\n) -&gt; Synced[Trial, Path]:\n    \"\"\"Put a new Trial into the repository.\n\n    Args:\n        trial: The trial to put.\n        lock_poll: The poll time for the file lock.\n        lock_timeout: The timeout for the file lock.\n\n    Returns:\n        The synced trial.\n\n    Raises:\n        TrialRepo.TrialAlreadyExistsError: If the trial already exists in the\n            repository.\n    \"\"\"\n    config_path = self.directory.absolute().resolve() / f\"config_{trial.metadata.id}\"\n    if config_path.exists():\n        raise TrialRepo.TrialAlreadyExistsError(\n            f\"Trial '{trial.metadata.id}' already exists as '{config_path}'.\"\n        )\n\n    # HACK: We do this here as there is no way to know where a Trial will\n    # be located when it's created...\n    trial.metadata.location = str(config_path)\n    shared_trial = Synced.new(\n        data=trial,\n        location=config_path,\n        locker=FileLocker(\n            lock_path=config_path / \".lock\",\n            poll=lock_poll,\n            timeout=lock_timeout,\n        ),\n        versioner=FileVersioner(version_file=config_path / \".version\"),\n        reader_writer=ReaderWriterTrial(),\n    )\n    self._cache[trial.metadata.id] = shared_trial\n    return shared_trial\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.create_or_load_filebased_neps_state","title":"create_or_load_filebased_neps_state","text":"<pre><code>create_or_load_filebased_neps_state(\n    directory: Path,\n    *,\n    optimizer_info: OptimizerInfo,\n    optimizer_state: OptimizationState\n) -&gt; NePSState[Path]\n</code></pre> <p>Create a new NePSState in a directory or load the existing one if it already exists.</p> <p>Warning</p> <p>We check that the optimizer info in the NePSState on disk matches the one that is passed. However we do not lock this check so it is possible that if two processes try to create a NePSState at the same time, both with different optimizer infos, that one will fail to create the NePSState. This is a limitation of the current design.</p> <p>In principal, we could allow multiple optimizers to be run and share the same set of trials.</p> PARAMETER DESCRIPTION <code>directory</code> <p>The directory to create the state in.</p> <p> TYPE: <code>Path</code> </p> <code>optimizer_info</code> <p>The optimizer info to use.</p> <p> TYPE: <code>OptimizerInfo</code> </p> <code>optimizer_state</code> <p>The optimizer state to use.</p> <p> TYPE: <code>OptimizationState</code> </p> RETURNS DESCRIPTION <code>NePSState[Path]</code> <p>The NePSState.</p> RAISES DESCRIPTION <code>NePSError</code> <p>If the optimizer info on disk does not match the one provided.</p> Source code in <code>neps/state/filebased.py</code> <pre><code>def create_or_load_filebased_neps_state(\n    directory: Path,\n    *,\n    optimizer_info: OptimizerInfo,\n    optimizer_state: OptimizationState,\n) -&gt; NePSState[Path]:\n    \"\"\"Create a new NePSState in a directory or load the existing one\n    if it already exists.\n\n    !!! warning\n\n        We check that the optimizer info in the NePSState on disk matches\n        the one that is passed. However we do not lock this check so it\n        is possible that if two processes try to create a NePSState at the\n        same time, both with different optimizer infos, that one will fail\n        to create the NePSState. This is a limitation of the current design.\n\n        In principal, we could allow multiple optimizers to be run and share\n        the same set of trials.\n\n    Args:\n        directory: The directory to create the state in.\n        optimizer_info: The optimizer info to use.\n        optimizer_state: The optimizer state to use.\n\n    Returns:\n        The NePSState.\n\n    Raises:\n        NePSError: If the optimizer info on disk does not match the one provided.\n    \"\"\"\n    is_new = not directory.exists()\n    directory.mkdir(parents=True, exist_ok=True)\n    config_dir = directory / \"configs\"\n    config_dir.mkdir(parents=True, exist_ok=True)\n    seed_dir = directory / \".seed_state\"\n    seed_dir.mkdir(parents=True, exist_ok=True)\n    error_dir = directory / \".errors\"\n    error_dir.mkdir(parents=True, exist_ok=True)\n    optimizer_state_dir = directory / \".optimizer_state\"\n    optimizer_state_dir.mkdir(parents=True, exist_ok=True)\n    optimizer_info_dir = directory / \".optimizer_info\"\n    optimizer_info_dir.mkdir(parents=True, exist_ok=True)\n\n    # We have to do one bit of sanity checking to ensure that the optimzier\n    # info on disk manages the one we have recieved, otherwise we are unsure which\n    # optimizer is being used.\n    # NOTE: We assume that we do not have to worry about a race condition\n    # here where we have two different NePSState objects with two different optimizer\n    # infos trying to be created at the same time. This avoids the need to lock to\n    # check the optimizer info. If this assumption changes, then we would have\n    # to first lock before we do this check\n    optimizer_info_reader_writer = ReaderWriterOptimizerInfo()\n    if not is_new:\n        existing_info = optimizer_info_reader_writer.read(optimizer_info_dir)\n        if existing_info != optimizer_info:\n            raise NePSError(\n                \"The optimizer info on disk does not match the one provided.\"\n                f\"\\nOn disk: {existing_info}\\nProvided: {optimizer_info}\"\n                f\"\\n\\nLoaded the one on disk from {optimizer_info_dir}.\"\n            )\n\n    return NePSState(\n        location=str(directory.absolute().resolve()),\n        _trials=TrialRepoInDirectory(config_dir),\n        _optimizer_info=Synced.new_or_load(\n            data=optimizer_info,  # type: ignore\n            location=optimizer_info_dir,\n            versioner=FileVersioner(version_file=optimizer_info_dir / \".version\"),\n            locker=FileLocker(\n                lock_path=optimizer_info_dir / \".lock\",\n                poll=OPTIMIZER_INFO_FILELOCK_POLL,\n                timeout=OPTIMIZER_INFO_FILELOCK_TIMEOUT,\n            ),\n            reader_writer=ReaderWriterOptimizerInfo(),\n        ),\n        _seed_state=Synced.new_or_load(\n            data=SeedSnapshot.new_capture(),\n            location=seed_dir,\n            reader_writer=ReaderWriterSeedSnapshot(),\n            versioner=FileVersioner(version_file=seed_dir / \".version\"),\n            locker=FileLocker(\n                lock_path=seed_dir / \".lock\",\n                poll=SEED_SNAPSHOT_FILELOCK_POLL,\n                timeout=SEED_SNAPSHOT_FILELOCK_TIMEOUT,\n            ),\n        ),\n        _shared_errors=Synced.new_or_load(\n            data=ErrDump(),\n            location=error_dir,\n            reader_writer=ReaderWriterErrDump(\"all\"),\n            versioner=FileVersioner(version_file=error_dir / \".all.version\"),\n            locker=FileLocker(\n                lock_path=error_dir / \".all.lock\",\n                poll=GLOBAL_ERR_FILELOCK_POLL,\n                timeout=GLOBAL_ERR_FILELOCK_TIMEOUT,\n            ),\n        ),\n        _optimizer_state=Synced.new_or_load(\n            data=optimizer_state,\n            location=optimizer_state_dir,\n            reader_writer=ReaderWriterOptimizationState(),\n            versioner=FileVersioner(version_file=optimizer_state_dir / \".version\"),\n            locker=FileLocker(\n                lock_path=optimizer_state_dir / \".lock\",\n                poll=OPTIMIZER_STATE_FILELOCK_POLL,\n                timeout=OPTIMIZER_STATE_FILELOCK_TIMEOUT,\n            ),\n        ),\n    )\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.load_filebased_neps_state","title":"load_filebased_neps_state","text":"<pre><code>load_filebased_neps_state(\n    directory: Path,\n) -&gt; NePSState[Path]\n</code></pre> <p>Load a NePSState from a directory.</p> PARAMETER DESCRIPTION <code>directory</code> <p>The directory to load the state from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>NePSState[Path]</code> <p>The loaded NePSState.</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If no NePSState is found at the given directory.</p> Source code in <code>neps/state/filebased.py</code> <pre><code>def load_filebased_neps_state(directory: Path) -&gt; NePSState[Path]:\n    \"\"\"Load a NePSState from a directory.\n\n    Args:\n        directory: The directory to load the state from.\n\n    Returns:\n        The loaded NePSState.\n\n    Raises:\n        FileNotFoundError: If no NePSState is found at the given directory.\n    \"\"\"\n    if not directory.exists():\n        raise FileNotFoundError(f\"No NePSState found at '{directory}'.\")\n    directory.mkdir(parents=True, exist_ok=True)\n    config_dir = directory / \"configs\"\n    config_dir.mkdir(parents=True, exist_ok=True)\n    seed_dir = directory / \".seed_state\"\n    seed_dir.mkdir(parents=True, exist_ok=True)\n    error_dir = directory / \".errors\"\n    error_dir.mkdir(parents=True, exist_ok=True)\n    optimizer_state_dir = directory / \".optimizer_state\"\n    optimizer_state_dir.mkdir(parents=True, exist_ok=True)\n    optimizer_info_dir = directory / \".optimizer_info\"\n    optimizer_info_dir.mkdir(parents=True, exist_ok=True)\n\n    return NePSState(\n        location=str(directory.absolute().resolve()),\n        _trials=TrialRepoInDirectory(config_dir),\n        _optimizer_info=Synced.load(\n            location=optimizer_info_dir,\n            versioner=FileVersioner(version_file=optimizer_info_dir / \".version\"),\n            locker=FileLocker(\n                lock_path=optimizer_info_dir / \".lock\",\n                poll=OPTIMIZER_INFO_FILELOCK_POLL,\n                timeout=OPTIMIZER_INFO_FILELOCK_TIMEOUT,\n            ),\n            reader_writer=ReaderWriterOptimizerInfo(),\n        ),\n        _seed_state=Synced.load(\n            location=seed_dir,\n            reader_writer=ReaderWriterSeedSnapshot(),\n            versioner=FileVersioner(version_file=seed_dir / \".version\"),\n            locker=FileLocker(\n                lock_path=seed_dir / \".lock\",\n                poll=SEED_SNAPSHOT_FILELOCK_POLL,\n                timeout=SEED_SNAPSHOT_FILELOCK_TIMEOUT,\n            ),\n        ),\n        _shared_errors=Synced.load(\n            location=error_dir,\n            reader_writer=ReaderWriterErrDump(\"all\"),\n            versioner=FileVersioner(version_file=error_dir / \".all.version\"),\n            locker=FileLocker(\n                lock_path=error_dir / \".all.lock\",\n                poll=GLOBAL_ERR_FILELOCK_POLL,\n                timeout=GLOBAL_ERR_FILELOCK_TIMEOUT,\n            ),\n        ),\n        _optimizer_state=Synced.load(\n            location=optimizer_state_dir,\n            reader_writer=ReaderWriterOptimizationState(),\n            versioner=FileVersioner(version_file=optimizer_state_dir / \".version\"),\n            locker=FileLocker(\n                lock_path=optimizer_state_dir / \".lock\",\n                poll=OPTIMIZER_STATE_FILELOCK_POLL,\n                timeout=OPTIMIZER_STATE_FILELOCK_TIMEOUT,\n            ),\n        ),\n    )\n</code></pre>"},{"location":"api/neps/state/filebased/#neps.state.filebased.make_sha","title":"make_sha","text":"<pre><code>make_sha() -&gt; str\n</code></pre> <p>Generate a str hex sha.</p> Source code in <code>neps/state/filebased.py</code> <pre><code>def make_sha() -&gt; str:\n    \"\"\"Generate a str hex sha.\"\"\"\n    return uuid4().hex\n</code></pre>"},{"location":"api/neps/state/neps_state/","title":"Neps state","text":""},{"location":"api/neps/state/neps_state/#neps.state.neps_state","title":"neps.state.neps_state","text":"<p>The main state object that holds all the shared state objects.</p> <p>This object is used to interact with the shared state objects in a safe atomic manner, such that each worker can create an identical NePSState and interact with it without having to worry about locking or out-dated information.</p> <p>For an actual instantiation of this object, see <code>create_or_load_filebased_neps_state</code>.</p>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState","title":"NePSState  <code>dataclass</code>","text":"<pre><code>NePSState(\n    location: str,\n    _trials: TrialRepo[Loc],\n    _optimizer_info: Synced[OptimizerInfo, Loc],\n    _seed_state: Synced[SeedSnapshot, Loc],\n    _optimizer_state: Synced[OptimizationState, Loc],\n    _shared_errors: Synced[ErrDump, Loc],\n)\n</code></pre> <p>               Bases: <code>Generic[Loc]</code></p> <p>The main state object that holds all the shared state objects.</p>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.all_trial_ids","title":"all_trial_ids","text":"<pre><code>all_trial_ids() -&gt; set[str]\n</code></pre> <p>Get all the trial ids that are known about.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def all_trial_ids(self) -&gt; set[str]:\n    \"\"\"Get all the trial ids that are known about.\"\"\"\n    return self._trials.all_trial_ids()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.get_all_trials","title":"get_all_trials","text":"<pre><code>get_all_trials() -&gt; dict[str, Trial]\n</code></pre> <p>Get all the trials that are known about.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def get_all_trials(self) -&gt; dict[str, Trial]:\n    \"\"\"Get all the trials that are known about.\"\"\"\n    return {_id: trial.synced() for _id, trial in self._trials.all().items()}\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.get_errors","title":"get_errors","text":"<pre><code>get_errors() -&gt; ErrDump\n</code></pre> <p>Get all the errors that have occurred during the optimization.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def get_errors(self) -&gt; ErrDump:\n    \"\"\"Get all the errors that have occurred during the optimization.\"\"\"\n    return self._shared_errors.synced()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.get_next_pending_trial","title":"get_next_pending_trial","text":"<pre><code>get_next_pending_trial() -&gt; Trial | None\n</code></pre><pre><code>get_next_pending_trial(n: int | None = None) -&gt; list[Trial]\n</code></pre> <pre><code>get_next_pending_trial(\n    n: int | None = None,\n) -&gt; Trial | list[Trial] | None\n</code></pre> <p>Get the next pending trial to evaluate.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of trials to get. If <code>None</code>, get the next trial.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial | list[Trial] | None</code> <p>The next trial or a list of trials if <code>n</code> is not <code>None</code>.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def get_next_pending_trial(self, n: int | None = None) -&gt; Trial | list[Trial] | None:\n    \"\"\"Get the next pending trial to evaluate.\n\n    Args:\n        n: The number of trials to get. If `None`, get the next trial.\n\n    Returns:\n        The next trial or a list of trials if `n` is not `None`.\n    \"\"\"\n    _pending_itr = (\n        shared_trial.synced() for _, shared_trial in self._trials.pending()\n    )\n    if n is not None:\n        return take(n, _pending_itr)\n    return next(_pending_itr, None)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.get_trial_by_id","title":"get_trial_by_id","text":"<pre><code>get_trial_by_id(trial_id: str) -&gt; Trial\n</code></pre> <p>Get a trial by its id.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def get_trial_by_id(self, trial_id: str, /) -&gt; Trial:\n    \"\"\"Get a trial by its id.\"\"\"\n    return self._trials.get_by_id(trial_id).synced()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.optimizer_info","title":"optimizer_info","text":"<pre><code>optimizer_info() -&gt; OptimizerInfo\n</code></pre> <p>Get the optimizer information.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def optimizer_info(self) -&gt; OptimizerInfo:\n    \"\"\"Get the optimizer information.\"\"\"\n    return self._optimizer_info.synced()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.optimizer_state","title":"optimizer_state","text":"<pre><code>optimizer_state() -&gt; OptimizationState\n</code></pre> <p>Get the optimizer state.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def optimizer_state(self) -&gt; OptimizationState:\n    \"\"\"Get the optimizer state.\"\"\"\n    return self._optimizer_state.synced()\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.put_updated_trial","title":"put_updated_trial","text":"<pre><code>put_updated_trial(trial: Trial) -&gt; None\n</code></pre> <p>Update the trial with the new information.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial to update.</p> <p> TYPE: <code>Trial</code> </p> RAISES DESCRIPTION <code>VersionMismatchError</code> <p>If the trial has been updated since it was last fetched by the worker using this state. This indicates that some other worker has updated the trial in the meantime and the changes from this worker are rejected.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def put_updated_trial(self, trial: Trial, /) -&gt; None:\n    \"\"\"Update the trial with the new information.\n\n    Args:\n        trial: The trial to update.\n\n    Raises:\n        VersionMismatchError: If the trial has been updated since it was last\n            fetched by the worker using this state. This indicates that some other\n            worker has updated the trial in the meantime and the changes from\n            this worker are rejected.\n    \"\"\"\n    shared_trial = self._trials.get_by_id(trial.id)\n    shared_trial.put(trial)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.report_trial_evaluation","title":"report_trial_evaluation","text":"<pre><code>report_trial_evaluation(\n    trial: Trial, report: Report, *, worker_id: str\n) -&gt; None\n</code></pre> <p>Update the trial with the evaluation report and update the optimizer state accordingly.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial that was evaluated.</p> <p> TYPE: <code>Trial</code> </p> <code>report</code> <p>The evaluation report.</p> <p> TYPE: <code>Report</code> </p> <code>optimizer</code> <p>The optimizer to update and get the state from</p> <p> </p> <code>worker_id</code> <p>The worker that evaluated the trial.</p> <p> TYPE: <code>str</code> </p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def report_trial_evaluation(\n    self,\n    trial: Trial,\n    report: Trial.Report,\n    *,\n    worker_id: str,\n) -&gt; None:\n    \"\"\"Update the trial with the evaluation report and update the optimizer state\n    accordingly.\n\n    Args:\n        trial: The trial that was evaluated.\n        report: The evaluation report.\n        optimizer: The optimizer to update and get the state from\n        worker_id: The worker that evaluated the trial.\n    \"\"\"\n    shared_trial = self._trials.get_by_id(trial.id)\n    # TODO: This would fail if some other worker has already updated the trial.\n\n    # IMPORTANT: We need to attach the report to the trial before updating the things.\n    trial.report = report\n    shared_trial.put(trial)\n    logger.debug(\"Updated trial '%s' with status '%s'\", trial.id, trial.state)\n    with self._optimizer_state.acquire() as (opt_state, put_opt_state):\n        # TODO: If an optimizer doesn't use the state, this is a waste of time.\n        # Update the budget if we have one.\n        if opt_state.budget is not None:\n            budget_info = opt_state.budget\n\n            if report.cost is not None:\n                budget_info.used_cost_budget += report.cost\n        put_opt_state(opt_state)\n\n    if report.err is not None:\n        with self._shared_errors.acquire() as (errs, put_errs):\n            trial_err = ErrDump.SerializableTrialError(\n                trial_id=trial.id,\n                worker_id=worker_id,\n                err_type=type(report.err).__name__,\n                err=str(report.err),\n                tb=report.tb,\n            )\n            errs.append(trial_err)\n            put_errs(errs)\n</code></pre>"},{"location":"api/neps/state/neps_state/#neps.state.neps_state.NePSState.sample_trial","title":"sample_trial","text":"<pre><code>sample_trial(\n    optimizer: BaseOptimizer,\n    *,\n    worker_id: str,\n    _sample_hooks: list[Callable] | None = None\n) -&gt; Trial\n</code></pre> <p>Sample a new trial from the optimizer.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The optimizer to sample the trial from.</p> <p> TYPE: <code>BaseOptimizer</code> </p> <code>worker_id</code> <p>The worker that is sampling the trial.</p> <p> TYPE: <code>str</code> </p> <code>_sample_hooks</code> <p>A list of hooks to apply to the optimizer before sampling.</p> <p> TYPE: <code>list[Callable] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial</code> <p>The new trial.</p> Source code in <code>neps/state/neps_state.py</code> <pre><code>def sample_trial(\n    self,\n    optimizer: BaseOptimizer,\n    *,\n    worker_id: str,\n    _sample_hooks: list[Callable] | None = None,\n) -&gt; Trial:\n    \"\"\"Sample a new trial from the optimizer.\n\n    Args:\n        optimizer: The optimizer to sample the trial from.\n        worker_id: The worker that is sampling the trial.\n        _sample_hooks: A list of hooks to apply to the optimizer before sampling.\n\n    Returns:\n        The new trial.\n    \"\"\"\n    with (\n        self._optimizer_state.acquire() as (\n            opt_state,\n            put_opt,\n        ),\n        self._seed_state.acquire() as (seed_state, put_seed_state),\n    ):\n        trials: dict[str, Trial] = {}\n        for trial_id, shared_trial in self._trials.all().items():\n            trial = shared_trial.synced()\n            trials[trial_id] = trial\n\n        seed_state.set_as_global_seed_state()\n\n        # TODO: Not sure if any existing pre_load hooks required\n        # it to be done after `load_results`... I hope not.\n        if _sample_hooks is not None:\n            for hook in _sample_hooks:\n                optimizer = hook(optimizer)\n\n        # NOTE: We don't want optimizers mutating this before serialization\n        budget = opt_state.budget.clone() if opt_state.budget is not None else None\n        sampled_config_maybe_new_opt_state = optimizer.ask(\n            trials=trials,\n            budget_info=budget,\n        )\n\n        if isinstance(sampled_config_maybe_new_opt_state, tuple):\n            sampled_config, new_opt_state = sampled_config_maybe_new_opt_state\n        else:\n            sampled_config = sampled_config_maybe_new_opt_state\n            new_opt_state = opt_state.shared_state\n\n        if sampled_config.previous_config_id is not None:\n            previous_trial = trials.get(sampled_config.previous_config_id)\n            if previous_trial is None:\n                raise ValueError(\n                    f\"Previous trial '{sampled_config.previous_config_id}' not found.\"\n                )\n            previous_trial_location = previous_trial.metadata.location\n        else:\n            previous_trial_location = None\n\n        trial = Trial.new(\n            trial_id=sampled_config.id,\n            location=\"\",  # HACK: This will be set by the `TrialRepo`\n            config=sampled_config.config,\n            previous_trial=sampled_config.previous_config_id,\n            previous_trial_location=previous_trial_location,\n            time_sampled=time.time(),\n            worker_id=worker_id,\n        )\n        shared_trial = self._trials.put_new(trial)\n        seed_state.recapture()\n        put_seed_state(seed_state)\n        put_opt(\n            OptimizationState(budget=opt_state.budget, shared_state=new_opt_state)\n        )\n\n    return trial\n</code></pre>"},{"location":"api/neps/state/optimizer/","title":"Optimizer","text":""},{"location":"api/neps/state/optimizer/#neps.state.optimizer","title":"neps.state.optimizer","text":"<p>Optimizer state and info dataclasses.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.BudgetInfo","title":"BudgetInfo  <code>dataclass</code>","text":"<pre><code>BudgetInfo(\n    max_cost_budget: float | None = None,\n    used_cost_budget: float = 0.0,\n    max_evaluations: int | None = None,\n    used_evaluations: int = 0,\n)\n</code></pre> <p>Information about the budget of an optimizer.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.BudgetInfo.clone","title":"clone","text":"<pre><code>clone() -&gt; BudgetInfo\n</code></pre> <p>Create a copy of the budget info.</p> Source code in <code>neps/state/optimizer.py</code> <pre><code>def clone(self) -&gt; BudgetInfo:\n    \"\"\"Create a copy of the budget info.\"\"\"\n    return replace(self)\n</code></pre>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizationState","title":"OptimizationState  <code>dataclass</code>","text":"<pre><code>OptimizationState(\n    budget: BudgetInfo | None, shared_state: dict[str, Any]\n)\n</code></pre> <p>The current state of an optimizer.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizationState.budget","title":"budget  <code>instance-attribute</code>","text":"<pre><code>budget: BudgetInfo | None\n</code></pre> <p>Information regarind the budget used by the optimization trajectory.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizationState.shared_state","title":"shared_state  <code>instance-attribute</code>","text":"<pre><code>shared_state: dict[str, Any]\n</code></pre> <p>Any information the optimizer wants to store between calls to sample and post evaluations.</p> <p>For example, an optimizer may wish to store running totals here or various other bits of information that may be expensive to recompute.</p> <p>Right now there's no support for tensors/arrays and almost no optimizer uses this feature. Only cost-cooling uses information out of <code>.budget</code>.</p> <p>Please reach out to @eddiebergman if you have a use case for this so we can make it more robust.</p>"},{"location":"api/neps/state/optimizer/#neps.state.optimizer.OptimizerInfo","title":"OptimizerInfo  <code>dataclass</code>","text":"<pre><code>OptimizerInfo(info: Mapping[str, Any])\n</code></pre> <p>Meta-information about an optimizer.</p>"},{"location":"api/neps/state/protocols/","title":"Protocols","text":""},{"location":"api/neps/state/protocols/#neps.state.protocols","title":"neps.state.protocols","text":"<p>This module defines the protocols used by <code>NePSState</code> and <code>Synced</code> to ensure atomic operations to the state itself.</p>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Locker","title":"Locker","text":"<p>               Bases: <code>Protocol</code></p> <p>A locker that can be used to communicate between workers.</p>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Locker.is_locked","title":"is_locked","text":"<pre><code>is_locked() -&gt; bool\n</code></pre> <p>Check if lock is...well, locked.</p> <p>Should return True if the resource is locked, even if the lock is held by the current worker/process.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def is_locked(self) -&gt; bool:\n    \"\"\"Check if lock is...well, locked.\n\n    Should return True if the resource is locked, even if the lock is held by the\n    current worker/process.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Locker.lock","title":"lock","text":"<pre><code>lock() -&gt; Iterator[None]\n</code></pre> <p>Initiate the lock as a context manager, releasing it when done.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>@contextmanager\ndef lock(self) -&gt; Iterator[None]:\n    \"\"\"Initiate the lock as a context manager, releasing it when done.\"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.ReaderWriter","title":"ReaderWriter","text":"<p>               Bases: <code>Protocol[T, Loc_contra]</code></p> <p>A reader-writer that can read and write some resource T with location Loc.</p> <p>For example, a <code>ReaderWriter[Trial, Path]</code> indicates a class that can read and write trials, given some <code>Path</code>.</p>"},{"location":"api/neps/state/protocols/#neps.state.protocols.ReaderWriter.read","title":"read","text":"<pre><code>read(loc: Loc_contra) -&gt; T\n</code></pre> <p>Read the resource at the given location.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def read(self, loc: Loc_contra, /) -&gt; T:\n    \"\"\"Read the resource at the given location.\"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.ReaderWriter.write","title":"write","text":"<pre><code>write(value: T, loc: Loc_contra) -&gt; None\n</code></pre> <p>Write the resource at the given location.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def write(self, value: T, loc: Loc_contra, /) -&gt; None:\n    \"\"\"Write the resource at the given location.\"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced","title":"Synced  <code>dataclass</code>","text":"<pre><code>Synced(\n    LockFailedError: ClassVar = LockFailedError,\n    VersionedResourceRemovedError: ClassVar = VersionedResourceRemovedError,\n    VersionMismatchError: ClassVar = VersionMismatchError,\n    VersionedResourceAlreadyExistsError: ClassVar = VersionedResourceAlreadyExistsError,\n    VersionedResourceDoesNotExistsError: ClassVar = VersionedResourceDoesNotExistsError,\n    _resource: VersionedResource[T, K],\n    _locker: Locker,\n)\n</code></pre> <p>               Bases: <code>Generic[T, K]</code></p> <p>Manages a versioned resource but it's methods also implement locking procedures for accessing it.</p> <p>Its types are parametrized by two type variables:</p> <ul> <li><code>T</code> is the type of the data stored in the resource.</li> <li><code>K</code> is the type of the location of the resource, for example <code>Path</code></li> </ul> <p>This wraps a <code>VersionedResource</code> and additionally provides utility to perform atmoic operations on it using a <code>Locker</code>.</p> <p>This is used by <code>NePSState</code> to manage the state of trials and other shared resources.</p> <p>It consists of 2 main components:</p> <ul> <li>A <code>VersionedResource</code> to manage the     versioning of the resource.</li> <li>A <code>Locker</code> to manage the locking of the resource.</li> </ul> <p>The primary methods to interact with a resource that is behined a <code>Synced</code> are:</p> <ul> <li><code>synced()</code> to get the data of the resource     after syncing it to it's latest verison.</li> <li><code>acquire()</code> context manager to get latest     version of the data while also mainting a lock on it. This additionally provides     a <code>put()</code> operation to put the data back. This can primarily be used to get the     data, perform some mutation on it and then put it back, while not allowing other     workers access to the data.</li> </ul>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.acquire","title":"acquire","text":"<pre><code>acquire() -&gt; Iterator[tuple[T, Callable[[T], None]]]\n</code></pre> <p>Acquire the lock and get the data of the resource.</p> <p>This is a context manager that returns the data of the resource and a function to put the data back.</p> <p>Note</p> <p>This is the primary way to get the resource, mutate it and put it back. Otherwise you likely want <code>synced()</code> or <code>put()</code>.</p> YIELDS DESCRIPTION <code>tuple[T, Callable[[T], None]]</code> <p>A tuple containing the data of the resource and a function to put the data back.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>@contextmanager\ndef acquire(self) -&gt; Iterator[tuple[T, Callable[[T], None]]]:\n    \"\"\"Acquire the lock and get the data of the resource.\n\n    This is a context manager that returns the data of the resource and a function\n    to put the data back.\n\n    !!! note\n        This is the primary way to get the resource, mutate it and put it back.\n        Otherwise you likely want [`synced()`][neps.state.protocols.Synced.synced]\n        or [`put()`][neps.state.protocols.Synced.put].\n\n    Yields:\n        A tuple containing the data of the resource and a function to put the data\n        back.\n    \"\"\"\n    with self._locker.lock():\n        self._resource.sync()\n        yield self._resource.current(), self._put_unsafe\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy() -&gt; Self\n</code></pre> <p>Create a deep copy of the shared resource.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def deepcopy(self) -&gt; Self:\n    \"\"\"Create a deep copy of the shared resource.\"\"\"\n    return deepcopy(self)\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(\n    *,\n    locker: Locker,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2]\n) -&gt; Synced[T2, K2]\n</code></pre> <p>Load an existing Synced resource.</p> <p>This will load an existing resource if it exists, otherwise, it will raise an error.</p> <p>Use <code>new()</code> if you want to create a new resource. Use <code>new_or_load()</code> if you want to create a new resource if it doesn't exist, otherwise load an existing resource.</p> PARAMETER DESCRIPTION <code>locker</code> <p>The locker to be used.</p> <p> TYPE: <code>Locker</code> </p> <code>location</code> <p>The location of the resource.</p> <p> TYPE: <code>K2</code> </p> <code>versioner</code> <p>The versioner to be used.</p> <p> TYPE: <code>Versioner</code> </p> <code>reader_writer</code> <p>The reader-writer to be used.</p> <p> TYPE: <code>ReaderWriter[T2, K2]</code> </p> RETURNS DESCRIPTION <code>Synced[T2, K2]</code> <p>A Synced resource.</p> RAISES DESCRIPTION <code>VersionedResourceDoesNotExistsError</code> <p>If no versioned resource exists at the given location.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    *,\n    locker: Locker,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2],\n) -&gt; Synced[T2, K2]:\n    \"\"\"Load an existing Synced resource.\n\n    This will load an existing resource if it exists, otherwise, it will raise an\n    error.\n\n    Use [`new()`][neps.state.protocols.Synced.new] if you want to create a new\n    resource. Use [`new_or_load()`][neps.state.protocols.Synced.new_or_load] if you\n    want to create a new resource if it doesn't exist, otherwise load an existing\n    resource.\n\n    Args:\n        locker: The locker to be used.\n        location: The location of the resource.\n        versioner: The versioner to be used.\n        reader_writer: The reader-writer to be used.\n\n    Returns:\n        A Synced resource.\n\n    Raises:\n        VersionedResourceDoesNotExistsError: If no versioned resource exists at\n            the given location.\n    \"\"\"\n    with locker.lock():\n        return Synced(\n            _resource=VersionedResource.load(\n                location=location,\n                versioner=versioner,\n                reader_writer=reader_writer,\n            ),\n            _locker=locker,\n        )\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.location","title":"location","text":"<pre><code>location() -&gt; K\n</code></pre> <p>Get the location of the resource.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def location(self) -&gt; K:\n    \"\"\"Get the location of the resource.\"\"\"\n    return self._resource.location()\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(\n    *,\n    locker: Locker,\n    data: T2,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2]\n) -&gt; Synced[T2, K2]\n</code></pre> <p>Create a new Synced resource.</p> <p>This will create a new resource if it doesn't exist, otherwise, if it already exists, it will raise an error.</p> <p>Use <code>load()</code> if you want to load an existing resource. Use <code>new_or_load()</code> if you want to create a new resource if it doesn't exist, otherwise load an existing resource.</p> PARAMETER DESCRIPTION <code>locker</code> <p>The locker to be used.</p> <p> TYPE: <code>Locker</code> </p> <code>data</code> <p>The data to be stored.</p> <p> TYPE: <code>T2</code> </p> <code>location</code> <p>The location where the data will be stored.</p> <p> TYPE: <code>K2</code> </p> <code>versioner</code> <p>The versioner to be used.</p> <p> TYPE: <code>Versioner</code> </p> <code>reader_writer</code> <p>The reader-writer to be used.</p> <p> TYPE: <code>ReaderWriter[T2, K2]</code> </p> RETURNS DESCRIPTION <code>Synced[T2, K2]</code> <p>A new Synced resource.</p> RAISES DESCRIPTION <code>VersionedResourceAlreadyExistsError</code> <p>If a versioned resource already exists at the given location.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>@classmethod\ndef new(\n    cls,\n    *,\n    locker: Locker,\n    data: T2,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2],\n) -&gt; Synced[T2, K2]:\n    \"\"\"Create a new Synced resource.\n\n    This will create a new resource if it doesn't exist, otherwise,\n    if it already exists, it will raise an error.\n\n    Use [`load()`][neps.state.protocols.Synced.load] if you want to load an existing\n    resource. Use [`new_or_load()`][neps.state.protocols.Synced.new_or_load] if you\n    want to create a new resource if it doesn't exist, otherwise load an existing\n    resource.\n\n    Args:\n        locker: The locker to be used.\n        data: The data to be stored.\n        location: The location where the data will be stored.\n        versioner: The versioner to be used.\n        reader_writer: The reader-writer to be used.\n\n    Returns:\n        A new Synced resource.\n\n    Raises:\n        VersionedResourceAlreadyExistsError: If a versioned resource already exists\n            at the given location.\n    \"\"\"\n    with locker.lock():\n        vr = VersionedResource.new(\n            data=data,\n            location=location,\n            versioner=versioner,\n            reader_writer=reader_writer,\n        )\n        return Synced(_resource=vr, _locker=locker)\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.new_or_load","title":"new_or_load  <code>classmethod</code>","text":"<pre><code>new_or_load(\n    *,\n    locker: Locker,\n    data: T2,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2]\n) -&gt; Synced[T2, K2]\n</code></pre> <p>Create a new Synced resource if it doesn't exist, otherwise load it.</p> <p>This will create a new resource if it doesn't exist, otherwise, it will load an existing resource.</p> <p>Use <code>new()</code> if you want to create a new resource and fail otherwise. Use <code>load()</code> if you want to load an existing resource and fail if it doesn't exist.</p> PARAMETER DESCRIPTION <code>locker</code> <p>The locker to be used.</p> <p> TYPE: <code>Locker</code> </p> <code>data</code> <p>The data to be stored.</p> <p>Warning</p> <p>This will be ignored if the data already exists.</p> <p> TYPE: <code>T2</code> </p> <code>location</code> <p>The location where the data will be stored.</p> <p> TYPE: <code>K2</code> </p> <code>versioner</code> <p>The versioner to be used.</p> <p> TYPE: <code>Versioner</code> </p> <code>reader_writer</code> <p>The reader-writer to be used.</p> <p> TYPE: <code>ReaderWriter[T2, K2]</code> </p> RETURNS DESCRIPTION <code>Synced[T2, K2]</code> <p>A Synced resource.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>@classmethod\ndef new_or_load(\n    cls,\n    *,\n    locker: Locker,\n    data: T2,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2],\n) -&gt; Synced[T2, K2]:\n    \"\"\"Create a new Synced resource if it doesn't exist, otherwise load it.\n\n    This will create a new resource if it doesn't exist, otherwise, it will load\n    an existing resource.\n\n    Use [`new()`][neps.state.protocols.Synced.new] if you want to create a new\n    resource and fail otherwise. Use [`load()`][neps.state.protocols.Synced.load]\n    if you want to load an existing resource and fail if it doesn't exist.\n\n    Args:\n        locker: The locker to be used.\n        data: The data to be stored.\n\n            !!! warning\n\n                This will be ignored if the data already exists.\n\n        location: The location where the data will be stored.\n        versioner: The versioner to be used.\n        reader_writer: The reader-writer to be used.\n\n    Returns:\n        A Synced resource.\n    \"\"\"\n    try:\n        return Synced.new(\n            locker=locker,\n            data=data,\n            location=location,\n            versioner=versioner,\n            reader_writer=reader_writer,\n        )\n    except VersionedResourceAlreadyExistsError:\n        return Synced.load(\n            locker=locker,\n            location=location,\n            versioner=versioner,\n            reader_writer=reader_writer,\n        )\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.put","title":"put","text":"<pre><code>put(data: T) -&gt; None\n</code></pre> <p>Update the data atomically.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def put(self, data: T) -&gt; None:\n    \"\"\"Update the data atomically.\"\"\"\n    with self._locker.lock():\n        self._resource.put(data)\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Synced.synced","title":"synced","text":"<pre><code>synced() -&gt; T\n</code></pre> <p>Get the data of the resource atomically.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def synced(self) -&gt; T:\n    \"\"\"Get the data of the resource atomically.\"\"\"\n    with self._locker.lock():\n        return self._resource.sync_and_get()\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.TrialRepo","title":"TrialRepo","text":"<p>               Bases: <code>Protocol[K]</code></p> <p>A repository of trials.</p> <p>The primary purpose of this protocol is to ensure consistent access to trial, the ability to put in a new trial and know about the trials that are stored there.</p>"},{"location":"api/neps/state/protocols/#neps.state.protocols.TrialRepo.all","title":"all","text":"<pre><code>all() -&gt; dict[str, Synced[Trial, K]]\n</code></pre> <p>Get all trials in the repo.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def all(self) -&gt; dict[str, Synced[Trial, K]]:\n    \"\"\"Get all trials in the repo.\"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.TrialRepo.all_trial_ids","title":"all_trial_ids","text":"<pre><code>all_trial_ids() -&gt; set[str]\n</code></pre> <p>List all the trial ids in this trial Repo.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def all_trial_ids(self) -&gt; set[str]:\n    \"\"\"List all the trial ids in this trial Repo.\"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.TrialRepo.get_by_id","title":"get_by_id","text":"<pre><code>get_by_id(trial_id: str) -&gt; Synced[Trial, K]\n</code></pre> <p>Get a trial by its id.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def get_by_id(self, trial_id: str) -&gt; Synced[Trial, K]:\n    \"\"\"Get a trial by its id.\"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.TrialRepo.pending","title":"pending","text":"<pre><code>pending() -&gt; Iterable[tuple[str, Synced[Trial, K]]]\n</code></pre> <p>Get all pending trials in the repo.</p> <p>Note</p> <p>This should return trials in the order in which they should be next evaluated, usually the order in which they were put in the repo.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def pending(self) -&gt; Iterable[tuple[str, Synced[Trial, K]]]:\n    \"\"\"Get all pending trials in the repo.\n\n    !!! note\n        This should return trials in the order in which they should be next evaluated,\n        usually the order in which they were put in the repo.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.TrialRepo.put_new","title":"put_new","text":"<pre><code>put_new(trial: Trial) -&gt; Synced[Trial, K]\n</code></pre> <p>Put a new trial in the repo.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def put_new(self, trial: Trial) -&gt; Synced[Trial, K]:\n    \"\"\"Put a new trial in the repo.\"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource","title":"VersionedResource  <code>dataclass</code>","text":"<pre><code>VersionedResource(\n    VersionMismatchError: ClassVar = VersionMismatchError,\n    VersionedResourceDoesNotExistsError: ClassVar = VersionedResourceDoesNotExistsError,\n    VersionedResourceAlreadyExistsError: ClassVar = VersionedResourceAlreadyExistsError,\n    VersionedResourceRemovedError: ClassVar = VersionedResourceRemovedError,\n    _current: T,\n    _location: K,\n    _version: str,\n    _versioner: Versioner,\n    _reader_writer: ReaderWriter[T, K],\n)\n</code></pre> <p>               Bases: <code>Generic[T, K]</code></p> <p>A resource that will be read if it needs to update to the latest version.</p> <p>Relies on 3 main components: * A <code>Versioner</code> to manage the versioning of the     resource. * A <code>ReaderWriter</code> to read and write the     resource. * The location of the resource that can be used for the reader-writer.</p>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.current","title":"current","text":"<pre><code>current() -&gt; T\n</code></pre> <p>Get the current data of the resource.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def current(self) -&gt; T:\n    \"\"\"Get the current data of the resource.\"\"\"\n    return self._current\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.is_stale","title":"is_stale","text":"<pre><code>is_stale() -&gt; bool\n</code></pre> <p>Check if the resource is stale.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def is_stale(self) -&gt; bool:\n    \"\"\"Check if the resource is stale.\"\"\"\n    return self._version != self._versioner.current()\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(\n    *,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2]\n) -&gt; VersionedResource[T2, K2]\n</code></pre> <p>Load an existing VersionedResource.</p> <p>This will load an existing resource if it exists, otherwise, it will raise an error.</p> <p>Use <code>new()</code> if you want to create a new resource.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the resource.</p> <p> TYPE: <code>K2</code> </p> <code>versioner</code> <p>The versioner to be used.</p> <p> TYPE: <code>Versioner</code> </p> <code>reader_writer</code> <p>The reader-writer to be used.</p> <p> TYPE: <code>ReaderWriter[T2, K2]</code> </p> RETURNS DESCRIPTION <code>VersionedResource[T2, K2]</code> <p>A VersionedResource</p> RAISES DESCRIPTION <code>VersionedResourceDoesNotExistsError</code> <p>If no versioned resource exists at the given location.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    *,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2],\n) -&gt; VersionedResource[T2, K2]:\n    \"\"\"Load an existing VersionedResource.\n\n    This will load an existing resource if it exists, otherwise, it will raise an\n    error.\n\n    Use [`new()`][neps.state.protocols.VersionedResource.new] if you want to\n    create a new resource.\n\n    Args:\n        location: The location of the resource.\n        versioner: The versioner to be used.\n        reader_writer: The reader-writer to be used.\n\n    Returns:\n        A VersionedResource\n\n    Raises:\n        VersionedResourceDoesNotExistsError: If no versioned resource exists at\n            the given location.\n    \"\"\"\n    version = versioner.current()\n    if version is None:\n        raise cls.VersionedResourceDoesNotExistsError(\n            f\"No versioned resource exists at '{location}'.\"\n        )\n    data = reader_writer.read(location)\n    return VersionedResource(\n        _current=data,\n        _location=location,\n        _version=version,\n        _versioner=versioner,\n        _reader_writer=reader_writer,\n    )\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.location","title":"location","text":"<pre><code>location() -&gt; K\n</code></pre> <p>Get the location of the resource.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def location(self) -&gt; K:\n    \"\"\"Get the location of the resource.\"\"\"\n    return self._location\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.new","title":"new  <code>staticmethod</code>","text":"<pre><code>new(\n    *,\n    data: T2,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2]\n) -&gt; VersionedResource[T2, K2]\n</code></pre> <p>Create a new VersionedResource.</p> <p>This will create a new resource if it doesn't exist, otherwise, if it already exists, it will raise an error.</p> <p>Use <code>load()</code> if you want to load an existing resource.</p> PARAMETER DESCRIPTION <code>data</code> <p>The data to be stored.</p> <p> TYPE: <code>T2</code> </p> <code>location</code> <p>The location where the data will be stored.</p> <p> TYPE: <code>K2</code> </p> <code>versioner</code> <p>The versioner to be used.</p> <p> TYPE: <code>Versioner</code> </p> <code>reader_writer</code> <p>The reader-writer to be used.</p> <p> TYPE: <code>ReaderWriter[T2, K2]</code> </p> RETURNS DESCRIPTION <code>VersionedResource[T2, K2]</code> <p>A new VersionedResource</p> RAISES DESCRIPTION <code>VersionedResourceAlreadyExistsError</code> <p>If a versioned resource already exists at the given location.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>@staticmethod\ndef new(\n    *,\n    data: T2,\n    location: K2,\n    versioner: Versioner,\n    reader_writer: ReaderWriter[T2, K2],\n) -&gt; VersionedResource[T2, K2]:\n    \"\"\"Create a new VersionedResource.\n\n    This will create a new resource if it doesn't exist, otherwise,\n    if it already exists, it will raise an error.\n\n    Use [`load()`][neps.state.protocols.VersionedResource.load] if you want to\n    load an existing resource.\n\n    Args:\n        data: The data to be stored.\n        location: The location where the data will be stored.\n        versioner: The versioner to be used.\n        reader_writer: The reader-writer to be used.\n\n    Returns:\n        A new VersionedResource\n\n    Raises:\n        VersionedResourceAlreadyExistsError: If a versioned resource already exists\n            at the given location.\n    \"\"\"\n    current_version = versioner.current()\n    if current_version is not None:\n        raise VersionedResourceAlreadyExistsError(\n            f\"A versioend resource already already exists at '{location}'\"\n            f\" with version '{current_version}'\"\n        )\n\n    version = versioner.bump()\n    reader_writer.write(data, location)\n    return VersionedResource(\n        _current=data,\n        _location=location,\n        _version=version,\n        _versioner=versioner,\n        _reader_writer=reader_writer,\n    )\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.put","title":"put","text":"<pre><code>put(data: T) -&gt; None\n</code></pre> <p>Put the data and version of the resource.</p> RAISES DESCRIPTION <code>VersionMismatchError</code> <p>If the version of the resource is not the same as the current version. This implies that the resource has been updated by another worker.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def put(self, data: T) -&gt; None:\n    \"\"\"Put the data and version of the resource.\n\n    Raises:\n        VersionMismatchError: If the version of the resource is not the same as the\n            current version. This implies that the resource has been updated by\n            another worker.\n    \"\"\"\n    current_version = self._versioner.current()\n    if self._version != current_version:\n        raise self.VersionMismatchError(\n            f\"Version mismatch - ours: '{self._version}', remote: '{current_version}'\"\n            f\" Tried to put data at '{self._location}'. Doing so would overwrite\"\n            \" changes made by another worker. The solution is to pull the latest\"\n            \" version of the resource and try again.\"\n            \" The most possible reasons for this error is that a lock was not\"\n            \" utilized when getting this resource before putting it back.\"\n        )\n\n    self._reader_writer.write(data, self._location)\n    self._current = data\n    self._version = self._versioner.bump()\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.sync","title":"sync","text":"<pre><code>sync() -&gt; None\n</code></pre> <p>Sync the resource with the latest version.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def sync(self) -&gt; None:\n    \"\"\"Sync the resource with the latest version.\"\"\"\n    current_version = self._versioner.current()\n    if current_version is None:\n        raise self.VersionedResourceRemovedError(\n            f\"Versioned resource at '{self._location}' has been removed!\"\n            f\" Last known version was '{self._version}'.\"\n        )\n\n    if self._version != current_version:\n        self._current = self._reader_writer.read(self._location)\n        self._version = current_version\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.VersionedResource.sync_and_get","title":"sync_and_get","text":"<pre><code>sync_and_get() -&gt; T\n</code></pre> <p>Get the data and version of the resource.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def sync_and_get(self) -&gt; T:\n    \"\"\"Get the data and version of the resource.\"\"\"\n    self.sync()\n    return self._current\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Versioner","title":"Versioner","text":"<p>               Bases: <code>Protocol</code></p> <p>A versioner that can bump the version of a resource.</p> <p>It should have some <code>current()</code> method to give the current version tag of a resource and a <code>bump()</code> method to provide a new version tag.</p> <p>These <code>current()</code> and <code>bump()</code> methods do not need to be atomic but they should read/write to external state, i.e. file-system, database, etc.</p>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Versioner.bump","title":"bump","text":"<pre><code>bump() -&gt; str\n</code></pre> <p>Create a new external version tag.</p> RETURNS DESCRIPTION <code>str</code> <p>The new version tag.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def bump(self) -&gt; str:\n    \"\"\"Create a new external version tag.\n\n    Returns:\n        The new version tag.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/protocols/#neps.state.protocols.Versioner.current","title":"current","text":"<pre><code>current() -&gt; str | None\n</code></pre> <p>Return the current version as defined by the external state, i.e. the version of the tag on disk.</p> RETURNS DESCRIPTION <code>str | None</code> <p>The current version if there is one written.</p> Source code in <code>neps/state/protocols.py</code> <pre><code>def current(self) -&gt; str | None:\n    \"\"\"Return the current version as defined by the external state, i.e.\n    the version of the tag on disk.\n\n    Returns:\n        The current version if there is one written.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/neps/state/seed_snapshot/","title":"Seed snapshot","text":""},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot","title":"neps.state.seed_snapshot","text":"<p>Snapshot of the global rng state.</p>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot","title":"SeedSnapshot  <code>dataclass</code>","text":"<pre><code>SeedSnapshot(\n    np_rng: NP_RNG_STATE,\n    py_rng: PY_RNG_STATE,\n    torch_rng: TORCH_RNG_STATE | None,\n    torch_cuda_rng: TORCH_CUDA_RNG_STATE | None,\n)\n</code></pre> <p>State of the global rng.</p> <p>Primarly enables storing of the rng state to disk using a binary format native to each library, allowing for potential version mistmatches between processes loading the state, as long as they can read the binary format.</p>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot.new_capture","title":"new_capture  <code>classmethod</code>","text":"<pre><code>new_capture() -&gt; SeedSnapshot\n</code></pre> <p>Current state of the global rng.</p> <p>Takes a snapshot, including cloning or copying any arrays, tensors, etc.</p> Source code in <code>neps/state/seed_snapshot.py</code> <pre><code>@classmethod\ndef new_capture(cls) -&gt; SeedSnapshot:\n    \"\"\"Current state of the global rng.\n\n    Takes a snapshot, including cloning or copying any arrays, tensors, etc.\n    \"\"\"\n    self = cls(None, None, None, None)  # type: ignore\n    self.recapture()\n    return self\n</code></pre>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot.recapture","title":"recapture","text":"<pre><code>recapture() -&gt; None\n</code></pre> <p>Reread the state of the global rng into this snapshot.</p> Source code in <code>neps/state/seed_snapshot.py</code> <pre><code>def recapture(self) -&gt; None:\n    \"\"\"Reread the state of the global rng into this snapshot.\"\"\"\n    # https://numpy.org/doc/stable/reference/random/generated/numpy.random.get_state.html\n\n    self.py_rng = random.getstate()\n\n    np_keys = np.random.get_state(legacy=True)\n    assert np_keys[0] == \"MT19937\"  # type: ignore\n    self.np_rng = (np_keys[0], np_keys[1].copy(), *np_keys[2:])  # type: ignore\n\n    with contextlib.suppress(Exception):\n        import torch\n\n        self.torch_rng = torch.random.get_rng_state().clone()\n        torch_cuda_keys: list[torch.Tensor] | None = None\n        if torch.cuda.is_available():\n            torch_cuda_keys = [c.clone() for c in torch.cuda.get_rng_state_all()]\n        self.torch_cuda_rng = torch_cuda_keys\n</code></pre>"},{"location":"api/neps/state/seed_snapshot/#neps.state.seed_snapshot.SeedSnapshot.set_as_global_seed_state","title":"set_as_global_seed_state","text":"<pre><code>set_as_global_seed_state() -&gt; None\n</code></pre> <p>Set the global rng to the given state.</p> Source code in <code>neps/state/seed_snapshot.py</code> <pre><code>def set_as_global_seed_state(self) -&gt; None:\n    \"\"\"Set the global rng to the given state.\"\"\"\n    np.random.set_state(self.np_rng)\n    random.setstate(self.py_rng)\n\n    if self.torch_rng is not None or self.torch_cuda_rng is not None:\n        import torch\n\n        if self.torch_rng is not None:\n            torch.random.set_rng_state(self.torch_rng)\n\n        if self.torch_cuda_rng is not None and torch.cuda.is_available():\n            torch.cuda.set_rng_state_all(self.torch_cuda_rng)\n</code></pre>"},{"location":"api/neps/state/settings/","title":"Settings","text":""},{"location":"api/neps/state/settings/#neps.state.settings","title":"neps.state.settings","text":"<p>Settings for the worker and the global state of NePS.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues","title":"DefaultReportValues  <code>dataclass</code>","text":"<pre><code>DefaultReportValues(\n    loss_value_on_error: float | None = None,\n    cost_value_on_error: float | None = None,\n    cost_if_not_provided: float | None = None,\n    learning_curve_on_error: list[float] | None = None,\n    learning_curve_if_not_provided: (\n        Literal[\"loss\"] | list[float] | None\n    ) = None,\n)\n</code></pre> <p>Values to use when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.cost_if_not_provided","title":"cost_if_not_provided  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost_if_not_provided: float | None = None\n</code></pre> <p>The value to use for the cost when the evaluation function does not provide one.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.cost_value_on_error","title":"cost_value_on_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost_value_on_error: float | None = None\n</code></pre> <p>The value to use for the cost when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.learning_curve_if_not_provided","title":"learning_curve_if_not_provided  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_curve_if_not_provided: (\n    Literal[\"loss\"] | list[float] | None\n) = None\n</code></pre> <p>The value to use for the learning curve when the evaluation function does not provide one.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.learning_curve_on_error","title":"learning_curve_on_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_curve_on_error: list[float] | None = None\n</code></pre> <p>The value to use for the learning curve when an error occurs.</p> <p>If <code>'loss'</code>, the learning curve will be set to the loss value but as a list with a single value.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.DefaultReportValues.loss_value_on_error","title":"loss_value_on_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>loss_value_on_error: float | None = None\n</code></pre> <p>The value to use for the loss when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities","title":"OnErrorPossibilities","text":"<p>               Bases: <code>Enum</code></p> <p>Possible values for what to do when an error occurs.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.IGNORE","title":"IGNORE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IGNORE = 'ignore'\n</code></pre> <p>Ignore all errors and continue running.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.RAISE_ANY_ERROR","title":"RAISE_ANY_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RAISE_ANY_ERROR = 'raise_any_error'\n</code></pre> <p>Raise an error if there was an error from any worker, i.e. there is a trial in the NePSState that has an error.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.RAISE_WORKER_ERROR","title":"RAISE_WORKER_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RAISE_WORKER_ERROR = 'raise_worker_error'\n</code></pre> <p>Raise an error only if the error occurs in the worker.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.STOP_ANY_ERROR","title":"STOP_ANY_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STOP_ANY_ERROR = 'stop_any_error'\n</code></pre> <p>Stop the workers if any error occured from any worker, i.e. there is a trial in the NePSState that has an error.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.OnErrorPossibilities.STOP_WORKER_ERROR","title":"STOP_WORKER_ERROR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STOP_WORKER_ERROR = 'stop_worker_error'\n</code></pre> <p>Stop the worker if an error occurs in the worker, without raising</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings","title":"WorkerSettings  <code>dataclass</code>","text":"<pre><code>WorkerSettings(\n    on_error: OnErrorPossibilities,\n    default_report_values: DefaultReportValues,\n    max_evaluations_total: int | None,\n    include_in_progress_evaluations_towards_maximum: bool,\n    max_cost_total: float | None,\n    max_evaluation_time_total_seconds: float | None,\n    max_evaluations_for_worker: int | None,\n    max_cost_for_worker: float | None,\n    max_evaluation_time_for_worker_seconds: float | None,\n    max_wallclock_time_for_worker_seconds: float | None,\n)\n</code></pre> <p>Settings for a running instance of NePS.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.default_report_values","title":"default_report_values  <code>instance-attribute</code>","text":"<pre><code>default_report_values: DefaultReportValues\n</code></pre> <p>Values to use when an error occurs or was not specified.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.include_in_progress_evaluations_towards_maximum","title":"include_in_progress_evaluations_towards_maximum  <code>instance-attribute</code>","text":"<pre><code>include_in_progress_evaluations_towards_maximum: bool\n</code></pre> <p>Whether to include currently evaluating configurations towards the stopping criterion <code>max_evaluations_total</code></p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_cost_for_worker","title":"max_cost_for_worker  <code>instance-attribute</code>","text":"<pre><code>max_cost_for_worker: float | None\n</code></pre> <p>The maximum cost incurred by a worker before finisihng.</p> <p>Once this cost total is reached, only this worker will stop evaluating new configurations.</p> <p>This cost is the sum of <code>'cost'</code> values that are returned by evaluation of the target function.</p> <p>If <code>None</code>, there is no limit and the worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_cost_total","title":"max_cost_total  <code>instance-attribute</code>","text":"<pre><code>max_cost_total: float | None\n</code></pre> <p>The maximum cost to run in total.</p> <p>Once this cost total is reached, all workers will stop evaluating new configurations.</p> <p>This cost is the sum of <code>'cost'</code> values that are returned by evaluation of the target function.</p> <p>If <code>None</code>, there is no limit and workers will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluation_time_for_worker_seconds","title":"max_evaluation_time_for_worker_seconds  <code>instance-attribute</code>","text":"<pre><code>max_evaluation_time_for_worker_seconds: float | None\n</code></pre> <p>The maximum time to allow this worker for evaluating configurations.</p> <p>Note</p> <p>This does not include time for sampling new configurations.</p> <p>If <code>None</code>, there is no limit and this worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluation_time_total_seconds","title":"max_evaluation_time_total_seconds  <code>instance-attribute</code>","text":"<pre><code>max_evaluation_time_total_seconds: float | None\n</code></pre> <p>The maximum wallclock time allowed for evaluation in total.</p> <p>Note</p> <p>This does not include time for sampling new configurations.</p> <p>Once this wallclock time is reached, all workers will stop once their current evaluation is finished.</p> <p>If <code>None</code>, there is no limit and workers will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluations_for_worker","title":"max_evaluations_for_worker  <code>instance-attribute</code>","text":"<pre><code>max_evaluations_for_worker: int | None\n</code></pre> <p>The maximum number of evaluations to run for the worker.</p> <p>This count is specific to each worker spawned by NePS. only the current worker will stop evaluating new configurations once this limit is reached.</p> <p>If <code>None</code>, there is no limit and this worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_evaluations_total","title":"max_evaluations_total  <code>instance-attribute</code>","text":"<pre><code>max_evaluations_total: int | None\n</code></pre> <p>The maximum number of evaluations to run in total.</p> <p>Once this evaluation total is reached, all workers will stop evaluating new configurations.</p> <p>To control whether currently evaluating configurations are included in this total, see <code>include_in_progress_evaluations_towards_maximum</code>.</p> <p>If <code>None</code>, there is no limit and workers will continue to evaluate indefinitely.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.max_wallclock_time_for_worker_seconds","title":"max_wallclock_time_for_worker_seconds  <code>instance-attribute</code>","text":"<pre><code>max_wallclock_time_for_worker_seconds: float | None\n</code></pre> <p>The maximum wallclock time to run for this worker.</p> <p>Once this wallclock time is reached, only this worker will stop evaluating new configurations.</p> <p>Warning</p> <p>This will not stop the worker if it is currently evaluating a configuration.</p> <p>This is useful when the worker is deployed on some managed resource where there is a time limit.</p> <p>If <code>None</code>, there is no limit and this worker will continue to evaluate indefinitely or until another stopping criterion is met.</p>"},{"location":"api/neps/state/settings/#neps.state.settings.WorkerSettings.on_error","title":"on_error  <code>instance-attribute</code>","text":"<pre><code>on_error: OnErrorPossibilities\n</code></pre> <p>What to do when an error occurs.</p> <ul> <li><code>'raise_worker_error'</code>: Raise an error only if the error occurs in the worker.</li> <li><code>'raise_any_error'</code>: Raise an error if any error occurs from any worker, i.e.     there is a trial in the NePSState that has an error.</li> <li><code>'ignore'</code>: Ignore all errors and continue running.</li> </ul>"},{"location":"api/neps/state/trial/","title":"Trial","text":""},{"location":"api/neps/state/trial/#neps.state.trial","title":"neps.state.trial","text":"<p>A trial is a configuration and it's associated data.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.MetaData","title":"MetaData  <code>dataclass</code>","text":"<pre><code>MetaData(\n    id: str,\n    location: str,\n    previous_trial_id: str | None,\n    previous_trial_location: str | None,\n    sampling_worker_id: str,\n    time_sampled: float,\n    evaluating_worker_id: str | None = None,\n    evaluation_duration: float | None = None,\n    time_submitted: float | None = None,\n    time_started: float | None = None,\n    time_end: float | None = None,\n)\n</code></pre> <p>Metadata for a trial.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.NotReportedYetError","title":"NotReportedYetError","text":"<p>               Bases: <code>NePSError</code></p> <p>Raised when trying to access a report that has not been reported yet.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Report","title":"Report  <code>dataclass</code>","text":"<pre><code>Report(\n    trial_id: str,\n    loss: float | None,\n    cost: float | None,\n    learning_curve: list[float] | None,\n    extra: Mapping[str, Any],\n    err: Exception | None,\n    tb: str | None,\n    reported_as: Literal[\"success\", \"failed\", \"crashed\"],\n    evaluation_duration: float | None,\n)\n</code></pre> <p>A failed report of the evaluation of a configuration.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Report.to_deprecate_result_dict","title":"to_deprecate_result_dict","text":"<pre><code>to_deprecate_result_dict() -&gt; dict[str, Any] | ERROR\n</code></pre> <p>Return the report as a dictionary.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def to_deprecate_result_dict(self) -&gt; dict[str, Any] | ERROR:\n    \"\"\"Return the report as a dictionary.\"\"\"\n    if self.reported_as == \"success\":\n        d = {\"loss\": self.loss, \"cost\": self.cost, **self.extra}\n\n        # HACK: Backwards compatibility. Not sure how much this is needed\n        # but it should be removed once optimizers stop calling the\n        # `get_loss`, `get_cost`, `get_learning_curve` methods of `BaseOptimizer`\n        # and just use the `Report` directly.\n        if \"info_dict\" not in d or \"learning_curve\" not in d[\"info_dict\"]:\n            d.setdefault(\"info_dict\", {})[\"learning_curve\"] = self.learning_curve\n        return d\n\n    return \"error\"\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.State","title":"State","text":"<p>               Bases: <code>Enum</code></p> <p>The state of a trial.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.State.pending","title":"pending","text":"<pre><code>pending() -&gt; bool\n</code></pre> <p>Return True if the trial is pending.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def pending(self) -&gt; bool:\n    \"\"\"Return True if the trial is pending.\"\"\"\n    return self in (State.PENDING, State.SUBMITTED, State.EVALUATING)\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial","title":"Trial  <code>dataclass</code>","text":"<pre><code>Trial(\n    State: ClassVar = State,\n    Report: ClassVar = Report,\n    MetaData: ClassVar = MetaData,\n    NotReportedYetError: ClassVar = NotReportedYetError,\n    config: Mapping[str, Any],\n    metadata: MetaData,\n    state: State,\n    report: Report | None,\n)\n</code></pre> <p>A trial is a configuration and it's associated data.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Return the id of the trial.</p>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.into_config_result","title":"into_config_result","text":"<pre><code>into_config_result(\n    config_to_search_space: Callable[\n        [RawConfig], SearchSpace\n    ]\n) -&gt; ConfigResult\n</code></pre> <p>Convert the trial and report to a <code>ConfigResult</code> object.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def into_config_result(\n    self,\n    config_to_search_space: Callable[[RawConfig], SearchSpace],\n) -&gt; ConfigResult:\n    \"\"\"Convert the trial and report to a `ConfigResult` object.\"\"\"\n    if self.report is None:\n        raise self.NotReportedYetError(\"The trial has not been reported yet.\")\n    from neps.utils.types import ConfigResult\n\n    result: dict[str, Any] | ERROR\n    if self.report.reported_as == \"success\":\n        result = {\n            **self.report.extra,\n            \"loss\": self.report.loss,\n            \"cost\": self.report.cost,\n        }\n    else:\n        result = \"error\"\n\n    return ConfigResult(\n        self.id,\n        config=config_to_search_space(self.config),\n        result=result,\n        metadata=asdict(self.metadata),\n    )\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(\n    *,\n    trial_id: str,\n    config: Mapping[str, Any],\n    location: str,\n    previous_trial: str | None,\n    previous_trial_location: str | None,\n    time_sampled: float,\n    worker_id: int | str\n) -&gt; Self\n</code></pre> <p>Create a new trial object that was just sampled.</p> Source code in <code>neps/state/trial.py</code> <pre><code>@classmethod\ndef new(\n    cls,\n    *,\n    trial_id: str,\n    config: Mapping[str, Any],\n    location: str,\n    previous_trial: str | None,\n    previous_trial_location: str | None,\n    time_sampled: float,\n    worker_id: int | str,\n) -&gt; Self:\n    \"\"\"Create a new trial object that was just sampled.\"\"\"\n    worker_id = str(worker_id)\n    return cls(\n        state=State.PENDING,\n        config=config,\n        metadata=MetaData(\n            id=trial_id,\n            location=location,\n            time_sampled=time_sampled,\n            previous_trial_id=previous_trial,\n            previous_trial_location=previous_trial_location,\n            sampling_worker_id=worker_id,\n        ),\n        report=None,\n    )\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the trial to a pending state.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the trial to a pending state.\"\"\"\n    self.state = State.PENDING\n    self.metadata = MetaData(\n        id=self.metadata.id,\n        location=self.metadata.location,\n        previous_trial_id=self.metadata.previous_trial_id,\n        previous_trial_location=self.metadata.previous_trial_location,\n        time_sampled=self.metadata.time_sampled,\n        sampling_worker_id=self.metadata.sampling_worker_id,\n    )\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_complete","title":"set_complete","text":"<pre><code>set_complete(\n    *,\n    report_as: Literal[\"success\", \"failed\", \"crashed\"],\n    time_end: float,\n    loss: float | None,\n    cost: float | None,\n    learning_curve: list[float] | None,\n    err: Exception | None,\n    tb: str | None,\n    extra: Mapping[str, Any] | None,\n    evaluation_duration: float | None\n) -&gt; Report\n</code></pre> <p>Set the report for the trial.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def set_complete(\n    self,\n    *,\n    report_as: Literal[\"success\", \"failed\", \"crashed\"],\n    time_end: float,\n    loss: float | None,\n    cost: float | None,\n    learning_curve: list[float] | None,\n    err: Exception | None,\n    tb: str | None,\n    extra: Mapping[str, Any] | None,\n    evaluation_duration: float | None,\n) -&gt; Report:\n    \"\"\"Set the report for the trial.\"\"\"\n    if report_as == \"success\":\n        self.state = State.SUCCESS\n    elif report_as == \"failed\":\n        self.state = State.FAILED\n    elif report_as == \"crashed\":\n        self.state = State.CRASHED\n    else:\n        raise ValueError(f\"Invalid report_as: '{report_as}'\")\n\n    self.metadata.time_end = time_end\n    self.metadata.evaluation_duration = evaluation_duration\n\n    extra = {} if extra is None else extra\n\n    loss = float(loss) if loss is not None else None\n    cost = float(cost) if cost is not None else None\n    if learning_curve is not None:\n        learning_curve = [float(v) for v in learning_curve]\n\n    return Report(\n        trial_id=self.metadata.id,\n        reported_as=report_as,\n        evaluation_duration=evaluation_duration,\n        loss=loss,\n        cost=cost,\n        learning_curve=learning_curve,\n        extra=extra,\n        err=err,\n        tb=tb,\n    )\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_corrupted","title":"set_corrupted","text":"<pre><code>set_corrupted() -&gt; None\n</code></pre> <p>Set the trial as corrupted.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def set_corrupted(self) -&gt; None:\n    \"\"\"Set the trial as corrupted.\"\"\"\n    self.state = State.CORRUPTED\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_evaluating","title":"set_evaluating","text":"<pre><code>set_evaluating(\n    *, time_started: float, worker_id: int | str\n) -&gt; None\n</code></pre> <p>Set the trial as in progress.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def set_evaluating(self, *, time_started: float, worker_id: int | str) -&gt; None:\n    \"\"\"Set the trial as in progress.\"\"\"\n    self.metadata.time_started = time_started\n    self.metadata.evaluating_worker_id = str(worker_id)\n    self.state = State.EVALUATING\n</code></pre>"},{"location":"api/neps/state/trial/#neps.state.trial.Trial.set_submitted","title":"set_submitted","text":"<pre><code>set_submitted(*, time_submitted: float) -&gt; None\n</code></pre> <p>Set the trial as submitted.</p> Source code in <code>neps/state/trial.py</code> <pre><code>def set_submitted(self, *, time_submitted: float) -&gt; None:\n    \"\"\"Set the trial as submitted.\"\"\"\n    self.metadata.time_submitted = time_submitted\n    self.state = State.SUBMITTED\n</code></pre>"},{"location":"api/neps/status/status/","title":"Status","text":""},{"location":"api/neps/status/status/#neps.status.status","title":"neps.status.status","text":"<p>Functions to get the status of a run and save the status to CSV files.</p>"},{"location":"api/neps/status/status/#neps.status.status.get_summary_dict","title":"get_summary_dict","text":"<pre><code>get_summary_dict(\n    root_directory: str | Path, *, add_details: bool = False\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a dict that summarizes a run.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory given to neps.run.</p> <p> TYPE: <code>str | Path</code> </p> <code>add_details</code> <p>If true, add detailed dicts for previous_results, pending_configs, and pending_configs_free.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>summary_dict</code> <p>Information summarizing a run</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>neps/status/status.py</code> <pre><code>def get_summary_dict(\n    root_directory: str | Path,\n    *,\n    add_details: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Create a dict that summarizes a run.\n\n    Args:\n        root_directory: The root directory given to neps.run.\n        add_details: If true, add detailed dicts for previous_results, pending_configs,\n            and pending_configs_free.\n\n    Returns:\n        summary_dict: Information summarizing a run\n    \"\"\"\n    root_directory = Path(root_directory)\n\n    # NOTE: We don't lock the shared state since we are just reading and don't need to\n    # make decisions based on the state\n    shared_state = load_filebased_neps_state(root_directory)\n\n    trials = shared_state.get_all_trials()\n\n    evaluated: dict[ConfigID, _ConfigResultForStats] = {}\n\n    for trial in trials.values():\n        if trial.report is None:\n            continue\n\n        _result_for_stats = _ConfigResultForStats(\n            id=trial.id,\n            config=trial.config,\n            result=trial.report.to_deprecate_result_dict(),\n            metadata=asdict(trial.metadata),\n        )\n        evaluated[trial.id] = _result_for_stats\n\n    in_progress = {\n        trial.id: trial.config\n        for trial in trials.values()\n        if trial.State == Trial.State.EVALUATING\n    }\n    pending = {\n        trial.id: trial.config\n        for trial in trials.values()\n        if trial.State == Trial.State.PENDING\n    }\n\n    summary: dict[str, Any] = {}\n\n    if add_details:\n        summary[\"previous_results\"] = evaluated\n        summary[\"pending_configs\"] = {**in_progress, **pending}\n        summary[\"pending_configs_free\"] = pending\n\n    summary[\"num_evaluated_configs\"] = len(evaluated)\n    summary[\"num_pending_configs\"] = len(in_progress) + len(pending)\n    summary[\"num_pending_configs_with_worker\"] = len(in_progress)\n\n    summary[\"best_loss\"] = float(\"inf\")\n    summary[\"best_config_id\"] = None\n    summary[\"best_config_metadata\"] = None\n    summary[\"best_config\"] = None\n    summary[\"num_error\"] = 0\n    for evaluation in evaluated.values():\n        if evaluation.result == \"error\":\n            summary[\"num_error\"] += 1\n        loss = evaluation.loss\n        if isinstance(loss, float) and loss &lt; summary[\"best_loss\"]:\n            summary[\"best_loss\"] = loss\n            summary[\"best_config\"] = evaluation.config\n            summary[\"best_config_id\"] = evaluation.id\n            summary[\"best_config_metadata\"] = evaluation.metadata\n\n    return summary\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.post_run_csv","title":"post_run_csv","text":"<pre><code>post_run_csv(\n    root_directory: str | Path,\n) -&gt; tuple[Path, Path]\n</code></pre> <p>Create CSV files summarizing the run data.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory of the NePS run.</p> <p> TYPE: <code>str | Path</code> </p> RETURNS DESCRIPTION <code>tuple[Path, Path]</code> <p>The paths to the configuration data CSV and the run data CSV.</p> Source code in <code>neps/status/status.py</code> <pre><code>def post_run_csv(root_directory: str | Path) -&gt; tuple[Path, Path]:\n    \"\"\"Create CSV files summarizing the run data.\n\n    Args:\n        root_directory: The root directory of the NePS run.\n\n    Returns:\n        The paths to the configuration data CSV and the run data CSV.\n    \"\"\"\n    csv_config_data, csv_rundata, csv_locker = _initiate_summary_csv(root_directory)\n    csv_config_data = Path(csv_config_data).absolute().resolve()\n    csv_rundata = Path(csv_rundata).absolute().resolve()\n\n    df_config_data, df_run_data = _get_dataframes_from_summary(\n        root_directory,\n        include_metadatas=True,\n        include_results=True,\n        include_configs=True,\n    )\n\n    _save_data_to_csv(\n        csv_config_data,\n        csv_rundata,\n        csv_locker,\n        df_config_data,\n        df_run_data,\n    )\n    return csv_config_data, csv_rundata\n</code></pre>"},{"location":"api/neps/status/status/#neps.status.status.status","title":"status","text":"<pre><code>status(\n    root_directory: str | Path,\n    *,\n    best_losses: bool = False,\n    best_configs: bool = False,\n    all_configs: bool = False,\n    print_summary: bool = True\n) -&gt; tuple[\n    dict[str, _ConfigResultForStats], dict[str, SearchSpace]\n]\n</code></pre> <p>Print status information of a neps run and return results.</p> PARAMETER DESCRIPTION <code>root_directory</code> <p>The root directory given to neps.run.</p> <p> TYPE: <code>str | Path</code> </p> <code>best_losses</code> <p>If true, show the trajectory of the best loss across evaluations</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>best_configs</code> <p>If true, show the trajectory of the best configs and their losses across evaluations</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>all_configs</code> <p>If true, show all configs and their losses</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>print_summary</code> <p>If true, print a summary of the current run state</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>previous_results</code> <p>Already evaluated configurations and results. pending_configs: Configs that have been sampled, but have not finished evaluating</p> <p> TYPE: <code>tuple[dict[str, _ConfigResultForStats], dict[str, SearchSpace]]</code> </p> Source code in <code>neps/status/status.py</code> <pre><code>def status(\n    root_directory: str | Path,\n    *,\n    best_losses: bool = False,\n    best_configs: bool = False,\n    all_configs: bool = False,\n    print_summary: bool = True,\n) -&gt; tuple[dict[str, _ConfigResultForStats], dict[str, SearchSpace]]:\n    \"\"\"Print status information of a neps run and return results.\n\n    Args:\n        root_directory: The root directory given to neps.run.\n        best_losses: If true, show the trajectory of the best loss across evaluations\n        best_configs: If true, show the trajectory of the best configs and their losses\n            across evaluations\n        all_configs: If true, show all configs and their losses\n        print_summary: If true, print a summary of the current run state\n\n    Returns:\n        previous_results: Already evaluated configurations and results.\n        pending_configs: Configs that have been sampled, but have not finished evaluating\n    \"\"\"\n    root_directory = Path(root_directory)\n    summary = get_summary_dict(root_directory, add_details=True)\n\n    if print_summary:\n        print(f\"#Evaluated configs: {summary['num_evaluated_configs']}\")\n        print(f\"#Pending configs: {summary['num_pending_configs']}\")\n        print(\n            f\"#Pending configs with worker: {summary['num_pending_configs_with_worker']}\",\n        )\n\n        print(f\"#Crashed configs: {summary['num_error']}\")\n\n        if len(summary[\"previous_results\"]) == 0:\n            return summary[\"previous_results\"], summary[\"pending_configs\"]\n\n        print()\n        print(f\"Best loss: {summary['best_loss']}\")\n        print(f\"Best config id: {summary['best_config_id']}\")\n        print(f\"Best config: {summary['best_config']}\")\n\n        if best_losses:\n            print()\n            print(\"Best loss across evaluations:\")\n            best_loss_trajectory = root_directory / \"best_loss_trajectory.txt\"\n            print(best_loss_trajectory.read_text(encoding=\"utf-8\"))\n\n        if best_configs:\n            print()\n            print(\"Best configs and their losses across evaluations:\")\n            print(79 * \"-\")\n            best_loss_config = root_directory / \"best_loss_with_config_trajectory.txt\"\n            print(best_loss_config.read_text(encoding=\"utf-8\"))\n\n        if all_configs:\n            print()\n            print(\"All evaluated configs and their losses:\")\n            print(79 * \"-\")\n            all_loss_config = root_directory / \"all_losses_and_configs.txt\"\n            print(all_loss_config.read_text(encoding=\"utf-8\"))\n\n    return summary[\"previous_results\"], summary[\"pending_configs\"]\n</code></pre>"},{"location":"api/neps/utils/cli/","title":"Cli","text":""},{"location":"api/neps/utils/cli/#neps.utils.cli","title":"neps.utils.cli","text":"<p>This module provides a command-line interface (CLI) for NePS.</p>"},{"location":"api/neps/utils/cli/#neps.utils.cli.compute_duration","title":"compute_duration","text":"<pre><code>compute_duration(start_time: float) -&gt; str\n</code></pre> <p>Compute duration from start_time to current time.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def compute_duration(start_time: float) -&gt; str:\n    \"\"\"Compute duration from start_time to current time.\"\"\"\n    return format_duration(datetime.now().timestamp() - start_time)\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.convert_timestamp","title":"convert_timestamp","text":"<pre><code>convert_timestamp(timestamp: float) -&gt; str\n</code></pre> <p>Convert a UNIX timestamp to a human-readable datetime string.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def convert_timestamp(timestamp: float) -&gt; str:\n    \"\"\"Convert a UNIX timestamp to a human-readable datetime string.\"\"\"\n    return datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.format_duration","title":"format_duration","text":"<pre><code>format_duration(seconds: float) -&gt; str\n</code></pre> <p>Convert duration in seconds to a h:min:sec format.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def format_duration(seconds: float) -&gt; str:\n    \"\"\"Convert duration in seconds to a h:min:sec format.\"\"\"\n    duration = str(timedelta(seconds=seconds))\n    # Remove milliseconds for alignment\n    if \".\" in duration:\n        duration = duration.split(\".\")[0]\n    return duration\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.get_root_directory","title":"get_root_directory","text":"<pre><code>get_root_directory(args: Namespace) -&gt; Path\n</code></pre> <p>Load the root directory from the provided argument or from the config.yaml file.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def get_root_directory(args: argparse.Namespace) -&gt; Path:\n    \"\"\"Load the root directory from the provided argument or from the config.yaml file.\"\"\"\n    if args.root_directory:\n        return Path(args.root_directory)\n\n    config_path = Path(\"run_config.yaml\")\n    if config_path.exists():\n        with config_path.open(\"r\") as file:\n            config = yaml.safe_load(file)\n        root_directory = config.get(\"root_directory\")\n        if root_directory:\n            return Path(root_directory)\n        else:\n            raise ValueError(\n                \"The config.yaml file exists but does not contain 'root_directory'.\"\n            )\n    else:\n        raise ValueError(\n            \"Either the root_directory must be provided as an argument or config.yaml \"\n            \"must exist with a 'root_directory' key.\"\n        )\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.info_config","title":"info_config","text":"<pre><code>info_config(args: Namespace) -&gt; None\n</code></pre> <p>Handles the info-config command by providing information based on directory and id.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def info_config(args: argparse.Namespace) -&gt; None:\n    \"\"\"Handles the info-config command by providing information based on directory\n    and id.\"\"\"\n    directory_path = get_root_directory(args)\n    config_id = args.id\n\n    if not directory_path.exists() or not directory_path.is_dir():\n        print(\n            f\"Error: The directory {directory_path} does not exist or is not a \"\n            f\"directory.\"\n        )\n        return\n    try:\n        neps_state = load_filebased_neps_state(directory_path)\n    except VersionedResourceDoesNotExistsError:\n        print(f\"No NePS state found in the directory {directory_path}.\")\n        return\n    try:\n        trial = neps_state.get_trial_by_id(config_id)\n    except TrialNotFoundError:\n        print(f\"No trial found with ID {config_id}.\")\n        return\n\n    print(\"Trial Information:\")\n    print(f\"  Trial ID: {trial.metadata.id}\")\n    print(f\"  State: {trial.state}\")\n    print(f\"  Configurations:\")\n    for key, value in trial.config.items():\n        print(f\"    {key}: {value}\")\n\n    print(\"\\nMetadata:\")\n    print(f\"  Location: {trial.metadata.location}\")\n    print(f\"  Previous Trial ID: {trial.metadata.previous_trial_id}\")\n    print(f\"  Sampling Worker ID: {trial.metadata.sampling_worker_id}\")\n    print(f\"  Time Sampled: {convert_timestamp(trial.metadata.time_sampled)}\")\n    print(f\"  Evaluating Worker ID: {trial.metadata.evaluating_worker_id}\")\n    print(f\"  Evaluation Duration: {format_duration(trial.metadata.evaluation_duration)}\")\n    print(f\"  Time Started: {convert_timestamp(trial.metadata.time_started)}\")\n    print(f\"  Time End: {convert_timestamp(trial.metadata.time_end)}\")\n\n    if trial.report is not None:\n        print(\"\\nReport:\")\n        print(f\"  Loss: {trial.report.loss}\")\n        print(f\"  Cost: {trial.report.cost}\")\n        print(f\"  Reported As: {trial.report.reported_as}\")\n    else:\n        print(\"No report available.\")\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.init_config","title":"init_config","text":"<pre><code>init_config(args: Namespace) -&gt; None\n</code></pre> <p>Creates a 'run_args' configuration YAML file template if it does not already exist.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def init_config(args: argparse.Namespace) -&gt; None:\n    \"\"\"Creates a 'run_args' configuration YAML file template if it does not already\n    exist.\n    \"\"\"\n    config_path = Path(args.config_path) if args.config_path else Path(\"run_config.yaml\")\n    if not config_path.exists():\n        with config_path.open(\"w\") as file:\n            template = args.template if args.template else \"basic\"\n            if template == \"basic\":\n                file.write(\n                    \"\"\"# Add your NEPS configuration settings here\n\nrun_pipeline:\n  path: \"path/to/your/run_pipeline.py\"\n  name: name_of_your_pipeline_function\n\npipeline_space:\n  float_parameter_name:\n    type: \"float\"\n    lower:\n    upper:\n    log: false\n  int_parameter_name:\n    type: \"int\"\n    lower:\n    upper:\n  categorical_parameter_name:\n    choices: [\"choice1\", \"choice2\", \"choice3\"]\n  constant_parameter_name: 17\n\nroot_directory: \"set/path/for/root_directory\"\nmax_evaluations_total:\noverwrite_working_directory:\n\"\"\"\n                )\n            elif template == \"complete\":\n                file.write(\n                    \"\"\"# Full Configuration Template for NePS\n\nrun_pipeline:\n  path: path/to/your/run_pipeline.py  # Path to the function file\n  name: example_pipeline              # Function name within the file\n\npipeline_space:\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true\n  epochs:\n    lower: 5\n    upper: 20\n    is_fidelity: true\n  optimizer:\n    choices: [adam, sgd, adamw]\n  batch_size: 64\n\nroot_directory: path/to/results       # Directory for result storage\nmax_evaluations_total: 20             # Budget\nmax_cost_total:\n\n# Debug and Monitoring\noverwrite_working_directory: false\npost_run_summary: true\ndevelopment_stage_id:\ntask_id:\n\n# Parallelization Setup\nmax_evaluations_per_run:\ncontinue_until_max_evaluation_completed: true\n\n# Error Handling\nloss_value_on_error:\ncost_value_on_error:\nignore_errors:\n\n# Customization Options\nsearcher: hyperband       # Internal key to select a NePS optimizer.\n\n# Hooks\npre_load_hooks:\n\"\"\"\n                )\n    elif args.state_machine:\n        pass\n        # create_or_load_filebased_neps_state()\n    else:\n        print(f\"Path {config_path} does already exist.\")\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.load_neps_errors","title":"load_neps_errors","text":"<pre><code>load_neps_errors(args: Namespace) -&gt; None\n</code></pre> <p>Handles the 'errors' command by loading errors from the neps_state.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def load_neps_errors(args: argparse.Namespace) -&gt; None:\n    \"\"\"Handles the 'errors' command by loading errors from the neps_state.\"\"\"\n    directory_path = get_root_directory(args)\n\n    if not directory_path.exists() or not directory_path.is_dir():\n        print(\n            f\"Error: The directory {directory_path} does not exist or is not a \"\n            f\"directory.\"\n        )\n        return\n\n    try:\n        neps_state = load_filebased_neps_state(directory_path)\n    except VersionedResourceDoesNotExistsError:\n        print(f\"No NePS state found in the directory {directory_path}.\")\n        return\n    errors = neps_state.get_errors()\n\n    if not errors.errs:\n        print(\"No errors found.\")\n        return\n\n    # Print out the errors in a human-readable format\n    print(f\"Loaded Errors from directory: {directory_path}\\n\")\n\n    for error in errors.errs:\n        print(f\"Error in Trial ID: {error.trial_id}\")\n        print(f\"  Worker ID: {error.worker_id}\")\n        print(f\"  Error Type: {error.err_type}\")\n        print(f\"  Error Message: {error.err}\")\n        print(f\"  Traceback:\")\n        print(f\"{error.tb}\")\n        print(\"\\n\" + \"-\" * 50 + \"\\n\")\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>CLI entry point.</p> <p>This function sets up the command-line interface (CLI) for NePS using argparse. It defines the available subcommands and their respective arguments.</p> Available commands <ul> <li>init: Generates a 'run_args' YAML template file.</li> <li>run: Runs the optimization with specified configuration.</li> </ul> Source code in <code>neps/utils/cli.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"CLI entry point.\n\n    This function sets up the command-line interface (CLI) for NePS using argparse.\n    It defines the available subcommands and their respective arguments.\n\n    Available commands:\n        - init: Generates a 'run_args' YAML template file.\n        - run: Runs the optimization with specified configuration.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"NePS Command Line Interface\")\n    subparsers = parser.add_subparsers(\n        dest=\"command\", help=\"Available commands: init, run\"\n    )\n\n    # Subparser for \"init\" command\n    parser_init = subparsers.add_parser(\"init\", help=\"Generate 'run_args' \" \"YAML file\")\n    parser_init.add_argument(\n        \"--config-path\",\n        type=str,\n        default=None,\n        help=\"Optional custom path for generating the configuration file. \"\n        \"Default is 'run_config.yaml'.\",\n    )\n    parser_init.add_argument(\n        \"--template\",\n        type=str,\n        choices=[\"basic\", \"complete\"],\n        default=\"basic\",\n        help=\"Optional, options between different templates. Required configs(basic) vs \"\n        \"all neps configs (complete)\",\n    )\n    parser_init.add_argument(\n        \"--state-machine\",\n        action=\"store_true\",\n        help=\"If set, creates a NEPS state. Requires an existing config.yaml.\",\n    )\n    parser_init.set_defaults(func=init_config)\n\n    # Subparser for \"run\" command\n    parser_run = subparsers.add_parser(\"run\", help=\"Run a neural pipeline search.\")\n    # Adding arguments to the 'run' subparser with defaults\n    parser_run.add_argument(\n        \"--run-args\",\n        type=str,\n        help=\"Path to the YAML configuration file.\",\n        default=Default(None),\n    )\n    parser_run.add_argument(\n        \"--run-pipeline\",\n        type=str,\n        help=\"Optional: Provide the path to a Python file and a function name separated \"\n        \"by a colon, e.g., 'path/to/module.py:function_name'. \"\n        \"If provided, it overrides the run_pipeline setting from the YAML \"\n        \"configuration.\",\n        default=Default(None),\n    )\n\n    parser_run.add_argument(\n        \"--pipeline-space\",\n        type=str,\n        default=Default(None),\n        help=\"Path to the YAML file defining the search space for the optimization. \"\n        \"This can be provided here or defined within the 'run_args' YAML file. \"\n        \"(default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--root-directory\",\n        type=str,\n        default=Default(None),\n        help=\"The directory to save progress to. This is also used to synchronize \"\n        \"multiple calls for parallelization. (default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--overwrite-working-directory\",\n        action=\"store_true\",\n        default=Default(False),  # noqa: FBT003\n        help=\"If set, deletes the working directory at the start of the run. \"\n        \"This is useful, for example, when debugging a run_pipeline function. \"\n        \"(default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--development-stage-id\",\n        type=str,\n        default=Default(None),\n        help=\"Identifier for the current development stage, used in multi-stage \"\n        \"projects. (default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--task-id\",\n        type=str,\n        default=Default(None),\n        help=\"Identifier for the current task, useful in projects with multiple tasks. \"\n        \"(default: %(default)s)\",\n    )\n    # Create a mutually exclusive group for post-run summary flags\n    summary_group = parser_run.add_mutually_exclusive_group(required=False)\n    summary_group.add_argument(\n        \"--post-run-summary\",\n        action=\"store_true\",\n        default=Default(True),  # noqa: FBT003\n        help=\"Provide a summary of the results after running. (default: %(default)s)\",\n    )\n    summary_group.add_argument(\n        \"--no-post-run-summary\",\n        action=\"store_false\",\n        dest=\"post_run_summary\",\n        help=\"Do not provide a summary of the results after running.\",\n    )\n    parser_run.add_argument(\n        \"--max-evaluations-total\",\n        type=int,\n        default=Default(None),\n        help=\"Total number of evaluations to run. (default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--max-evaluations-per-run\",\n        type=int,\n        default=Default(None),\n        help=\"Number of evaluations a specific call should maximally do. \"\n        \"(default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--continue-until-max-evaluation-completed\",\n        action=\"store_true\",\n        default=Default(False),  # noqa: FBT003\n        help=\"If set, only stop after max-evaluations-total have been completed. This \"\n        \"is only relevant in the parallel setting. (default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--max-cost-total\",\n        type=float,\n        default=Default(None),\n        help=\"No new evaluations will start when this cost is exceeded. Requires \"\n        \"returning a cost in the run_pipeline function, e.g., `return dict(\"\n        \"loss=loss, cost=cost)`. (default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--ignore-errors\",\n        action=\"store_true\",\n        default=Default(False),  # noqa: FBT003\n        help=\"If set, ignore errors during the optimization process. (default: %(\"\n        \"default)s)\",\n    )\n    parser_run.add_argument(\n        \"--loss-value-on-error\",\n        type=float,\n        default=Default(None),\n        help=\"Loss value to assume on error. (default: %(default)s)\",\n    )\n    parser_run.add_argument(\n        \"--cost-value-on-error\",\n        type=float,\n        default=Default(None),\n        help=\"Cost value to assume on error. (default: %(default)s)\",\n    )\n\n    parser_run.add_argument(\n        \"--searcher\",\n        type=str,\n        default=Default(\"default\"),\n        help=\"String key of searcher algorithm to use for optimization. (default: %(\"\n        \"default)s)\",\n    )\n\n    parser_run.add_argument(\n        \"--searcher-kwargs\",\n        type=str,\n        nargs=\"+\",\n        help=\"Additional keyword arguments as key=value pairs for the searcher.\",\n    )\n\n    parser_run.set_defaults(func=run_optimization)\n\n    # Subparser for \"info-config\" command\n    parser_info_config = subparsers.add_parser(\n        \"info-config\", help=\"Provides information about \" \"specific config.\"\n    )\n    parser_info_config.add_argument(\n        \"id\", type=str, help=\"The configuration ID to be used.\"\n    )\n    parser_info_config.add_argument(\n        \"--root-directory\",\n        type=str,\n        help=\"Optional: The path to your root_directory. If not provided, \"\n        \"it will be loaded from run_config.yaml.\",\n    )\n    parser_info_config.set_defaults(func=info_config)\n\n    # Subparser for \"errors\" command\n    parser_errors = subparsers.add_parser(\"errors\", help=\"List all errors.\")\n    parser_errors.add_argument(\n        \"--root-directory\",\n        type=str,\n        help=\"Optional: The path to your \"\n        \"root_directory. If not provided, it will be \"\n        \"loaded from run_config.yaml.\",\n    )\n    parser_errors.set_defaults(func=load_neps_errors)\n\n    # Subparser for \"sample-config\" command\n    parser_sample_config = subparsers.add_parser(\n        \"sample-config\", help=\"Sample a configuration from existing neps state.\"\n    )\n    parser_sample_config.add_argument(\n        \"--root-directory\",\n        type=str,\n        help=\"Optional: The path to your root_directory. If not provided, \"\n        \"it will be loaded from run_config.yaml.\",\n    )\n    parser_sample_config.set_defaults(func=sample_config)\n\n    # Subparser for \"status\" command\n    parser_status = subparsers.add_parser(\n        \"status\", help=\"Check the status of the NePS run.\"\n    )\n    parser_status.add_argument(\n        \"--root-directory\",\n        type=str,\n        help=\"Optional: The path to your root_directory. If not provided, \"\n        \"it will be loaded from run_config.yaml.\",\n    )\n    parser_status.add_argument(\n        \"--pending\", action=\"store_true\", help=\"Show only pending trials.\"\n    )\n    parser_status.add_argument(\n        \"--evaluating\", action=\"store_true\", help=\"Show only evaluating trials.\"\n    )\n    parser_status.add_argument(\n        \"--succeeded\", action=\"store_true\", help=\"Show only succeeded trials.\"\n    )\n    parser_status.set_defaults(func=status)\n\n    # Subparser for \"results\" command\n    parser_results = subparsers.add_parser(\n        \"results\", help=\"Display results of the NePS run.\"\n    )\n    parser_results.add_argument(\n        \"--root-directory\",\n        type=str,\n        help=\"Optional: The path to your root_directory. If not provided, \"\n        \"it will be loaded from run_config.yaml.\",\n    )\n    parser_results.add_argument(\n        \"--plot\", action=\"store_true\", help=\"Plot the results if set.\"\n    )\n    parser_results.set_defaults(func=results)\n\n    # Subparser for \"help\" command\n    parser_help = subparsers.add_parser(\"help\", help=\"Displays help information.\")\n    parser_help.set_defaults(func=print_help)\n\n    # updating documentation\n    generate_markdown_from_parser(parser, \"cli.md\")\n    args = parser.parse_args()\n\n    if hasattr(args, \"func\"):\n        args.func(args)\n    else:\n        parser.print_help()\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.parse_kv_pairs","title":"parse_kv_pairs","text":"<pre><code>parse_kv_pairs(kv_list: list[str]) -&gt; dict\n</code></pre> <p>Parse a list of key=value strings into a dictionary with appropriate types.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def parse_kv_pairs(kv_list: list[str]) -&gt; dict:\n    \"\"\"Parse a list of key=value strings into a dictionary with appropriate types.\"\"\"\n\n    def convert_value(value: str) -&gt; int | float | str:\n        \"\"\"Convert the value to the appropriate type.\"\"\"\n        # Check for boolean\n        if value.lower() in (\"true\", \"false\"):\n            return value.lower() == \"true\"\n\n        # Check for float if value contains '.' or 'e'\n        if \".\" in value or \"e\" in value.lower():\n            try:\n                return float(value)\n            except ValueError:\n                return value  # Return as string if conversion fails\n\n        # Check for integer\n        try:\n            return int(value)\n        except ValueError:\n            return value  # Return as string if conversion fails\n\n    result = {}\n    for item in kv_list:\n        if \"=\" in item:\n            key, value = item.split(\"=\", 1)\n            result[key] = convert_value(value)\n        else:\n            raise ValueError(\"Each kwarg must be in key=value format.\")\n    return result\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.plot_incumbents","title":"plot_incumbents","text":"<pre><code>plot_incumbents(\n    all_trials: List[Trial],\n    incumbents: List[Trial],\n    directory_path: Path,\n) -&gt; str\n</code></pre> <p>Plot the evolution of incumbent trials over the total number of trials.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def plot_incumbents(\n    all_trials: List[Trial], incumbents: List[Trial], directory_path: Path\n) -&gt; str:\n    \"\"\"Plot the evolution of incumbent trials over the total number of trials.\"\"\"\n    id_to_index = {trial.id: idx + 1 for idx, trial in enumerate(all_trials)}\n\n    # Collect data for plotting\n    x_values = [id_to_index[incumbent.id] for incumbent in incumbents]\n    y_values = [\n        incumbent.report.loss\n        for incumbent in incumbents\n        if incumbent.report is not None and incumbent.report.loss is not None\n    ]\n\n    plt.figure(figsize=(12, 6))\n    sns.lineplot(\n        x=x_values,\n        y=y_values,\n        marker=\"o\",\n        linestyle=\"-\",\n        markersize=8,\n        color=\"dodgerblue\",\n    )\n    plt.xlabel(\"Number of Trials\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Evolution of Incumbents Over Trials\")\n\n    # Dynamically set x-axis ticks based on the number of trials\n    num_trials = len(all_trials)\n    if num_trials &lt; 20:\n        tick_spacing = 1  # Every trial is labeled if fewer than 20 trials\n    else:\n        tick_spacing = max(\n            5, round(num_trials / 10 / 5) * 5\n        )  # Round to nearest multiple of 5\n\n    ticks = np.arange(0, num_trials + 1, tick_spacing)\n    ticks[0] = 1\n    plt.xticks(ticks)\n\n    sns.set_style(\"whitegrid\")\n    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n    plt.tight_layout()\n\n    # Save the figure\n    plot_file_name = \"incumbents_evolution.png\"\n    plot_path = os.path.join(directory_path, plot_file_name)\n    plt.savefig(plot_path)\n    plt.close()\n\n    return plot_path\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.print_help","title":"print_help","text":"<pre><code>print_help(args: Optional[Namespace] = None) -&gt; None\n</code></pre> <p>Prints help information for the NEPS CLI.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def print_help(args: Optional[argparse.Namespace] = None) -&gt; None:\n    \"\"\"Prints help information for the NEPS CLI.\"\"\"\n    help_text = \"\"\"\nUsage: neps [COMMAND] [OPTIONS]\n\nAvailable Commands:\n-------------------\n\nneps init [OPTIONS]\n    Generates a 'run_args' YAML template file.\n    Options:\n    --config-path &lt;path/to/config.yaml&gt; (Optional: Specify the path for the config\n    file. Default is run_config.yaml)\n    --template [basic|complete] (Optional: Choose between a basic or complete template.)\n    --state-machine (Optional: Creates a NEPS state. Requires an existing config.yaml.)\n\nneps run [OPTIONS]\n    Runs a neural pipeline search.\n    Options:\n    --run-args &lt;path_to_run_args&gt; (Path to the YAML configuration file.)\n    --run-pipeline &lt;path_to_module:function_name&gt; (Path and function for the pipeline.)\n    --pipeline-space &lt;path_to_yaml&gt; (Path to the YAML defining the search space.)\n    --root-directory &lt;path&gt; (Optional: Directory for saving progress and\n    synchronization. Default is 'root_directory' from run_config.yaml if not provided.)\n    --overwrite-working-directory (Deletes the working directory at the start of the run.)\n    --development-stage-id &lt;id&gt; (Identifier for the development stage.)\n    --task-id &lt;id&gt; (Identifier for the task.)\n    --post-run-summary/--no-post-run-summary (Toggle summary after running.)\n    --max-evaluations-total &lt;int&gt; (Total number of evaluations to run.)\n    --max-evaluations-per-run &lt;int&gt; (Max evaluations per run call.)\n    --continue-until-max-evaluation-completed (Continue until max evaluations are completed.)\n    --max-cost-total &lt;float&gt; (Max cost before halting new evaluations.)\n    --ignore-errors (Ignore errors during optimization.)\n    --loss-value-on-error &lt;float&gt; (Assumed loss value on error.)\n    --cost-value-on-error &lt;float&gt; (Assumed cost value on error.)\n    --searcher &lt;key&gt; (Searcher algorithm key for optimization.)\n    --searcher-kwargs &lt;key=value&gt;... (Additional kwargs for the searcher.)\n\nneps info-config &lt;id&gt; [OPTIONS]\n    Provides detailed information about a specific configuration by its ID.\n    Options:\n    --root-directory &lt;path&gt; (Optional: Path to your root_directory. Default is\n    'root_directory' from run_config.yaml if not provided.)\n\nneps errors [OPTIONS]\n    Lists all errors from the specified NePS run.\n    Options:\n    --root-directory &lt;path&gt; (Optional: Path to your root_directory. Default is\n    'root_directory' from run_config.yaml if not provided.)\n\nneps sample-config [OPTIONS]\n    Sample a configuration from the existing NePS state.\n    Options:\n    --root-directory &lt;path&gt; (Optional: Path to your root_directory. Default is\n    'root_directory' from run_config.yaml if not provided.)\n\nneps status [OPTIONS]\n    Check the status of the NePS run.\n    Options:\n    --root-directory &lt;path&gt; (Optional: Path to your root_directory. Default is\n    'root_directory' from run_config.yaml if not provided.)\n    --pending (Show only pending trials.)\n    --evaluating (Show only evaluating trials.)\n    --succeeded (Show only succeeded trials.)\n\nneps results [OPTIONS]\n    Display results of the NePS run.\n    Options:\n    --root-directory &lt;path&gt; (Optional: Path to your root_directory. Defaults is\n    'root_directory' from run_config.yaml if not provided.)\n    --plot (Plot the results if set.)\n\nneps help\n    Displays this help message.\n    \"\"\"\n    print(help_text)\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.results","title":"results","text":"<pre><code>results(args: Namespace) -&gt; None\n</code></pre> <p>Handles the 'results' command by displaying incumbents in reverse order and optionally plotting and saving the results.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def results(args: argparse.Namespace) -&gt; None:\n    \"\"\"Handles the 'results' command by displaying incumbents in\n    reverse order and\n    optionally plotting and saving the results.\"\"\"\n    directory_path = get_root_directory(args)\n\n    if not directory_path.exists() or not directory_path.is_dir():\n        print(\n            f\"Error: The directory {directory_path} does not exist or is not a \"\n            f\"directory.\"\n        )\n        return\n\n    try:\n        neps_state = load_filebased_neps_state(directory_path)\n    except VersionedResourceDoesNotExistsError:\n        print(f\"No NePS state found in the directory {directory_path}.\")\n        return\n\n    trials = neps_state.get_all_trials()\n    # Sort trials by trial ID\n    sorted_trials = sorted(trials.values(), key=lambda x: int(x.id))\n\n    # Compute incumbents\n    best_loss = float(\"inf\")\n    incumbents = []\n    for trial in sorted_trials:\n        if trial.report and trial.report.loss &lt; best_loss:\n            best_loss = trial.report.loss\n            incumbents.append(trial)\n\n    # Reverse the list for displaying, so the most recent incumbent is shown first\n    incumbents_display = incumbents[::-1]\n\n    if not args.plot:\n        print(f\"Results for NePS run: {directory_path}\")\n        print(\"--------------------\")\n        print(\"All Incumbent Trials:\")\n        header = f\"{'ID':&lt;6} {'Loss':&lt;12} {'Config':&lt;60}\"\n        print(header)\n        print(\"-\" * len(header))\n        if len(incumbents_display) &gt; 0:\n            for trial in incumbents_display:\n                if trial.report is not None and trial.report.loss is not None:\n                    config = \", \".join(f\"{k}: {v}\" for k, v in trial.config.items())\n                    print(f\"{trial.id:&lt;6} {trial.report.loss:&lt;12.6f} {config:&lt;60}\")\n                else:\n                    print(f\"Trial {trial.id} has no valid loss.\")\n        else:\n            print(\"No Incumbent Trials found.\")\n    else:\n        plot_path = plot_incumbents(sorted_trials, incumbents, directory_path)\n        print(f\"Plot saved to {plot_path}\")\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.run_optimization","title":"run_optimization","text":"<pre><code>run_optimization(args: Namespace) -&gt; None\n</code></pre> <p>Collects arguments from the parser and runs the NePS optimization. Args: args (argparse.Namespace): Parsed command-line arguments.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def run_optimization(args: argparse.Namespace) -&gt; None:\n    \"\"\"Collects arguments from the parser and runs the NePS optimization.\n    Args: args (argparse.Namespace): Parsed command-line arguments.\n    \"\"\"\n    if not isinstance(args.run_pipeline, Default):\n        print(\"fehler\")\n        module_path, function_name = args.run_pipeline.split(\":\")\n        run_pipeline = load_and_return_object(module_path, function_name, \"run_pipeline\")\n\n    else:\n        run_pipeline = args.run_pipeline\n\n    kwargs = {}\n    if args.searcher_kwargs:\n        kwargs = parse_kv_pairs(args.searcher_kwargs)  # convert kwargs\n\n    # Collect arguments from args and prepare them for neps.run\n    options = {\n        \"run_args\": args.run_args,\n        \"run_pipeline\": run_pipeline,\n        \"pipeline_space\": args.pipeline_space,\n        \"root_directory\": args.root_directory,\n        \"overwrite_working_directory\": args.overwrite_working_directory,\n        \"post_run_summary\": args.post_run_summary,\n        \"development_stage_id\": args.development_stage_id,\n        \"task_id\": args.task_id,\n        \"max_evaluations_total\": args.max_evaluations_total,\n        \"max_evaluations_per_run\": args.max_evaluations_per_run,\n        \"continue_until_max_evaluation_completed\": (\n            args.continue_until_max_evaluation_completed\n        ),\n        \"max_cost_total\": args.max_cost_total,\n        \"ignore_errors\": args.ignore_errors,\n        \"loss_value_on_error\": args.loss_value_on_error,\n        \"cost_value_on_error\": args.cost_value_on_error,\n        \"searcher\": args.searcher,\n        **kwargs,\n    }\n    logging.basicConfig(level=logging.INFO)\n    neps.run(**options)\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.sample_config","title":"sample_config","text":"<pre><code>sample_config(args: Namespace) -&gt; None\n</code></pre> <p>Handles the sample-config command</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def sample_config(args: argparse.Namespace) -&gt; None:\n    \"\"\"Handles the sample-config command\"\"\"\n    # Get the root_directory from args or load it from run_config.yaml\n    directory_path = get_root_directory(args)\n    neps_state = load_filebased_neps_state(directory_path)\n\n    # Placeholder for the logic that will be implemented\n    pass\n</code></pre>"},{"location":"api/neps/utils/cli/#neps.utils.cli.status","title":"status","text":"<pre><code>status(args: Namespace) -&gt; None\n</code></pre> <p>Handles the status command, providing a summary of the NEPS run.</p> Source code in <code>neps/utils/cli.py</code> <pre><code>def status(args: argparse.Namespace) -&gt; None:\n    \"\"\"Handles the status command, providing a summary of the NEPS run.\"\"\"\n    # Get the root_directory from args or load it from run_config.yaml\n    directory_path = get_root_directory(args)\n\n    if not directory_path.exists() or not directory_path.is_dir():\n        print(\n            f\"Error: The directory {directory_path} does not exist or is not a \"\n            f\"directory.\"\n        )\n        return\n\n    try:\n        neps_state = load_filebased_neps_state(directory_path)\n    except VersionedResourceDoesNotExistsError:\n        print(f\"No NePS state found in the directory {directory_path}.\")\n        return\n\n    summary = get_summary_dict(directory_path, add_details=True)\n\n    # Calculate the number of trials in different states\n    evaluating_trials_count = sum(\n        1\n        for trial in neps_state.get_all_trials().values()\n        if trial.state.name == \"EVALUATING\"\n    )\n    pending_trials_count = summary[\"num_pending_configs\"]\n    succeeded_trials_count = summary[\"num_evaluated_configs\"] - summary[\"num_error\"]\n    failed_trials_count = summary[\"num_error\"]\n    pending_with_worker_count = summary[\"num_pending_configs_with_worker\"]\n\n    # Print summary\n    print(\"NePS Status:\")\n    print(\"-----------------------------\")\n    print(f\"Optimizer: {neps_state.optimizer_info().info['searcher_alg']}\")\n    print(f\"Succeeded Trials: {succeeded_trials_count}\")\n    print(f\"Failed Trials (Errors): {failed_trials_count}\")\n    print(f\"Active Trials: {evaluating_trials_count}\")\n    print(f\"Pending Trials: {pending_trials_count}\")\n    print(f\"Pending Trials with Worker: {pending_with_worker_count}\")\n    print(f\"Best Loss Achieved: {summary['best_loss']}\")\n\n    print(\"\\nLatest Trials:\")\n    print(\"-----------------------------\")\n\n    # Retrieve and sort the trials by time_sampled\n    all_trials = neps_state.get_all_trials()\n    sorted_trials = sorted(\n        all_trials.values(), key=lambda t: t.metadata.time_sampled, reverse=True\n    )\n\n    # Filter trials based on state\n    if args.pending:\n        filtered_trials = [\n            trial for trial in sorted_trials if trial.state.name == \"PENDING\"\n        ]\n    elif args.evaluating:\n        filtered_trials = [\n            trial for trial in sorted_trials if trial.state.name == \"EVALUATING\"\n        ]\n    elif args.succeeded:\n        filtered_trials = [\n            trial for trial in sorted_trials if trial.state.name == \"SUCCESS\"\n        ]\n    else:\n        filtered_trials = sorted_trials[:7]\n\n    # Define column headers with fixed width\n    header_format = \"{:&lt;20} {:&lt;10} {:&lt;10} {:&lt;40} {:&lt;12} {:&lt;10}\"\n    row_format = \"{:&lt;20} {:&lt;10} {:&lt;10} {:&lt;40} {:&lt;12} {:&lt;10}\"\n\n    # Print the header\n    print(\n        header_format.format(\n            \"Sampled Time\", \"Duration\", \"Trial ID\", \"Worker ID\", \"State\", \"Loss\"\n        )\n    )\n\n    # Print the details of the filtered trials\n    for trial in filtered_trials:\n        time_sampled = convert_timestamp(trial.metadata.time_sampled)\n        if trial.state.name in [\"PENDING\", \"EVALUATING\"]:\n            duration = compute_duration(trial.metadata.time_sampled)\n        else:\n            duration = (\n                format_duration(trial.metadata.evaluation_duration)\n                if trial.metadata.evaluation_duration\n                else \"N/A\"\n            )\n        trial_id = trial.id\n        worker_id = trial.metadata.sampling_worker_id\n        state = trial.state.name\n        loss = (\n            f\"{trial.report.loss:.6f}\"\n            if (trial.report and trial.report.loss is not None)\n            else \"N/A\"\n        )\n\n        print(row_format.format(time_sampled, duration, trial_id, worker_id, state, loss))\n\n    # If no specific filter is applied, print the best trial and optimizer info\n    if not args.pending and not args.evaluating and not args.succeeded:\n        if summary[\"best_config_id\"] is not None:\n            print(\"\\nBest Trial:\")\n            print(\"-----------------------------\")\n            print(f\"ID: {summary['best_config_id']}\")\n            print(f\"Loss: {summary['best_loss']}\")\n            print(\"Config:\")\n            for key, value in summary[\"best_config\"].items():\n                print(f\"  {key}: {value}\")\n\n            print(\n                f\"\\nMore details: neps info-config {summary['best_config_id']} \"\n                f\"(use --root-directory if not using run_config.yaml)\"\n            )\n        else:\n            print(\"\\nBest Trial:\")\n            print(\"-----------------------------\")\n            print(\"\\nNo successful trial found.\")\n\n        # Display optimizer information\n        optimizer_info = neps_state.optimizer_info().info\n        searcher_name = optimizer_info.get(\"searcher_name\", \"N/A\")\n        searcher_alg = optimizer_info.get(\"searcher_alg\", \"N/A\")\n        searcher_args = optimizer_info.get(\"searcher_args\", {})\n\n        print(\"\\nOptimizer Information:\")\n        print(\"-----------------------------\")\n        print(f\"Name: {searcher_name}\")\n        print(f\"Algorithm: {searcher_alg}\")\n        print(\"Parameter:\")\n        for arg, value in searcher_args.items():\n            print(f\"  {arg}: {value}\")\n\n        print(\"-----------------------------\")\n</code></pre>"},{"location":"api/neps/utils/common/","title":"Common","text":""},{"location":"api/neps/utils/common/#neps.utils.common","title":"neps.utils.common","text":"<p>Common utility functions used across the library.</p>"},{"location":"api/neps/utils/common/#neps.utils.common.get_initial_directory","title":"get_initial_directory","text":"<pre><code>get_initial_directory(\n    pipeline_directory: Path | str | None = None,\n) -&gt; Path\n</code></pre> <p>Find the initial directory based on its existence and the presence of the \"previous_config.id\" file.</p> PARAMETER DESCRIPTION <code>pipeline_directory</code> <p>The current config directory.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>The initial directory.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def get_initial_directory(pipeline_directory: Path | str | None = None) -&gt; Path:\n    \"\"\"Find the initial directory based on its existence and the presence of\n    the \"previous_config.id\" file.\n\n    Args:\n        pipeline_directory: The current config directory.\n\n    Returns:\n        The initial directory.\n    \"\"\"\n    neps_state = get_workers_neps_state()\n    if pipeline_directory is not None:\n        # TODO: Hard coded assumption\n        config_id = Path(pipeline_directory).name.split(\"_\", maxsplit=1)[-1]\n        trial = neps_state.get_trial_by_id(config_id)\n    else:\n        trial = get_in_progress_trial()\n\n    if trial.metadata.id in _INTIAL_DIRECTORY_CACHE:\n        return _INTIAL_DIRECTORY_CACHE[trial.metadata.id]\n\n    # Recursively find the initial directory\n    while (prev_trial_id := trial.metadata.previous_trial_id) is not None:\n        trial = neps_state.get_trial_by_id(prev_trial_id)\n\n    initial_dir = trial.metadata.location\n\n    # TODO: Hard coded assumption that we are operating in a filebased neps\n    assert isinstance(initial_dir, str)\n    path = Path(initial_dir)\n\n    _INTIAL_DIRECTORY_CACHE[trial.metadata.id] = path\n    return path\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.get_searcher_data","title":"get_searcher_data","text":"<pre><code>get_searcher_data(\n    searcher: str | Path,\n    *,\n    loading_custom_searcher: bool = False\n) -&gt; tuple[dict[str, Any], str]\n</code></pre> <p>Returns the data from the YAML file associated with the specified searcher.</p> PARAMETER DESCRIPTION <code>searcher</code> <p>The name of the searcher.</p> <p> TYPE: <code>str | Path</code> </p> <code>loading_custom_searcher</code> <p>Flag if searcher contains a custom yaml</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[dict[str, Any], str]</code> <p>The content of the YAML file and searcher name.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def get_searcher_data(\n    searcher: str | Path, *, loading_custom_searcher: bool = False\n) -&gt; tuple[dict[str, Any], str]:\n    \"\"\"Returns the data from the YAML file associated with the specified searcher.\n\n    Args:\n        searcher: The name of the searcher.\n        loading_custom_searcher: Flag if searcher contains a custom yaml\n\n    Returns:\n        The content of the YAML file and searcher name.\n    \"\"\"\n    if loading_custom_searcher:\n        user_yaml_path = Path(searcher).with_suffix(\".yaml\")\n\n        if not user_yaml_path.exists():\n            raise FileNotFoundError(\n                \"Failed to get info for searcher from user-defined YAML file. \"\n                f\"File '{searcher}.yaml' does not exist at '{user_yaml_path}'\"\n            )\n\n        with user_yaml_path.open(\"r\") as file:\n            data = yaml.safe_load(file)\n\n        file_name = user_yaml_path.stem\n        searcher = data.pop(\"name\", file_name)\n\n    else:\n        # TODO(eddiebergman): This is a bad idea as it relies on folder structure to be\n        # correct, we should either have a dedicated resource folder or at least have\n        # this defined as a constant somewhere, incase we access elsewhere.\n        # Seems like we could just include this as a method on `SearcherConfigs` class.\n        # TODO(eddiebergman): Need to make sure that these yaml files are actually\n        # included in a source dist when published to PyPI.\n\n        # This is pointing to yaml file directory elsewhere in the source code.\n        resource_path = (\n            Path(__file__).parent.parent.absolute()\n            / \"optimizers\"\n            / \"default_searchers\"\n            / searcher\n        ).with_suffix(\".yaml\")\n\n        from neps.optimizers.info import SearcherConfigs\n\n        searchers = SearcherConfigs.get_searchers()\n\n        if not resource_path.exists():\n            raise FileNotFoundError(\n                f\"Searcher '{searcher}' not in:\\n{', '.join(searchers)}\"\n            )\n\n        with resource_path.open() as file:\n            data = yaml.safe_load(file)\n\n    return data, searcher  # type: ignore\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.get_value","title":"get_value","text":"<pre><code>get_value(obj: Any) -&gt; Any\n</code></pre> <p>Honestly, don't know why you would use this. Please try not to.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def get_value(obj: Any) -&gt; Any:\n    \"\"\"Honestly, don't know why you would use this. Please try not to.\"\"\"\n    if obj is None:\n        return None\n    if isinstance(obj, str | int | float | bool):\n        return obj\n    if isinstance(obj, dict):\n        return {key: get_value(value) for key, value in obj.items()}\n    if isinstance(obj, list):\n        return [get_value(item) for item in obj]\n\n    return obj.__name__\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.instance_from_map","title":"instance_from_map","text":"<pre><code>instance_from_map(\n    mapping: dict[str, Any],\n    request: str | list | tuple | type,\n    name: str = \"mapping\",\n    *,\n    allow_any: bool = True,\n    as_class: bool = False,\n    kwargs: dict | None = None\n) -&gt; Any\n</code></pre> <p>Get an instance of an class from a mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Mapping from string keys to classes or instances</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>request</code> <p>A key from the mapping. If allow_any is True, could also be an object or a class, to use a custom object.</p> <p> TYPE: <code>str | list | tuple | type</code> </p> <code>name</code> <p>Name of the mapping used in error messages</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mapping'</code> </p> <code>allow_any</code> <p>If set to True, allows using custom classes/objects.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_class</code> <p>If the class should be returned without beeing instanciated</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Arguments used for the new instance, if created. Its purpose is to serve at default arguments if the user doesn't built the object.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the request is invalid (not a string if allow_any is False), or invalid key.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def instance_from_map(  # noqa: C901\n    mapping: dict[str, Any],\n    request: str | list | tuple | type,\n    name: str = \"mapping\",\n    *,\n    allow_any: bool = True,\n    as_class: bool = False,\n    kwargs: dict | None = None,\n) -&gt; Any:\n    \"\"\"Get an instance of an class from a mapping.\n\n    Arguments:\n        mapping: Mapping from string keys to classes or instances\n        request: A key from the mapping. If allow_any is True, could also be an\n            object or a class, to use a custom object.\n        name: Name of the mapping used in error messages\n        allow_any: If set to True, allows using custom classes/objects.\n        as_class: If the class should be returned without beeing instanciated\n        kwargs: Arguments used for the new instance, if created. Its purpose is\n            to serve at default arguments if the user doesn't built the object.\n\n    Raises:\n        ValueError: if the request is invalid (not a string if allow_any is False),\n            or invalid key.\n    \"\"\"\n    # Split arguments of the form (request, kwargs)\n    args_dict = kwargs or {}\n    if isinstance(request, Sequence) and not isinstance(request, str):\n        if len(request) != 2:\n            raise ValueError(\n                \"When building an instance and specifying arguments, \"\n                \"you should give a pair (class, arguments)\"\n            )\n        request, req_args_dict = request\n\n        if not isinstance(req_args_dict, Mapping):\n            raise ValueError(\"The arguments should be given as a dictionary\")\n\n        args_dict = {**args_dict, **req_args_dict}\n\n    # Then, get the class/instance from the request\n    if isinstance(request, str):\n        if request not in mapping:\n            raise ValueError(f\"{request} doesn't exists for {name}\")\n\n        instance = mapping[request]\n    elif allow_any:\n        instance = request\n    else:\n        raise ValueError(f\"Object {request} invalid key for {name}\")\n\n    # Check if the request is a class if it is mandatory\n    if (args_dict or as_class) and not is_partial_class(instance):\n        raise ValueError(\n            f\"{instance} is not a class and can't be used with additional arguments\"\n        )\n\n    # Give the arguments to the class\n    if args_dict:\n        instance = partial(instance, **args_dict)  # type: ignore\n\n    if as_class:\n        return instance\n\n    if is_partial_class(instance):\n        try:\n            instance = instance()  # type: ignore\n        except TypeError as e:\n            raise TypeError(f\"{e} when calling {instance} with {args_dict}\") from e\n\n    return instance\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.is_partial_class","title":"is_partial_class","text":"<pre><code>is_partial_class(obj: Any) -&gt; bool\n</code></pre> <p>Check if the object is a (partial) class, or an instance.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def is_partial_class(obj: Any) -&gt; bool:\n    \"\"\"Check if the object is a (partial) class, or an instance.\"\"\"\n    if isinstance(obj, partial):\n        obj = obj.func\n    return inspect.isclass(obj)\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    model: Module | None = None,\n    optimizer: Optimizer | None = None,\n) -&gt; dict | None\n</code></pre> <p>Load a checkpoint and return the model state_dict and checkpoint values.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Directory where the checkpoint is located.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_name</code> <p>The name of the checkpoint file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'checkpoint'</code> </p> <code>model</code> <p>The PyTorch model to load.</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to load.</p> <p> TYPE: <code>Optimizer | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict | None</code> <p>A dictionary containing the checkpoint values, or None if the checkpoint file does not exist hence no checkpointing was previously done.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def load_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    model: torch.nn.Module | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n) -&gt; dict | None:\n    \"\"\"Load a checkpoint and return the model state_dict and checkpoint values.\n\n    Args:\n        directory: Directory where the checkpoint is located.\n        checkpoint_name: The name of the checkpoint file.\n        model: The PyTorch model to load.\n        optimizer: The optimizer to load.\n\n    Returns:\n        A dictionary containing the checkpoint values, or None if the checkpoint file\n        does not exist hence no checkpointing was previously done.\n    \"\"\"\n    if directory is None:\n        trial = get_in_progress_trial()\n        directory = trial.metadata.previous_trial_location\n        if directory is None:\n            return None\n        assert isinstance(directory, str)\n\n    directory = Path(directory)\n    checkpoint_path = (directory / checkpoint_name).with_suffix(\".pth\")\n\n    if not checkpoint_path.exists():\n        return None\n\n    checkpoint = torch.load(checkpoint_path, weights_only=True)\n\n    if model is not None and \"model_state_dict\" in checkpoint:\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n    return checkpoint  # type: ignore\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.load_lightning_checkpoint","title":"load_lightning_checkpoint","text":"<pre><code>load_lightning_checkpoint(\n    checkpoint_dir: Path | str,\n    previous_pipeline_directory: Path | str | None = None,\n) -&gt; tuple[Path, dict] | tuple[None, None]\n</code></pre> <p>Load the latest checkpoint file from the specified directory.</p> <p>This function searches for possible checkpoint files in the <code>checkpoint_dir</code> and loads the latest one if found. It returns a tuple with the checkpoint path and the loaded checkpoint data.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The directory where checkpoint files are stored.</p> <p> TYPE: <code>Path | str</code> </p> <code>previous_pipeline_directory</code> <p>The previous pipeline directory.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Path, dict] | tuple[None, None]</code> <p>A tuple containing the checkpoint path (str) and the loaded checkpoint data (dict) or (None, None) if no checkpoint files are found in the directory.</p> Source code in <code>neps/utils/common.py</code> <pre><code>def load_lightning_checkpoint(\n    checkpoint_dir: Path | str,\n    previous_pipeline_directory: Path | str | None = None,\n) -&gt; tuple[Path, dict] | tuple[None, None]:\n    \"\"\"Load the latest checkpoint file from the specified directory.\n\n    This function searches for possible checkpoint files in the `checkpoint_dir` and loads\n    the latest one if found. It returns a tuple with the checkpoint path and the loaded\n    checkpoint data.\n\n    Args:\n        checkpoint_dir: The directory where checkpoint files are stored.\n        previous_pipeline_directory: The previous pipeline directory.\n\n    Returns:\n        A tuple containing the checkpoint path (str) and the loaded checkpoint data (dict)\n        or (None, None) if no checkpoint files are found in the directory.\n    \"\"\"\n    if previous_pipeline_directory is None:\n        trial = get_in_progress_trial()\n        previous_pipeline_directory = trial.metadata.previous_trial_location\n        if previous_pipeline_directory is None:\n            return None, None\n\n    # Search for possible checkpoints to continue training\n    ckpt_files = list(Path(checkpoint_dir).glob(\"*.ckpt\"))\n\n    if len(ckpt_files) == 0:\n        raise FileNotFoundError(\n            \"No checkpoint files were located in the checkpoint directory\"\n        )\n\n    if len(ckpt_files) &gt; 1:\n        raise ValueError(\n            \"The number of checkpoint files is more than expected (1) \"\n            \"which makes if difficult to find the correct file.\"\n            \" Please save other checkpoint files in a different directory.\"\n        )\n\n    assert len(ckpt_files) == 1\n    checkpoint_path = ckpt_files[0]\n    checkpoint = torch.load(checkpoint_path, weights_only=True)\n    return checkpoint_path, checkpoint\n</code></pre>"},{"location":"api/neps/utils/common/#neps.utils.common.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    values_to_save: dict | None = None,\n    model: Module | None = None,\n    optimizer: Optimizer | None = None,\n) -&gt; None\n</code></pre> <p>Save a checkpoint including model state_dict and optimizer state_dict to a file.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Directory where the checkpoint will be saved.</p> <p> TYPE: <code>Path | str | None</code> DEFAULT: <code>None</code> </p> <code>values_to_save</code> <p>Additional values to save in the checkpoint.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The PyTorch model to save.</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to save.</p> <p> TYPE: <code>Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_name</code> <p>The name of the checkpoint file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'checkpoint'</code> </p> Source code in <code>neps/utils/common.py</code> <pre><code>def save_checkpoint(\n    directory: Path | str | None = None,\n    checkpoint_name: str = \"checkpoint\",\n    values_to_save: dict | None = None,\n    model: torch.nn.Module | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n) -&gt; None:\n    \"\"\"Save a checkpoint including model state_dict and optimizer state_dict to a file.\n\n    Args:\n        directory: Directory where the checkpoint will be saved.\n        values_to_save: Additional values to save in the checkpoint.\n        model: The PyTorch model to save.\n        optimizer: The optimizer to save.\n        checkpoint_name: The name of the checkpoint file.\n    \"\"\"\n    if directory is None:\n        in_progress_trial = get_in_progress_trial()\n        directory = in_progress_trial.metadata.location\n\n    directory = Path(directory)\n    checkpoint_path = (directory / checkpoint_name).with_suffix(\".pth\")\n\n    saved_dict = {}\n\n    if model is not None:\n        saved_dict[\"model_state_dict\"] = model.state_dict()\n    if optimizer is not None:\n        saved_dict[\"optimizer_state_dict\"] = optimizer.state_dict()\n\n    if values_to_save is not None:\n        saved_dict.update(values_to_save)\n\n    torch.save(saved_dict, checkpoint_path)\n</code></pre>"},{"location":"api/neps/utils/files/","title":"Files","text":""},{"location":"api/neps/utils/files/#neps.utils.files","title":"neps.utils.files","text":"<p>Utilities for file operations.</p>"},{"location":"api/neps/utils/files/#neps.utils.files.deserialize","title":"deserialize","text":"<pre><code>deserialize(path: Path | str) -&gt; dict[str, Any]\n</code></pre> <p>Deserialize data from a yaml file.</p> Source code in <code>neps/utils/files.py</code> <pre><code>def deserialize(path: Path | str) -&gt; dict[str, Any]:\n    \"\"\"Deserialize data from a yaml file.\"\"\"\n    with Path(path).open(\"r\") as file_stream:\n        data = yaml.full_load(file_stream)\n\n    if not isinstance(data, dict):\n        raise TypeError(\n            f\"Deserialized data at {path} is not a dictionary!\"\n            f\" Got {type(data)} instead.\\n{data}\"\n        )\n\n    return data\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.serializable_format","title":"serializable_format","text":"<pre><code>serializable_format(data: Any) -&gt; Any\n</code></pre> <p>Format data to be serializable.</p> Source code in <code>neps/utils/files.py</code> <pre><code>def serializable_format(data: Any) -&gt; Any:  # noqa: PLR0911\n    \"\"\"Format data to be serializable.\"\"\"\n    if hasattr(data, \"serialize\"):\n        return serializable_format(data.serialize())\n\n    if dataclasses.is_dataclass(data) and not isinstance(data, type):\n        return serializable_format(dataclasses.asdict(data))  # type: ignore\n\n    if isinstance(data, Exception):\n        return str(data)\n\n    if isinstance(data, Enum):\n        return data.value\n\n    if isinstance(data, Mapping):\n        return {key: serializable_format(val) for key, val in data.items()}\n\n    if not isinstance(data, str) and isinstance(data, Iterable):\n        return [serializable_format(val) for val in data]\n\n    if type(data).__module__ in [\"numpy\", \"torch\"]:\n        data = data.tolist()  # type: ignore\n        if type(data).__module__ == \"numpy\":\n            data = data.item()\n\n        return serializable_format(data)\n\n    return data\n</code></pre>"},{"location":"api/neps/utils/files/#neps.utils.files.serialize","title":"serialize","text":"<pre><code>serialize(\n    data: Any, path: Path | str, *, sort_keys: bool = True\n) -&gt; None\n</code></pre> <p>Serialize data to a yaml file.</p> Source code in <code>neps/utils/files.py</code> <pre><code>def serialize(data: Any, path: Path | str, *, sort_keys: bool = True) -&gt; None:\n    \"\"\"Serialize data to a yaml file.\"\"\"\n    data = serializable_format(data)\n    path = Path(path)\n    with path.open(\"w\") as file_stream:\n        try:\n            return yaml.safe_dump(data, file_stream, sort_keys=sort_keys)\n        except yaml.representer.RepresenterError as e:\n            raise TypeError(\n                \"Could not serialize to yaml! The object \"\n                f\"{e.args[1]} of type {type(e.args[1])} is not.\"\n            ) from e\n</code></pre>"},{"location":"api/neps/utils/run_args/","title":"Run args","text":""},{"location":"api/neps/utils/run_args/#neps.utils.run_args","title":"neps.utils.run_args","text":"<p>This module provides utility functions for handling yaml content of run_args. It includes functions for loading and processing configurations.</p>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Default","title":"Default","text":"<pre><code>Default(value: Any)\n</code></pre> <p>A class to enable default detection.</p> ATTRIBUTE DESCRIPTION <code>value</code> <p>The value to be stored as the default.</p> <p> </p> METHOD DESCRIPTION <code>__repr__</code> <p>Returns a string representation of the Default object.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to store as default. Can be any data type.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def __init__(self, value: Any):\n    \"\"\"Initialize the Default object with the specified value.\n\n    Args:\n        value: The value to store as default. Can be any data type.\n    \"\"\"\n    self.value = value\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Default.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return the string representation of the Default object.</p> RETURNS DESCRIPTION <code>str</code> <p>A string that represents the Default object in the format . Source code in <code>neps/utils/run_args.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Default object.\n\n    Returns:\n        A string that represents the Default object in the format &lt;default: value&gt;.\n    \"\"\"\n    return f\"&lt;default: {self.value}&gt;\"\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Sentinel","title":"Sentinel","text":"<p>Introduce a sentinel object as default value for checking variable assignment.</p>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Settings","title":"Settings","text":"<pre><code>Settings(\n    func_args: dict,\n    yaml_args: Path | str | Default | None = None,\n)\n</code></pre> <p>Centralizes and manages configuration settings from various sources of NePS arguments (run_args (yaml) and neps func_args).</p> <p>necessary configurations and handles default values where specified.</p> <p>Args: func_args (dict): The function arguments directly passed to NePS. yaml_args (dict | None): Optional. YAML file arguments provided via run_args.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def __init__(self, func_args: dict, yaml_args: Path | str | Default | None = None):\n    \"\"\"Initializes the Settings object by merging function arguments with YAML\n    configuration settings and assigning them to class attributes. It checks for\n    necessary configurations and handles default values where specified.\n\n    Args:\n    func_args (dict): The function arguments directly passed to NePS.\n    yaml_args (dict | None): Optional. YAML file arguments provided via run_args.\n    \"\"\"\n    self.run_pipeline = UNSET\n    self.root_directory = UNSET\n    self.pipeline_space = UNSET\n    self.overwrite_working_directory = UNSET\n    self.post_run_summary = UNSET\n    self.development_stage_id = UNSET\n    self.task_id = UNSET\n    self.max_evaluations_total = UNSET\n    self.max_evaluations_per_run = UNSET\n    self.continue_until_max_evaluation_completed = UNSET\n    self.max_cost_total = UNSET\n    self.ignore_errors = UNSET\n    self.loss_value_on_error = UNSET\n    self.cost_value_on_error = UNSET\n    self.pre_load_hooks = UNSET\n    self.searcher = UNSET\n    self.searcher_kwargs = UNSET\n\n    if not isinstance(yaml_args, Default) and yaml_args is not None:\n        yaml_settings = get_run_args_from_yaml(yaml_args)\n        dict_settings = self.merge(func_args, yaml_settings)\n    else:\n        dict_settings = {}\n        for key, value in func_args.items():\n            if isinstance(value, Default):\n                dict_settings[key] = value.value\n            else:\n                dict_settings[key] = value\n\n    # drop run_args, not needed as a setting attribute\n    del dict_settings[RUN_ARGS]\n    self.assign(dict_settings)\n    self.check()\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Settings.assign","title":"assign","text":"<pre><code>assign(dict_settings: dict) -&gt; None\n</code></pre> <p>Updates existing attributes with values from <code>dict_settings</code>. Raises AttributeError if any attribute in <code>dict_settings</code> does not exist.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def assign(self, dict_settings: dict) -&gt; None:\n    \"\"\"Updates existing attributes with values from `dict_settings`.\n    Raises AttributeError if any attribute in `dict_settings` does not exist.\n    \"\"\"\n    for key, value in dict_settings.items():\n        if hasattr(self, key):\n            setattr(self, key, value)\n        else:\n            raise AttributeError(f\"'Settings' object has no attribute '{key}'\")\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Settings.check","title":"check","text":"<pre><code>check() -&gt; None\n</code></pre> <p>Check if all values are assigned and if the essentials are provided correctly.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def check(self) -&gt; None:\n    \"\"\"Check if all values are assigned and if the essentials are provided\n    correctly.\n    \"\"\"\n    unassigned_attributes = self.check_unassigned_attributes()\n    if unassigned_attributes:\n        raise ValueError(\n            f\"Unassigned or default-initialized attributes detected: \"\n            f\"{', '.join(unassigned_attributes)}\"\n        )\n    check_essential_arguments(\n        self.run_pipeline,  # type: ignore\n        self.root_directory,  # type: ignore\n        self.pipeline_space,  # type: ignore\n        self.max_cost_total,  # type: ignore\n        self.max_evaluations_total,  # type: ignore\n        self.searcher,  # type: ignore\n    )\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Settings.check_unassigned_attributes","title":"check_unassigned_attributes","text":"<pre><code>check_unassigned_attributes() -&gt; list\n</code></pre> <p>Check for UNSET and Default class.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def check_unassigned_attributes(self) -&gt; list:\n    \"\"\"Check for UNSET and Default class.\"\"\"\n    return [\n        key\n        for key, value in self.__dict__.items()\n        if value is UNSET or isinstance(value, Default)\n    ]\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.Settings.merge","title":"merge","text":"<pre><code>merge(func_args: dict, yaml_args: dict) -&gt; dict\n</code></pre> <p>Merge func_args and yaml_args. func_args gets priority over yaml_args.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def merge(self, func_args: dict, yaml_args: dict) -&gt; dict:\n    \"\"\"Merge func_args and yaml_args. func_args gets priority over yaml_args.\"\"\"\n    # Initialize with YAML settings\n    merged_settings = yaml_args.copy()\n\n    # overwrite or merge keys\n    for key, value in func_args.items():\n        # Handle searcher_kwargs for BaseOptimizer case\n        if key == SEARCHER_KWARGS:\n            merged_settings[SEARCHER_KWARGS] = {\n                **yaml_args.pop(SEARCHER_KWARGS, {}),\n                **func_args[SEARCHER_KWARGS],\n            }\n        elif not isinstance(value, Default):\n            merged_settings[key] = value\n        elif key not in yaml_args:\n            # If the key is not in yaml_args, set it from Default\n            merged_settings[key] = value.value\n    return merged_settings\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.check_essential_arguments","title":"check_essential_arguments","text":"<pre><code>check_essential_arguments(\n    run_pipeline: Callable | None,\n    root_directory: str | None,\n    pipeline_space: dict | None,\n    max_cost_total: int | None,\n    max_evaluations_total: int | None,\n    searcher: BaseOptimizer | dict | str | None,\n) -&gt; None\n</code></pre> <p>Validates essential NePS configuration arguments.</p> <p>Ensures 'run_pipeline', 'root_directory', 'pipeline_space', and either 'max_cost_total' or 'max_evaluations_total' are provided for NePS execution. Raises ValueError with missing argument details. Additionally, checks 'searcher' is a BaseOptimizer if 'pipeline_space' is absent.</p> PARAMETER DESCRIPTION <code>run_pipeline</code> <p>Function for the pipeline execution.</p> <p> TYPE: <code>Callable | None</code> </p> <code>root_directory</code> <p>Directory path for data storage.</p> <p> TYPE: <code>str</code> </p> <code>pipeline_space</code> <p>search space for this run.</p> <p> TYPE: <code>dict | None</code> </p> <code>max_cost_total</code> <p>Max allowed total cost for experiments.</p> <p> TYPE: <code>int | None</code> </p> <code>max_evaluations_total</code> <p>Max allowed evaluations.</p> <p> TYPE: <code>int | None</code> </p> <code>searcher</code> <p>Optimizer for the configuration space.</p> <p> TYPE: <code>BaseOptimizer | dict | str | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Missing or invalid essential arguments.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def check_essential_arguments(\n    run_pipeline: Callable | None,\n    root_directory: str | None,\n    pipeline_space: dict | None,\n    max_cost_total: int | None,\n    max_evaluations_total: int | None,\n    searcher: BaseOptimizer | dict | str | None,\n) -&gt; None:\n    \"\"\"Validates essential NePS configuration arguments.\n\n    Ensures 'run_pipeline', 'root_directory', 'pipeline_space', and either\n    'max_cost_total' or 'max_evaluations_total' are provided for NePS execution.\n    Raises ValueError with missing argument details. Additionally, checks 'searcher'\n    is a BaseOptimizer if 'pipeline_space' is absent.\n\n    Args:\n        run_pipeline: Function for the pipeline execution.\n        root_directory (str): Directory path for data storage.\n        pipeline_space: search space for this run.\n        max_cost_total: Max allowed total cost for experiments.\n        max_evaluations_total: Max allowed evaluations.\n        searcher: Optimizer for the configuration space.\n\n    Raises:\n        ValueError: Missing or invalid essential arguments.\n    \"\"\"\n    if not run_pipeline:\n        raise ValueError(\"'run_pipeline' is required but was not provided.\")\n    if not root_directory:\n        raise ValueError(\"'root_directory' is required but was not provided.\")\n    if not pipeline_space and not isinstance(searcher, BaseOptimizer):\n        # handling special case for searcher instance, in which user doesn't have to\n        # provide the search_space because it's the argument of the searcher.\n        raise ValueError(\"'pipeline_space' is required but was not provided.\")\n\n    if not max_evaluations_total and not max_cost_total:\n        raise ValueError(\n            \"'max_evaluations_total' or 'max_cost_total' is required but \"\n            \"both were not provided.\"\n        )\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.check_run_args","title":"check_run_args","text":"<pre><code>check_run_args(settings: dict) -&gt; None\n</code></pre> <p>Validates the types of NePS configuration settings.</p> <p>Checks that each setting's value type matches its expected type. Raises TypeError for type mismatches.</p> PARAMETER DESCRIPTION <code>settings</code> <p>NePS configuration settings.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>For mismatched setting value types.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def check_run_args(settings: dict) -&gt; None:\n    \"\"\"Validates the types of NePS configuration settings.\n\n    Checks that each setting's value type matches its expected type. Raises\n    TypeError for type mismatches.\n\n    Args:\n        settings (dict): NePS configuration settings.\n\n    Raises:\n        TypeError: For mismatched setting value types.\n    \"\"\"\n    # Mapping parameter names to their allowed types\n    # [task_id, development_stage_id, pre_load_hooks] require special handling of type,\n    # that's why they are not listed\n    expected_types = {\n        RUN_PIPELINE: Callable,\n        ROOT_DIRECTORY: str,\n        # TODO: Support CS.ConfigurationSpace for pipeline_space\n        PIPELINE_SPACE: (str, dict),\n        OVERWRITE_WORKING_DIRECTORY: bool,\n        POST_RUN_SUMMARY: bool,\n        MAX_EVALUATIONS_TOTAL: int,\n        MAX_COST_TOTAL: (int, float),\n        MAX_EVALUATIONS_PER_RUN: int,\n        CONTINUE_UNTIL_MAX_EVALUATION_COMPLETED: bool,\n        LOSS_VALUE_ON_ERROR: float,\n        COST_VALUE_ON_ERROR: float,\n        IGNORE_ERROR: bool,\n        SEARCHER_KWARGS: dict,\n    }\n    for param, value in settings.items():\n        if param in (DEVELOPMENT_STAGE_ID, TASK_ID):\n            # this argument can be Any\n            continue\n        elif param == PRE_LOAD_HOOKS:  # noqa: RET507\n            # check if all items in pre_load_hooks are callable objects\n            if not all(callable(item) for item in value):\n                raise TypeError(\"All items in 'pre_load_hooks' must be callable.\")\n        elif param == SEARCHER:\n            if not (isinstance(value, str | dict) or issubclass(value, BaseOptimizer)):\n                raise TypeError(\n                    \"Parameter 'searcher' must be a string or a class that is a subclass \"\n                    \"of BaseOptimizer.\"\n                )\n        else:\n            try:\n                expected_type = expected_types[param]\n            except KeyError as e:\n                raise KeyError(f\"{param} is not a valid argument of neps\") from e\n            if not isinstance(value, expected_type):  # type: ignore\n                raise TypeError(\n                    f\"Parameter '{param}' expects a value of type {expected_type}, got \"\n                    f\"{type(value)} instead.\"\n                )\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.config_loader","title":"config_loader","text":"<pre><code>config_loader(path: str | Path) -&gt; dict\n</code></pre> <p>Loads a YAML file and returns the contents under the 'run_args' key.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to the YAML file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Content of the yaml (dict)</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the file at 'path' does not exist.</p> <code>ValueError</code> <p>If the file is not a valid YAML.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def config_loader(path: str | Path) -&gt; dict:\n    \"\"\"Loads a YAML file and returns the contents under the 'run_args' key.\n\n    Args:\n        path (str): Path to the YAML file.\n\n    Returns:\n        Content of the yaml (dict)\n\n    Raises:\n        FileNotFoundError: If the file at 'path' does not exist.\n        ValueError: If the file is not a valid YAML.\n    \"\"\"\n    try:\n        with open(path) as file:  # noqa: PTH123\n            config = yaml.safe_load(file)\n    except FileNotFoundError as e:\n        raise FileNotFoundError(\n            f\"The specified file was not found: '{path}'.\"\n            f\" Please make sure that the path is correct and \"\n            f\"try again.\"\n        ) from e\n    except yaml.YAMLError as e:\n        raise ValueError(f\"The file at {path} is not a valid YAML file.\") from e\n\n    return config\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.extract_leaf_keys","title":"extract_leaf_keys","text":"<pre><code>extract_leaf_keys(\n    d: dict, special_keys: dict | None = None\n) -&gt; tuple[dict, dict]\n</code></pre> <p>Recursive function to extract leaf keys and their values from a nested dictionary. Special keys (e.g.'run_pipeline') are also extracted if present and their corresponding values (dict) at any level in the nested structure.</p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary to extract values from.</p> <p> TYPE: <code>dict</code> </p> <code>special_keys</code> <p>A dictionary to store values of special keys.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[dict, dict]</code> <p>A tuple containing the leaf keys dictionary and the dictionary for special keys.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def extract_leaf_keys(d: dict, special_keys: dict | None = None) -&gt; tuple[dict, dict]:\n    \"\"\"Recursive function to extract leaf keys and their values from a nested dictionary.\n    Special keys (e.g.'run_pipeline') are also extracted if present\n    and their corresponding values (dict) at any level in the nested structure.\n\n    Args:\n        d (dict): The dictionary to extract values from.\n        special_keys (dict|None): A dictionary to store values of special keys.\n\n    Returns:\n        A tuple containing the leaf keys dictionary and the dictionary for\n        special keys.\n    \"\"\"\n    if special_keys is None:\n        special_keys = {\n            RUN_PIPELINE: None,\n            PRE_LOAD_HOOKS: None,\n            SEARCHER: None,\n            PIPELINE_SPACE: None,\n        }\n\n    leaf_keys = {}\n    for k, v in d.items():\n        if k in special_keys and v != \"None\":\n            special_keys[k] = v\n        elif isinstance(v, dict):\n            # Recursively call to explore nested dictionaries\n            nested_leaf_keys, _ = extract_leaf_keys(v, special_keys)\n            leaf_keys.update(nested_leaf_keys)\n        elif v is not None and v != \"None\":\n            leaf_keys[k] = v\n    return leaf_keys, special_keys\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.get_run_args_from_yaml","title":"get_run_args_from_yaml","text":"<pre><code>get_run_args_from_yaml(path: str | Path) -&gt; dict\n</code></pre> <p>Load and validate NEPS run arguments from a specified YAML configuration file provided via run_args.</p> <p>This function reads a YAML file, extracts the arguments required by NePS, validates these arguments, and then returns them in a dictionary. It checks for the presence and validity of expected parameters, and distinctively handles more complex configurations, specifically those that are dictionaries(e.g. pipeline_space) or objects(e.g. run_pipeline) requiring loading.</p> PARAMETER DESCRIPTION <code>path</code> <p>The file path to the YAML configuration file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary of validated run arguments.</p> RAISES DESCRIPTION <code>KeyError</code> <p>If any parameter name is invalid.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def get_run_args_from_yaml(path: str | Path) -&gt; dict:\n    \"\"\"Load and validate NEPS run arguments from a specified YAML configuration file\n    provided via run_args.\n\n    This function reads a YAML file, extracts the arguments required by NePS,\n    validates these arguments, and then returns them in a dictionary. It checks for the\n    presence and validity of expected parameters, and distinctively handles more complex\n    configurations, specifically those that are dictionaries(e.g. pipeline_space) or\n    objects(e.g. run_pipeline) requiring loading.\n\n    Args:\n        path (str): The file path to the YAML configuration file.\n\n    Returns:\n        A dictionary of validated run arguments.\n\n    Raises:\n        KeyError: If any parameter name is invalid.\n    \"\"\"\n    # Load the YAML configuration file\n    config = config_loader(path)\n\n    # Initialize an empty dictionary to hold the extracted settings\n    settings = {}\n\n    # List allowed NePS run arguments with simple types (e.g., string, int). Parameters\n    # like 'run_pipeline', 'preload_hooks', 'pipeline_space',\n    # and 'searcher' are excluded due to needing specialized processing.\n    expected_parameters = [\n        ROOT_DIRECTORY,\n        MAX_EVALUATIONS_TOTAL,\n        MAX_COST_TOTAL,\n        OVERWRITE_WORKING_DIRECTORY,\n        POST_RUN_SUMMARY,\n        DEVELOPMENT_STAGE_ID,\n        TASK_ID,\n        MAX_EVALUATIONS_PER_RUN,\n        CONTINUE_UNTIL_MAX_EVALUATION_COMPLETED,\n        LOSS_VALUE_ON_ERROR,\n        COST_VALUE_ON_ERROR,\n        IGNORE_ERROR,\n    ]\n\n    # Flatten the YAML file's structure to separate flat parameters (flat_config) and\n    # those needing special handling (special_configs).\n    flat_config, special_configs = extract_leaf_keys(config)\n\n    # Check if flatten dict (flat_config) just contains the expected parameters\n    for parameter, value in flat_config.items():\n        if parameter in expected_parameters:\n            settings[parameter] = value\n        else:\n            raise KeyError(\n                f\"Parameter '{parameter}' is not an argument of neps.run() \"\n                f\"provided via run_args.\"\n                f\"See here all valid arguments:\"\n                f\" {', '.join(expected_parameters)}, \"\n                f\"'run_pipeline', 'preload_hooks', 'pipeline_space'\"\n            )\n\n    # Process complex configurations (e.g., 'pipeline_space', 'searcher') and integrate\n    # them into 'settings'.\n    handle_special_argument_cases(settings, special_configs)\n\n    # check if all provided arguments have legal types\n    check_run_args(settings)\n\n    logger.debug(\n        f\"The 'run_args' arguments: {settings} are now extracted and type-tested from \"\n        f\"referenced YAML.\"\n    )\n\n    return settings\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.handle_special_argument_cases","title":"handle_special_argument_cases","text":"<pre><code>handle_special_argument_cases(\n    settings: dict, special_configs: dict\n) -&gt; None\n</code></pre> <p>Process and integrate special configuration cases into the 'settings' dictionary.</p> <p>This function updates 'settings' with values from 'special_configs'. It handles specific keys that require more complex processing, such as 'pipeline_space' and 'searcher', which may need to load a function/dict from paths. It also manages nested configurations like 'pre_load_hooks' which need individual processing or function loading.</p> PARAMETER DESCRIPTION <code>settings</code> <p>The dictionary to be updated with processed configurations.</p> <p> TYPE: <code>dict</code> </p> <code>special_configs</code> <p>A dictionary containing configuration keys and values                   that require special processing.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def handle_special_argument_cases(settings: dict, special_configs: dict) -&gt; None:\n    \"\"\"Process and integrate special configuration cases into the 'settings' dictionary.\n\n    This function updates 'settings' with values from 'special_configs'. It handles\n    specific keys that require more complex processing, such as 'pipeline_space' and\n    'searcher', which may need to load a function/dict from paths. It also manages nested\n    configurations like 'pre_load_hooks' which need individual processing or function\n    loading.\n\n    Args:\n        settings (dict): The dictionary to be updated with processed configurations.\n        special_configs (dict): A dictionary containing configuration keys and values\n                              that require special processing.\n\n    \"\"\"\n    # process special configs\n    process_run_pipeline(RUN_PIPELINE, special_configs, settings)\n    process_pipeline_space(PIPELINE_SPACE, special_configs, settings)\n    process_searcher(SEARCHER, special_configs, settings)\n\n    if special_configs[PRE_LOAD_HOOKS] is not None:\n        # Loads the pre_load_hooks functions and add them in a list to settings.\n        settings[PRE_LOAD_HOOKS] = load_hooks_from_config(special_configs[PRE_LOAD_HOOKS])\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.load_and_return_object","title":"load_and_return_object","text":"<pre><code>load_and_return_object(\n    module_path: str, object_name: str, key: str\n) -&gt; object\n</code></pre> <p>Dynamically loads an object from a given module file path.</p> <p>This function attempts to dynamically import an object by its name from a specified module path. If the initial import fails, it retries with a '.py' extension appended to the path.</p> PARAMETER DESCRIPTION <code>module_path</code> <p>File system path to the Python module.</p> <p> TYPE: <code>str</code> </p> <code>object_name</code> <p>Name of the object to import from the module.</p> <p> TYPE: <code>str</code> </p> <code>key</code> <p>Identifier for the argument causing the error, for enhanced error</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>object</code> <p>The imported object from the module.</p> <p> TYPE: <code>object</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If the module or object cannot be found, with a message detailing</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def load_and_return_object(module_path: str, object_name: str, key: str) -&gt; object:\n    \"\"\"Dynamically loads an object from a given module file path.\n\n    This function attempts to dynamically import an object by its name from a specified\n    module path. If the initial import fails, it retries with a '.py' extension appended\n    to the path.\n\n    Args:\n        module_path (str): File system path to the Python module.\n        object_name (str): Name of the object to import from the module.\n        key (str): Identifier for the argument causing the error, for enhanced error\n        feedback.\n\n    Returns:\n        object: The imported object from the module.\n\n    Raises:\n        ImportError: If the module or object cannot be found, with a message detailing\n        the issue.\n    \"\"\"\n\n    def import_object(path: str) -&gt; object | None:\n        try:\n            # Convert file system path to module path, removing '.py' if present.\n            module_name = (\n                path[:-3].replace(\"/\", \".\")\n                if path.endswith(\".py\")\n                else path.replace(\"/\", \".\")\n            )\n\n            # Dynamically import the module.\n            spec = importlib.util.spec_from_file_location(module_name, path)\n            if spec is None or spec.loader is None:\n                return None  # Failed to load module spec.\n            module = importlib.util.module_from_spec(spec)\n            sys.modules[module_name] = module\n            spec.loader.exec_module(module)\n\n            # Retrieve the object.\n            imported_object = getattr(module, object_name, None)\n            if imported_object is None:\n                return None  # Object not found in module.\n            return imported_object\n        except FileNotFoundError:\n            return None  # File not found.\n\n    # Attempt to import the object using the provided path.\n    imported_object = import_object(module_path)\n    if imported_object is None:\n        # If the object could not be imported, attempt again by appending '.py',\n        # if not already present.\n        if not module_path.endswith(\".py\"):\n            module_path += \".py\"\n            imported_object = import_object(module_path)\n\n        if imported_object is None:\n            raise ImportError(\n                f\"Failed to import '{object_name}' for argument '{key}'. \"\n                f\"Module path '{module_path}' not found or object does not \"\n                f\"exist.\"\n            )\n\n    return imported_object\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.load_hooks_from_config","title":"load_hooks_from_config","text":"<pre><code>load_hooks_from_config(pre_load_hooks_dict: dict) -&gt; list\n</code></pre> <p>Loads hook functions from a dictionary of configurations.</p> PARAMETER DESCRIPTION <code>pre_load_hooks_dict</code> <p>Dictionary with hook names as keys and paths as values</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>List</code> <p>List of loaded hook functions.</p> <p> TYPE: <code>list</code> </p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def load_hooks_from_config(pre_load_hooks_dict: dict) -&gt; list:\n    \"\"\"Loads hook functions from a dictionary of configurations.\n\n    Args:\n        pre_load_hooks_dict (Dict): Dictionary with hook names as keys and paths as values\n\n    Returns:\n        List: List of loaded hook functions.\n    \"\"\"\n    loaded_hooks = []\n    for name, path in pre_load_hooks_dict.items():\n        hook_func = load_and_return_object(path, name, PRE_LOAD_HOOKS)\n        loaded_hooks.append(hook_func)\n    return loaded_hooks\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.process_pipeline_space","title":"process_pipeline_space","text":"<pre><code>process_pipeline_space(\n    key: str, special_configs: dict, settings: dict\n) -&gt; None\n</code></pre> <p>Process or load the pipeline space configuration.</p> <p>This function checks if the given key exists in the <code>special_configs</code> dictionary. If it exists, it processes the associated value, which can be either a dictionary or a string. Based on the keys of the dictionary it decides if the pipeline_space have to be loaded or needs to be converted into a neps search_space structure. The processed pipeline space is then stored in the <code>settings</code> dictionary under the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to check in the <code>special_configs</code> dictionary.</p> <p> TYPE: <code>str</code> </p> <code>special_configs</code> <p>The dictionary containing special configuration values.</p> <p> TYPE: <code>dict</code> </p> <code>settings</code> <p>The dictionary where the processed pipeline space will be stored.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the value associated with the key is neither a string nor a</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def process_pipeline_space(key: str, special_configs: dict, settings: dict) -&gt; None:\n    \"\"\"Process or load the pipeline space configuration.\n\n    This function checks if the given key exists in the `special_configs` dictionary.\n    If it exists, it processes the associated value, which can be either a dictionary\n    or a string. Based on the keys of the dictionary it decides if the pipeline_space\n    have to be loaded or needs to be converted into a neps search_space structure.\n    The processed pipeline space is then stored in the `settings`\n    dictionary under the given key.\n\n    Args:\n        key (str): The key to check in the `special_configs` dictionary.\n        special_configs (dict): The dictionary containing special configuration values.\n        settings (dict): The dictionary where the processed pipeline space will be stored.\n\n    Raises:\n        TypeError: If the value associated with the key is neither a string nor a\n        dictionary.\n    \"\"\"\n    if special_configs.get(key) is not None:\n        pipeline_space = special_configs[key]\n        # Define the type of processed_pipeline_space to accommodate both situations\n        if isinstance(pipeline_space, dict):\n            # determine if dict contains path_loading or the actual search space\n            expected_keys = {\"path\", \"name\"}\n            actual_keys = set(pipeline_space.keys())\n            if expected_keys != actual_keys:\n                # pipeline_space directly defined in run_args yaml\n                processed_pipeline_space = pipeline_space_from_yaml(pipeline_space)\n            else:\n                # pipeline_space stored in a python dict, not using a yaml\n                processed_pipeline_space = load_and_return_object(\n                    pipeline_space[\"path\"], pipeline_space[\"name\"], key\n                )  # type: ignore\n        elif isinstance(pipeline_space, str):\n            # load yaml from path\n            processed_pipeline_space = pipeline_space_from_yaml(pipeline_space)\n        else:\n            raise TypeError(\n                f\"Value for {key} must be a string or a dictionary, \"\n                f\"but got {type(pipeline_space).__name__}.\"\n            )\n        settings[key] = processed_pipeline_space\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.process_run_pipeline","title":"process_run_pipeline","text":"<pre><code>process_run_pipeline(\n    key: str, special_configs: dict, settings: dict\n) -&gt; None\n</code></pre> <p>Processes the run pipeline configuration and updates the settings dictionary.</p> PARAMETER DESCRIPTION <code>key</code> <p>Key to look up in special_configs.</p> <p> TYPE: <code>str</code> </p> <code>special_configs</code> <p>Dictionary of special configurations.</p> <p> TYPE: <code>dict</code> </p> <code>settings</code> <p>Dictionary to update with the processed function.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>KeyError</code> <p>If required keys ('path' and 'name') are missing in the config.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def process_run_pipeline(key: str, special_configs: dict, settings: dict) -&gt; None:\n    \"\"\"Processes the run pipeline configuration and updates the settings dictionary.\n\n    Args:\n        key (str): Key to look up in special_configs.\n        special_configs (dict): Dictionary of special configurations.\n        settings (dict): Dictionary to update with the processed function.\n\n    Raises:\n        KeyError: If required keys ('path' and 'name') are missing in the config.\n    \"\"\"\n    if special_configs.get(key) is not None:\n        config = special_configs[key]\n        try:\n            func = load_and_return_object(config[\"path\"], config[\"name\"], key)\n            settings[key] = func\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing key for argument {key}: {e}. Expect 'path' \"\n                f\"and 'name' as keys when loading '{key}' \"\n                f\"from 'run_args'\"\n            ) from e\n</code></pre>"},{"location":"api/neps/utils/run_args/#neps.utils.run_args.process_searcher","title":"process_searcher","text":"<pre><code>process_searcher(\n    key: str, special_configs: dict, settings: dict\n) -&gt; None\n</code></pre> <p>Processes the searcher configuration and updates the settings dictionary.</p> <p>Checks if the key exists in special_configs. If found, it processes the value based on its type. Updates settings with the processed searcher.</p> PARAMETER DESCRIPTION <code>key</code> <p>Key to look up in special_configs.</p> <p> TYPE: <code>str</code> </p> <code>special_configs</code> <p>Dictionary of special configurations.</p> <p> TYPE: <code>dict</code> </p> <code>settings</code> <p>Dictionary to update with the processed searcher.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the value for the key is neither a string, Path, nor a dictionary.</p> Source code in <code>neps/utils/run_args.py</code> <pre><code>def process_searcher(key: str, special_configs: dict, settings: dict) -&gt; None:\n    \"\"\"Processes the searcher configuration and updates the settings dictionary.\n\n    Checks if the key exists in special_configs. If found, it processes the\n    value based on its type. Updates settings with the processed searcher.\n\n    Args:\n        key (str): Key to look up in special_configs.\n        special_configs (dict): Dictionary of special configurations.\n        settings (dict): Dictionary to update with the processed searcher.\n\n    Raises:\n        TypeError: If the value for the key is neither a string, Path, nor a dictionary.\n    \"\"\"\n    if special_configs.get(key) is not None:\n        searcher = special_configs[key]\n        if isinstance(searcher, dict):\n            # determine if dict contains path_loading or the actual searcher config\n            expected_keys = {\"path\", \"name\"}\n            actual_keys = set(searcher.keys())\n            if expected_keys.issubset(actual_keys):\n                path = searcher.pop(\"path\")\n                name = searcher.pop(\"name\")\n                settings[SEARCHER_KWARGS] = searcher\n                searcher = load_and_return_object(path, name, key)\n\n        elif isinstance(searcher, str | Path):\n            pass\n        else:\n            raise TypeError(\n                f\"Value for {key} must be a string or a dictionary, \"\n                f\"but got {type(searcher).__name__}.\"\n            )\n        settings[key] = searcher\n</code></pre>"},{"location":"api/neps/utils/types/","title":"Types","text":""},{"location":"api/neps/utils/types/#neps.utils.types","title":"neps.utils.types","text":"<p>Primitive types to be used in NePS or consumers of NePS.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult","title":"ConfigResult  <code>dataclass</code>","text":"<pre><code>ConfigResult(\n    id: str,\n    config: SearchSpace,\n    result: Report | ResultDict | ERROR,\n    metadata: dict,\n)\n</code></pre> <p>Primary class through which optimizers recieve results.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: SearchSpace\n</code></pre> <p>Configuration that was evaluated.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>Unique identifier for the configuration.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: dict\n</code></pre> <p>Any additional data to store with this config and result.</p>"},{"location":"api/neps/utils/types/#neps.utils.types.ConfigResult.result","title":"result  <code>instance-attribute</code>","text":"<pre><code>result: Report | ResultDict | ERROR\n</code></pre> <p>Some dictionary of results.</p>"},{"location":"dev_docs/contributing/","title":"Introduction","text":""},{"location":"dev_docs/contributing/#getting-help","title":"Getting Help","text":"<p>Please use our github and raise an issue at: automl/neps</p>"},{"location":"dev_docs/contributing/#development-workflow","title":"Development Workflow","text":"<p>We use one main branch <code>master</code> and feature branches for development. We use pull requests to merge feature branches into <code>master</code>. Versions released to PyPI are tagged with a version number.</p> <p>Automatic checks are run on every pull request and on every commit to <code>master</code>.</p>"},{"location":"dev_docs/contributing/#installation","title":"Installation","text":"<p>There are three required steps and one optional:</p> <ol> <li>Optional: Install miniconda and create an environment</li> <li>Install poetry</li> <li>Install the neps package using poetry</li> <li>Activate pre-commit for the repository</li> </ol> <p>For instructions see below.</p>"},{"location":"dev_docs/contributing/#1-optional-install-miniconda-and-create-a-virtual-environment","title":"1. Optional: Install miniconda and create a virtual environment","text":"<p>To manage python versions install e.g., miniconda with</p> <pre><code>wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O install_miniconda.sh\nbash install_miniconda.sh -b -p $HOME/.conda  # Change to place of preference\nrm install_miniconda.sh\n</code></pre> <p>Consider running <code>~/.conda/bin/conda init</code> or <code>~/.conda/bin/conda init zsh</code> .</p> <p>Then finally create the environment and activate it</p> <pre><code>conda create -n neps python=3.10\nconda activate neps\n</code></pre>"},{"location":"dev_docs/contributing/#2-install-poetry","title":"2. Install poetry","text":"<p>First, install poetry, e.g., via</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n# or directly into your virtual env using `pip install poetry`\n</code></pre> <p>Then consider appending</p> <pre><code>export PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre> <p>to your <code>.zshrc</code> / <code>.bashrc</code> or alternatively simply running the export manually.</p>"},{"location":"dev_docs/contributing/#3-install-the-neps-package-using-poetry","title":"3. Install the neps Package Using poetry","text":"<p>Clone the repository, e.g.,</p> <pre><code>git clone https://github.com/automl/neps.git\ncd neps\n</code></pre> <p>Then, inside the main directory of neps run</p> <pre><code>poetry install\n</code></pre> <p>This will installthe neps package but also additional dev dependencies.</p>"},{"location":"dev_docs/contributing/#4-activate-pre-commit-for-the-repository","title":"4. Activate pre-commit for the repository","text":"<p>With the python environment used to install the neps package run in the main directory of neps</p> <pre><code>pre-commit install\n</code></pre> <p>This install a set of hooks that will run basic linting and type checking before every comment. If you ever need to unsinstall the hooks, you can do so with <code>pre-commit uninstall</code>. These mostly consist of <code>ruff</code> for formatting and linting and <code>mypy</code> for type checking.</p> <p>We highly recommend you install at least <code>ruff</code> either on command line, or in the editor of your choice, e.g. VSCode, PyCharm.</p>"},{"location":"dev_docs/contributing/#checks-and-tests","title":"Checks and Tests","text":"<p>We have setup checks and tests at several points in the development flow:</p> <ul> <li>At every commit we automatically run a suite of pre-commit hooks that perform static code analysis, autoformating, and sanity checks. This is setup during our installation process.</li> <li>At every commit / push locally running a minimal suite of integration tests is encouraged. The tests correspond directly to examples in neps_examples and only check for crash-causing errors.</li> <li>At every push all integration tests and regression tests are run automatically using github actions.</li> </ul>"},{"location":"dev_docs/contributing/#linting-ruff","title":"Linting (Ruff)","text":"<p>For linting we use <code>ruff</code> for checking code quality. You can install it locally and use it as so:</p> <pre><code>pip install ruff\nruff check --fix neps  # the --fix flag will try to fix issues it can automatically\n</code></pre> <p>This will also be run using <code>pre-commit</code> hooks.</p> <p>To ignore a rule for a specific line, you can add a comment with <code>ruff: disable</code> at the end of the line, e.g.</p> <pre><code>for x, y in zip(a, b):  # noqa: &lt;ERRCODE&gt;\n    pass\n</code></pre> <p>The configuration of <code>ruff</code> is in the <code>pyproject.toml</code> file and we refer you to the documentation if you require any changes to be made.</p> <p>There you can find the documentation for all of the rules employed.</p>"},{"location":"dev_docs/contributing/#type-checking-mypy","title":"Type Checking (Mypy)","text":"<p>For type checking we use <code>mypy</code>. You can install it locally and use it as so:</p> <pre><code>pip install mypy\nmypy neps\n</code></pre> <p>Types are helpful for making your code more understandable by your editor and tools, allowing them to warn you of potential issues, as well as allow for safer refactoring. Copilot also works better with types.</p> <p>To ignore some error you can use <code># type: ignore</code> at the end of the line, e.g.</p> <pre><code>code = \"foo\"  # type: ignore\n</code></pre> <p>A common place to ignore types is when dealing with numpy arrays, tensors and pandas, where the type checker can not be sure of the return type.</p> <pre><code>df.mean()  # Is this another dataframe, a series or a single number?\n</code></pre> <p>In the worse case, please just use <code>Any</code> and move on with your life, the type checker is meant to help you catch bugs, not hinder you. However it will take some experience to know whe it's trying to tell you something useful vs. something it just can not infer properly. A good rule of thumb is that you're only dealing with simple native types from python or types defined from NePS, there is probably a good reason for a mypy error.</p> <p>If you have issues regarding typing, please feel free to reach out for help <code>@eddiebergman</code>.</p>"},{"location":"dev_docs/contributing/#examples-and-integration-tests","title":"Examples and Integration Tests","text":"<p>We use examples in neps_examples as integration tests, which we run from the main directory via</p> <pre><code>pytest\n</code></pre> <p>If tests fail for you on the master, please raise an issue on github, preferabbly with some informationon the error, traceback and the environment in which you are running, i.e. python version, OS, etc.</p>"},{"location":"dev_docs/contributing/#regression-tests","title":"Regression Tests","text":"<p>Regression tests are run on each push to the repository to assure the performance of the optimizers don't degrade.</p> <p>Currently, regression runs are recorded on JAHS-Bench-201 data for 2 tasks: <code>cifar10</code> and <code>fashion_mnist</code> and only for optimizers: <code>random_search</code>, <code>bayesian_optimization</code>, <code>mf_bayesian_optimization</code>. This information is stored in the <code>tests/regression_runner.py</code> as two lists: <code>TASKS</code>, <code>OPTIMIZERS</code>. The recorded results are stored as a json dictionary in the <code>tests/losses.json</code> file.</p>"},{"location":"dev_docs/contributing/#adding-new-optimizer-algorithms","title":"Adding new optimizer algorithms","text":"<p>Once a new algorithm is added to NEPS library, we need to first record the performance of the algorithm for 100 optimization runs.</p> <ul> <li> <p>If the algorithm expects standard loss function (pipeline) and accepts fidelity hyperparameters in pipeline space, then recording results only requires adding the optimizer name into <code>OPTIMIZERS</code> list in <code>tests/regression_runner.py</code> and running <code>tests/regression_runner.py</code></p> </li> <li> <p>In case your algorithm requires custom pipeline and/or pipeline space you can modify the <code>runner.run_pipeline</code> and <code>runner.pipeline_space</code> attributes of the <code>RegressionRunner</code> after initialization (around line <code>#322</code> in <code>tests/regression_runner.py</code>)</p> </li> </ul> <p>You can verify the optimizer is recorded by rerunning the <code>regression_runner.py</code>. Now regression test will be run on your new optimizer as well on every push.</p>"},{"location":"dev_docs/contributing/#regression-test-metrics","title":"Regression test metrics","text":"<p>For each regression test the algorithm is run 10 times to sample its performance, then they are statistically compared to the 100 recorded runs. We use these 3 boolean metrics to define the performance of the algorithm on any task:</p> <ol> <li>Kolmogorov-Smirnov test for goodness of fit - <code>pvalue</code> &gt;= 10%</li> <li>Absolute median distance - bounded within 92.5% confidence range of the expected median distance</li> <li>Median improvement - Median improvement over the recorded median</li> </ol> <p>Test metrics are run for each <code>(optimizer, task)</code> combination separately and then collected. The collected metrics are then further combined into 2 metrics</p> <ol> <li>Task pass - either both <code>Kolmogorov-Smirnov test</code> and <code>Absolute median distance</code> test passes or just <code>Median improvement</code></li> <li>Test aggregate - Sum_over_tasks(<code>Kolmogorov-Smirnov test</code> + <code>Absolute median distance</code> + 2 * <code>Median improvement</code>)</li> </ol> <p>Finally, a test for an optimizer only passes when at least for one of the tasks <code>Task pass</code> is true, and <code>Test aggregate</code> is higher than 1 + <code>number of tasks</code></p>"},{"location":"dev_docs/contributing/#on-regression-test-failures","title":"On regression test failures","text":"<p>Regression tests are stochastic by nature, so they might fail occasionally even the algorithm performance didn't degrade. In the case of regression test failure, try running it again first, if the problem still persists, then you can contact Danny Stoll or Samir. You can also run tests locally by running:</p> <pre><code>poetry run pytest -m regression_all\n</code></pre>"},{"location":"dev_docs/contributing/#disabling-and-skipping-checks-etc","title":"Disabling and Skipping Checks etc.","text":""},{"location":"dev_docs/contributing/#pre-commit-how-to-not-run-hooks","title":"Pre-commit: How to not run hooks?","text":"<p>To commit without running <code>pre-commit</code> use <code>git commit --no-verify -m &lt;COMMIT MESSAGE&gt;</code>.</p>"},{"location":"dev_docs/contributing/#mypy-how-to-ignore-warnings","title":"Mypy: How to ignore warnings?","text":"<p>There are two options:</p> <ul> <li>Disable the warning locally:</li> </ul> <pre><code>code = \"foo\"  # type: ignore\n</code></pre>"},{"location":"dev_docs/contributing/#managing-dependencies","title":"Managing Dependencies","text":"<p>To manage dependencies and for package distribution we use poetry (replaces pip).</p>"},{"location":"dev_docs/contributing/#add-dependencies","title":"Add dependencies","text":"<p>To install a dependency use</p> <pre><code>poetry add dependency\n</code></pre> <p>and commit the updated <code>pyproject.toml</code> to git.</p> <p>For more advanced dependency management see examples in <code>pyproject.toml</code> or have a look at the poetry documentation.</p>"},{"location":"dev_docs/contributing/#install-dependencies-added-by-others","title":"Install dependencies added by others","text":"<p>When other contributors added dependencies to <code>pyproject.toml</code>, you can install them via</p> <pre><code>poetry lock\npoetry install\n</code></pre>"},{"location":"dev_docs/contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs, more specifically Material for MkDocs for documentation. To support documentation for multiple versions, we use the plugin mike.</p> <p>Source files for the documentation are under <code>/docs</code> and configuration at  mkdocs.yml.</p> <p>To build and view the documentation run</p> <pre><code>mike deploy 0.5.1 latest\nmike serve\n</code></pre> <p>and open the URL shown by the <code>mike serve</code> command.</p> <p>To publish the documentation run</p> <pre><code>mike deploy 0.5.1 latest -p\n</code></pre>"},{"location":"dev_docs/contributing/#releasing-a-new-version","title":"Releasing a New Version","text":"<p>There are four steps to releasing a new version of neps:</p> <ol> <li>Understand Semantic Versioning</li> <li>Update the Package Version</li> <li>Commit and Push With a Version Tag</li> <li>Update Documentation</li> <li>Publish on PyPI</li> </ol>"},{"location":"dev_docs/contributing/#0-understand-semantic-versioning","title":"0. Understand Semantic Versioning","text":"<p>We follow the semantic versioning scheme.</p>"},{"location":"dev_docs/contributing/#1-update-the-package-version-and-citationcff","title":"1. Update the Package Version and CITATION.cff","text":"<pre><code>poetry version v0.9.0\n</code></pre> <p>and manually change the version specified in <code>CITATION.cff</code>.</p>"},{"location":"dev_docs/contributing/#2-commit-with-a-version-tag","title":"2. Commit with a Version Tag","text":"<p>First commit and test</p> <pre><code>git add pyproject.toml\ngit commit -m \"Bump version from v0.8.4 to v0.9.0\"\npytest\n</code></pre> <p>Then tag and push</p> <pre><code>git tag v0.9.0\ngit push --tags\ngit push\n</code></pre>"},{"location":"dev_docs/contributing/#3-update-documentation","title":"3. Update Documentation","text":"<p>First check if the documentation has any issues via</p> <pre><code>mike deploy 0.9.0 latest -u\nmike serve\n</code></pre> <p>and then looking at it.</p> <p>Afterwards, publish it via</p> <pre><code>mike deploy 0.9.0 latest -up\n</code></pre>"},{"location":"dev_docs/contributing/#4-publish-on-pypi","title":"4. Publish on PyPI","text":"<p>To publish to PyPI:</p> <ol> <li>Get publishing rights, e.g., asking Danny or Maciej or Neeratyoy.</li> <li>Be careful, once on PyPI we can not change things.</li> <li>Run</li> </ol> <pre><code>poetry publish --build\n</code></pre> <p>This will ask for your PyPI credentials.</p>"},{"location":"dev_docs/roadmap/","title":"Roadmap","text":""},{"location":"dev_docs/roadmap/#next-up","title":"Next up","text":""},{"location":"dev_docs/roadmap/#features","title":"Features","text":"<ul> <li>Improve large scale experience</li> <li>Result saving function (Samir)</li> <li>Priorband default sampling / pass evaluated configs to neps.run (Samir)</li> <li>Document large scale</li> <li>Evaluate and maybe improve ease-of-use of NePS for DDP (Gopalji)</li> <li>Optimize dependencies (Anton)</li> <li>Tensorboard st no one has to Touch it anymore (Tarek)</li> </ul>"},{"location":"dev_docs/roadmap/#fixes","title":"Fixes","text":"<ul> <li>ignore_errors should work seamlessly with all optimizers, also check different error handling Flags (Gopalji)</li> <li>Install all dependencies to run core examples always (Anton)</li> </ul>"},{"location":"dev_docs/roadmap/#refactoring","title":"Refactoring","text":"<p>(Anton) - Rename: run_pipeline = evaluate_pipeline - Rename: loss = objective_to_minimize - Rename: default = prior, default_confidence = prior_confidence - Rename: budget = max_cost_total</p>"},{"location":"dev_docs/roadmap/#documentation","title":"Documentation","text":"<ul> <li>Update citations (also docs) (Danny)</li> <li>Notebooks add (Danny)</li> <li>Remove templates (Danny)e</li> <li>Rework readme (remove declarative API) (Danny)</li> <li>Improved examples</li> <li>New Lightning example (Gopalji)</li> <li>DDP examples (Gopalji)</li> <li>Larger examples (Gopalji)</li> <li>Tensorboard into new lightning example (Tarek)</li> <li>Example spawning cloud instances via run pipeline</li> </ul>"},{"location":"dev_docs/roadmap/#tests","title":"Tests","text":"<ul> <li>Pytest needs to work on a fresh install (Anton)</li> <li>Regression tests to run on cluster on each version release</li> </ul>"},{"location":"dev_docs/roadmap/#before-100-version","title":"Before 1.0.0 version","text":""},{"location":"dev_docs/roadmap/#features_1","title":"Features","text":"<ul> <li>Utility neps.clean to manage existing run results</li> <li>Generate pdf plot after each evaluation</li> <li>Finegrained control over user prior</li> <li>Print search space upon run</li> <li>Utility to generate code for best architecture</li> <li>Core algorithmic feature set (research)</li> </ul>"},{"location":"dev_docs/roadmap/#documentation_1","title":"Documentation","text":"<ul> <li>NAS documentation</li> <li>Optimizer pages (Anton, Neeratyoy)</li> </ul>"},{"location":"dev_docs/roadmap/#tests_1","title":"Tests","text":""},{"location":"dev_docs/roadmap/#later","title":"Later","text":""},{"location":"dev_docs/roadmap/#documentation_2","title":"Documentation","text":"<ul> <li>Keep a changelog, add to it before each release</li> </ul>"},{"location":"examples/","title":"Overview","text":"<ol> <li> <p>Basic usage examples demonstrate fundamental usage. Learn how to perform Hyperparameter Optimization (HPO), Neural Architecture Search (NAS), and Joint Architecture and Hyperparameter Search (JAHS). Understand how to analyze runs on a basic level, emphasizing that no neural network training is involved at this stage; the search is performed on functions to introduce NePS.</p> </li> <li> <p>Efficiency examples showcase how to enhance efficiency in NePS. Learn about expert priors, multi-fidelity, and parallelization to streamline your pipeline and optimize search processes.</p> </li> <li> <p>Convenience examples show tensorboard compatibility and its integration, explore the compatibility with PyTorch Lightning, and understand file management within the run pipeline function used in NePS.</p> </li> <li> <p>Experimental examples tailored for NePS contributors. These examples provide insights and practices for experimental scenarios.</p> </li> <li> <p>Templates to find a basic fill-in template to kickstart your hyperparameter search with NePS. Use this template as a foundation for your projects, saving time and ensuring a structured starting point.</p> </li> <li> <p>YAML usage examples to define NePS configurations and search spaces with YAML files, streamlining the setup and execution of experiments.</p> </li> </ol>"},{"location":"examples/basic_usage/analyse/","title":"Analyse","text":"<pre><code>\"\"\" How to generate a summary (neps.status) and visualizations (neps.plot) of a run.\n\nBefore running this example analysis, run the hyperparameters example with:\n\n    python -m neps_examples.basic_usage.hyperparameters\n\"\"\"\nimport neps\n\n# 1. At all times, NePS maintains several files in the root directory that are human\n# read-able and can be useful\n\n# 2. Printing a summary and reading in results.\n# Alternatively use `python -m neps.status results/hyperparameters_example`\nresults, pending_configs = neps.status(\"results/hyperparameters_example\")\nconfig_id = \"1\"\nprint(results[config_id].config)\nprint(results[config_id].result)\nprint(results[config_id].metadata)\n\n# 3. Get the summary as a dictionary\nsummary = neps.get_summary_dict(\"results/hyperparameters_example\")\nprint(summary)\n\n# 4. Generating plots to the root directory (results/hyperparameters_example)\n# Alternatively use `python -m neps.plot results/hyperparameters_example`\nneps.plot(\"results/hyperparameters_example\")\n</code></pre>"},{"location":"examples/basic_usage/architecture/","title":"Architecture","text":"<pre><code>raise NotImplementedError(\n    \"Support for graphs was temporarily removed, if you'd like to use a version\"\n    \" of NePS that supports graphs, please use version v0.12.2\"\n)\n\nimport logging\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\nfrom neps.search_spaces.architecture.primitives import AbstractPrimitive\n\n\nclass DownSampleBlock(AbstractPrimitive):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__(locals())\n        self.conv_a = ReLUConvBN(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1\n        )\n        self.conv_b = ReLUConvBN(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.downsample = nn.Sequential(\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False\n            ),\n        )\n\n    def forward(self, inputs):\n        basicblock = self.conv_a(inputs)\n        basicblock = self.conv_b(basicblock)\n        residual = self.downsample(inputs)\n        return residual + basicblock\n\n\nclass ReLUConvBN(AbstractPrimitive):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super().__init__(locals())\n\n        self.kernel_size = kernel_size\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels, affine=True, track_running_stats=True),\n        )\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass AvgPool(AbstractPrimitive):\n    def __init__(self, **kwargs):\n        super().__init__(kwargs)\n        self.op = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n    def forward(self, x):\n        return self.op(x)\n\n\nprimitives = {\n    \"Sequential15\": topos.get_sequential_n_edge(15),\n    \"DenseCell\": topos.get_dense_n_node_dag(4),\n    \"down\": {\"op\": DownSampleBlock},\n    \"avg_pool\": {\"op\": AvgPool},\n    \"id\": {\"op\": ops.Identity},\n    \"conv3x3\": {\"op\": ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ReLUConvBN, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0},\n}\n\n\nstructure = {\n    \"S\": [\"Sequential15(C, C, C, C, C, down, C, C, C, C, C, down, C, C, C, C, C)\"],\n    \"C\": [\"DenseCell(OPS, OPS, OPS, OPS, OPS, OPS)\"],\n    \"OPS\": [\"id\", \"conv3x3\", \"conv1x1\", \"avg_pool\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 16 if predecessor_values is None else predecessor_values[\"out_channels\"]\n    out_channels = in_channels * 2 if op_name == \"DownSampleBlock\" else in_channels\n    return dict(in_channels=in_channels, out_channels=out_channels)\n\n\ndef run_pipeline(architecture):\n    in_channels = 3\n    base_channels = 16\n    n_classes = 10\n    out_channels_factor = 4\n\n    # E.g., in shape = (N, 3, 32, 32) =&gt; out shape = (N, 10)\n    model = architecture.to_pytorch()\n    model = nn.Sequential(\n        nn.Conv2d(in_channels, base_channels, 3, padding=1, bias=False),\n        nn.BatchNorm2d(base_channels),\n        model,\n        nn.BatchNorm2d(base_channels * out_channels_factor),\n        nn.ReLU(inplace=True),\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(base_channels * out_channels_factor, n_classes),\n    )\n    return 1\n\n\npipeline_space = dict(\n    architecture=neps.Architecture(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n    )\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/architecture\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/basic_usage/architecture_and_hyperparameters/","title":"Architecture and hyperparameters","text":"<pre><code>raise NotImplementedError(\n    \"Support for graphs was temporarily removed, if you'd like to use a version\"\n    \" of NePS that supports graphs, please use version v0.12.2\"\n)\n\nimport logging\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\nfrom neps.search_spaces.architecture.primitives import AbstractPrimitive\n\n\nclass DownSampleBlock(AbstractPrimitive):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__(locals())\n        self.conv_a = ReLUConvBN(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1\n        )\n        self.conv_b = ReLUConvBN(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.downsample = nn.Sequential(\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False\n            ),\n        )\n\n    def forward(self, inputs):\n        basicblock = self.conv_a(inputs)\n        basicblock = self.conv_b(basicblock)\n        residual = self.downsample(inputs)\n        return residual + basicblock\n\n\nclass ReLUConvBN(AbstractPrimitive):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super().__init__(locals())\n\n        self.kernel_size = kernel_size\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels, affine=True, track_running_stats=True),\n        )\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass AvgPool(AbstractPrimitive):\n    def __init__(self, **kwargs):\n        super().__init__(kwargs)\n        self.op = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n    def forward(self, x):\n        return self.op(x)\n\n\nprimitives = {\n    \"Sequential15\": topos.get_sequential_n_edge(15),\n    \"DenseCell\": topos.get_dense_n_node_dag(4),\n    \"down\": {\"op\": DownSampleBlock},\n    \"avg_pool\": {\"op\": AvgPool},\n    \"id\": {\"op\": ops.Identity},\n    \"conv3x3\": {\"op\": ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ReLUConvBN, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0},\n}\n\n\nstructure = {\n    \"S\": [\"Sequential15(C, C, C, C, C, down, C, C, C, C, C, down, C, C, C, C, C)\"],\n    \"C\": [\"DenseCell(OPS, OPS, OPS, OPS, OPS, OPS)\"],\n    \"OPS\": [\"id\", \"conv3x3\", \"conv1x1\", \"avg_pool\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 16 if predecessor_values is None else predecessor_values[\"out_channels\"]\n    out_channels = in_channels * 2 if op_name == \"DownSampleBlock\" else in_channels\n    return dict(in_channels=in_channels, out_channels=out_channels)\n\n\ndef run_pipeline(**config):\n    optimizer = config[\"optimizer\"]\n    learning_rate = config[\"learning_rate\"]\n    model = config[\"architecture\"].to_pytorch()\n\n    target_params = 1531258\n    number_of_params = sum(p.numel() for p in model.parameters())\n    validation_error = abs(target_params - number_of_params) / target_params\n\n    target_lr = 10e-3\n    validation_error += abs(target_lr - learning_rate) / target_lr\n    validation_error += int(optimizer == \"sgd\")\n\n    return validation_error\n\n\npipeline_space = dict(\n    architecture=neps.Architecture(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n    ),\n    optimizer=neps.Categorical(choices=[\"sgd\", \"adam\"]),\n    learning_rate=neps.Float(lower=10e-7, upper=10e-3, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hyperparameters_architecture_example\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/basic_usage/hpo_usage_example/","title":"Hpo usage example","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\ndef run_pipeline(\n    float_name1,\n    float_name2,\n    categorical_name1,\n    categorical_name2,\n    integer_name1,\n    integer_name2,\n):\n    # neps optimize to find values that maximizes sum, for demonstration only\n    loss = -float(\n        np.sum(\n            [float_name1, float_name2, categorical_name1, integer_name1, integer_name2]\n        )\n    )\n    if categorical_name2 == \"a\":\n        loss += 1\n\n    return loss\n\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=\"search_space_example.yaml\",\n    root_directory=\"results/hyperparameters_example\",\n    post_run_summary=True,\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/basic_usage/hyperparameters/","title":"Hyperparameters","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(float1, float2, categorical, integer1, integer2):\n    loss = -float(np.sum([float1, float2, int(categorical), integer1, integer2]))\n    # time.sleep(0.7)  # For demonstration purposes\n    return loss\n\n\npipeline_space = dict(\n    float1=neps.Float(lower=0, upper=1),\n    float2=neps.Float(lower=-10, upper=10),\n    categorical=neps.Categorical(choices=[0, 1]),\n    integer1=neps.Integer(lower=0, upper=1),\n    integer2=neps.Integer(lower=1, upper=1000, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hyperparameters_example\",\n    post_run_summary=True,\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/convenience/logging_additional_info/","title":"Logging additional info","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(float1, float2, categorical, integer1, integer2):\n    start = time.time()\n    loss = -float(np.sum([float1, float2, int(categorical), integer1, integer2]))\n    end = time.time()\n    return {\n        \"loss\": loss,\n        \"info_dict\": {  # Optionally include additional information as an info_dict\n            \"train_time\": end - start,\n        },\n    }\n\n\npipeline_space = dict(\n    float1=neps.Float(lower=0, upper=1),\n    float2=neps.Float(lower=-10, upper=10),\n    categorical=neps.Categorical(choices=[0, 1]),\n    integer1=neps.Integer(lower=0, upper=1),\n    integer2=neps.Integer(lower=1, upper=1000, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/logging_additional_info\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/convenience/neps_tblogger_tutorial/","title":"Neps tblogger tutorial","text":"<pre><code>\"\"\"\nNePS tblogger With TensorBoard\n==============================\n\n1- Introduction\n---------------\nWelcome to the NePS tblogger with TensorBoard tutorial. This guide will walk you\nthrough the process of using the NePS tblogger class to monitor performance\ndata for different hyperparameter configurations during optimization.\n\nAssuming you have experience with NePS, this tutorial aims to showcase the power\nof visualization using tblogger. To go directly to that part, check lines 244-264\nor search for 'Start Tensorboard Logging'.\n\n2- Learning Objectives\n----------------------\nBy completing this tutorial, you will:\n\n- Understand the role of NePS tblogger in HPO and NAS.\n- Learn to define search spaces within NePS for different model configurations.\n- Build a comprehensive run pipeline to train and evaluate models.\n- Utilize TensorBoard to visualize and compare performance metrics of different\n  model configurations.\n\n3- Setup\n--------\nBefore we begin, ensure you have the necessary dependencies installed. To install\nthe 'NePS' package, use the following command:\n\n```bash\npip install neural-pipeline-search\n```\n\nAdditionally, note that 'NePS' does not include 'torchvision' as a dependency.\nYou can install it with this command:\n\n```bash\npip install torchvision\n```\n\nMake sure to download the torchvision version that fits with your pytorch\nversion. More info on this link:\n\nhttps://pypi.org/project/torchvision/\n\nThese dependencies ensure you have everything you need for this tutorial.\n\n\"\"\"\n\nimport logging\nimport random\nimport time\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.transforms import transforms\n\nimport neps\nfrom neps.plot.tensorboard_eval import tblogger\n\n\"\"\"\nSteps for a successful training pipeline:\n\n#1 Define the seeds for reproducibility.\n#2 Prepare the input data.\n#3 Design the model.\n#4 Design the pipeline search spaces.\n#5 Design the run pipeline function.\n#6 Use neps.run the run the entire search using your specified searcher.\n\nEach step will be covered in detail thourghout the code\n\n\"\"\"\n\n#############################################################\n# Definig the seeds for reproducibility\n\n\ndef set_seed(seed=123):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\n#############################################################\n# Prepare the input data. For this tutorial we use the MNIST dataset.\n\n\ndef MNIST(\n    batch_size: int = 256,\n    n_train_size: float = 0.9,\n    data_reduction_factor: float = 0.5,\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    # Download MNIST training and test datasets if not already downloaded.\n    train_dataset = torchvision.datasets.MNIST(\n        root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\n    )\n    test_dataset = torchvision.datasets.MNIST(\n        root=\"./data\", train=False, transform=transforms.ToTensor(), download=True\n    )\n\n    # Determine the size of the reduced training dataset for faster training\n    # and calculate the size of the training subset from the reduced dataset\n    reduced_dataset_train = int(data_reduction_factor * len(train_dataset))\n    train_size = int(n_train_size * reduced_dataset_train)\n\n    # Create a random sampler for the training and validation data\n    train_sampler = SubsetRandomSampler(range(train_size))\n    valid_sampler = SubsetRandomSampler(range(train_size, reduced_dataset_train))\n\n    # Create DataLoaders for training, validation, and test datasets.\n    train_dataloader = DataLoader(\n        dataset=train_dataset, batch_size=batch_size, shuffle=False, sampler=train_sampler\n    )\n    val_dataloader = DataLoader(\n        dataset=train_dataset, batch_size=batch_size, shuffle=False, sampler=valid_sampler\n    )\n    test_dataloader = DataLoader(\n        dataset=test_dataset, batch_size=batch_size, shuffle=False\n    )\n\n    return train_dataloader, val_dataloader, test_dataloader\n\n\n#############################################################\n# Design small MLP model to be able to represent the input data.\n\n\nclass MLP(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.linear1 = nn.Linear(in_features=784, out_features=196)\n        self.linear2 = nn.Linear(in_features=196, out_features=98)\n        self.linear3 = nn.Linear(in_features=98, out_features=10)\n\n    def forward(self, x: torch.Tensor):\n        # Flattening the grayscaled image from 1x28x28 (CxWxH) to 784.\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.linear1(x))\n        x = F.relu(self.linear2(x))\n        x = self.linear3(x)\n\n        return x\n\n\n#############################################################\n# Define the training step. Return the validation error and\n# misclassified images.\n\n\ndef loss_ev(model: nn.Module, data_loader: DataLoader) -&gt; float:\n    # Set the model in evaluation mode (no gradient computation).\n    model.eval()\n\n    correct = 0\n    total = 0\n\n    # Disable gradient computation for efficiency.\n    with torch.no_grad():\n        for x, y in data_loader:\n            output = model(x)\n\n            # Get the predicted class for each input.\n            _, predicted = torch.max(output.data, 1)\n\n            # Update the correct and total counts.\n            correct += (predicted == y).sum().item()\n            total += y.size(0)\n\n    # Calculate the accuracy and return the error rate.\n    accuracy = correct / total\n    error_rate = 1 - accuracy\n    return error_rate\n\n\ndef training(\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    criterion: nn.Module,\n    train_loader: DataLoader,\n    validation_loader: DataLoader,\n) -&gt; Tuple[float, torch.Tensor]:\n    \"\"\"\n    Function that trains the model for one epoch and evaluates the model\n    on the validation set.\n\n    Args:\n        model (nn.Module): Model to be trained.\n        optimizer (torch.optim.Optimizer): Optimizer used to train the weights.\n        criterion (nn.Module) : Loss function to use.\n        train_loader (DataLoader): DataLoader containing the training data.\n        validation_loader (DataLoader): DataLoader containing the validation data.\n\n    Returns:\n    Tuple[float, torch.Tensor]: A tuple containing the validation error (float)\n                                and a tensor of misclassified images.\n    \"\"\"\n    incorrect_images = []\n    model.train()\n\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n\n        predicted_labels = torch.argmax(output, dim=1)\n        incorrect_mask = predicted_labels != y\n        incorrect_images.append(x[incorrect_mask])\n\n    # Calculate validation loss using the loss_ev function.\n    validation_loss = loss_ev(model, validation_loader)\n\n    # Return the misclassified image by during model training.\n    if len(incorrect_images) &gt; 0:\n        incorrect_images = torch.cat(incorrect_images, dim=0)\n\n    return (validation_loss, incorrect_images)\n\n\n#############################################################\n# Design the pipeline search spaces.\n\n\ndef pipeline_space() -&gt; dict:\n    pipeline = dict(\n        lr=neps.Float(lower=1e-5, upper=1e-1, log=True),\n        optim=neps.Categorical(choices=[\"Adam\", \"SGD\"]),\n        weight_decay=neps.Float(lower=1e-4, upper=1e-1, log=True),\n    )\n\n    return pipeline\n\n\n#############################################################\n# Implement the pipeline run search.\n\n\ndef run_pipeline(lr, optim, weight_decay):\n    # Create the network model.\n    model = MLP()\n\n    if optim == \"Adam\":\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optim == \"SGD\":\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(\n            \"Optimizer choices are defined differently in the pipeline_space\"\n        )\n\n    max_epochs = 2  # Epochs to train the model, can be parameterized as fidelity\n\n    # Load the MNIST dataset for training, validation, and testing.\n    train_loader, validation_loader, test_loader = MNIST(\n        batch_size=96, n_train_size=0.6, data_reduction_factor=0.75\n    )\n\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.75)\n    criterion = nn.CrossEntropyLoss()\n\n    for i in range(max_epochs):\n        loss, miss_img = training(\n            optimizer=optimizer,\n            model=model,\n            criterion=criterion,\n            train_loader=train_loader,\n            validation_loader=validation_loader,\n        )\n\n        # Gathering the gradient mean in each layer\n        mean_gradient = []\n        for layer in model.children():\n            layer_gradients = [param.grad for param in layer.parameters()]\n            if layer_gradients:\n                mean_gradient.append(\n                    torch.mean(torch.cat([grad.view(-1) for grad in layer_gradients]))\n                )\n\n        ###################### Start Tensorboard Logging ######################\n\n        # The following tblogge` will result in:\n\n        # 1. Loss curves of each configuration at each epoch.\n        # 2. Decay curve of the learning rate at each epoch.\n        # 3. Wrongly classified images by the model.\n        # 4. First two layer gradients passed as scalar configs.\n\n        tblogger.log(\n            loss=loss,\n            current_epoch=i,\n            write_summary_incumbent=False,  # Set to `True` for a live incumbent trajectory.\n            writer_config_scalar=True,  # Set to `True` for a live loss trajectory for each config.\n            writer_config_hparam=True,  # Set to `True` for live parallel coordinate, scatter plot matrix, and table view.\n            # Appending extra data\n            extra_data={\n                \"lr_decay\": tblogger.scalar_logging(value=scheduler.get_last_lr()[0]),\n                \"miss_img\": tblogger.image_logging(image=miss_img, counter=2, seed=2),\n                \"layer_gradient1\": tblogger.scalar_logging(value=mean_gradient[0]),\n                \"layer_gradient2\": tblogger.scalar_logging(value=mean_gradient[1]),\n            },\n        )\n\n        ###################### End Tensorboard Logging ######################\n\n        scheduler.step()\n\n        print(f\"  Epoch {i + 1} / {max_epochs} Val Error: {loss} \")\n\n    # Calculate training and test accuracy.\n    train_accuracy = loss_ev(model, train_loader)\n    test_accuracy = loss_ev(model, test_loader)\n\n    # Return a dictionary with relevant metrics and information.\n    return {\n        \"loss\": loss,\n        \"info_dict\": {\n            \"train_accuracy\": train_accuracy,\n            \"test_accuracy\": test_accuracy,\n            \"cost\": max_epochs,\n        },\n    }\n\n\n#############################################################\n# Running neps with BO as the searcher.\n\nif __name__ == \"__main__\":\n    \"\"\"\n    When running this code without any arguments, it will by default\n    run bayesian optimization with 3 evaluations total.\n\n    ```bash\n    python neps_tblogger_tutorial.py\n    ```\n    \"\"\"\n    start_time = time.time()\n\n    set_seed(112)\n    logging.basicConfig(level=logging.INFO)\n\n    # To check the status of tblogger:\n    # tblogger.get_status()\n\n    run_args = dict(\n        run_pipeline=run_pipeline,\n        pipeline_space=pipeline_space(),\n        root_directory=\"results/neps_tblogger_example\",\n        searcher=\"random_search\",\n    )\n\n    neps.run(\n        **run_args,\n        max_evaluations_total=2,\n    )\n\n    \"\"\"\n    To check live plots during this search, please open a new terminal\n    and make sure to be at the same level directory of your project and\n    run the following command on the file created by neps root_directory.\n\n    ```bash:\n    tensorboard --logdir output\n    ```\n\n    To be able to check the visualization of tensorboard make sure to\n    follow the local link provided.\n\n    http://localhost:6006/\n\n    Double-check the directory path you've provided; if you're not seeing\n    any visualizations and have followed the tutorial closely, there\n    might be an error in the directory specification. Remember that\n    TensorBoard runs in the command line without checking if the directory\n    actually exists.\n    \"\"\"\n\n    # Disables tblogger for the continued run\n    tblogger.disable()\n\n    neps.run(\n        **run_args,\n        max_evaluations_total=3,  # continues the previous run for 1 more evaluation\n    )\n\n    \"\"\"\n    This second run of one more configuration will not add to the tensorboard logs.\n    \"\"\"\n\n    end_time = time.time()  # Record the end time\n    execution_time = end_time - start_time\n    logging.info(f\"Execution time: {execution_time} seconds\\n\")\n</code></pre>"},{"location":"examples/convenience/neps_x_lightning/","title":"Neps x lightning","text":"<pre><code>\"\"\"\nExploring NePS Compatibility with PyTorch Lightning\n=======================================================\n\n1. Introduction:\n----------------\nWelcome to this tutorial on utilizing NePS-generated files and directories\nin conjunction with PyTorch Lightning.\n\n2. Setup:\n---------\nEnsure you have the necessary dependencies installed. You can install the 'NePS'\npackage by executing the following command:\n\n```bash\npip install neural-pipeline-search\n```\n\nAdditionally, note that 'NePS' does not include 'torchvision' as a dependency.\nYou can install it with this command:\n\n```bash\npip install torchvision==0.14\n```\n\nMake sure to download the torchvision version that is compatible with your\npytorch version. More info on this link:\n\nhttps://pypi.org/project/torchvision/\n\nAdditionally, you will need to install the PyTorch Lightning package. This can\nbe achieved with the following command:\n\n```bash\npip install lightning\n```\n\nThese dependencies ensure you have everything you need for this tutorial.\n\"\"\"\nimport argparse\nimport glob\nimport logging\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport lightning as L\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import TensorBoardLogger\nfrom torch.utils.data import SubsetRandomSampler\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import transforms\n\nimport neps\nfrom neps.utils.common import get_initial_directory, load_lightning_checkpoint\n\n#############################################################\n# Definig the seeds for reproducibility\n\n\ndef set_seed(seed=123):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\n#############################################################\n# Define the lightning model\n\n\nclass LitMNIST(L.LightningModule):\n    def __init__(\n        self,\n        configuration: dict,\n        n_train: int = 8192,\n        n_valid: int = 1024,\n    ):\n        super().__init__()\n\n        # Initialize the model's hyperparameters with the configuration\n        self.save_hyperparameters(configuration)\n\n        self.n_train = n_train\n        self.n_valid = n_valid\n\n        # Define data transformation and loss function\n        self.transform = transforms.ToTensor()\n        self.criterion = nn.NLLLoss()\n\n        # Define the model's architecture\n        self.linear1 = nn.Linear(in_features=784, out_features=392)\n        self.linear2 = nn.Linear(in_features=392, out_features=196)\n        self.linear3 = nn.Linear(in_features=196, out_features=10)\n\n        # Define PyTorch Lightning metrics for training, validation, and testing\n        metric = Accuracy(task=\"multiclass\", num_classes=10)\n        self.train_accuracy = metric.clone()\n        self.val_accuracy = metric.clone()\n        self.test_accuracy = metric.clone()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Forward pass function\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.linear1(x))\n        x = F.relu(self.linear2(x))\n        x = self.linear3(x)\n\n        return F.log_softmax(x, dim=1)\n\n    def common_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Perform a forward pass and compute loss, predictions, and get the ground\n        truth labels for a batch of data.\n        \"\"\"\n        x, y = batch\n        logits = self.forward(x)\n        loss = self.criterion(logits, y)\n        preds = torch.argmax(logits, dim=1)\n\n        return loss, preds, y\n\n    def training_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; float:\n        loss, preds, y = self.common_step(batch, batch_idx)\n        self.train_accuracy.update(preds, y)\n\n        self.log_dict(\n            {\"train_loss\": loss, \"train_acc\": self.val_accuracy.compute()},\n            on_epoch=True,\n            on_step=False,\n            prog_bar=True,\n        )\n\n        return loss\n\n    def validation_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        loss, preds, y = self.common_step(batch, batch_idx)\n        self.val_accuracy.update(preds, y)\n\n        self.log_dict(\n            {\"val_loss\": loss, \"val_acc\": self.val_accuracy.compute()},\n            on_epoch=True,\n            on_step=False,\n            prog_bar=True,\n        )\n\n    def test_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        _, preds, y = self.common_step(batch, batch_idx)\n        self.test_accuracy.update(preds, y)\n\n        self.log(name=\"test_acc\", value=self.test_accuracy.compute())\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        # Configure and return the optimizer based on the configuration\n        if self.hparams.optimizer == \"Adam\":\n            optimizer = torch.optim.Adam(\n                self.parameters(),\n                lr=self.hparams.lr,\n                weight_decay=self.hparams.weight_decay,\n            )\n        elif self.hparams.optimizer == \"SGD\":\n            optimizer = torch.optim.SGD(\n                self.parameters(),\n                lr=self.hparams.lr,\n                weight_decay=self.hparams.weight_decay,\n            )\n        else:\n            raise ValueError(\n                \"The optimizer choices is not one of the available optimizers\"\n            )\n        return optimizer\n\n    def on_train_end(self):\n        # Get the metric at the end of the training and log it with respect to\n        # it's hyperparameters\n        val_acc_metric = {\n            \"val_accuracy\": self.val_accuracy.compute(),\n        }\n\n        # Log hyperparameters\n        self.logger.log_hyperparams(self.hparams, metrics=val_acc_metric)\n\n    def prepare_data(self) -&gt; None:\n        # Downloading the dataste if not already downloaded\n        MNIST(self.hparams.data_dir, train=True, download=True)\n        MNIST(self.hparams.data_dir, train=False, download=True)\n\n    def setup(self, stage: str = None) -&gt; None:\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            self.mnist_full = MNIST(\n                self.hparams.data_dir, train=True, transform=self.transform\n            )\n\n            # Create random subsets of the training dataset for validation.\n            self.train_sampler = SubsetRandomSampler(range(self.n_train))\n            self.val_sampler = SubsetRandomSampler(\n                range(self.n_train, self.n_train + self.n_valid)\n            )\n\n        # Assign test dataset for use in dataloader\n        if stage == \"test\" or stage is None:\n            self.mnist_test = MNIST(\n                self.hparams.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self) -&gt; DataLoader:\n        return DataLoader(\n            self.mnist_full,\n            batch_size=self.hparams.batch_size,\n            sampler=self.train_sampler,\n            num_workers=16,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        return DataLoader(\n            self.mnist_full,\n            batch_size=self.hparams.batch_size,\n            sampler=self.val_sampler,\n            num_workers=16,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        return DataLoader(\n            self.mnist_test,\n            batch_size=self.hparams.batch_size,\n            num_workers=16,\n        )\n\n\n#############################################################\n# Define search space\n\n\ndef search_space() -&gt; dict:\n    # Define a dictionary to represent the hyperparameter search space\n    space = dict(\n        data_dir=neps.Constant(\"./data\"),\n        batch_size=neps.Constant(64),\n        lr=neps.Float(lower=1e-5, upper=1e-2, log=True, default=1e-3),\n        weight_decay=neps.Float(\n            lower=1e-5, upper=1e-3, log=True, default=5e-4\n        ),\n        optimizer=neps.Categorical(choices=[\"Adam\", \"SGD\"], default=\"Adam\"),\n        epochs=neps.Integer(lower=1, upper=9, log=False, is_fidelity=True),\n    )\n    return space\n\n\n#############################################################\n# Define the run pipeline function\n\n\ndef run_pipeline(pipeline_directory, previous_pipeline_directory, **config) -&gt; dict:\n    # Initialize the first directory to store the event and checkpoints files\n    init_dir = get_initial_directory(pipeline_directory)\n    checkpoint_dir = init_dir / \"checkpoints\"\n\n    # Initialize the model and checkpoint dir\n    model = LitMNIST(config)\n\n    # Create the TensorBoard logger for logging\n    logger = TensorBoardLogger(\n        save_dir=init_dir, name=\"data\", version=\"logs\", default_hp_metric=False\n    )\n\n    # Add checkpoints at the end of training\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=checkpoint_dir,\n        filename=\"{epoch}-{val_loss:.2f}\",\n    )\n\n    # Use this function to load the previous checkpoint if it exists\n    checkpoint_path, checkpoint = load_lightning_checkpoint(\n        previous_pipeline_directory=previous_pipeline_directory,\n        checkpoint_dir=checkpoint_dir,\n    )\n\n    if checkpoint is None:\n        previously_spent_epochs = 0\n    else:\n        previously_spent_epochs = checkpoint[\"epoch\"]\n\n    # Create a PyTorch Lightning Trainer\n    epochs = config[\"epochs\"]\n\n    trainer = L.Trainer(\n        logger=logger,\n        max_epochs=epochs,\n        callbacks=[checkpoint_callback],\n    )\n\n    # Train the model and retrieve training/validation metrics\n    if checkpoint_path:\n        trainer.fit(model, ckpt_path=checkpoint_path)\n    else:\n        trainer.fit(model)\n\n    train_accuracy = trainer.logged_metrics.get(\"train_acc\", None)\n    val_loss = trainer.logged_metrics.get(\"val_loss\", None)\n    val_accuracy = trainer.logged_metrics.get(\"val_acc\", None)\n\n    # Test the model and retrieve test metrics\n    trainer.test(model)\n\n    test_accuracy = trainer.logged_metrics.get(\"test_acc\", None)\n\n    return {\n        \"loss\": val_loss,\n        \"cost\": epochs - previously_spent_epochs,\n        \"info_dict\": {\n            \"train_accuracy\": train_accuracy,\n            \"val_accuracy\": val_accuracy,\n            \"test_accuracy\": test_accuracy,\n        },\n    }\n\n\nif __name__ == \"__main__\":\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--max_evaluations_total\",\n        type=int,\n        default=15,\n        help=\"Number of different configurations to train\",\n    )\n    args = parser.parse_args()\n\n    # Initialize the logger and record start time\n    start_time = time.time()\n    set_seed(112)\n    logging.basicConfig(level=logging.INFO)\n\n    # Run NePS with specified parameters\n    neps.run(\n        run_pipeline=run_pipeline,\n        pipeline_space=search_space(),\n        root_directory=\"results/hyperband\",\n        max_evaluations_total=args.max_evaluations_total,\n        searcher=\"hyperband\",\n    )\n\n    # Record the end time and calculate execution time\n    end_time = time.time()\n    execution_time = end_time - start_time\n\n    # Log the execution time\n    logging.info(f\"Execution time: {execution_time} seconds\")\n</code></pre>"},{"location":"examples/convenience/running_on_slurm_scripts/","title":"Running on slurm scripts","text":"<pre><code>\"\"\" Example that shows HPO with NePS based on a slurm script.\n\"\"\"\n\nimport logging\nimport os\nimport time\nfrom pathlib import Path\n\nimport neps\n\n\ndef _ask_to_submit_slurm_script(pipeline_directory: Path, script: str):\n    script_path = pipeline_directory / \"submit.sh\"\n    logging.info(f\"Submitting the script {script_path} (see below): \\n\\n{script}\")\n\n    # You may want to remove the below check and not ask before submitting every time\n    if input(\"Ok to submit? [Y|n] -- \").lower() in {\"y\", \"\"}:\n        script_path.write_text(script)\n        os.system(f\"sbatch {script_path}\")\n    else:\n        raise ValueError(\"We generated a slurm script that should not be submitted.\")\n\n\ndef _get_validation_error(pipeline_directory: Path):\n    validation_error_file = pipeline_directory / \"validation_error_from_slurm_job.txt\"\n    if validation_error_file.exists():\n        return float(validation_error_file.read_text())\n    return None\n\n\ndef run_pipeline_via_slurm(\n    pipeline_directory: Path, optimizer: str, learning_rate: float\n):\n    script = f\"\"\"#!/bin/bash\n#SBATCH --time 0-00:05\n#SBATCH --job-name test\n#SBATCH --partition cpu-cascadelake\n#SBATCH --error \"{pipeline_directory}/%N_%A_%x_%a.oe\"\n#SBATCH --output \"{pipeline_directory}/%N_%A_%x_%a.oe\"\n# Plugin your python script here\npython -c \"print('Learning rate {learning_rate} and optimizer {optimizer}')\"\n# At the end of training and validation create this file\necho -10 &gt; {pipeline_directory}/validation_error_from_slurm_job.txt\n\"\"\"\n\n    # Now we submit and wait until the job has created validation_error_from_slurm_job.txt\n    _ask_to_submit_slurm_script(pipeline_directory, script)\n    while validation_error := _get_validation_error(pipeline_directory) is None:\n        logging.info(\"Waiting until the job has finished.\")\n        time.sleep(60)  # Adjust to something reasonable\n    return validation_error\n\n\npipeline_space = dict(\n    optimizer=neps.Categorical(choices=[\"sgd\", \"adam\"]),\n    learning_rate=neps.Float(lower=10e-7, upper=10e-3, log=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline_via_slurm,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/slurm_script_example\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/convenience/working_directory_per_pipeline/","title":"Working directory per pipeline","text":"<pre><code>import logging\nfrom pathlib import Path\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(pipeline_directory: Path, float1, categorical, integer1):\n    # When adding pipeline_directory to run_pipeline, neps detects its presence and\n    # passes a directory unique for each pipeline configuration. You can then use this\n    # pipeline_directory to create / save files pertaining to a specific pipeline, e.g.:\n    weight_file = pipeline_directory / \"weight_file.txt\"\n    weight_file.write_text(\"0\")\n\n    loss = -float(np.sum([float1, int(categorical), integer1]))\n    return loss\n\n\npipeline_space = dict(\n    float1=neps.Float(lower=0, upper=1),\n    categorical=neps.Categorical(choices=[0, 1]),\n    integer1=neps.Integer(lower=0, upper=1),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/working_directory_per_pipeline\",\n    max_evaluations_total=5,\n)\n</code></pre>"},{"location":"examples/declarative_usage/","title":"Declarative Usage in NePS for Neural Network Optimization","text":"<p>This folder contains examples and templates for optimizing neural networks using NePS, configured via YAML files. These configurations allow easy adjustments to experiment parameters and search spaces, enabling fine-tuning of your models without modifying Python code.</p>"},{"location":"examples/declarative_usage/#hpo_examplepy","title":"<code>hpo_example.py</code>","text":"<p>This Python script demonstrates how to integrate NePS with a neural network training pipeline for hyperparameter optimization. It utilizes a YAML configuration file to set up and run the experiments.</p> <pre><code>import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport neps\n\n\"\"\"\nThis script demonstrates the integration of a simple neural network training pipeline\nwith NePS for hyperparameter optimization, focusing on the MNIST dataset.\n\n- SimpleNN Class: A PyTorch neural network model that constructs a feedforward\n  architecture based on input size, number of layers, and neurons per layer.\n\n- Training Pipeline: A function that takes hyperparameters (number of layers, neurons,\n  epochs, learning rate, optimizer type) to train and validate the SimpleNN model on\n  the MNIST dataset. Supports Adam and SGD optimizers.\n\n- NEPS Integration: Shows how to automate hyperparameter tuning using NEPS. Configuration\n  settings are specified in a YAML file ('run_args.yaml'), which is passed to the NePS\n  optimization process via the `run_args` parameter.\n\nUsage:\n1. Define model architecture and training logic in `SimpleNN` and `training_pipeline`.\n2. Configure hyperparameters and optimization settings in 'config.yaml'.\n3. Launch optimization with NePS by calling `neps.run`, specifying the training pipeline,\nand configuration file(config.yaml).\n\"\"\"\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, num_layers, num_neurons):\n        super().__init__()\n        layers = [nn.Flatten()]\n\n        for _ in range(num_layers):\n            layers.append(nn.Linear(input_size, num_neurons))\n            layers.append(nn.ReLU())\n            input_size = num_neurons  # Set input size for the next layer\n\n        layers.append(nn.Linear(num_neurons, 10))  # Output layer for 10 classes\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndef training_pipeline(num_layers, num_neurons, epochs, learning_rate, optimizer):\n    \"\"\"\n    Trains and validates a simple neural network on the MNIST dataset.\n\n    Args:\n        num_layers (int): Number of hidden layers in the network.\n        num_neurons (int): Number of neurons in each hidden layer.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate for the optimizer.\n        optimizer (str): Name of the optimizer to use ('adam' or 'sgd').\n\n    Returns:\n        float: The average loss over the validation set after training.\n\n    Raises:\n        KeyError: If the specified optimizer is not supported.\n    \"\"\"\n    # Transformations applied on each image\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                (0.1307,), (0.3081,)\n            ),  # Mean and Std Deviation for MNIST\n        ]\n    )\n\n    # Loading MNIST dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=True, download=True, transform=transform\n    )\n    train_set, val_set = torch.utils.data.random_split(dataset, [55000, 5000])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=1000, shuffle=False)\n\n    model = SimpleNN(28 * 28, num_layers, num_neurons)\n    criterion = nn.CrossEntropyLoss()\n\n    # Select optimizer\n    if optimizer == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    else:\n        raise KeyError(f\"optimizer {optimizer} is not available\")\n\n    # Training loop\n\n    for epoch in range(epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            output = model(data)\n            val_loss += criterion(output, target).item()\n\n    val_loss /= len(val_loader.dataset)\n    return val_loss\n\n\nif __name__ == \"__main__\":\n    # Configure logging to display important messages from NePS.\n    logging.basicConfig(level=logging.INFO)\n\n    # Run optimization using neps.run(...). Arguments can be provided directly to neps.run\n    # or defined in a configuration file (e.g., \"config.yaml\") passed through\n    # the run_args parameter.\n    neps.run(training_pipeline, run_args=\"config.yaml\")\n</code></pre>"},{"location":"examples/declarative_usage/#configyaml","title":"<code>config.yaml</code>","text":"<p>This YAML file defines the NePS arguments for the experiment. By editing this file, users can customize their experiments without modifying the Python script.</p> <pre><code>experiment:\n  root_directory: \"results/example_run\"\n  max_evaluations_total: 20\n  overwrite_working_directory: true\n  post_run_summary: true\n  development_stage_id: \"beta\"\n\npipeline_space:\n  epochs: 5\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true\n  num_layers:\n    lower: 1\n    upper: 5\n  optimizer:\n    choices: [\"adam\", \"sgd\"]\n  num_neurons:\n    lower: 64\n    upper: 128\n\nsearcher:\n  strategy: \"bayesian_optimization\"\n  initial_design_size: 5\n  surrogate_model: gp\n</code></pre>"},{"location":"examples/declarative_usage/#quick-start-guide","title":"Quick Start Guide","text":"<ol> <li>Review the YAML File: Examine <code>config.yaml</code> to understand the available configurations and how they are structured.</li> <li>Run the Example Script: Execute hpo_example.py, by providing <code>config.yaml</code> via the run_args agrument to NePS.    This will initiate a hyperparameter optimization task based on your YAML configurations.</li> <li>Modify YAML File: Experiment with adjusting the parameters in the YAML file to see how changes affect your    search experiments. This is a great way to learn about the impact of different configurations on your results.</li> </ol> <p>By following these steps and utilizing the provided YAML files, you'll be able to efficiently set up, run, and modify your NePS experiments. Enjoy the flexibility and simplicity that comes with managing your experiment configurations in YAML!</p>"},{"location":"examples/declarative_usage/hpo_example/","title":"Hpo example","text":"<pre><code>import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport neps\n\n\"\"\"\nThis script demonstrates the integration of a simple neural network training pipeline\nwith NePS for hyperparameter optimization, focusing on the MNIST dataset.\n\n- SimpleNN Class: A PyTorch neural network model that constructs a feedforward\n  architecture based on input size, number of layers, and neurons per layer.\n\n- Training Pipeline: A function that takes hyperparameters (number of layers, neurons,\n  epochs, learning rate, optimizer type) to train and validate the SimpleNN model on\n  the MNIST dataset. Supports Adam and SGD optimizers.\n\n- NEPS Integration: Shows how to automate hyperparameter tuning using NEPS. Configuration\n  settings are specified in a YAML file ('run_args.yaml'), which is passed to the NePS\n  optimization process via the `run_args` parameter.\n\nUsage:\n1. Define model architecture and training logic in `SimpleNN` and `training_pipeline`.\n2. Configure hyperparameters and optimization settings in 'config.yaml'.\n3. Launch optimization with NePS by calling `neps.run`, specifying the training pipeline,\nand configuration file(config.yaml).\n\"\"\"\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, num_layers, num_neurons):\n        super().__init__()\n        layers = [nn.Flatten()]\n\n        for _ in range(num_layers):\n            layers.append(nn.Linear(input_size, num_neurons))\n            layers.append(nn.ReLU())\n            input_size = num_neurons  # Set input size for the next layer\n\n        layers.append(nn.Linear(num_neurons, 10))  # Output layer for 10 classes\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndef training_pipeline(num_layers, num_neurons, epochs, learning_rate, optimizer):\n    \"\"\"\n    Trains and validates a simple neural network on the MNIST dataset.\n\n    Args:\n        num_layers (int): Number of hidden layers in the network.\n        num_neurons (int): Number of neurons in each hidden layer.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate for the optimizer.\n        optimizer (str): Name of the optimizer to use ('adam' or 'sgd').\n\n    Returns:\n        float: The average loss over the validation set after training.\n\n    Raises:\n        KeyError: If the specified optimizer is not supported.\n    \"\"\"\n    # Transformations applied on each image\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                (0.1307,), (0.3081,)\n            ),  # Mean and Std Deviation for MNIST\n        ]\n    )\n\n    # Loading MNIST dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=True, download=True, transform=transform\n    )\n    train_set, val_set = torch.utils.data.random_split(dataset, [55000, 5000])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=1000, shuffle=False)\n\n    model = SimpleNN(28 * 28, num_layers, num_neurons)\n    criterion = nn.CrossEntropyLoss()\n\n    # Select optimizer\n    if optimizer == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer == \"sgd\":\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    else:\n        raise KeyError(f\"optimizer {optimizer} is not available\")\n\n    # Training loop\n\n    for epoch in range(epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            output = model(data)\n            val_loss += criterion(output, target).item()\n\n    val_loss /= len(val_loader.dataset)\n    return val_loss\n\n\nif __name__ == \"__main__\":\n    # Configure logging to display important messages from NePS.\n    logging.basicConfig(level=logging.INFO)\n\n    # Run optimization using neps.run(...). Arguments can be provided directly to neps.run\n    # or defined in a configuration file (e.g., \"config.yaml\") passed through\n    # the run_args parameter.\n    neps.run(training_pipeline, run_args=\"config.yaml\")\n</code></pre>"},{"location":"examples/efficiency/","title":"Parallelization","text":"<p>In order to run neps in parallel on multiple processes or multiple machines, simply call <code>neps.run</code> multiple times. All calls to <code>neps.run</code> need to use the same <code>root_directory</code> on the same filesystem to synchronize between the <code>neps.run</code>'s.</p> <p>For example, start the HPO example in two shells from the same directory as below.</p> <p>In shell 1:</p> <pre><code>python -m neps_examples.basic_usage.hyperparameters\n</code></pre> <p>In shell 2:</p> <pre><code>python -m neps_examples.basic_usage.hyperparameters\n</code></pre>"},{"location":"examples/efficiency/expert_priors_for_hyperparameters/","title":"Expert priors for hyperparameters","text":"<pre><code>import logging\nimport time\n\nimport neps\n\n\ndef run_pipeline(some_float, some_integer, some_cat):\n    start = time.time()\n    if some_cat != \"a\":\n        y = some_float + some_integer\n    else:\n        y = -some_float - some_integer\n    end = time.time()\n    return {\n        \"loss\": y,\n        \"info_dict\": {\n            \"test_score\": y,\n            \"train_time\": end - start,\n        },\n    }\n\n\n# neps uses the default values and a confidence in this default value to construct a prior\n# that speeds up the search\npipeline_space = dict(\n    some_float=neps.Float(\n        lower=1, upper=1000, log=True, default=900, default_confidence=\"medium\"\n    ),\n    some_integer=neps.Integer(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    some_cat=neps.Categorical(\n        choices=[\"a\", \"b\", \"c\"], default=\"a\", default_confidence=\"high\"\n    ),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/user_priors_example\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/efficiency/freeze_thaw/","title":"Freeze thaw","text":"<pre><code>import logging\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport neps\nfrom neps import tblogger\nfrom neps.plot.plot3D import Plotter3D\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, num_layers, num_neurons):\n        super().__init__()\n        layers = [nn.Flatten()]\n\n        for _ in range(num_layers):\n            layers.append(nn.Linear(input_size, num_neurons))\n            layers.append(nn.ReLU())\n            input_size = num_neurons  # Set input size for the next layer\n\n        layers.append(nn.Linear(num_neurons, 10))  # Output layer for 10 classes\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndef training_pipeline(\n    pipeline_directory,\n    previous_pipeline_directory,\n    num_layers,\n    num_neurons,\n    epochs,\n    learning_rate,\n    weight_decay\n):\n    \"\"\"\n    Trains and validates a simple neural network on the MNIST dataset.\n\n    Args:\n        num_layers (int): Number of hidden layers in the network.\n        num_neurons (int): Number of neurons in each hidden layer.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate for the optimizer.\n        optimizer (str): Name of the optimizer to use ('adam' or 'sgd').\n\n    Returns:\n        float: The average loss over the validation set after training.\n\n    Raises:\n        KeyError: If the specified optimizer is not supported.\n    \"\"\"\n    # Transformations applied on each image\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                (0.1307,), (0.3081,)\n            ),  # Mean and Std Deviation for MNIST\n        ]\n    )\n\n    # Loading MNIST dataset\n    dataset = datasets.MNIST(\n        root=\"./.data\", train=True, download=True, transform=transform\n    )\n    train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=1000, shuffle=False)\n\n    model = SimpleNN(28 * 28, num_layers, num_neurons)\n    criterion = nn.CrossEntropyLoss()\n\n    # Select optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n    # Loading potential checkpoint\n    start_epoch = 1\n    if previous_pipeline_directory is not None:\n       if (Path(previous_pipeline_directory) / \"checkpoint.pt\").exists():\n          states = torch.load(Path(previous_pipeline_directory) / \"checkpoint.pt\")\n          model = states[\"model\"]\n          optimizer = states[\"optimizer\"]\n          start_epoch = states[\"epochs\"]\n\n    # Training loop\n    for epoch in range(start_epoch, epochs + 1):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            output = model(data)\n            val_loss += criterion(output, target).item()\n\n            # Get the predicted class\n            _, predicted = torch.max(output.data, 1)\n\n            # Count correct predictions\n            val_total += target.size(0)\n            val_correct += (predicted == target).sum().item()\n\n    val_loss /= len(val_loader.dataset)\n    val_err = 1 - val_correct / val_total\n\n    # Saving checkpoint\n    states = {\n       \"model\": model,\n       \"optimizer\": optimizer,\n       \"epochs\": epochs,\n    }\n    torch.save(states, Path(pipeline_directory) / \"checkpoint.pt\")\n\n    # Logging\n    tblogger.log(\n        loss=val_loss,\n        current_epoch=epochs,\n        # Set to `True` for a live incumbent trajectory.\n        write_summary_incumbent=True,\n        # Set to `True` for a live loss trajectory for each config.\n        writer_config_scalar=True,\n        # Set to `True` for live parallel coordinate, scatter plot matrix, and table view.\n        writer_config_hparam=True,\n        # Appending extra data\n        extra_data={\n            \"train_loss\": tblogger.scalar_logging(loss.item()),\n            \"val_err\": tblogger.scalar_logging(val_err),\n        },\n    )\n\n    return val_err\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n\n    pipeline_space = {\n        \"learning_rate\": neps.Float(1e-5, 1e-1, log=True),\n        \"num_layers\": neps.Integer(1, 5),\n        \"num_neurons\": neps.Integer(64, 128),\n        \"weight_decay\": neps.Float(1e-5, 0.1, log=True),\n        \"epochs\": neps.Integer(1, 10, is_fidelity=True),\n    }\n\n    neps.run(\n        pipeline_space=pipeline_space,\n        run_pipeline=training_pipeline,\n        searcher=\"ifbo\",\n        max_evaluations_total=50,\n        root_directory=\"./debug/ifbo-mnist/\",\n        overwrite_working_directory=False,  # set to False for a multi-worker run\n        # (optional) ifbo hyperparameters\n        step_size=1,\n        # (optional) ifbo surrogate model hyperparameters (for FT-PFN)\n        surrogate_model_args=dict(\n            version=\"0.0.1\",\n            target_path=None,\n        ),\n    )\n\n    # NOTE: this is `experimental` and may not work as expected\n    ## plotting a 3D plot for learning curves explored by ifbo\n    plotter = Plotter3D(\n        run_path=\"./debug/ifbo-mnist/\",  # same as `root_directory` above\n        fidelity_key=\"epochs\",  # same as `pipeline_space`\n    )\n    plotter.plot3D(filename=\"ifbo\")\n</code></pre>"},{"location":"examples/efficiency/multi_fidelity/","title":"Multi fidelity","text":"<pre><code>import logging\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nimport neps\n\n\nclass TheModelClass(nn.Module):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef get_model_and_optimizer(learning_rate):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n    model = TheModelClass()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    return model, optimizer\n\n\n# Important: Include the \"pipeline_directory\" and \"previous_pipeline_directory\" arguments\n# in your run_pipeline function. This grants access to NePS's folder system and is\n# critical for leveraging efficient multi-fidelity optimization strategies.\n\n\ndef run_pipeline(pipeline_directory, previous_pipeline_directory, learning_rate, epoch):\n    model, optimizer = get_model_and_optimizer(learning_rate)\n    checkpoint_name = \"checkpoint.pth\"\n\n    if previous_pipeline_directory is not None:\n        # Read in state of the model after the previous fidelity rung\n        checkpoint = torch.load(previous_pipeline_directory / checkpoint_name)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        epochs_previously_spent = checkpoint[\"epoch\"]\n    else:\n        epochs_previously_spent = 0\n\n    # Train model here ...\n\n    # Save model to disk\n    torch.save(\n        {\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        },\n        pipeline_directory / checkpoint_name,\n    )\n\n    loss = np.log(learning_rate / epoch)  # Replace with actual error\n    epochs_spent_in_this_call = epoch - epochs_previously_spent  # Optional for stopping\n    return dict(loss=loss, cost=epochs_spent_in_this_call)\n\n\npipeline_space = dict(\n    learning_rate=neps.Float(lower=1e-4, upper=1e0, log=True),\n    epoch=neps.Integer(lower=1, upper=10, is_fidelity=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/multi_fidelity_example\",\n    # Optional: Do not start another evaluation after &lt;=50 epochs, corresponds to cost\n    # field above.\n    max_cost_total=50,\n)\n</code></pre>"},{"location":"examples/efficiency/multi_fidelity_and_expert_priors/","title":"Multi fidelity and expert priors","text":"<pre><code>import logging\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(float1, float2, integer1, fidelity):\n    loss = -float(np.sum([float1, float2, integer1])) / fidelity\n    return loss\n\n\npipeline_space = dict(\n    float1=neps.Float(\n        lower=1, upper=1000, log=False, default=600, default_confidence=\"medium\"\n    ),\n    float2=neps.Float(\n        lower=-10, upper=10, default=0, default_confidence=\"medium\"\n    ),\n    integer1=neps.Integer(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    fidelity=neps.Integer(lower=1, upper=10, is_fidelity=True),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/multifidelity_priors\",\n    max_evaluations_total=25,  # For an alternate stopping method see multi_fidelity.py\n)\n</code></pre>"},{"location":"examples/experimental/cost_aware/","title":"Cost aware","text":"<pre><code>import logging\nimport time\n\nimport numpy as np\n\nimport neps\n\n\ndef run_pipeline(\n    pipeline_directory, float1, float2, categorical, integer1, integer2\n):\n    start = time.time()\n    y = -float(np.sum([float1, float2, int(categorical), integer1, integer2]))\n    end = time.time()\n    return {\n        \"loss\": y,\n        \"cost\": (end - start) + float1,\n    }\n\n\npipeline_space = dict(\n    float1=neps.Float(lower=0, upper=1, log=False),\n    float2=neps.Float(\n        lower=0, upper=10, log=False, default=10, default_confidence=\"medium\"\n    ),\n    categorical=neps.Categorical(choices=[0, 1]),\n    integer1=neps.Integer(lower=0, upper=1, log=False),\n    integer2=neps.Integer(lower=0, upper=1, log=False),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/cost_aware_example\",\n    searcher=\"cost_cooling\",\n    max_evaluations_total=12,  # TODO(Jan): remove\n    initial_design_size=5,\n    budget=100,\n)\nprevious_results, pending_configs = neps.status(\"results/cost_aware_example\")\n</code></pre>"},{"location":"examples/experimental/expert_priors_for_architecture_and_hyperparameters/","title":"Expert priors for architecture and hyperparameters","text":"<pre><code>import logging\nimport time\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\n\nprimitives = {\n    \"id\": ops.Identity(),\n    \"conv3x3\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 1},\n    \"avg_pool\": {\"op\": ops.AvgPool1x1, \"kernel_size\": 3, \"stride\": 1},\n    \"downsample\": {\"op\": ops.ResNetBasicblock, \"stride\": 2},\n    \"residual\": topos.Residual,\n    \"diamond\": topos.Diamond,\n    \"linear\": topos.get_sequential_n_edge(2),\n    \"diamond_mid\": topos.DiamondMid,\n}\n\nstructure = {\n    \"S\": [\n        \"diamond D2 D2 D1 D1\",\n        \"diamond D1 D2 D2 D1\",\n        \"diamond D1 D1 D2 D2\",\n        \"linear D2 D1\",\n        \"linear D1 D2\",\n        \"diamond_mid D1 D2 D1 D2 D1\",\n        \"diamond_mid D2 D2 Cell D1 D1\",\n    ],\n    \"D2\": [\n        \"diamond D1 D1 D1 D1\",\n        \"linear D1 D1\",\n        \"diamond_mid D1 D1 Cell D1 D1\",\n    ],\n    \"D1\": [\n        \"diamond D1Helper D1Helper Cell Cell\",\n        \"diamond Cell Cell D1Helper D1Helper\",\n        \"diamond D1Helper Cell Cell D1Helper\",\n        \"linear D1Helper Cell\",\n        \"linear Cell D1Helper\",\n        \"diamond_mid D1Helper D1Helper Cell Cell Cell\",\n        \"diamond_mid Cell D1Helper D1Helper D1Helper Cell\",\n    ],\n    \"D1Helper\": [\"linear Cell downsample\"],\n    \"Cell\": [\n        \"residual OPS OPS OPS\",\n        \"diamond OPS OPS OPS OPS\",\n        \"linear OPS OPS\",\n        \"diamond_mid OPS OPS OPS OPS OPS\",\n    ],\n    \"OPS\": [\"conv3x3\", \"conv1x1\", \"avg_pool\", \"id\"],\n}\n\nprior_distr = {\n    \"S\": [1 / 7 for _ in range(7)],\n    \"D2\": [1 / 3 for _ in range(3)],\n    \"D1\": [1 / 7 for _ in range(7)],\n    \"D1Helper\": [1],\n    \"Cell\": [1 / 4 for _ in range(4)],\n    \"OPS\": [1 / 4 for _ in range(4)],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 64 if predecessor_values is None else predecessor_values[\"c_out\"]\n    out_channels = in_channels * 2 if op_name == \"ResNetBasicblock\" else in_channels\n    return dict(c_in=in_channels, c_out=out_channels)\n\n\ndef run_pipeline(some_architecture, some_float, some_integer, some_cat):\n    start = time.time()\n\n    in_channels = 3\n    n_classes = 20\n    base_channels = 64\n    out_channels = 512\n\n    model = some_architecture.to_pytorch()\n    model = nn.Sequential(\n        ops.Stem(base_channels, c_in=in_channels),\n        model,\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(out_channels, n_classes),\n    )\n\n    number_of_params = sum(p.numel() for p in model.parameters())\n    y = abs(1.5e7 - number_of_params)\n\n    if some_cat != \"a\":\n        y *= some_float + some_integer\n    else:\n        y *= -some_float - some_integer\n\n    end = time.time()\n\n    return {\n        \"loss\": y,\n        \"info_dict\": {\n            \"test_score\": y,\n            \"train_time\": end - start,\n        },\n    }\n\n\npipeline_space = dict(\n    some_architecture=neps.Function(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n        name=\"pibo\",\n        prior=prior_distr,\n    ),\n    some_float=neps.Float(\n        lower=1, upper=1000, log=True, default=900, default_confidence=\"medium\"\n    ),\n    some_integer=neps.Integer(\n        lower=0, upper=50, default=35, default_confidence=\"low\"\n    ),\n    some_cat=neps.Categorical(\n        choices=[\"a\", \"b\", \"c\"], default=\"a\", default_confidence=\"high\"\n    ),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/user_priors_with_graphs\",\n    max_evaluations_total=15,\n    use_priors=True,\n)\n</code></pre>"},{"location":"examples/experimental/fault_tolerance/","title":"Fault tolerance","text":"<pre><code>\"\"\" To test the fault tolerance, run this script multiple times.\n\"\"\"\n\nimport logging\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nimport neps\n\n\nclass TheModelClass(nn.Module):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef get_model_and_optimizer(learning_rate):\n    \"\"\"Taken from https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\"\"\n    model = TheModelClass()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    return model, optimizer\n\n\ndef run_pipeline(pipeline_directory, learning_rate):\n    model, optimizer = get_model_and_optimizer(learning_rate)\n    checkpoint_path = pipeline_directory / \"checkpoint.pth\"\n\n    # Check if there is a previous state of the model training that crashed\n    if checkpoint_path.exists():\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        epoch_already_trained = checkpoint[\"epoch\"]\n        print(f\"Read in model trained for {epoch_already_trained} epochs\")\n    else:\n        epoch_already_trained = 0\n\n    for epoch in range(epoch_already_trained, 101):\n        epoch += 1\n\n        # Train model here ....\n\n        # Repeatedly save your progress\n        if epoch % 10 == 0:\n            torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": optimizer.state_dict(),\n                },\n                checkpoint_path,\n            )\n\n        # Here we simulate a crash! E.g., due to job runtime limits\n        if epoch == 50 and learning_rate &lt; 0.2:\n            print(\"Oh no! A simulated crash!\")\n            exit()\n\n    return learning_rate  # Replace with actual error\n\n\npipeline_space = dict(\n    learning_rate=neps.Float(lower=0, upper=1),\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/fault_tolerance_example\",\n    max_evaluations_total=15,\n)\nprevious_results, pending_configs = neps.status(\"results/fault_tolerance_example\")\n</code></pre>"},{"location":"examples/experimental/hierarchical_architecture/","title":"Hierarchical architecture","text":"<pre><code>raise NotImplementedError(\n    \"Support for graphs was temporarily removed, if you'd like to use a version\"\n    \" of NePS that supports graphs, please use version v0.12.2\"\n)\n\nimport logging\n\nfrom torch import nn\n\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\n\nprimitives = {\n    \"id\": ops.Identity(),\n    \"conv3x3\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ops.ReLUConvBN, \"kernel_size\": 1},\n    \"avg_pool\": {\"op\": ops.AvgPool1x1, \"kernel_size\": 3, \"stride\": 1},\n    \"downsample\": {\"op\": ops.ResNetBasicblock, \"stride\": 2},\n    \"residual\": topos.Residual,\n    \"diamond\": topos.Diamond,\n    \"linear\": topos.get_sequential_n_edge(2),\n    \"diamond_mid\": topos.DiamondMid,\n}\n\nstructure = {\n    \"S\": [\n        \"diamond D2 D2 D1 D1\",\n        \"diamond D1 D2 D2 D1\",\n        \"diamond D1 D1 D2 D2\",\n        \"linear D2 D1\",\n        \"linear D1 D2\",\n        \"diamond_mid D1 D2 D1 D2 D1\",\n        \"diamond_mid D2 D2 Cell D1 D1\",\n    ],\n    \"D2\": [\n        \"diamond D1 D1 D1 D1\",\n        \"linear D1 D1\",\n        \"diamond_mid D1 D1 Cell D1 D1\",\n    ],\n    \"D1\": [\n        \"diamond D1Helper D1Helper Cell Cell\",\n        \"diamond Cell Cell D1Helper D1Helper\",\n        \"diamond D1Helper Cell Cell D1Helper\",\n        \"linear D1Helper Cell\",\n        \"linear Cell D1Helper\",\n        \"diamond_mid D1Helper D1Helper Cell Cell Cell\",\n        \"diamond_mid Cell D1Helper D1Helper D1Helper Cell\",\n    ],\n    \"D1Helper\": [\"linear Cell downsample\"],\n    \"Cell\": [\n        \"residual OPS OPS OPS\",\n        \"diamond OPS OPS OPS OPS\",\n        \"linear OPS OPS\",\n        \"diamond_mid OPS OPS OPS OPS OPS\",\n    ],\n    \"OPS\": [\"conv3x3\", \"conv1x1\", \"avg_pool\", \"id\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 64 if predecessor_values is None else predecessor_values[\"c_out\"]\n    out_channels = in_channels * 2 if op_name == \"ResNetBasicblock\" else in_channels\n    return dict(c_in=in_channels, c_out=out_channels)\n\n\ndef run_pipeline(architecture: neps.Function):\n    in_channels = 3\n    n_classes = 20\n    base_channels = 64\n    out_channels = 512\n\n    model = architecture.to_pytorch()\n    model = nn.Sequential(\n        ops.Stem(base_channels, c_in=in_channels),\n        model,\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(out_channels, n_classes),\n    )\n\n    number_of_params = sum(p.numel() for p in model.parameters())\n    validation_error = abs(1.5e7 - number_of_params)\n\n    return validation_error\n\n\npipeline_space = dict(\n    architecture=neps.Function(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n        name=\"makrograph\",\n    )\n)\n\nlogging.basicConfig(level=logging.INFO)\nneps.run(\n    run_pipeline=run_pipeline,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/hierarchical_architecture_example\",\n    max_evaluations_total=15,\n)\n</code></pre>"},{"location":"examples/template/basic_template/","title":"Basic template","text":"<pre><code>\"\"\"\nNOTE!!! This code is not meant to be executed.\nIt is only to serve as a template to help interface NePS with an existing ML/DL pipeline.\n\nThe following script is designed as a template for using NePS.\nIt describes the crucial components that a user needs to provide in order to interface\na NePS optimizer.\n\nThe 2 crucial components are:\n* The search space, called the `pipeline_space` in NePS\n  * This defines the set of hyperparameters that the optimizer will search over\n  * This declaration also allows injecting priors in the form of defaults per hyperparameter\n* The `run_pipeline` function\n  * This function is called by the optimizer and is responsible for running the pipeline\n  * The function should at the minimum expect the hyperparameters as keyword arguments\n  * The function should return the loss of the pipeline as a float\n    * If the return value is a dictionary, it should have a key called \"loss\" with the loss as a float\n\n\nOverall, running an optimizer from NePS involves 4 clear steps:\n1. Importing neccessary packages including neps.\n2. Designing the search space as a dictionary.\n3. Creating the run_pipeline and returning the loss and other wanted metrics.\n4. Using neps run with the optimizer of choice.\n\"\"\"\n\nimport logging\n\nimport neps\n\n\nlogger = logging.getLogger(\"neps_template.run\")\n\n\ndef pipeline_space() -&gt; dict:\n    # Create the search space based on NEPS parameters and return the dictionary.\n    # Example:\n    space = dict(\n        lr=neps.Float(\n            lower=1e-5,\n            upper=1e-2,\n            log=True,      # If True, the search space is sampled in log space\n            default=1e-3,  # a non-None value here acts as the mode of the prior distribution\n        ),\n    )\n    return space\n\n\ndef run_pipeline(**config) -&gt; dict | float:\n    # Run pipeline should include the following steps:\n\n    # 1. Defining the model.\n    # 1.1 Load any checkpoint if necessary\n    # 2. Each optimization variable should get its values from the pipeline space.\n    #   Example:\n    #   learning_rate = config[\"lr\"]\n    # 3. The training loop\n    # 3.1 Save any checkpoint if necessary\n    # 4. Returning the loss, which can be either as a single float or as part of\n    #   an info dictionary containing other metrics.\n\n    # Can use global logger to log any information\n    logger.info(f\"Running pipeline with config: {config}\")\n\n    return dict or float\n\n\nif __name__ == \"__main__\":\n    # 1. Creating the logger\n\n\n    # 2. Passing the correct arguments to the neps.run function\n    # For more information on the searcher, please take a look at this link:\n    # https://github.com/automl/neps/tree/master/neps/optimizers/README.md\n\n    neps.run(\n        run_pipeline=run_pipeline,        # User TODO (defined above)\n        pipeline_space=pipeline_space(),  # User TODO (defined above)\n        root_directory=\"results\",\n        max_evaluations_total=10,\n    )\n</code></pre>"},{"location":"examples/template/ifbo_template/","title":"Ifbo template","text":"<pre><code>import numpy as np\n\nfrom neps.plot.plot3D import Plotter3D\n\nfrom .priorband_template import pipeline_space, run_pipeline\n\n\nASSUMED_MAX_LOSS = 10\n\n\ndef ifbo_run_pipeline(\n    pipeline_directory,  # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the config's immediate lower fidelity\n    **config,  # The hyperparameters to be used in the pipeline\n) -&gt; dict | float:\n    result_dict = run_pipeline(\n        pipeline_directory=pipeline_directory,  # NOTE: can only support &lt;=10 HPs and no categoricals\n        previous_pipeline_directory=previous_pipeline_directory,\n        **config,\n    )\n    # NOTE: Normalize the loss to be between 0 and 1\n    ## crucial for ifBO's FT-PFN surrogate to work as expected\n    result_dict[\"loss\"] = np.clip(result_dict[\"loss\"], 0, ASSUMED_MAX_LOSS) / ASSUMED_MAX_LOSS\n    return result_dict\n\n\nif __name__ == \"__main__\":\n    import neps\n\n    neps.run(\n        run_pipeline=run_pipeline,\n        pipeline_space=pipeline_space(),\n        root_directory=\"results\",\n        max_evaluations_total=50,\n        searcher=\"ifbo\",\n    )\n# end of ifbo_run_pipeline\n</code></pre>"},{"location":"examples/template/lightning_template/","title":"Lightning template","text":"<pre><code>\"\"\" Boilerplate code to optimize a simple PyTorch Lightning model.\n\nNOTE!!! This code is not meant to be executed.\nIt is only to serve as a template to help interface NePS with an existing ML/DL pipeline.\n\n\nThe following script describes the crucial components that a user needs to provide\nin order to interface with Lightning.\n\nThe 3 crucial components are:\n* The search space, called the `pipeline_space` in NePS\n  * This defines the set of hyperparameters that the optimizer will search over\n  * This declaration also allows injecting priors in the form of defaults per hyperparameter\n* The `lightning module`\n  * This defines the training, validation, and testing of the model\n  * This distributes the hyperparameters\n  * This can be used to create the Dataloaders for training, validation, and testing\n* The `run_pipeline` function\n  * This function is called by the optimizer and is responsible for running the pipeline\n  * The function should at the minimum expect the hyperparameters as keyword arguments\n  * The function should return the loss of the pipeline as a float\n    * If the return value is a dictionary, it should have a key called \"loss\" with the loss as a float\n\nOverall, running an optimizer from NePS with Lightning involves 5 clear steps:\n1. Importing neccessary packages including NePS and Lightning.\n2. Designing the search space as a dictionary.\n3. Creating the LightningModule with the required parameters\n4. Creating the run_pipeline and returning the loss and other wanted metrics.\n5. Using neps run with the optimizer of choice.\n\nFor a more detailed guide, please refer to:\nhttps://github.com/automl/neps/blob/master/neps_examples/convenience/neps_x_lightning.py\n\"\"\"\nimport logging\n\nimport lightning as L\nimport torch\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\nimport neps\nfrom neps.utils.common import get_initial_directory, load_lightning_checkpoint\n\nlogger = logging.getLogger(\"neps_template.run\")\n\n\ndef pipeline_space() -&gt; dict:\n    # Create the search space based on NEPS parameters and return the dictionary.\n    # IMPORTANT:\n    space = dict(\n        lr=neps.Float(\n            lower=1e-5,\n            upper=1e-2,\n            log=True,  # If True, the search space is sampled in log space\n            default=1e-3,  # a non-None value here acts as the mode of the prior distribution\n        ),\n        optimizer=neps.Categorical(choices=[\"Adam\", \"SGD\"], default=\"Adam\"),\n        epochs=neps.Integer(\n            lower=1,\n            upper=9,\n            is_fidelity=True,  # IMPORTANT to set this to True for the fidelity parameter\n        ),\n    )\n    return space\n\n\nclass LitModel(L.LightningModule):\n    def __init__(self, configuration: dict):\n        super().__init__()\n\n        self.save_hyperparameters(configuration)\n\n        # You can now define your criterion, data transforms, model layers, and\n        # metrics obtained during training\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Forward pass function\n        pass\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        # Training step function\n        # Training metric of choice\n        pass\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        # Validation step function\n        # Validation metric of choice\n        pass\n\n    def test_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        # Test step function\n        # Test metric of choice\n        pass\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        # Define the optimizer base on the configuration\n        if self.hparams.optimizer == \"Adam\":\n            optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n        elif self.hparams.optimizer == \"SGD\":\n            optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams.lr)\n        else:\n            raise ValueError(f\"{self.hparams.optimizer} is not a valid optimizer\")\n        return optimizer\n\n    # Here one can now configure the dataloaders for the model\n    # Further details can be found here:\n    # https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n    # https://github.com/automl/neps/blob/master/neps_examples/convenience/neps_x_lightning.py\n\n\ndef run_pipeline(\n    pipeline_directory,  # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the config's immediate lower fidelity\n    **config,  # The hyperparameters to be used in the pipeline\n) -&gt; dict | float:\n    # Start by getting the initial directory which will be used to store tensorboard\n    # event files and checkpoint files\n    init_dir = get_initial_directory(pipeline_directory)\n    checkpoint_dir = init_dir / \"checkpoints\"\n    tensorboard_dir = init_dir / \"tensorboard\"\n\n    # Create the model\n    model = LitModel(config)\n\n    # Create the TensorBoard logger and the checkpoint callback\n    logger = TensorBoardLogger(\n        save_dir=tensorboard_dir, name=\"data\", version=\"logs\", default_hp_metric=False\n    )\n    checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir)\n\n    # Checking for any checkpoint files and checkpoint data, returns None if\n    # no checkpoint files exist.\n    checkpoint_path, checkpoint_data = load_lightning_checkpoint(\n        previous_pipeline_directory=previous_pipeline_directory,\n        checkpoint_dir=checkpoint_dir,\n    )\n\n    # Create a PyTorch Lightning Trainer\n    epochs = config[\"epochs\"]\n\n    trainer = L.Trainer(\n        logger=logger,\n        max_epochs=epochs,\n        callbacks=[checkpoint_callback],\n    )\n\n    # Train, test, and get their corresponding metrics\n    if checkpoint_path:\n        trainer.fit(model, ckpt_path=checkpoint_path)\n    else:\n        trainer.fit(model)\n    val_loss = trainer.logged_metrics.get(\"val_loss\", None)\n\n    trainer.test(model)\n    test_loss = trainer.logged_metrics.get(\"test_loss\", None)\n\n    # Return a dictionary with the results, or a single float value (loss)\n    return {\n        \"loss\": val_loss,\n        \"info_dict\": {\n            \"test_loss\": test_loss,\n        },\n    }\n\n\n# end of run_pipeline\n\nif __name__ == \"__main__\":\n    neps.run(\n        run_pipeline=run_pipeline,  # User TODO (defined above)\n        pipeline_space=pipeline_space(),  # User TODO (defined above)\n        root_directory=\"results\",\n        max_evaluations_total=25,  # total number of times `run_pipeline` is called\n        searcher=\"priorband\",  # \"priorband_bo\" for longer budgets, and set `initial_design_size``\n    )\n</code></pre>"},{"location":"examples/template/priorband_template/","title":"Priorband template","text":"<pre><code>\"\"\"Boilerplate code to optimize a simple PyTorch model using PriorBand.\n\nNOTE!!! This code is not meant to be executed.\nIt is only to serve as a template to help interface NePS with an existing ML/DL pipeline.\n\n\nThe following script is designed as a template for using `PriorBand` from NePS.\nIt describes the crucial components that a user needs to provide in order to interface PriorBand.\n\nThe 2 crucial components are:\n* The search space, called the `pipeline_space` in NePS\n  * This defines the set of hyperparameters that the optimizer will search over\n  * This declaration also allows injecting priors in the form of defaults per hyperparameter\n* The `run_pipeline` function\n  * This function is called by the optimizer and is responsible for running the pipeline\n  * The function should at the minimum expect the hyperparameters as keyword arguments\n  * The function should return the loss of the pipeline as a float\n    * If the return value is a dictionary, it should have a key called \"loss\" with the loss as a float\n\n\nOverall, running an optimizer from NePS involves 4 clear steps:\n1. Importing neccessary packages including neps.\n2. Designing the search space as a dictionary.\n3. Creating the run_pipeline and returning the loss and other wanted metrics.\n4. Using neps run with the optimizer of choice.\n\"\"\"\n\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport neps\nfrom neps.utils.common import load_checkpoint, save_checkpoint\n\nlogger = logging.getLogger(\"neps_template.run\")\n\n\ndef pipeline_space() -&gt; dict:\n    # Create the search space based on NEPS parameters and return the dictionary.\n    # IMPORTANT:\n    space = dict(\n        lr=neps.Float(\n            lower=1e-5,\n            upper=1e-2,\n            log=True,  # If True, the search space is sampled in log space\n            default=1e-3,  # a non-None value here acts as the mode of the prior distribution\n        ),\n        wd=neps.Float(\n            lower=0,\n            upper=1e-1,\n            log=True,\n            default=1e-3,\n        ),\n        epoch=neps.Integer(\n            lower=1,\n            upper=10,\n            is_fidelity=True,  # IMPORTANT to set this to True for the fidelity parameter\n        ),\n    )\n    return space\n\n\ndef run_pipeline(\n    pipeline_directory,  # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the config's immediate lower fidelity\n    **config,  # The hyperparameters to be used in the pipeline\n) -&gt; dict | float:\n    # Defining the model\n    #  Can define outside the function or import from a file, package, etc.\n    class my_model(nn.Module):\n        def __init__(self) -&gt; None:\n            super().__init__()\n            self.linear1 = nn.Linear(in_features=224, out_features=512)\n            self.linear2 = nn.Linear(in_features=512, out_features=10)\n\n        def forward(self, x):\n            x = F.relu(self.linear1(x))\n            x = self.linear2(x)\n            return x\n\n    # Instantiates the model\n    model = my_model()\n\n    # IMPORTANT: Extracting hyperparameters from passed config\n    learning_rate = config[\"lr\"]\n    weight_decay = config[\"wd\"]\n\n    # Initializing the optimizer\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n    )\n\n    ## Checkpointing\n    # loading the checkpoint if it exists\n    previous_state = load_checkpoint(  # predefined function from neps\n        directory=previous_pipeline_directory,\n        model=model,  # relies on pass-by-reference\n        optimizer=optimizer,  # relies on pass-by-reference\n    )\n    # adjusting run budget based on checkpoint\n    if previous_state is not None:\n        epoch_already_trained = previous_state[\"epochs\"]\n        # + Anything else saved in the checkpoint.\n    else:\n        epoch_already_trained = 0\n        # + Anything else with default value.\n\n    # Extracting target epochs from config\n    max_epochs = config[\"epoch\"]\n\n    # User TODO:\n    #  Load relevant data for training and validation\n\n    # Actual model training\n    for epoch in range(epoch_already_trained, max_epochs):\n        # Training loop\n        ...\n        # Validation loop\n        ...\n        logger.info(f\"Epoch: {epoch}, Loss: {...}, Val. acc.: {...}\")\n\n    # Save the checkpoint data in the current directory\n    save_checkpoint(\n        directory=pipeline_directory,\n        values_to_save={\"epochs\": max_epochs},\n        model=model,\n        optimizer=optimizer,\n    )\n\n    # Return a dictionary with the results, or a single float value (loss)\n    return {\n        \"loss\": ...,\n        \"info_dict\": {\n            \"train_accuracy\": ...,\n            \"test_accuracy\": ...,\n        },\n    }\n\n\n# end of run_pipeline\n\n\nif __name__ == \"__main__\":\n    neps.run(\n        run_pipeline=run_pipeline,  # User TODO (defined above)\n        pipeline_space=pipeline_space(),  # User TODO (defined above)\n        root_directory=\"results\",\n        max_evaluations_total=25,  # total number of times `run_pipeline` is called\n        searcher=\"priorband\",  # \"priorband_bo\" for longer budgets, and set `initial_design_size``\n    )\n</code></pre>"},{"location":"reference/analyse/","title":"Analysing Runs","text":"<p>NePS has some convenient utilities to help you to understand the results after you've run your runs. All of the results and state are stored and communicated on disk, which you can access using the <code>python -m neps.status ROOT_DIRECTORY</code> command or integrate live logging directly into your training loop and visualize the results using TensorBoard.</p> <p>To get a quick overview of the results, you can use the <code>python -m neps.plot ROOT_DIRECTORY</code> command.</p>"},{"location":"reference/analyse/#status","title":"Status","text":"<p>To show status information about a neural pipeline search run, use</p> <pre><code>python -m neps.status ROOT_DIRECTORY\n</code></pre> <p>If you need more status information than is printed per default (e.g., the best config over time), please have a look at</p> <pre><code>python -m neps.status --help\n</code></pre> <p>Using <code>watch</code></p> <p>To show the status repeatedly, on unix systems you can use</p> <pre><code>watch --interval 30 python -m neps.status ROOT_DIRECTORY\n</code></pre>"},{"location":"reference/analyse/#cli-commands","title":"CLI commands","text":"<p>To generate plots to the root directory, run</p> <pre><code>python -m neps.plot ROOT_DIRECTORY\n</code></pre> <p>Currently, this creates one plot that shows the best error value across the number of evaluations.</p>"},{"location":"reference/analyse/#whats-on-disk","title":"What's on disk?","text":"<p>In the root directory, NePS maintains several files at all times that are human readable and can be useful If you pass the <code>post_run_summary=</code> argument to <code>neps.run()</code>, NePS will also generate a summary CSV file for you.</p> <code>neps.run(..., post_run_summary=True)</code><code>neps.run(..., post_run_summary=False)</code> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 result.yaml\n\u251c\u2500\u2500 summary_csv\n\u2502  \u251c\u2500\u2500 config_data.csv\n\u2502  \u2514\u2500\u2500 run_status.csv\n\u251c\u2500\u2500 all_losses_and_configs.txt\n\u251c\u2500\u2500 best_loss_trajectory.txt\n\u2514\u2500\u2500 best_loss_with_config_trajectory.txt\n</code></pre> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 result.yaml\n\u251c\u2500\u2500 all_losses_and_configs.txt\n\u251c\u2500\u2500 best_loss_trajectory.txt\n\u2514\u2500\u2500 best_loss_with_config_trajectory.txt\n</code></pre> <p>The <code>config_data.csv</code> contains all configuration details in CSV format, ordered by ascending <code>loss</code>. Details include configuration hyperparameters, any returned result from the <code>run_pipeline</code> function, and metadata information.</p> <p>The <code>run_status.csv</code> provides general run details, such as the number of sampled configs, best configs, number of failed configs, best loss, etc.</p>"},{"location":"reference/analyse/#tensorboard-integration","title":"TensorBoard Integration","text":"<p>TensorBoard serves as a valuable tool for visualizing machine learning experiments, offering the ability to observe losses and metrics throughout the model training process. In NePS, we use this to show metrics of configurations during training in addition to comparisons to different hyperparameters used in the search for better diagnosis of the model.</p>"},{"location":"reference/analyse/#logging-things","title":"Logging Things","text":"<p>The <code>tblogger.log()</code> function is invoked within the model's training loop to facilitate logging of key metrics.</p> <p>We also provide some utility functions to make it easier to log things like:</p> <ul> <li>Scalars through <code>tblogger.scalar_logging()</code></li> <li>Images through <code>tblogger.image_logging()</code></li> </ul> <p>You can provide these through the <code>extra_data=</code> argument in the <code>tblogger.log()</code> function.</p> <p>For an example usage of all these features please refer to the example!</p> <pre><code>tblogger.log(\n    loss=loss,\n    current_epoch=i,\n    write_summary_incumbent=False,  # Set to `True` for a live incumbent trajectory.\n    writer_config_scalar=True,  # Set to `True` for a live loss trajectory for each config.\n    writer_config_hparam=True,  # Set to `True` for live parallel coordinate, scatter plot matrix, and table view.\n\n    # Name the dictionary keys as the names of the values\n    # you want to log and pass one of the following functions\n    # as the values for a successful logging process.\n    extra_data={\n        \"lr_decay\": tblogger.scalar_logging(value=scheduler.get_last_lr()[0]),\n        \"miss_img\": tblogger.image_logging(image=miss_img, counter=2, seed=2),\n        \"layer_gradient1\": tblogger.scalar_logging(value=mean_gradient[0]),\n        \"layer_gradient2\": tblogger.scalar_logging(value=mean_gradient[1]),\n    },\n)\n</code></pre> <p>Tip</p> <p>The logger function is primarily designed for use within the <code>run_pipeline</code> function during the training of the neural network.</p> Quick Reference <code>tblogger.log()</code><code>tblogger.scalar_logging()</code><code>tblogger.image_logging()</code>"},{"location":"reference/analyse/#neps.plot.tensorboard_eval.tblogger.log","title":"neps.plot.tensorboard_eval.tblogger.log  <code>staticmethod</code>","text":"<pre><code>log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None\n) -&gt; None\n</code></pre> <p>Log experiment data to the logger, including scalar values, hyperparameters, and images.</p> PARAMETER DESCRIPTION <code>loss</code> <p>Current loss value.</p> <p> TYPE: <code>float</code> </p> <code>current_epoch</code> <p>Current epoch of the experiment (used as the global step).</p> <p> TYPE: <code>int</code> </p> <code>writer_config_scalar</code> <p>Displaying the loss or accuracy curve on tensorboard (default: True)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>writer_config_hparam</code> <p>Write hyperparameters logging of the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>write_summary_incumbent</code> <p>Set to <code>True</code> for a live incumbent trajectory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>extra_data</code> <p>Additional experiment data for logging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef log(\n    loss: float,\n    current_epoch: int,\n    *,\n    writer_config_scalar: bool = True,\n    writer_config_hparam: bool = True,\n    write_summary_incumbent: bool = False,\n    extra_data: dict | None = None,\n) -&gt; None:\n    \"\"\"Log experiment data to the logger, including scalar values,\n    hyperparameters, and images.\n\n    Args:\n        loss: Current loss value.\n        current_epoch: Current epoch of the experiment (used as the global step).\n        writer_config_scalar: Displaying the loss or accuracy\n            curve on tensorboard (default: True)\n        writer_config_hparam: Write hyperparameters logging of the configs.\n        write_summary_incumbent: Set to `True` for a live incumbent trajectory.\n        extra_data: Additional experiment data for logging.\n    \"\"\"\n    if tblogger.disable_logging:\n        return\n\n    tblogger.current_epoch = current_epoch\n    tblogger.loss = loss\n    tblogger.write_incumbent = write_summary_incumbent\n\n    tblogger._initiate_internal_configurations()\n\n    if writer_config_scalar:\n        tblogger._write_scalar_config(tag=\"Loss\", value=loss)\n\n    if writer_config_hparam:\n        tblogger._write_hparam_config()\n\n    if extra_data is not None:\n        for key in extra_data:\n            if extra_data[key][0] == \"scalar\":\n                tblogger._write_scalar_config(tag=str(key), value=extra_data[key][1])\n\n            elif extra_data[key][0] == \"image\":\n                tblogger._write_image_config(\n                    tag=str(key),\n                    image=extra_data[key][1],\n                    counter=extra_data[key][2],\n                    resize_images=extra_data[key][3],\n                    random_images=extra_data[key][4],\n                    num_images=extra_data[key][5],\n                    seed=extra_data[key][6],\n                )\n</code></pre>"},{"location":"reference/analyse/#neps.plot.tensorboard_eval.tblogger.scalar_logging","title":"neps.plot.tensorboard_eval.tblogger.scalar_logging  <code>staticmethod</code>","text":"<pre><code>scalar_logging(value: float) -&gt; tuple[str, float]\n</code></pre> <p>Prepare a scalar value for logging.</p> PARAMETER DESCRIPTION <code>value</code> <p>The scalar value to be logged.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tuple</code> <p>A tuple containing the logging mode and the value for logging.     The tuple format is (logging_mode, value).</p> <p> TYPE: <code>tuple[str, float]</code> </p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef scalar_logging(value: float) -&gt; tuple[str, float]:\n    \"\"\"Prepare a scalar value for logging.\n\n    Args:\n        value (float): The scalar value to be logged.\n\n    Returns:\n        Tuple: A tuple containing the logging mode and the value for logging.\n            The tuple format is (logging_mode, value).\n    \"\"\"\n    logging_mode = \"scalar\"\n    return (logging_mode, value)\n</code></pre>"},{"location":"reference/analyse/#neps.plot.tensorboard_eval.tblogger.image_logging","title":"neps.plot.tensorboard_eval.tblogger.image_logging  <code>staticmethod</code>","text":"<pre><code>image_logging(\n    image: Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | RandomState | None = None\n) -&gt; tuple[\n    str,\n    Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | RandomState | None,\n]\n</code></pre> <p>Prepare an image tensor for logging.</p> PARAMETER DESCRIPTION <code>image</code> <p>Image tensor to be logged.</p> <p> TYPE: <code>Tensor</code> </p> <code>counter</code> <p>Counter value associated with the images.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>resize_images</code> <p>List of integers for image sizes after resizing.</p> <p> TYPE: <code>list[None | int] | None</code> DEFAULT: <code>None</code> </p> <code>random_images</code> <p>Images are randomly selected if True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_images</code> <p>Number of images to log.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>seed</code> <p>Seed value or RandomState instance to control randomness.</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[str, Tensor, int, list[None | int] | None, bool, int, int | RandomState | None]</code> <p>A tuple containing the logging mode and all the necessary parameters for image logging. Tuple: (logging_mode, img_tensor, counter, resize_images,                 random_images, num_images, seed).</p> Source code in <code>neps/plot/tensorboard_eval.py</code> <pre><code>@staticmethod\ndef image_logging(\n    image: torch.Tensor,\n    counter: int = 1,\n    *,\n    resize_images: list[None | int] | None = None,\n    random_images: bool = True,\n    num_images: int = 20,\n    seed: int | np.random.RandomState | None = None,\n) -&gt; tuple[\n    str,\n    torch.Tensor,\n    int,\n    list[None | int] | None,\n    bool,\n    int,\n    int | np.random.RandomState | None,\n]:\n    \"\"\"Prepare an image tensor for logging.\n\n    Args:\n        image: Image tensor to be logged.\n        counter: Counter value associated with the images.\n        resize_images: List of integers for image sizes after resizing.\n        random_images: Images are randomly selected if True.\n        num_images: Number of images to log.\n        seed: Seed value or RandomState instance to control randomness.\n\n    Returns:\n        A tuple containing the logging mode and all the necessary parameters for\n        image logging.\n        Tuple: (logging_mode, img_tensor, counter, resize_images,\n                        random_images, num_images, seed).\n    \"\"\"\n    logging_mode = \"image\"\n    return (\n        logging_mode,\n        image,\n        counter,\n        resize_images,\n        random_images,\n        num_images,\n        seed,\n    )\n</code></pre>"},{"location":"reference/analyse/#visualizing-results","title":"Visualizing Results","text":"<p>The following command will open a local host for TensorBoard visualizations, allowing you to view them either in real-time or after the run is complete.</p> <pre><code>tensorboard --logdir path/to/root_directory\n</code></pre> <p>This image shows visualizations related to scalar values logged during training. Scalars typically include metrics such as loss, incumbent trajectory, a summary of losses for all configurations, and any additional data provided via the <code>extra_data</code> argument in the <code>tblogger.log</code> function.</p> <p></p> <p>This image represents visualizations related to logged images during training. It could include snapshots of input data, model predictions, or any other image-related information. In our case, we use images to depict instances of incorrect predictions made by the model.</p> <p></p> <p>The following images showcase visualizations related to hyperparameter logging in TensorBoard. These plots include three different views, providing insights into the relationship between different hyperparameters and their impact on the model.</p> <p>In the table view, you can explore hyperparameter configurations across five different trials. The table displays various hyperparameter values alongside corresponding evaluation metrics.</p> <p></p> <p>The parallel coordinate plot offers a holistic perspective on hyperparameter configurations. By presenting multiple hyperparameters simultaneously, this view allows you to observe the interactions between variables, providing insights into their combined influence on the model.</p> <p></p> <p>The scatter plot matrix view provides an in-depth analysis of pairwise relationships between different hyperparameters. By visualizing correlations and patterns, this view aids in identifying key interactions that may influence the model's performance.</p> <p></p>"},{"location":"reference/cli/","title":"NePS Command Line Interface","text":"<p>This section provides a brief overview of the commands available in the NePS CLI.</p>"},{"location":"reference/cli/#init-command","title":"<code>init</code> Command","text":"<p>Generates a default <code>run_args</code> YAML configuration file, providing a template that you can customize for your experiments.</p> <p>Arguments:</p> <ul> <li><code>-h, --help</code> (Optional): show this help message and exit</li> <li><code>--config-path</code> (Optional): Optional custom path for generating the configuration file. Default is 'run_config.yaml'.</li> <li><code>--template</code> (Optional): Optional, options between different templates. Required configs(basic) vs all neps configs (complete)</li> <li><code>--state-machine</code> (Optional): If set, creates a NEPS state. Requires an existing config.yaml.</li> </ul> <p>Example Usage:</p> <pre><code>neps init --config-path custom/path/config.yaml --template complete\n</code></pre>"},{"location":"reference/cli/#run-command","title":"<code>run</code> Command","text":"<p>Executes the optimization based on the provided configuration. This command serves as a CLI wrapper around <code>neps.run</code>, effectively mapping each CLI argument to a parameter in <code>neps.run</code>. It offers a flexible interface that allows you to override the existing settings specified in the YAML configuration file, facilitating dynamic adjustments for managing your experiments.</p> <p>Arguments:</p> <ul> <li><code>-h, --help</code> (Optional): show this help message and exit</li> <li><code>--run-args</code> (Optional): Path to the YAML configuration file.</li> <li><code>--run-pipeline</code> (Optional): Optional: Provide the path to a Python file and a function name separated by a colon, e.g., 'path/to/module.py:function_name'. If provided, it overrides the run_pipeline setting from the YAML configuration.</li> <li><code>--pipeline-space</code> (Optional): Path to the YAML file defining the search space for the optimization. This can be provided here or defined within the 'run_args' YAML file.</li> <li><code>--root-directory</code> (Optional): The directory to save progress to. This is also used to synchronize multiple calls for parallelization.</li> <li><code>--overwrite-working-directory</code> (Optional): If set, deletes the working directory at the start of the run. This is useful, for example, when debugging a run_pipeline function.</li> <li><code>--development-stage-id</code> (Optional): Identifier for the current development stage, used in multi-stage projects.</li> <li><code>--task-id</code> (Optional): Identifier for the current task, useful in projects with multiple tasks.</li> <li><code>--post-run-summary</code> (Optional): Provide a summary of the results after running.</li> <li><code>--no-post-run-summary</code> (Optional): Do not provide a summary of the results after running.</li> <li><code>--max-evaluations-total</code> (Optional): Total number of evaluations to run.</li> <li><code>--max-evaluations-per-run</code> (Optional): Number of evaluations a specific call should maximally do.</li> <li><code>--continue-until-max-evaluation-completed</code> (Optional): If set, only stop after max-evaluations-total have been completed. This is only relevant in the parallel setting.</li> <li><code>--max-cost-total</code> (Optional): No new evaluations will start when this cost is exceeded. Requires returning a cost   in the run_pipeline function.</li> <li><code>--ignore-errors</code> (Optional): If set, ignore errors during the optimization process.</li> <li><code>--loss-value-on-error</code> (Optional): Loss value to assume on error.</li> <li><code>--cost-value-on-error</code> (Optional): Cost value to assume on error.</li> <li><code>--searcher</code> (Optional): String key of searcher algorithm to use for optimization.</li> <li><code>--searcher-kwargs</code> (Optional): Additional keyword arguments as key=value pairs for the searcher.</li> </ul> <p>Example Usage:</p> <pre><code>neps run --run-args path/to/config.yaml --max-evaluations-total 50\n</code></pre>"},{"location":"reference/cli/#status-command","title":"<code>status</code> Command","text":"<p>Check the status of the NePS run. This command provides a summary of trials, including pending, evaluating, succeeded, and failed trials. You can filter the trials displayed based on their state.</p> <p>Arguments:</p> <ul> <li><code>-h, --help</code> (Optional): show this help message and exit</li> <li><code>--root-directory</code> (Optional): Optional: The path to your root_directory. If not provided, it will be loaded from run_config.yaml.</li> <li><code>--pending</code> (Optional): Show only pending trials.</li> <li><code>--evaluating</code> (Optional): Show only evaluating trials.</li> <li><code>--succeeded</code> (Optional): Show only succeeded trials.</li> </ul> <p>Example Usage: <pre><code>neps status --root-directory path/to/directory --succeeded\n</code></pre></p>"},{"location":"reference/cli/#info-config-command","title":"<code>info-config</code> Command","text":"<p>Provides detailed information about a specific configuration identified by its ID. This includes metadata, configuration values, and trial status.</p> <p>Arguments:</p> <ul> <li> <p>id (Required): The configuration ID to be used.</p> </li> <li> <p><code>-h, --help</code> (Optional): show this help message and exit</p> </li> <li><code>--root-directory</code> (Optional): Optional: The path to your root_directory. If not provided, it will be loaded from run_config.yaml.</li> </ul> <p>Example Usage: <pre><code>neps info-config 42 --root-directory path/to/directory\n</code></pre></p>"},{"location":"reference/cli/#results-command","title":"<code>results</code> Command","text":"<p>Displays the results of the NePS run, listing all incumbent trials in reverse order (most recent first). Optionally, you can plot the results to visualize the progression of incumbents over trials.</p> <p>Arguments:</p> <ul> <li><code>-h, --help</code> (Optional): show this help message and exit</li> <li><code>--root-directory</code> (Optional): Optional: The path to your root_directory. If not provided, it will be loaded from run_config.yaml.</li> <li><code>--plot</code> (Optional): Plot the results if set.</li> </ul> <p>Example Usage:</p> <pre><code>neps results --root-directory path/to/directory --plot\n</code></pre>"},{"location":"reference/cli/#errors-command","title":"<code>errors</code> Command","text":"<p>Lists all errors found in the specified NePS run. This is useful for debugging or reviewing failed trials.</p> <p>Arguments:</p> <ul> <li><code>-h, --help</code> (Optional): show this help message and exit</li> <li><code>--root-directory</code> (Optional): Optional: The path to your root_directory. If not provided, it will be loaded from run_config.yaml.</li> </ul> <p>Example Usage:</p> <pre><code>neps errors --root-directory path/to/directory\n</code></pre>"},{"location":"reference/cli/#sample-config-command","title":"<code>sample-config</code> Command","text":"<p>Arguments:</p> <ul> <li><code>-h, --help</code> (Optional): show this help message and exit</li> <li><code>--root-directory</code> (Optional): Optional: The path to your root_directory. If not provided, it will be loaded from run_config.yaml.</li> </ul> <p>Example Usage:</p> <pre><code>neps sample-config --help\n</code></pre>"},{"location":"reference/cli/#help-command","title":"<code>help</code> Command","text":"<p>Displays help information for the NePS CLI, including a list of available commands and their descriptions.</p> <p>Arguments:</p> <ul> <li><code>-h, --help</code> (Optional): show this help message and exit</li> </ul> <p>Example Usage:</p> <pre><code>neps help --help\n</code></pre>"},{"location":"reference/declarative_usage/","title":"Declarative Usage","text":""},{"location":"reference/declarative_usage/#introduction","title":"Introduction","text":""},{"location":"reference/declarative_usage/#configuring-with-yaml","title":"Configuring with YAML","text":"<p>Configure your experiments using a YAML file, which serves as a central reference for setting up your project. This approach simplifies sharing, reproducing and modifying configurations.</p> <p>Argument Handling and Prioritization</p> <p>You can partially define and provide arguments via <code>run_args</code> (YAML file) and partially provide the arguments directly to <code>neps.run</code>. Arguments directly provided to <code>neps.run</code> get prioritized over those defined in the YAML file. An exception to this is for <code>searcher_kwargs</code> where a merge happens between the configurations. In this case, the directly provided arguments are still prioritized, but the values from both the directly provided arguments and the YAML file are merged.</p>"},{"location":"reference/declarative_usage/#simple-yaml-example","title":"Simple YAML Example","text":"<p>Below is a straightforward YAML configuration example for NePS covering the required arguments.</p> config.yamlrun_neps.py <pre><code># Basic NePS Configuration Example\npipeline_space:\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true  # Log scale for learning rate\n  epochs:\n    lower: 5\n    upper: 20\n    is_fidelity: true\n  optimizer:\n    choices: [adam, sgd, adamw]\n  batch_size: 64\n\nroot_directory: path/to/results       # Directory for result storage\nmax_evaluations_total: 20             # Budget\n</code></pre> <pre><code>import neps\n\ndef run_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n\nif __name__ == \"__main__\":\n    neps.run(run_pipeline, run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#including-run_pipeline-in-run_args-for-external-referencing","title":"Including <code>run_pipeline</code> in <code>run_args</code> for External Referencing","text":"<p>In addition to setting experimental parameters via YAML, this configuration example also specifies the pipeline function and its location, enabling more flexible project structures.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code># Simple NePS configuration including run_pipeline\nrun_pipeline:\n  path: path/to/your/run_pipeline.py  # Path to the function file\n  name: example_pipeline              # Function name within the file\n\npipeline_space:\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true  # Log scale for learning rate\n  epochs:\n    lower: 5\n    upper: 20\n    is_fidelity: true\n  optimizer:\n    choices: [adam, sgd, adamw]\n  batch_size: 64\n\nroot_directory: path/to/results       # Directory for result storage\nmax_evaluations_total: 20             # Budget\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\n# No need to define run_pipeline here. NePS loads it directly from the specified path.\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#comprehensive-yaml-configuration-template","title":"Comprehensive YAML Configuration Template","text":"<p>This example showcases a more comprehensive YAML configuration, which includes not only the essential parameters but also advanced settings for more complex setups.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code># Full Configuration Template for NePS\nrun_pipeline:\n  path: path/to/your/run_pipeline.py  # Path to the function file\n  name: example_pipeline              # Function name within the file\n\npipeline_space:\n  learning_rate:\n    lower: 1e-5\n    upper: 1e-1\n    log: true\n  epochs:\n    lower: 5\n    upper: 20\n    is_fidelity: true\n  optimizer:\n    choices: [adam, sgd, adamw]\n  batch_size: 64\n\nroot_directory: path/to/results       # Directory for result storage\nmax_evaluations_total: 20             # Budget\nmax_cost_total:\n\n# Debug and Monitoring\noverwrite_working_directory: true\npost_run_summary: false\ndevelopment_stage_id:\ntask_id:\n\n# Parallelization Setup\nmax_evaluations_per_run:\ncontinue_until_max_evaluation_completed: false\n\n# Error Handling\nloss_value_on_error:\ncost_value_on_error:\nignore_errors:\n\n# Customization Options\nsearcher: hyperband       # Internal key to select a NePS optimizer.\n\n# Hooks\npre_load_hooks:\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\n# Executes the configuration specified in your YAML file\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre> <p>The <code>searcher</code> key used in the YAML configuration corresponds to the same keys used for selecting an optimizer directly through <code>neps.run</code>. For a detailed list of integrated optimizers, see here</p> <p>Note on undefined keys in <code>run_args</code> (config.yaml)</p> <p>Not all configurations are explicitly defined in this template. Any undefined key in the YAML file is mapped to the internal default settings of NePS. This ensures that your experiments can run even if certain parameters are omitted.</p>"},{"location":"reference/declarative_usage/#different-use-cases","title":"Different Use Cases","text":""},{"location":"reference/declarative_usage/#customizing-neps-optimizer","title":"Customizing NePS optimizer","text":"<p>Customize an internal NePS optimizer by specifying its parameters directly under the key <code>searcher</code> in the <code>config.yaml</code> file.</p> <p>Note</p> <p>For <code>searcher_kwargs</code> of <code>neps.run</code>, the optimizer arguments passed via the YAML file and those passed directly via <code>neps.run</code> will be merged. In this special case, if the same argument is referenced in both places, <code>searcher_kwargs</code> will be prioritized and set for this argument.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Customizing NePS Searcher\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      optimizer:\n        choices: [adam, sgd, adamw]\n      epochs: 50\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n    searcher:\n      strategy: bayesian_optimization     # key for neps searcher\n      name: \"my_bayesian\"                 # optional; changing the searcher_name for better recognition\n      # Specific arguments depending on the searcher\n      initial_design_size: 7\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre> <p>For detailed information about the available optimizers and their parameters, please visit the optimizer page</p>"},{"location":"reference/declarative_usage/#testing-multiple-optimizer-configurations","title":"Testing Multiple Optimizer Configurations","text":"<p>Simplify experiments with multiple optimizer settings by outsourcing the optimizer configuration.</p> config.yamlsearcher_setup.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Optimizer settings from YAML configuration\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      optimizer:\n        choices: [adam, sgd, adamw]\n      epochs: 50\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n\n    searcher: path/to/your/searcher_setup.yaml\n</code></pre> <pre><code>strategy: bayesian_optimization\n# Specific arguments depending on the searcher\ninitial_design_size: 7\nuse_priors: true\nsample_default_first: false\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#handling-large-search-spaces","title":"Handling Large Search Spaces","text":"<p>Manage large search spaces by outsourcing the pipeline space configuration in a separate YAML file or for keeping track of your experiments.</p> config.yamlpipeline_space.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Pipeline space settings from separate YAML\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space: path/to/your/pipeline_space.yaml\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n</code></pre> <pre><code># Pipeline_space including priors and fidelity\nlearning_rate:\n  lower: 1e-5\n  upper: 1e-1\n  log: true  # Log scale for learning rate\n  default: 1e-2\n  default_confidence: \"medium\"\nepochs:\n  lower: 5\n  upper: 20\n  is_fidelity: true\ndropout_rate:\n  lower: 0.1\n  upper: 0.5\n  default: 0.2\n  default_confidence: \"high\"\noptimizer:\n  choices: [adam, sgd, adamw]\n  default: adam\n  # if default confidence is not defined it gets its default 'low'\nbatch_size: 64\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size, dropout_rate):\n    model = initialize_model(dropout_rate)\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#using-architecture-search-spaces","title":"Using Architecture Search Spaces","text":"<p>Since the option for defining the search space via YAML is limited to HPO, grammar-based search spaces or architecture search spaces must be loaded via a dictionary, which is then referenced in the <code>config.yaml</code>.</p> config.yamlsearch_space.pyrun_pipeline.pyrun_neps.py <pre><code>    # Loading pipeline space from a python dict\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      path: path/to/your/search_space.py  # Path to the dict file\n      name: pipeline_space                # Name of the dict instance\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n</code></pre> <pre><code>from torch import nn\nimport neps\nfrom neps.search_spaces.architecture import primitives as ops\nfrom neps.search_spaces.architecture import topologies as topos\nfrom neps.search_spaces.architecture.primitives import AbstractPrimitive\n\n\nclass DownSampleBlock(AbstractPrimitive):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__(locals())\n        self.conv_a = ReLUConvBN(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1\n        )\n        self.conv_b = ReLUConvBN(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.downsample = nn.Sequential(\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False\n            ),\n        )\n\n    def forward(self, inputs):\n        basicblock = self.conv_a(inputs)\n        basicblock = self.conv_b(basicblock)\n        residual = self.downsample(inputs)\n        return residual + basicblock\n\n\nclass ReLUConvBN(AbstractPrimitive):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super().__init__(locals())\n\n        self.kernel_size = kernel_size\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels, affine=True, track_running_stats=True),\n        )\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass AvgPool(AbstractPrimitive):\n    def __init__(self, **kwargs):\n        super().__init__(kwargs)\n        self.op = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n    def forward(self, x):\n        return self.op(x)\n\n\nprimitives = {\n    \"Sequential15\": topos.get_sequential_n_edge(15),\n    \"DenseCell\": topos.get_dense_n_node_dag(4),\n    \"down\": {\"op\": DownSampleBlock},\n    \"avg_pool\": {\"op\": AvgPool},\n    \"id\": {\"op\": ops.Identity},\n    \"conv3x3\": {\"op\": ReLUConvBN, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n    \"conv1x1\": {\"op\": ReLUConvBN, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0},\n}\n\n\nstructure = {\n    \"S\": [\"Sequential15(C, C, C, C, C, down, C, C, C, C, C, down, C, C, C, C, C)\"],\n    \"C\": [\"DenseCell(OPS, OPS, OPS, OPS, OPS, OPS)\"],\n    \"OPS\": [\"id\", \"conv3x3\", \"conv1x1\", \"avg_pool\"],\n}\n\n\ndef set_recursive_attribute(op_name, predecessor_values):\n    in_channels = 16 if predecessor_values is None else predecessor_values[\"out_channels\"]\n    out_channels = in_channels * 2 if op_name == \"DownSampleBlock\" else in_channels\n    return dict(in_channels=in_channels, out_channels=out_channels)\n\n\npipeline_space = dict(\n    architecture=neps.Architecture(\n        set_recursive_attribute=set_recursive_attribute,\n        structure=structure,\n        primitives=primitives,\n    ),\n    optimizer=neps.Categorical(choices=[\"sgd\", \"adam\"]),\n    learning_rate=neps.Float(lower=10e-7, upper=10e-3, log=True),\n)\n</code></pre> <pre><code>from torch import nn\n\n\ndef example_pipeline(architecture, optimizer, learning_rate):\n    in_channels = 3\n    base_channels = 16\n    n_classes = 10\n    out_channels_factor = 4\n\n    # E.g., in shape = (N, 3, 32, 32) =&gt; out shape = (N, 10)\n    model = architecture.to_pytorch()\n    model = nn.Sequential(\n        nn.Conv2d(in_channels, base_channels, 3, padding=1, bias=False),\n        nn.BatchNorm2d(base_channels),\n        model,\n        nn.BatchNorm2d(base_channels * out_channels_factor),\n        nn.ReLU(inplace=True),\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(base_channels * out_channels_factor, n_classes),\n    )\n    training_loss = train_model(model, optimizer, learning_rate)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#integrating-custom-optimizers","title":"Integrating Custom Optimizers","text":"<p>For people who want to write their own optimizer class as a subclass of the base optimizer, you can load your own custom optimizer class and define its arguments in <code>config.yaml</code>.</p> <p>Note: You can still overwrite arguments via searcher_kwargs of <code>neps.run</code> like for the internal searchers.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Loading Optimizer Class\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      optimizer:\n        choices: [adam, sgd, adamw]\n      epochs: 50\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n    searcher:\n      path: path/to/your/searcher.py      # Path to the class\n      name: CustomOptimizer               # class name within the file\n      # Specific arguments depending on your searcher\n      initial_design_size: 7\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#adding-custom-hooks-to-your-configuration","title":"Adding Custom Hooks to Your Configuration","text":"<p>Define hooks in your YAML configuration to extend the functionality of your experiment.</p> config.yamlrun_pipeline.pyrun_neps.py <pre><code>    # Hooks\n    run_pipeline:\n      path: path/to/your/run_pipeline.py  # Path to the function file\n      name: example_pipeline              # Function name within the file\n\n    pipeline_space:\n      learning_rate:\n        lower: 1e-5\n        upper: 1e-1\n        log: true  # Log scale for learning rate\n      epochs:\n        lower: 5\n        upper: 20\n        is_fidelity: true\n      optimizer:\n        choices: [adam, sgd, adamw]\n      batch_size: 64\n\n    root_directory: path/to/results       # Directory for result storage\n    max_evaluations_total: 20             # Budget\n\n    pre_load_hooks:\n        hook1: path/to/your/hooks.py # (function_name: Path to the function's file)\n        hook2: path/to/your/hooks.py # Different function name 'hook2' from the same file source\n</code></pre> <pre><code>def example_pipeline(learning_rate, optimizer, epochs, batch_size):\n    model = initialize_model()\n    training_loss = train_model(model, optimizer, learning_rate, epochs, batch_size)\n    evaluation_loss = evaluate_model(model)\n    return {\"loss\": evaluation_loss, \"training_loss\": training_loss}\n</code></pre> <pre><code>import neps\nneps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/declarative_usage/#cli-usage","title":"CLI Usage","text":"<p>This section provides a brief overview of the primary commands available in the NePS CLI. For additional command options, you can directly refer to the help documentation provided by each command using --help.</p>"},{"location":"reference/declarative_usage/#init-command","title":"<code>init</code> Command","text":"<p>Generates a default <code>run_args</code> YAML configuration file, providing a template that you can customize for your experiments.</p> <p>Options:</p> <ul> <li><code>--config-path &lt;path&gt;</code>: Optional. Specify the custom path for generating the configuration file. Defaults to   <code>run_config.yaml</code> in the current working directory.</li> <li><code>--template [basic|complete]</code>: Optional. Choose between a basic or complete template. The basic template includes only required settings, while the complete template includes all NePS configurations.</li> <li><code>--state-machine</code>: Optional. Creates a NEPS state if set, which requires an existing <code>run_config.yaml</code>.</li> </ul> <p>Example Usage: <pre><code>neps init --config-path custom/path/config.yaml --template complete\n</code></pre></p>"},{"location":"reference/declarative_usage/#run-command","title":"<code>run</code> Command","text":"<p>Executes the optimization based on the provided configuration. This command serves as a CLI wrapper around <code>neps.run</code>, effectively mapping each CLI argument to a parameter in <code>neps.run</code>. It offers a flexible interface that allows you to override the existing settings specified in the YAML configuration file, facilitating dynamic adjustments for managing your experiments.</p> <p>Options:</p> <ul> <li><code>--run-args &lt;path&gt;</code>: Path to the YAML configuration file containing the run arguments.</li> <li><code>--run-pipeline &lt;path/to/module:function_name&gt;</code>: Optional. Specify the path to the Python module and function to use for running the pipeline. Overrides any settings in the YAML file.</li> <li><code>--pipeline-space &lt;path/to/yaml&gt;</code>: Path to the YAML file defining the search space for the optimization.</li> <li><code>--root-directory &lt;path&gt;</code>: Optional. Directory for saving progress and synchronizing multiple processes. Defaults to the <code>root_directory</code> from <code>run_config.yaml</code> if not provided.</li> <li><code>--overwrite-working-directory</code>: Optional. If set, deletes the working directory at the start of the run.</li> <li><code>--development-stage-id &lt;id&gt;</code>: Optional. Identifier for the current development stage, useful for multi-stage projects.</li> <li><code>--task-id &lt;id&gt;</code>: Optional. Identifier for the current task, useful for managing projects with multiple tasks.</li> <li><code>--post-run-summary/--no-post-run-summary</code>: Optional. Provides a summary of the run after execution. Enabled by default.</li> <li><code>--max-evaluations-total &lt;int&gt;</code>: Optional. Specifies the total number of evaluations to run.</li> <li><code>--max-evaluations-per-run &lt;int&gt;</code>: Optional. Number of evaluations to run per call.</li> <li><code>--continue-until-max-evaluation-completed</code>: Optional. If set, ensures the run continues until <code>max-evaluations-total</code> has been reached.</li> <li><code>--max-cost-total &lt;float&gt;</code>: Optional. Specifies a cost threshold. No new evaluations will start if this cost is exceeded.</li> <li><code>--ignore-errors</code>: Optional. If set, errors during the optimization will be ignored.</li> <li><code>--loss-value-on-error &lt;float&gt;</code>: Optional. Specifies the loss value to assume in case of an error.</li> <li><code>--cost-value-on-error &lt;float&gt;</code>: Optional. Specifies the cost value to assume in case of an error.</li> <li><code>--searcher &lt;key&gt;</code>: Specifies the searcher algorithm for optimization.</li> <li><code>--searcher-kwargs &lt;key=value&gt;</code>: Optional. Additional keyword arguments for the searcher.</li> </ul> <p>Example Usage: <pre><code>neps run --run-args path/to/config.yaml --max-evaluations-total 50\n</code></pre></p>"},{"location":"reference/declarative_usage/#status-command","title":"<code>status</code> Command","text":"<p>Executes the optimization based on the provided configuration. This command serves as a CLI wrapper around neps.run, effectively mapping each CLI argument to a parameter in neps.run. This setup offers a flexible interface that allows you to override the existing settings specified in the YAML configuration file, facilitating dynamic adjustments for managing your experiments.</p> <p>Example Usage: <pre><code>neps run --run-args path/to/config.yaml\n</code></pre></p>"},{"location":"reference/neps_run/","title":"Configuring and Running Optimizations","text":"<p>The <code>neps.run()</code> function is the core interface for running Hyperparameter and/or architecture search using optimizers in NePS.</p> <p>This document breaks down the core arguments that allow users to control the optimization process in NePS. Please see the documentation of <code>neps.run()</code> for a full list.</p>"},{"location":"reference/neps_run/#required-arguments","title":"Required Arguments","text":"<p>To operate, NePS requires at minimum the following three arguments <code>neps.run(run_pipeline=..., pipeline_space=..., root_directory=...)</code>:</p> <pre><code>import neps\n\ndef run(learning_rate: float, epochs: int) -&gt; float:\n    # Your code here\n\n    return loss\n\nneps.run(\n    run_pipeline=run, # (1)!\n    pipeline_space={, # (2)!\n        \"learning_rate\": neps.Float(1e-3, 1e-1, log=True),\n        \"epochs\": neps.Integer(10, 100)\n    },\n    root_directory=\"path/to/result_dir\" # (3)!\n)\n</code></pre> <ol> <li>The objective function, targeted by NePS for minimization, by evaluation various configurations.     It requires these configurations as input and should return either a dictionary or a sole loss value as the output.     For correct setup instructions, refer to the run pipeline page</li> <li>This defines the search space for the configurations from which the optimizer samples.     It accepts either a dictionary with the configuration names as keys, a path to a YAML configuration file, or a <code>configSpace.ConfigurationSpace</code> object.     For comprehensive information and examples, please refer to the detailed guide available here</li> <li>The directory path where the information about the optimization and its progress gets stored.     This is also used to synchronize multiple calls to <code>neps.run()</code> for parallelization.</li> </ol> <p>To learn more about the <code>run_pipeline</code> function and the <code>pipeline_space</code> configuration, please refer to the run pipeline and pipeline space pages.</p>"},{"location":"reference/neps_run/#budget-how-long-to-run","title":"Budget, how long to run?","text":"<p>To define a budget, provide <code>max_evaluations_total=</code> to <code>neps.run()</code>, to specify the total number of evaluations to conduct before halting the optimization process, or <code>max_cost_total=</code> to specify a cost threshold for your own custom cost metric, such as time, energy, or monetary.</p> <pre><code>```python\ndef run(learning_rate: float, epochs: int) -&gt; float:\n    start = time.time()\n\n    # Your code here\n    end = time.time()\n    duration = end - start\n    return {\"loss\": loss, \"cost\": duration}\n\nneps.run(\n    max_evaluations_total=10, # (1)!\n    max_cost_total=1000, # (2)!\n)\n</code></pre> <ol> <li>Specifies the total number of evaluations to conduct before halting the optimization process.</li> <li>Prevents the initiation of new evaluations once this cost threshold is surpassed.     This can be any kind of cost metric you like, such as time, energy, or monetary, as long as you can calculate it.     This requires adding a cost value to the output of the <code>run_pipeline</code> function, for example, return <code>{'loss': loss, 'cost': cost}</code>.     For more details, please refer here</li> </ol>"},{"location":"reference/neps_run/#getting-some-feedback-logging","title":"Getting some feedback, logging","text":"<p>Most of NePS will not print anything to the console. To view the progress of workers, you can do so by enabling logging through logging.basicConfig.</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nneps.run(...)\n</code></pre> <p>Please refer to Python's logging documentation for more information on how to customize the logging output.</p>"},{"location":"reference/neps_run/#continuing-runs","title":"Continuing Runs","text":"<p>To continue a run, all you need to do is provide the same <code>root_directory=</code> to <code>neps.run()</code> as before, with an increased <code>max_evaluations_total=</code> or <code>max_cost_total=</code>.</p> <pre><code>def run(learning_rate: float, epochs: int) -&gt; float:\n    start = time.time()\n\n    # Your code here\n    end = time.time()\n    duration = end - start\n    return {\"loss\": loss, \"cost\": duration}\n\nneps.run(\n    # Increase the total number of trials from 10 as set previously to 50\n    max_evaluations_total=50,\n)\n</code></pre> <p>If the run previously stopped due to reaching a budget and you specify the same budget, the worker will immediatly stop as it will remember the amount of budget it used previously.</p>"},{"location":"reference/neps_run/#overwriting-a-run","title":"Overwriting a Run","text":"<p>To overwrite a run, simply provide the same <code>root_directory=</code> to <code>neps.run()</code> as before, with the <code>overwrite_working_directory=True</code> argument.</p> <pre><code>neps.run(\n    ...,\n    root_directory=\"path/to/previous_result_dir\",\n    overwrite_working_directory=True,\n)\n</code></pre> <p>Warning</p> <p>This will delete the folder specified by <code>root_directory=</code> and all its contents.</p>"},{"location":"reference/neps_run/#getting-the-results","title":"Getting the results","text":"<p>The results of the optimization process are stored in the <code>root_directory=</code> provided to <code>neps.run()</code>. To obtain a summary of the optimization process, you can enable the <code>post_run_summary=True</code> argument in <code>neps.run()</code>, while will generate a summary csv after the run has finished.</p> Result Directorypython <p>The root directory after utilizing this argument will look like the following:</p> <pre><code>ROOT_DIRECTORY\n\u251c\u2500\u2500 results\n\u2502  \u2514\u2500\u2500 config_1\n\u2502      \u251c\u2500\u2500 config.yaml\n\u2502      \u251c\u2500\u2500 metadata.yaml\n\u2502      \u2514\u2500\u2500 result.yaml\n\u251c\u2500\u2500 summary_csv         # Only if post_run_summary=True\n\u2502  \u251c\u2500\u2500 config_data.csv\n\u2502  \u2514\u2500\u2500 run_status.csv\n\u251c\u2500\u2500 all_losses_and_configs.txt\n\u251c\u2500\u2500 best_loss_trajectory.txt\n\u2514\u2500\u2500 best_loss_with_config_trajectory.txt\n</code></pre> <pre><code>neps.run(..., post_run_summary=True)\n</code></pre> <p>To capture the results of the optimization process, you can use tensorbaord logging with various utilities to integrate closer to NePS. For more information, please refer to the analyses page page.</p>"},{"location":"reference/neps_run/#parallelization","title":"Parallelization","text":"<p>NePS utilizes the file-system and locks as a means of communication for implementing parallelization and resuming runs. As a result, you can start multiple <code>neps.run()</code> from different processes however you like and they will synchronize, as long as they share the same <code>root_directory=</code>. Any new workers that come online will automatically pick up work and work together to until the budget is exhausted.</p> Worker scriptShell <pre><code># worker.py\nneps.run(\n    run_pipeline=...,\n    pipeline_space=...,\n    root_directory=\"some/path\",\n    max_evaluations_total=100,\n    max_evaluations_per_run=10, # (1)!\n    continue_until_max_evaluation_completed=True, # (2)!\n    overwrite_working_directory=False, #!!!\n)\n</code></pre> <ol> <li>Limits the number of evaluations for this specific call of <code>neps.run()</code>.</li> <li>Evaluations in-progress count towards max_evaluations_total, halting new ones when this limit is reached.     Setting this to <code>True</code> enables continuous sampling of new evaluations until the total of completed ones meets max_evaluations_total, optimizing resource use in time-sensitive scenarios.</li> </ol> <p>Warning</p> <p>Ensure <code>overwrite_working_directory=False</code> to prevent newly spawned workers from deleting the shared directory!</p> <pre><code># Start 3 workers\npython worker.py &amp;\npython worker.py &amp;\npython worker.py &amp;\n</code></pre>"},{"location":"reference/neps_run/#yaml-configuration","title":"YAML Configuration","text":"<p>You have the option to configure all arguments using a YAML file through <code>neps.run(run_args=...)</code>. For more on yaml usage, please visit the dedicated page on usage of YAML with NePS.</p> <p>Parameters not explicitly defined within this file will receive their default values.</p> Yaml ConfigurationPython <pre><code># path/to/your/config.yaml\nrun_pipeline:\n  path: \"path/to/your/run_pipeline.py\" # File path of the run_pipeline function\n  name: \"name_of_your_run_pipeline\" # Function name\npipeline_space: \"path/to/your/search_space.yaml\" # Path of the search space yaml file\nroot_directory: \"neps_results\"  # Output directory for results\nmax_evaluations_total: 100\npost_run_summary: # Defaults applied if left empty\nsearcher:\n  strategy: \"bayesian_optimization\"\n  initial_design_size: 5\n  surrogate_model: \"gp\"\n</code></pre> <pre><code>neps.run(run_args=\"path/to/your/config.yaml\")\n</code></pre>"},{"location":"reference/neps_run/#handling-errors","title":"Handling Errors","text":"<p>Things go wrong during optimization runs and it's important to consider what to do in these cases. By default, NePS will halt the optimization process when an error but you can choose to <code>ignore_errors=</code>, providing a <code>loss_value_on_error=</code> and <code>cost_value_on_error=</code> to control what values should be reported to the optimization process.</p> <pre><code>def run(learning_rate: float, epochs: int) -&gt; float:\n    if whoops_my_gpu_died():\n        raise RuntimeError(\"Oh no! GPU died!\")\n\n    ...\n    return loss\n\nneps.run(\n    loss_value_on_error=100, # (1)!\n    cost_value_on_error=10, # (2)!\n    ignore_errors=True, # (3)!\n)\n</code></pre> <ol> <li>If an error occurs, the loss value for that configuration will be set to 100.</li> <li>If an error occurs, the cost value for that configuration will be set to 100.</li> <li>Continue the optimization process even if an error occurs, otherwise throwing an exception and halting the process.</li> </ol> <p>Note</p> <p>Any runs that error will still count towards the total <code>max_evaluations_total</code> or <code>max_evaluations_per_run</code>.</p>"},{"location":"reference/neps_run/#selecting-an-optimizer","title":"Selecting an Optimizer","text":"<p>By default NePS intelligently selects the most appropriate search strategy based on your defined configurations in <code>pipeline_space=</code>, one of the arguments to <code>neps.run()</code>.</p> <p>The characteristics of your search space, as represented in the <code>pipeline_space=</code>, play a crucial role in determining which optimizer NePS will choose. This automatic selection process ensures that the strategy aligns with the specific requirements and nuances of your search space, thereby optimizing the effectiveness of the hyperparameter and/or architecture optimization.</p> <p>You can also manually select a specific or custom optimizer that better matches your specific needs. For more information about the available searchers and how to customize your own, refer here.</p>"},{"location":"reference/neps_run/#managing-experiments","title":"Managing Experiments","text":"<p>While tuning pipelines, it is common to run multiple experiments, perhaps varying the search space, the metric, the model or any other factors of your development. We provide two extra arguments to help manage directories for these, <code>development_stage_id=</code> and <code>task_id=</code>.</p> <pre><code>def run1(learning_rate: float, epochs: int) -&gt; float:\n    # Only tuning learning rate\n\n    return\n\ndef run2(learning_rate: float, l2: float, epochs: int) -&gt; float:\n    # Tuning learning rate and l2 regularization\n\n    return\n\nneps.run(\n    ...,\n    task_id=\"l2_regularization\", # (1)!\n    development_stage_id=\"003\", # (2)!\n)\n</code></pre> <ol> <li>An identifier used when working with multiple development stages.     Instead of creating new root directories, use this identifier to save the results of an optimization run in a separate dev_id folder within the root_directory.</li> <li>An identifier used when the optimization process involves multiple tasks.     This functions similarly to <code>development_stage_id=</code>, but it creates a folder named after the <code>task_id=</code>, providing an organized way to separate results for different tasks within the <code>root_directory=</code>.</li> </ol>"},{"location":"reference/neps_run/#others","title":"Others","text":"<ul> <li><code>pre_load_hooks=</code>: A list of hook functions to be called before loading results.</li> </ul>"},{"location":"reference/optimizers/","title":"Optimizer Configuration","text":"<p>Before running the optimizer for your AutoML tasks, you have several configuration options to tailor the optimization process to your specific needs. These options allow you to customize the optimizer's behavior according to your preferences and requirements.</p>"},{"location":"reference/optimizers/#1-automatic-optimizer-selection","title":"1. Automatic Optimizer Selection","text":"<p>If you prefer not to specify a particular optimizer for your AutoML task, you can simply pass <code>\"default\"</code> or <code>None</code> for the neps searcher. NePS will automatically choose the best optimizer based on the characteristics of your search space. This provides a hassle-free way to get started quickly.</p> <p>The optimizer selection is based on the following characteristics of your <code>pipeline_space</code>:</p> <ul> <li>If it has fidelity: <code>hyperband</code></li> <li>If it has both fidelity and a prior: <code>priorband</code></li> <li>If it has a prior: <code>pibo</code></li> <li>If it has neither: <code>bayesian_optimization</code></li> </ul> <p>For example, running the following format, without specifying a searcher will choose an optimizer depending on the <code>pipeline_space</code> passed. <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    # no searcher specified\n)\n</code></pre></p>"},{"location":"reference/optimizers/#2-choosing-one-of-neps-optimizers","title":"2. Choosing one of NePS Optimizers","text":"<p>We have also prepared some optimizers with specific hyperparameters that we believe can generalize well to most AutoML tasks and use cases. For more details on the available default optimizers and the algorithms that can be called, please refer to the next section on SearcherConfigs.</p> <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    # searcher specified, along with an argument\n    searcher=\"bayesian_optimization\",\n    initial_design_size=5,\n)\n</code></pre> <p>For more optimizers, please refer here .</p>"},{"location":"reference/optimizers/#3-custom-optimizer-configuration-via-yaml","title":"3. Custom Optimizer Configuration via YAML","text":"<p>For users who want more control over the optimizer's hyperparameters, you can create your own YAML configuration file. In this file, you can specify the hyperparameters for your preferred optimizer. To use this custom configuration, provide the path to your YAML file using the <code>searcher</code> parameter when running the optimizer. The library will then load your custom settings and use them for optimization.</p> <p>Here's the format of a custom YAML (<code>custom_bo.yaml</code>) configuration using <code>Bayesian Optimization</code> as an example:</p> <pre><code>strategy: bayesian_optimization\nname: my_custom_bo  # optional; otherwise, your searcher will be named after your YAML file, here 'custom_bo'.\n# Specific arguments depending on the searcher\ninitial_design_size: 7\nsurrogate_model: gp\nacquisition: EI\nlog_prior_weighted: false\nrandom_interleave_prob: 0.1\ndisable_priors: false\nprior_confidence: high\nsample_default_first: false\n</code></pre> <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    searcher=\"path/to/custom_bo.yaml\",\n)\n</code></pre>"},{"location":"reference/optimizers/#4-hyperparameter-overrides","title":"4. Hyperparameter Overrides","text":"<p>If you want to make on-the-fly adjustments to the optimizer's hyperparameters without modifying the YAML configuration file, you can do so by passing keyword arguments (kwargs) to the neps.run function itself. This enables you to fine-tune specific hyperparameters without the need for YAML file updates. Any hyperparameter values provided as kwargs will take precedence over those specified in the YAML configuration.</p> <pre><code>neps.run(\n    run_pipeline=run_function,\n    pipeline_space=pipeline_space,\n    root_directory=\"results/\",\n    max_evaluations_total=25,\n    searcher=\"path/to/custom_bo.yaml\",\n    initial_design_size=5,        # overrides value in custom_bo.yaml\n    random_interleave_prob=0.25  # overrides value in custom_bo.yaml\n)\n</code></pre>"},{"location":"reference/optimizers/#note-for-contributors","title":"Note for Contributors","text":"<p>When designing a new optimizer, it's essential to create a YAML configuration file in the <code>default_searcher</code> folder under <code>neps.src.optimizers</code>. This YAML file should contain the default configuration settings that you believe should be used when the user chooses the  searcher.</p> <p>Even when many hyperparameters might be set to their default values as specified in the code, it is still considered good practice to include them in the YAML file. This is because the <code>SearcherConfigs</code> method relies on the arguments from the YAML file to display the optimizer's configuration to the user.</p>"},{"location":"reference/optimizers/#searcher-configurations","title":"Searcher Configurations","text":"<p>The <code>SearcherConfigs</code> class provides a set of useful functions to manage and retrieve default configuration details for NePS optimizers. These functions can help you understand and interact with the available searchers and their associated algorithms and configurations.</p>"},{"location":"reference/optimizers/#importing-searcherconfigs","title":"Importing <code>SearcherConfigs</code>","text":"<p>Before you can use the <code>SearcherConfigs</code> class to manage and retrieve default configuration details for NePS optimizers, make sure to import it into your Python script. You can do this with the following import statement:</p> <pre><code>from neps.optimizers.info import SearcherConfigs\n</code></pre> <p>Once you have imported the class, you can proceed to use its functions to explore the available searchers, algorithms, and configuration details.</p>"},{"location":"reference/optimizers/#list-available-searchers","title":"List Available Searchers","text":"<p>To list all the available searchers that can be used in NePS runs, you can use the <code>get_searchers</code> function. It provides you with a list of searcher names:</p> <pre><code>searchers = SearcherConfigs.get_searchers()\nprint(\"Available searchers:\", searchers)\n</code></pre>"},{"location":"reference/optimizers/#list-available-searching-algorithms","title":"List Available Searching Algorithms","text":"<p>The <code>get_available_algorithms</code> function helps you discover the searching algorithms available within the NePS searchers:</p> <pre><code>algorithms = SearcherConfigs.get_available_algorithms()\nprint(\"Available searching algorithms:\", algorithms)\n</code></pre>"},{"location":"reference/optimizers/#find-searchers-using-a-specific-algorithm","title":"Find Searchers Using a Specific Algorithm","text":"<p>If you want to identify which NePS searchers are using a specific searching algorithm (e.g., Bayesian Optimization, Hyperband, PriorBand...), you can use the <code>get_searcher_from_algorithm</code> function. It returns a list of searchers utilizing the specified algorithm:</p> <pre><code>algorithm = \"bayesian_optimization\"  # Replace with the desired algorithm\nsearchers = SearcherConfigs.get_searcher_from_algorithm(algorithm)\nprint(f\"Searchers using {algorithm}:\", searchers)\n</code></pre>"},{"location":"reference/optimizers/#retrieve-searcher-configuration-details","title":"Retrieve Searcher Configuration Details","text":"<p>To access the configuration details of a specific searcher, you can use the <code>get_searcher_kwargs</code> function. Provide the name of the searcher you are interested in, and it will return the searcher's configuration:</p> <pre><code>searcher_name = \"pibo\"  # Replace with the desired NePS searcher name\nsearcher_kwargs = SearcherConfigs.get_searcher_kwargs(searcher_name)\nprint(f\"Configuration of {searcher_name}:\", searcher_kwargs)\n</code></pre> <p>These functions empower you to explore and manage the available NePS searchers and their configurations effectively.</p>"},{"location":"reference/pipeline_space/","title":"Initializing the Pipeline Space","text":"<p>In NePS, we need to define a <code>pipeline_space</code>. This space can be structured through various approaches, including a Python dictionary, a YAML file, or ConfigSpace. Each of these methods allows you to specify a set of parameter types, ranging from Float and Categorical to specialized architecture parameters. Whether you choose a dictionary, YAML file, or ConfigSpace, your selected method serves as a container or framework within which these parameters are defined and organized. This section not only guides you through the process of setting up your <code>pipeline_space</code> using these methods but also provides detailed instructions and examples on how to effectively incorporate various parameter types, ensuring that NePS can utilize them in the optimization process.</p>"},{"location":"reference/pipeline_space/#parameters","title":"Parameters","text":"<p>NePS currently features 4 primary hyperparameter types:</p> <ul> <li><code>Categorical</code></li> <li><code>Float</code></li> <li><code>Integer</code></li> <li><code>Constant</code></li> </ul> <p>Using these types, you can define the parameters that NePS will optimize during the search process. The most basic way to pass these parameters is through a Python dictionary, where each key-value pair represents a parameter name and its respective type. For example, the following Python dictionary defines a <code>pipeline_space</code> with four parameters for optimizing a deep learning model:</p> <pre><code>pipeline_space = {\n    \"learning_rate\": neps.Float(0.00001, 0.1, log=True),\n    \"num_epochs\": neps.Integer(3, 30, is_fidelity=True),\n    \"optimizer\": neps.Categorical([\"adam\", \"sgd\", \"rmsprop\"]),\n    \"dropout_rate\": neps.Constant(0.5),\n}\n\nneps.run(.., pipeline_space = pipeline_space)\n</code></pre> Quick Parameter Reference <code>Categorical</code><code>Float</code><code>Integer</code><code>Constant</code>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.Categorical","title":"neps.search_spaces.hyperparameters.categorical.Categorical","text":"<pre><code>Categorical(\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>ParameterWithPrior[CategoricalTypes, CategoricalTypes]</code></p> <p>A list of unordered choices for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters that can take on a discrete set of unordered values. For example, the <code>optimizer</code> hyperparameter in a neural network search space can be a <code>Categorical</code> with choices like <code>[\"adam\", \"sgd\", \"rmsprop\"]</code>.</p> <pre><code>import neps\n\noptimizer_choice = neps.Categorical(\n    [\"adam\", \"sgd\", \"rmsprop\"],\n    default=\"adam\"\n)\n</code></pre> <p>Please see the <code>Parameter</code>, <code>ParameterWithPrior</code>, for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>choices</code> <p>choices for the hyperparameter.</p> <p> TYPE: <code>Iterable[float | int | str]</code> </p> <code>default</code> <p>default value for the hyperparameter, must be in <code>choices=</code> if provided.</p> <p> TYPE: <code>float | int | str | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/categorical.py</code> <pre><code>def __init__(\n    self,\n    choices: Iterable[float | int | str],\n    *,\n    default: float | int | str | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `Categorical`.\n\n    Args:\n        choices: choices for the hyperparameter.\n        default: default value for the hyperparameter, must be in `choices=`\n            if provided.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    choices = list(choices)\n    if len(choices) &lt;= 1:\n        raise ValueError(\"Categorical choices must have more than one value.\")\n\n    super().__init__(value=None, is_fidelity=False, default=default)\n\n    for choice in choices:\n        if not isinstance(choice, float | int | str):\n            raise TypeError(\n                f'Choice \"{choice}\" is not of a valid type (float, int, str)'\n            )\n\n    if not all_unique(choices):\n        raise ValueError(f\"Choices must be unique but got duplicates.\\n{choices}\")\n\n    if default is not None and default not in choices:\n        raise ValueError(\n            f\"Default value {default} is not in the provided choices {choices}\"\n        )\n\n    self.choices = list(choices)\n\n    # NOTE(eddiebergman): If there's ever a very large categorical,\n    # then it would be beneficial to have a lookup table for indices as\n    # currently we do a list.index() operation which is O(n).\n    # However for small sized categoricals this is likely faster than\n    # a lookup table.\n    # For now we can just cache the index of the value and default.\n    self._value_index: int | None = None\n\n    self.default_confidence_choice = default_confidence\n    self.default_confidence_score = self.DEFAULT_CONFIDENCE_SCORES[default_confidence]\n    self.has_prior = self.default is not None\n    self._default_index: int | None = (\n        self.choices.index(default) if default is not None else None\n    )\n    self.domain = Domain.indices(len(self.choices))\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.Categorical.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.Categorical.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.categorical.Categorical.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.Float","title":"neps.search_spaces.hyperparameters.float.Float","text":"<pre><code>Float(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Numerical[float]</code></p> <p>A float value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous float values, optionally specifying if it exists on a log scale. For example, <code>l2_norm</code> could be a value in <code>(0.1)</code>, while the <code>learning_rate</code> hyperparameter in a neural network search space can be a <code>Float</code> with a range of <code>(0.0001, 0.1)</code> but on a log scale.</p> <pre><code>import neps\n\nl2_norm = neps.Float(0, 1)\nlearning_rate = neps.Float(1e-4, 1e-1, log=True)\n</code></pre> <p>Please see the <code>Numerical</code> class for more details on the methods available for this class.</p> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsidering prior based optimization..</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/float.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `Float`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsidering prior based optimization..\n    \"\"\"\n    super().__init__(\n        lower=float(lower),\n        upper=float(upper),\n        log=log,\n        default=float(default) if default is not None else None,\n        default_confidence=default_confidence,\n        is_fidelity=is_fidelity,\n        domain=Domain.floating(lower, upper, log=log),\n    )\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.Float.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.Float.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.float.Float.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.Integer","title":"neps.search_spaces.hyperparameters.integer.Integer","text":"<pre><code>Integer(\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\n        \"low\", \"medium\", \"high\"\n    ] = \"low\"\n)\n</code></pre> <p>               Bases: <code>Numerical[int]</code></p> <p>An integer value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with continuous integer values, optionally specifying f it exists on a log scale. For example, <code>batch_size</code> could be a value in <code>(32, 128)</code>, while the <code>num_layers</code> hyperparameter in a neural network search space can be a <code>Integer</code> with a range of <code>(1, 1000)</code> but on a log scale.</p> <pre><code>import neps\n\nbatch_size = neps.Integer(32, 128)\nnum_layers = neps.Integer(1, 1000, log=True)\n</code></pre> PARAMETER DESCRIPTION <code>lower</code> <p>lower bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>upper</code> <p>upper bound for the hyperparameter.</p> <p> TYPE: <code>Number</code> </p> <code>log</code> <p>whether the hyperparameter is on a log scale.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_fidelity</code> <p>whether the hyperparameter is fidelity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default</code> <p>default value for the hyperparameter.</p> <p> TYPE: <code>Number | None</code> DEFAULT: <code>None</code> </p> <code>default_confidence</code> <p>confidence score for the default value, used when condsider prior based optimization.</p> <p> TYPE: <code>Literal['low', 'medium', 'high']</code> DEFAULT: <code>'low'</code> </p> Source code in <code>neps/search_spaces/hyperparameters/integer.py</code> <pre><code>def __init__(\n    self,\n    lower: Number,\n    upper: Number,\n    *,\n    log: bool = False,\n    is_fidelity: bool = False,\n    default: Number | None = None,\n    default_confidence: Literal[\"low\", \"medium\", \"high\"] = \"low\",\n):\n    \"\"\"Create a new `Integer`.\n\n    Args:\n        lower: lower bound for the hyperparameter.\n        upper: upper bound for the hyperparameter.\n        log: whether the hyperparameter is on a log scale.\n        is_fidelity: whether the hyperparameter is fidelity.\n        default: default value for the hyperparameter.\n        default_confidence: confidence score for the default value, used when\n            condsider prior based optimization.\n    \"\"\"\n    lower = int(np.rint(lower))\n    upper = int(np.rint(upper))\n    _size = upper - lower + 1\n    if _size &lt;= 1:\n        raise ValueError(\n            f\"Integer: expected at least 2 possible values in the range,\"\n            f\" got upper={upper}, lower={lower}.\"\n        )\n\n    super().__init__(\n        lower=int(np.rint(lower)),\n        upper=int(np.rint(upper)),\n        log=log,\n        is_fidelity=is_fidelity,\n        default=int(np.rint(default)) if default is not None else None,\n        default_confidence=default_confidence,\n        domain=Domain.integer(lower, upper, log=log),\n    )\n\n    # We subtract/add 0.499999 from lower/upper bounds respectively, such that\n    # sampling in the float space gives equal probability for all integer values,\n    # i.e. [x - 0.499999, x + 0.499999]\n    self.float_hp = Float(\n        lower=self.lower - 0.499999,\n        upper=self.upper + 0.499999,\n        log=self.log,\n        is_fidelity=is_fidelity,\n        default=default,\n        default_confidence=default_confidence,\n    )\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.Integer.value","title":"value  <code>property</code>","text":"<pre><code>value: ValueT | None\n</code></pre> <p>Get the value of the hyperparameter, or <code>None</code> if not set.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.integer.Integer.sample","title":"sample","text":"<pre><code>sample(*, user_priors: bool = False) -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Similar to <code>Parameter.sample()</code>, but a <code>ParameterWithPrior</code> can use the confidence score by setting <code>user_priors=True</code>.</p> PARAMETER DESCRIPTION <code>user_priors</code> <p>whether to use the confidence score when sampling a value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self, *, user_priors: bool = False) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Similar to\n    [`Parameter.sample()`][neps.search_spaces.Parameter.sample],\n    but a `ParameterWithPrior` can use the confidence score by setting\n    `user_priors=True`.\n\n    Args:\n        user_priors: whether to use the confidence score\n            when sampling a value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value(user_priors=user_priors)\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.Constant","title":"neps.search_spaces.hyperparameters.constant.Constant","text":"<pre><code>Constant(value: T)\n</code></pre> <p>               Bases: <code>Parameter[T, T]</code></p> <p>A constant value for a parameter.</p> <p>This kind of <code>Parameter</code> is used to represent hyperparameters with values that should not change during optimization. For example, the <code>batch_size</code> hyperparameter in a neural network search space can be a <code>Constant</code> with a value of <code>32</code>.</p> <pre><code>import neps\n\nbatch_size = neps.Constant(32)\n</code></pre> <p>Note</p> <p>As the name suggests, the value of a <code>Constant</code> only have one value and so its <code>.default</code> and <code>.value</code> should always be the same.</p> <p>This also implies that the <code>.default</code> can never be <code>None</code>.</p> <p>Please use <code>.set_constant_value()</code> if you need to change the value of the constant parameter.</p> PARAMETER DESCRIPTION <code>value</code> <p>value for the hyperparameter.</p> <p> TYPE: <code>T</code> </p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>def __init__(self, value: T):\n    \"\"\"Create a new `Constant`.\n\n    Args:\n        value: value for the hyperparameter.\n    \"\"\"\n    super().__init__(value=value, default=value, is_fidelity=False)  # type: ignore\n    self._value: T = value  # type: ignore\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.Constant.value","title":"value  <code>property</code>","text":"<pre><code>value: T\n</code></pre> <p>Get the value of the constant parameter.</p>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.Constant.load_from","title":"load_from","text":"<pre><code>load_from(value: Any) -&gt; None\n</code></pre> <p>Load a serialized value into the hyperparameter's value.</p> PARAMETER DESCRIPTION <code>value</code> <p>value to load.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def load_from(self, value: Any) -&gt; None:\n    \"\"\"Load a serialized value into the hyperparameter's value.\n\n    Args:\n        value: value to load.\n    \"\"\"\n    self.set_value(value)\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.Constant.sample","title":"sample","text":"<pre><code>sample() -&gt; Self\n</code></pre> <p>Sample a new version of this <code>Parameter</code> with a random value.</p> <p>Will set the <code>.value</code> to the sampled value.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>Parameter</code> with a sampled value.</p> Source code in <code>neps/search_spaces/parameter.py</code> <pre><code>def sample(self) -&gt; Self:\n    \"\"\"Sample a new version of this `Parameter` with a random value.\n\n    Will set the [`.value`][neps.search_spaces.Parameter.value] to the\n    sampled value.\n\n    Returns:\n        A new `Parameter` with a sampled value.\n    \"\"\"\n    value = self.sample_value()\n    copy_self = self.clone()\n    copy_self.set_value(value)\n    return copy_self\n</code></pre>"},{"location":"reference/pipeline_space/#neps.search_spaces.hyperparameters.constant.Constant.set_value","title":"set_value","text":"<pre><code>set_value(value: T | None) -&gt; None\n</code></pre> <p>Set the value of the constant parameter.</p> <p>Note</p> <p>This method is a no-op but will raise a <code>ValueError</code> if the value is different from the current value.</p> <p>Please see <code>.set_constant_value()</code> which can be used to set both the <code>.value</code> and the <code>.default</code> at once</p> PARAMETER DESCRIPTION <code>value</code> <p>value to set the parameter to.</p> <p> TYPE: <code>T | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the value is different from the current value.</p> Source code in <code>neps/search_spaces/hyperparameters/constant.py</code> <pre><code>@override\ndef set_value(self, value: T | None) -&gt; None:\n    \"\"\"Set the value of the constant parameter.\n\n    !!! note\n\n        This method is a no-op but will raise a `ValueError` if the value\n        is different from the current value.\n\n        Please see\n        [`.set_constant_value()`][neps.search_spaces.hyperparameters.constant.Constant.set_constant_value]\n        which can be used to set both the\n        [`.value`][neps.search_spaces.parameter.Parameter.value]\n        and the [`.default`][neps.search_spaces.parameter.Parameter.default] at once\n\n    Args:\n        value: value to set the parameter to.\n\n    Raises:\n        ValueError: if the value is different from the current value.\n    \"\"\"\n    if value != self._value:\n        raise ValueError(\n            f\"Constant does not allow chaning the set value. \"\n            f\"Tried to set value to {value}, but it is already {self.value}\"\n        )\n</code></pre>"},{"location":"reference/pipeline_space/#using-your-knowledge-providing-a-prior","title":"Using your knowledge, providing a Prior","text":"<p>When optimizing, you can provide your own knowledge using the parameters <code>default=</code>. By indicating a <code>default=</code> we take this to be your user prior, your knowledge about where a good value for this parameter lies.</p> <p>You can also specify a <code>default_confidence=</code> to indicate how strongly you want NePS, to focus on these, one of either <code>\"low\"</code>, <code>\"medium\"</code>, or <code>\"high\"</code>.</p> <p>Currently the two major algorithms that exploit this in NePS are <code>PriorBand</code> (prior-based <code>HyperBand</code>) and <code>PiBO</code>, a version of Bayesian Optimization which uses Priors.</p> <pre><code>import neps\n\nneps.run(\n    ...,\n    pipeline_space={\n        \"learning_rate\": neps.Float(1e-4, 1e-1, log=True, default=1e-2, default_confidence=\"medium\"),\n        \"num_epochs\": neps.Integer(3, 30, is_fidelity=True),\n        \"optimizer\": neps.Categorical([\"adam\", \"sgd\", \"rmsprop\"], default=\"adam\", default_confidence=\"low\"),\n        \"dropout_rate\": neps.Constant(0.5),\n    }\n)\n</code></pre> <p>Must set <code>default=</code> for all parameters, if any</p> <p>If you specify <code>default=</code> for one parameter, you must do so for all your variables. This will be improved in future versions.</p> <p>Interaction with <code>is_fidelity</code></p> <p>If you specify <code>is_fidelity=True</code> for one parameter, the <code>default=</code> and <code>default_confidence=</code> are ignored. This will be dissallowed in future versions.</p>"},{"location":"reference/pipeline_space/#defining-a-pipeline-space-using-yaml","title":"Defining a pipeline space using YAML","text":"<p>Create a YAML file (e.g., <code>./pipeline_space.yaml</code>) with the parameter definitions following this structure.</p> <code>./pipeline_space.yaml</code><code>run.py</code> <pre><code>learning_rate:\n  type: float\n  lower: 2e-3\n  upper: 0.1\n  log: true\n\nnum_epochs:\n  type: int\n  lower: 3\n  upper: 30\n  is_fidelity: true\n\noptimizer:\n  type: categorical\n  choices: [\"adam\", \"sgd\", \"rmsprop\"]\n\ndropout_rate: 0.5\n</code></pre> <pre><code>neps.run(.., pipeline_space=\"./pipeline_space.yaml\")\n</code></pre> <p>When defining the <code>pipeline_space</code> using a YAML file, if the <code>type</code> argument is not specified, the NePS will automatically infer the data type based on the value provided.</p> <ul> <li>If <code>lower</code> and <code>upper</code> are provided, then if they are both integers, the type will be inferred as <code>int</code>,     otherwise as <code>float</code>. You can provide scientific notation for floating-point numbers as well.</li> <li>If <code>choices</code> are provided, the type will be inferred as <code>categorical</code>.</li> <li>If just a numeric or string is provided, the type will be inferred as <code>constant</code>.</li> </ul> <p>If none of these hold, an error will be raised.</p>"},{"location":"reference/pipeline_space/#using-configspace","title":"Using ConfigSpace","text":"<p>For users familiar with the <code>ConfigSpace</code> library, can also define the <code>pipeline_space</code> through <code>ConfigurationSpace()</code></p> <pre><code>from configspace import ConfigurationSpace, Float\n\nconfigspace = ConfigurationSpace(\n    {\n        \"learning_rate\": Float(\"learning_rate\", bounds=(1e-4, 1e-1), log=True)\n        \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"],\n        \"dropout_rate\": 0.5,\n    }\n)\n</code></pre> <p>Warning</p> <p>Parameters you wish to use as a fidelity are not support through ConfigSpace at this time.</p> <p>For additional information on ConfigSpace and its features, please visit the following link.</p>"},{"location":"reference/pipeline_space/#supported-architecture-parameter-types","title":"Supported Architecture parameter Types","text":"<p>A comprehensive documentation for the Architecture parameter is not available at this point.</p> <p>If you are interested in exploring architecture parameters, you can find detailed examples and usage in the following resources:</p> <ul> <li>Basic Usage Examples - Basic usage     examples that can help you understand the fundamentals of Architecture parameters.</li> <li>Experimental Examples - For more advanced     and experimental use cases, including Hierarchical parameters, check out this collection of examples.</li> </ul> <p>Warning</p> <p>The configuration of <code>pipeline_space</code> from a YAML file does not currently support architecture parameter types.</p>"},{"location":"reference/run_pipeline/","title":"The run function","text":""},{"location":"reference/run_pipeline/#introduction","title":"Introduction","text":"<p>The <code>run_pipeline=</code> function is crucial for NePS. It encapsulates the objective function to be minimized, which could range from a regular equation to a full training and evaluation pipeline for a neural network.</p> <p>This function receives the configuration to be utilized from the parameters defined in the search space. Consequently, it executes the same set of instructions or equations based on the provided configuration to minimize the objective function.</p> <p>We will show some basic usages and some functionalites this function would require for successful implementation.</p>"},{"location":"reference/run_pipeline/#types-of-returns","title":"Types of Returns","text":""},{"location":"reference/run_pipeline/#1-single-value","title":"1. Single Value","text":"<p>Assuming the <code>pipeline_space=</code> was already created (have a look at pipeline space for more details). A <code>run_pipeline=</code> function with an objective of minimizing the loss will resemble the following:</p> <pre><code>def run_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n\n    return loss\n</code></pre>"},{"location":"reference/run_pipeline/#2-dictionary","title":"2. Dictionary","text":"<p>In this section, we will outline the special variables that are expected to be returned when the <code>run_pipeline=</code> function returns a dictionary.</p>"},{"location":"reference/run_pipeline/#loss","title":"Loss","text":"<p>One crucial return variable is the <code>loss</code>. This metric serves as a fundamental indicator for the optimizer. One option is to return a dictionary with the <code>loss</code> as a key, along with other user-chosen metrics.</p> <p>Note</p> <p>Loss can be any value that is to be minimized by the objective function.</p> <pre><code>def run_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n    reverse_loss = -loss\n\n    return {\n        \"loss\": loss,\n        \"info_dict\": {\n            \"reverse_loss\": reverse_loss\n            ...\n        }\n    }\n</code></pre>"},{"location":"reference/run_pipeline/#cost","title":"Cost","text":"<p>Along with the return of the <code>loss</code>, the <code>run_pipeline=</code> function would optionally need to return a <code>cost</code> in certain cases. Specifically when the <code>max_cost_total</code> parameter is being utilized in the <code>neps.run</code> function.</p> <p>Note</p> <p><code>max_cost_total</code> sums the cost from all returned configuration results and checks whether the maximum allowed cost has been reached (if so, the search will come to an end).</p> <pre><code>import neps\nimport logging\n\n\ndef run_pipeline(\n    **config,   # The hyperparameters to be used in the pipeline\n):\n\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    loss = element_1 - element_2 + element_3\n    cost = 2\n\n    return {\n        \"loss\": loss,\n        \"cost\": cost,\n    }\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    neps.run(\n        run_pipeline=run_pipeline,\n        pipeline_space=pipeline_space, # Assuming the pipeline space is defined\n        root_directory=\"results/bo\",\n        max_cost_total=10,\n        searcher=\"bayesian_optimization\",\n    )\n</code></pre> <p>Each evaluation carries a cost of 2. Hence in this example, the Bayesian optimization search is set to perform 5 evaluations.</p>"},{"location":"reference/run_pipeline/#arguments-for-convenience","title":"Arguments for Convenience","text":"<p>NePS also provides the <code>pipeline_directory</code> and the <code>previous_pipeline_directory</code> as arguments in the <code>run_pipeline=</code> function for user convenience.</p> <p>Regard an example to be run with a multi-fidelity searcher, some checkpointing would be advantageos such that one does not have to train the configuration from scratch when the configuration qualifies to higher fidelity brackets.</p> <pre><code>def run_pipeline(\n    pipeline_directory,           # The directory where the config is saved\n    previous_pipeline_directory,  # The directory of the immediate lower fidelity config\n    **config,                     # The hyperparameters to be used in the pipeline\n):\n    # Assume element3 is our fidelity element\n    element_1 = config[\"element_1\"]\n    element_2 = config[\"element_2\"]\n    element_3 = config[\"element_3\"]\n\n    # Load any saved checkpoints\n    checkpoint_name = \"checkpoint.pth\"\n    start_element_3 = 0\n\n    if previous_pipeline_directory is not None:\n        # Read in state of the model after the previous fidelity rung\n        checkpoint = torch.load(previous_pipeline_directory / checkpoint_name)\n        prev_element_3 = checkpoint[\"element_3\"]\n    else:\n        prev_element_3 = 0\n\n    start_element_3 += prev_element_3\n\n    loss = 0\n    for i in range(start_element_3, element_3):\n        loss += element_1 - element_2\n\n    torch.save(\n        {\n            \"element_3\": element_3,\n        },\n        pipeline_directory / checkpoint_name,\n    )\n\n    return loss\n</code></pre> <p>This could allow the proper navigation to the trained models and further train them on higher fidelities without repeating the entire training process.</p>"}]}